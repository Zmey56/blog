{
  
    
        "post0": {
            "title": "Курсовой проект на классификацию по одобрению кредита",
            "content": "&#1055;&#1086;&#1089;&#1090;&#1072;&#1085;&#1086;&#1074;&#1082;&#1072; &#1079;&#1072;&#1076;&#1072;&#1095;&#1080; . Задача . Требуется, на основании имеющихся данных о клиентах банка, построить модель, используя обучающий датасет, для прогнозирования невыполнения долговых обязательств по текущему кредиту. Выполнить прогноз для примеров из тестового датасета. . Наименование файлов с данными . course_project_train.csv - обучающий датасет course_project_test.csv - тестовый датасет . Целевая переменная . Credit Default - факт невыполнения кредитных обязательств . Метрика качества . F1-score (sklearn.metrics.f1_score) . Требования к решению . Целевая метрика . F1 &gt; 0.5 | Метрика оценивается по качеству прогноза для главного класса (1 - просрочка по кредиту) | . Решение должно содержать . Тетрадка Jupyter Notebook с кодом Вашего решения, названная по образцу {ФИО}_solution.ipynb, пример SShirkin_solution.ipynb | Файл CSV с прогнозами целевой переменной для тестового датасета, названный по образцу {ФИО}_predictions.csv, пример SShirkin_predictions.csv | Рекомендации для файла с кодом (ipynb) . Файл должен содержать заголовки и комментарии (markdown) | Повторяющиеся операции лучше оформлять в виде функций | Не делать вывод большого количества строк таблиц (5-10 достаточно) | По возможности добавлять графики, описывающие данные (около 3-5) | Добавлять только лучшую модель, то есть не включать в код все варианты решения проекта | Скрипт проекта должен отрабатывать от начала и до конца (от загрузки данных до выгрузки предсказаний) | Весь проект должен быть в одном скрипте (файл ipynb). | Допускается применение библиотек Python и моделей машинного обучения, которые были в данном курсе. | Сроки сдачи . Cдать проект нужно в течение 5 дней после окончания последнего вебинара. Оценки работ, сданных до дедлайна, будут представлены в виде рейтинга, ранжированного по заданной метрике качества. Проекты, сданные после дедлайна или сданные повторно, не попадают в рейтинг, но можно будет узнать результат. . &#1055;&#1088;&#1080;&#1084;&#1077;&#1088;&#1085;&#1086;&#1077; &#1086;&#1087;&#1080;&#1089;&#1072;&#1085;&#1080;&#1077; &#1101;&#1090;&#1072;&#1087;&#1086;&#1074; &#1074;&#1099;&#1087;&#1086;&#1083;&#1085;&#1077;&#1085;&#1080;&#1103; &#1082;&#1091;&#1088;&#1089;&#1086;&#1074;&#1086;&#1075;&#1086; &#1087;&#1088;&#1086;&#1077;&#1082;&#1090;&#1072; . Построение модели классификации . Обзор обучающего датасета | Обработка выбросов | Обработка пропусков | Анализ данных | Отбор признаков | Балансировка классов | Подбор моделей, получение бейзлана | Выбор наилучшей модели, настройка гиперпараметров | Проверка качества, борьба с переобучением | Интерпретация результатов | Прогнозирование на тестовом датасете . Выполнить для тестового датасета те же этапы обработки и постронияния признаков | Спрогнозировать целевую переменную, используя модель, построенную на обучающем датасете | Прогнозы должны быть для всех примеров из тестового датасета (для всех строк) | Соблюдать исходный порядок примеров из тестового датасета | &#1054;&#1073;&#1079;&#1086;&#1088; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Описание датасета . Home Ownership - домовладение | Annual Income - годовой доход | Years in current job - количество лет на текущем месте работы | Tax Liens - налоговые обременения | Number of Open Accounts - количество открытых счетов | Years of Credit History - количество лет кредитной истории | Maximum Open Credit - наибольший открытый кредит | Number of Credit Problems - количество проблем с кредитом | Months since last delinquent - количество месяцев с последней просрочки платежа | Bankruptcies - банкротства | Purpose - цель кредита | Term - срок кредита | Current Loan Amount - текущая сумма кредита | Current Credit Balance - текущий кредитный баланс | Monthly Debt - ежемесячный долг | Credit Default - факт невыполнения кредитных обязательств (0 - погашен вовремя, 1 - просрочка) | . Подключение библиотек и скриптов . import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, learning_curve from sklearn.metrics import classification_report, f1_score, precision_score, recall_score #from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import RandomizedSearchCV, KFold import xgboost as xgb, lightgbm as lgbm, catboost as catb from scipy.stats import uniform as sp_randFloat from scipy.stats import randint as sp_randInt . import warnings warnings.filterwarnings(&#39;ignore&#39;) sns.set(style=&#39;whitegrid&#39;) sns.set_context(&quot;paper&quot;, font_scale=1.5) pd.options.display.float_format = &#39;{:,.2f}&#39;.format pd.set_option(&#39;display.max_rows&#39;, 50) . def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred): print(&#39;TRAIN n n&#39; + classification_report(y_train_true, y_train_pred)) print(&#39;TEST n n&#39; + classification_report(y_test_true, y_test_pred)) print(&#39;CONFUSION MATRIX n&#39;) print(pd.crosstab(y_test_true, y_test_pred)) . def balance_df_by_target(df, target_name): target_counts = df[target_name].value_counts() major_class_name = target_counts.argmax() minor_class_name = target_counts.argmin() disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1 for i in range(disbalance_coeff): sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name]) df = df.append(sample, ignore_index=True) return df.sample(frac=1) . def show_learning_curve_plot(estimator, X, y, cv=3, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)): train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=&#39;f1&#39;, train_sizes=train_sizes, n_jobs=n_jobs) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(15,8)) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;) plt.plot(train_sizes, train_scores_mean, &#39;o-&#39;, color=&quot;r&quot;, label=&quot;Training score&quot;) plt.plot(train_sizes, test_scores_mean, &#39;o-&#39;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;) plt.title(f&quot;Learning curves ({type(estimator).__name__})&quot;) plt.xlabel(&quot;Training examples&quot;) plt.ylabel(&quot;Score&quot;) plt.legend(loc=&quot;best&quot;) plt.grid() plt.show() . def show_proba_calibration_plots(y_predicted_probs, y_true_labels): preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels))) thresholds = [] precisions = [] recalls = [] f1_scores = [] for threshold in np.linspace(0.1, 0.9, 9): thresholds.append(threshold) precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) scores_table = pd.DataFrame({&#39;f1&#39;:f1_scores, &#39;precision&#39;:precisions, &#39;recall&#39;:recalls, &#39;probability&#39;:thresholds}).sort_values(&#39;f1&#39;, ascending=False).round(3) figure = plt.figure(figsize = (15, 5)) plt1 = figure.add_subplot(121) plt1.plot(thresholds, precisions, label=&#39;Precision&#39;, linewidth=4) plt1.plot(thresholds, recalls, label=&#39;Recall&#39;, linewidth=4) plt1.plot(thresholds, f1_scores, label=&#39;F1&#39;, linewidth=4) plt1.set_ylabel(&#39;Scores&#39;) plt1.set_xlabel(&#39;Probability threshold&#39;) plt1.set_title(&#39;Probabilities threshold calibration&#39;) plt1.legend(bbox_to_anchor=(0.25, 0.25)) plt1.table(cellText = scores_table.values, colLabels = scores_table.columns, colLoc = &#39;center&#39;, cellLoc = &#39;center&#39;, loc = &#39;bottom&#39;, bbox = [0, -1.3, 1, 1]) plt2 = figure.add_subplot(122) plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], label=&#39;Another class&#39;, color=&#39;royalblue&#39;, alpha=1) plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], label=&#39;Main class&#39;, color=&#39;darkcyan&#39;, alpha=0.8) plt2.set_ylabel(&#39;Number of examples&#39;) plt2.set_xlabel(&#39;Probabilities&#39;) plt2.set_title(&#39;Probability histogram&#39;) plt2.legend(bbox_to_anchor=(1, 1)) plt.show() . def show_feature_importances(feature_names, feature_importances, get_top=None): feature_importances = pd.DataFrame({&#39;feature&#39;: feature_names, &#39;importance&#39;: feature_importances}) feature_importances = feature_importances.sort_values(&#39;importance&#39;, ascending=False) plt.figure(figsize = (20, len(feature_importances) * 0.355)) sns.barplot(feature_importances[&#39;importance&#39;], feature_importances[&#39;feature&#39;]) plt.xlabel(&#39;Importance&#39;) plt.title(&#39;Importance of features&#39;) plt.show() if get_top is not None: return feature_importances[&#39;feature&#39;][:get_top].tolist() . def plot_feature_importance(importance,names,model_type): #Create arrays from feature importance and feature names feature_importance = np.array(importance) feature_names = np.array(names) #Create a DataFrame using a Dictionary data={&#39;feature_names&#39;:feature_names,&#39;feature_importance&#39;:feature_importance} fi_df = pd.DataFrame(data) #Sort the DataFrame in order decreasing feature importance fi_df.sort_values(by=[&#39;feature_importance&#39;], ascending=False,inplace=True) #Define size of bar plot plt.figure(figsize=(10,8)) #Plot Searborn bar chart sns.barplot(x=fi_df[&#39;feature_importance&#39;], y=fi_df[&#39;feature_names&#39;]) #Add chart labels plt.title(model_type + &#39;FEATURE IMPORTANCE&#39;) plt.xlabel(&#39;FEATURE IMPORTANCE&#39;) plt.ylabel(&#39;FEATURE NAMES&#39;) . Пути к директориям и файлам . TARGET_NAME = &#39;Credit Default&#39; TRAIN_DATASET_PATH = &#39;data/train.csv&#39; TEST_DATASET_PATH = &#39;data/test.csv&#39; SCALER_FILE_PATH = &#39;data/scaler.pkl&#39; TRAIN_PART_PATH = &#39;data/training_project_train_part.csv&#39; TEST_PART_PATH = &#39;data/training_project_test_part.csv&#39; . Загрузка данных . train_df = pd.read_csv(TRAIN_DATASET_PATH) train_df.head() . Home Ownership Annual Income Years in current job Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Purpose Term Current Loan Amount Current Credit Balance Monthly Debt Credit Score Credit Default . 0 Own Home | 482,087.00 | NaN | 0.00 | 11.00 | 26.30 | 685,960.00 | 1.00 | NaN | 1.00 | debt consolidation | Short Term | 99,999,999.00 | 47,386.00 | 7,914.00 | 749.00 | 0 | . 1 Own Home | 1,025,487.00 | 10+ years | 0.00 | 15.00 | 15.30 | 1,181,730.00 | 0.00 | NaN | 0.00 | debt consolidation | Long Term | 264,968.00 | 394,972.00 | 18,373.00 | 737.00 | 1 | . 2 Home Mortgage | 751,412.00 | 8 years | 0.00 | 11.00 | 35.00 | 1,182,434.00 | 0.00 | NaN | 0.00 | debt consolidation | Short Term | 99,999,999.00 | 308,389.00 | 13,651.00 | 742.00 | 0 | . 3 Own Home | 805,068.00 | 6 years | 0.00 | 8.00 | 22.50 | 147,400.00 | 1.00 | NaN | 1.00 | debt consolidation | Short Term | 121,396.00 | 95,855.00 | 11,338.00 | 694.00 | 0 | . 4 Rent | 776,264.00 | 8 years | 0.00 | 13.00 | 13.60 | 385,836.00 | 1.00 | NaN | 0.00 | debt consolidation | Short Term | 125,840.00 | 93,309.00 | 7,180.00 | 719.00 | 0 | . test_df = pd.read_csv(TEST_DATASET_PATH) test_df.head() . Home Ownership Annual Income Years in current job Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Purpose Term Current Loan Amount Current Credit Balance Monthly Debt Credit Score . 0 Rent | NaN | 4 years | 0.00 | 9.00 | 12.50 | 220,968.00 | 0.00 | 70.00 | 0.00 | debt consolidation | Short Term | 162,470.00 | 105,906.00 | 6,813.00 | NaN | . 1 Rent | 231,838.00 | 1 year | 0.00 | 6.00 | 32.70 | 55,946.00 | 0.00 | 8.00 | 0.00 | educational expenses | Short Term | 78,298.00 | 46,037.00 | 2,318.00 | 699.00 | . 2 Home Mortgage | 1,152,540.00 | 3 years | 0.00 | 10.00 | 13.70 | 204,600.00 | 0.00 | NaN | 0.00 | debt consolidation | Short Term | 200,178.00 | 146,490.00 | 18,729.00 | 7,260.00 | . 3 Home Mortgage | 1,220,313.00 | 10+ years | 0.00 | 16.00 | 17.00 | 456,302.00 | 0.00 | 70.00 | 0.00 | debt consolidation | Short Term | 217,382.00 | 213,199.00 | 27,559.00 | 739.00 | . 4 Home Mortgage | 2,340,952.00 | 6 years | 0.00 | 11.00 | 23.60 | 1,207,272.00 | 0.00 | NaN | 0.00 | debt consolidation | Long Term | 777,634.00 | 425,391.00 | 42,605.00 | 706.00 | . 1. &#1054;&#1073;&#1079;&#1086;&#1088; &#1086;&#1073;&#1091;&#1095;&#1072;&#1102;&#1097;&#1077;&#1075;&#1086; &#1076;&#1072;&#1090;&#1072;&#1089;&#1077;&#1090;&#1072; . print(train_df.shape, test_df.shape) . (7500, 17) (2500, 16) . train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7500 entries, 0 to 7499 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 Home Ownership 7500 non-null object 1 Annual Income 5943 non-null float64 2 Years in current job 7129 non-null object 3 Tax Liens 7500 non-null float64 4 Number of Open Accounts 7500 non-null float64 5 Years of Credit History 7500 non-null float64 6 Maximum Open Credit 7500 non-null float64 7 Number of Credit Problems 7500 non-null float64 8 Months since last delinquent 3419 non-null float64 9 Bankruptcies 7486 non-null float64 10 Purpose 7500 non-null object 11 Term 7500 non-null object 12 Current Loan Amount 7500 non-null float64 13 Current Credit Balance 7500 non-null float64 14 Monthly Debt 7500 non-null float64 15 Credit Score 5943 non-null float64 16 Credit Default 7500 non-null int64 dtypes: float64(12), int64(1), object(4) memory usage: 996.2+ KB . columns_name = train_df.columns . train_df.nunique(dropna=False) . Home Ownership 4 Annual Income 5479 Years in current job 12 Tax Liens 8 Number of Open Accounts 39 Years of Credit History 408 Maximum Open Credit 6963 Number of Credit Problems 8 Months since last delinquent 90 Bankruptcies 6 Purpose 15 Term 2 Current Loan Amount 5386 Current Credit Balance 6592 Monthly Debt 6716 Credit Score 269 Credit Default 2 dtype: int64 . columns_name . Index([&#39;Home Ownership&#39;, &#39;Annual Income&#39;, &#39;Years in current job&#39;, &#39;Tax Liens&#39;, &#39;Number of Open Accounts&#39;, &#39;Years of Credit History&#39;, &#39;Maximum Open Credit&#39;, &#39;Number of Credit Problems&#39;, &#39;Months since last delinquent&#39;, &#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;, &#39;Current Loan Amount&#39;, &#39;Current Credit Balance&#39;, &#39;Monthly Debt&#39;, &#39;Credit Score&#39;, &#39;Credit Default&#39;], dtype=&#39;object&#39;) . # Так же выделю анализируюмую переменную y = train_df[&#39;Credit Default&#39;] cat_col = [&#39;Home Ownership&#39;, &#39;Years in current job&#39;, &#39;Tax Liens&#39;, &#39;Number of Credit Problems&#39;,&#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;] num_col = [&#39;Annual Income&#39;, &#39;Number of Open Accounts&#39;, &#39;Maximum Open Credit&#39;, &#39;Years of Credit History&#39;, &#39;Months since last delinquent&#39;, &#39;Current Loan Amount&#39;, &#39;Current Credit Balance&#39;, &#39;Monthly Debt&#39;, &#39;Credit Score&#39;,] . cat_df = train_df[cat_col] cat_df = cat_df.astype(str) num_df = train_df[num_col] num_df = num_df.astype(float) . fig, ax = plt.subplots(4,2, figsize=(40,35)) sns.countplot(train_df[&#39;Home Ownership&#39;], ax=ax[0,0]) sns.countplot(train_df[&#39;Years in current job&#39;], ax=ax[0,1]) sns.countplot(train_df[&#39;Tax Liens&#39;], ax=ax[1,0]) sns.countplot(train_df[&#39;Number of Credit Problems&#39;], ax=ax[1,1]) sns.countplot(train_df[&#39;Bankruptcies&#39;], ax=ax[2,0]) sns.countplot(train_df[&#39;Purpose&#39;], ax=ax[2,1]) sns.countplot(train_df[&#39;Term&#39;], ax=ax[3,0]) fig.show() . for c in cat_df.columns: print (&quot;- %s &quot; % c) print (cat_df[c].value_counts()) . - Home Ownership Home Mortgage 3637 Rent 3204 Own Home 647 Have Mortgage 12 Name: Home Ownership, dtype: int64 - Years in current job 10+ years 2332 2 years 705 3 years 620 &lt; 1 year 563 5 years 516 1 year 504 4 years 469 6 years 426 7 years 396 nan 371 8 years 339 9 years 259 Name: Years in current job, dtype: int64 - Tax Liens 0.0 7366 1.0 83 2.0 30 3.0 10 4.0 6 6.0 2 5.0 2 7.0 1 Name: Tax Liens, dtype: int64 - Number of Credit Problems 0.0 6469 1.0 882 2.0 93 3.0 35 4.0 9 5.0 7 6.0 4 7.0 1 Name: Number of Credit Problems, dtype: int64 - Bankruptcies 0.0 6660 1.0 786 2.0 31 nan 14 3.0 7 4.0 2 Name: Bankruptcies, dtype: int64 - Purpose debt consolidation 5944 other 665 home improvements 412 business loan 129 buy a car 96 medical bills 71 major purchase 40 take a trip 37 buy house 34 small business 26 wedding 15 moving 11 educational expenses 10 vacation 8 renewable energy 2 Name: Purpose, dtype: int64 - Term Short Term 5556 Long Term 1944 Name: Term, dtype: int64 . h = num_df.hist(bins=25,figsize=(16,16),xlabelsize=&#39;10&#39;,ylabelsize=&#39;10&#39;,xrot=-15) sns.despine(left=True, bottom=True) [x.title.set_size(12) for x in h.ravel()]; [x.yaxis.tick_left() for x in h.ravel()]; . mask = np.zeros_like(num_df.corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(16, 12)) plt.title(&#39;Pearson Correlation Matrix&#39;,fontsize=25) sns.heatmap(num_df.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=&quot;BuGn&quot;, #&quot;BuGn_r&quot; to reverse linecolor=&#39;w&#39;,annot=True,annot_kws={&quot;size&quot;:8},mask=mask,cbar_kws={&quot;shrink&quot;: .9}); . 2. &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1087;&#1088;&#1086;&#1087;&#1091;&#1089;&#1082;&#1086;&#1074; . train_df.isna().sum() . Home Ownership 0 Annual Income 1557 Years in current job 371 Tax Liens 0 Number of Open Accounts 0 Years of Credit History 0 Maximum Open Credit 0 Number of Credit Problems 0 Months since last delinquent 4081 Bankruptcies 14 Purpose 0 Term 0 Current Loan Amount 0 Current Credit Balance 0 Monthly Debt 0 Credit Score 1557 Credit Default 0 dtype: int64 . train_df.describe() . Annual Income Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Current Loan Amount Current Credit Balance Monthly Debt Credit Score Credit Default . count 5,943.00 | 7,500.00 | 7,500.00 | 7,500.00 | 7,500.00 | 7,500.00 | 3,419.00 | 7,486.00 | 7,500.00 | 7,500.00 | 7,500.00 | 5,943.00 | 7,500.00 | . mean 1,366,391.72 | 0.03 | 11.13 | 18.32 | 945,153.73 | 0.17 | 34.69 | 0.12 | 11,873,177.45 | 289,833.24 | 18,314.45 | 1,151.09 | 0.28 | . std 845,339.20 | 0.27 | 4.91 | 7.04 | 16,026,216.67 | 0.50 | 21.69 | 0.35 | 31,926,122.97 | 317,871.38 | 11,926.76 | 1,604.45 | 0.45 | . min 164,597.00 | 0.00 | 2.00 | 4.00 | 0.00 | 0.00 | 0.00 | 0.00 | 11,242.00 | 0.00 | 0.00 | 585.00 | 0.00 | . 25% 844,341.00 | 0.00 | 8.00 | 13.50 | 279,229.50 | 0.00 | 16.00 | 0.00 | 180,169.00 | 114,256.50 | 10,067.50 | 711.00 | 0.00 | . 50% 1,168,386.00 | 0.00 | 10.00 | 17.00 | 478,159.00 | 0.00 | 32.00 | 0.00 | 309,573.00 | 209,323.00 | 16,076.50 | 731.00 | 0.00 | . 75% 1,640,137.00 | 0.00 | 14.00 | 21.80 | 793,501.50 | 0.00 | 50.00 | 0.00 | 519,882.00 | 360,406.25 | 23,818.00 | 743.00 | 1.00 | . max 10,149,344.00 | 7.00 | 43.00 | 57.70 | 1,304,726,170.00 | 7.00 | 118.00 | 4.00 | 99,999,999.00 | 6,506,797.00 | 136,679.00 | 7,510.00 | 1.00 | . Первое заполню Annual Income при помощи медианных значений . median_income = train_df[&#39;Annual Income&#39;].median() train_df[&#39;Annual Income&#39;] = train_df[&#39;Annual Income&#39;].fillna(median_income) test_df[&#39;Annual Income&#39;] = test_df[&#39;Annual Income&#39;].fillna(median_income) . Дальше занимаюсь Years in current job. В данный момент они имеют текстовое значение. Так что выделю по частоте наиболее часто встречающиеся и заменю им пропущенное значение . max_YCJ = train_df[&#39;Years in current job&#39;].value_counts().index[0] train_df[&#39;Years in current job&#39;] = train_df[&#39;Years in current job&#39;].fillna(max_YCJ) test_df[&#39;Years in current job&#39;] = test_df[&#39;Years in current job&#39;].fillna(max_YCJ) . Так же заменю значения на более приемлемые для дальнейшего использования в расчетах - цифры. . train_df[&#39;Years in current job&#39;] = train_df[&#39;Years in current job&#39;].replace({&#39;10+ years&#39;:10,&#39;2 years&#39;:2, &#39;3 years&#39;:3, &#39;&lt; 1 year&#39;:0, &#39;5 years&#39;:5, &#39;1 year&#39;:1, &#39;4 years&#39;:4, &#39;6 years&#39;:6,&#39;7 years&#39;:7, &#39;8 years&#39;:8, &#39;9 years&#39;:9}) test_df[&#39;Years in current job&#39;] = test_df[&#39;Years in current job&#39;].replace({&#39;10+ years&#39;:10,&#39;2 years&#39;:2, &#39;3 years&#39;:3, &#39;&lt; 1 year&#39;:0, &#39;5 years&#39;:5, &#39;1 year&#39;:1, &#39;4 years&#39;:4, &#39;6 years&#39;:6,&#39;7 years&#39;:7, &#39;8 years&#39;:8, &#39;9 years&#39;:9}) . С Months since last delinquent поступлю так же, как и Annual Income - использую медиану. Но может имеет смысл вообще удалить данную колонку, так как большая часть значений отсутствует . median_delinquent = train_df[&#39;Months since last delinquent&#39;].median() train_df[&#39;Months since last delinquent&#39;] = train_df[&#39;Months since last delinquent&#39;].fillna(median_delinquent) test_df[&#39;Months since last delinquent&#39;] = test_df[&#39;Months since last delinquent&#39;].fillna(median_delinquent) . Для банкротов при проверки на частоту выясняется, что большинство не было банкротами. Соответственно пропущенные значения заменю на 0. . train_df[&#39;Bankruptcies&#39;].value_counts() . 0.00 6660 1.00 786 2.00 31 3.00 7 4.00 2 Name: Bankruptcies, dtype: int64 . train_df[&#39;Bankruptcies&#39;] = train_df[&#39;Bankruptcies&#39;].fillna(0.00) test_df[&#39;Bankruptcies&#39;] = test_df[&#39;Bankruptcies&#39;].fillna(0.00) . Осталась последняя колонка с NaN значениями - Credit Score. Ее заполню уже привычным методом - медианы . median_CS = train_df[&#39;Credit Score&#39;].median() train_df[&#39;Credit Score&#39;] = train_df[&#39;Credit Score&#39;].fillna(median_CS) test_df[&#39;Credit Score&#39;] = test_df[&#39;Credit Score&#39;].fillna(median_CS) . 3. &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1074;&#1099;&#1073;&#1088;&#1086;&#1089;&#1086;&#1074; . Выбросы смотрю по графикам и частоте. . Категориальные колонки оставляю без изменений - &#39;Years in current job&#39;, &#39;Tax Liens&#39;,&#39;Number of Credit Problems&#39;,&#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;. . Из подсчета по частоте в Home Ownership выбивается значение для Have Mortgage как слишком низкое. Вероятнее всего это ошибочная запись для Home Mortgage. . test_df[&#39;Home Ownership&#39;].value_counts() . Home Mortgage 1225 Rent 1020 Own Home 248 Have Mortgage 7 Name: Home Ownership, dtype: int64 . train_df.loc[train_df[&#39;Home Ownership&#39;] == &#39;Have Mortgage&#39;, &#39;Home Ownership&#39;] = &#39;Home Mortgage&#39; test_df.loc[test_df[&#39;Home Ownership&#39;] == &#39;Have Mortgage&#39;, &#39;Home Ownership&#39;] = &#39;Home Mortgage&#39; . Так как в DataFrame тестовом отсутствует такая цель, как возобновляемая энергия, то объединяю их с путишествиями и называю - другое . train_df[&#39;Purpose&#39;] = train_df[&#39;Purpose&#39;].replace({&#39;vacation&#39;:&#39;other&#39;,&#39;renewable energy&#39;:&#39;other&#39;}) test_df[&#39;Purpose&#39;] = test_df[&#39;Purpose&#39;].replace({&#39;vacation&#39;:&#39;other&#39;,&#39;renewable energy&#39;:&#39;other&#39;}) . Дальше убираю сильные выбросы по годовому доходу, выданным кредитам, наибольшему открытому кредиту, количеству лет кредитной истории, количеству месяцев с последней просрочки платежа, текущем кредитном балансе, ежемесячном долге и кредитной оценке. Если значение вылетает за три сигмы, то присваиваю ему среднее плюс три сигмы. . В текущей сумме кредита превышения больше трех сигм отсутствуют, так что оставляю без изменений. . mean_AI = train_df[&#39;Annual Income&#39;].mean() sigma_AI = train_df[&#39;Annual Income&#39;].std() train_df.loc[train_df[&#39;Annual Income&#39;] &gt; (mean_AI + 3 * sigma_AI), &#39;Annual Income&#39;] = (mean_AI + 3 * sigma_AI) test_df.loc[test_df[&#39;Annual Income&#39;] &gt; (mean_AI + 3 * sigma_AI), &#39;Annual Income&#39;] = (mean_AI + 3 * sigma_AI) . mean_NOA = train_df[&#39;Number of Open Accounts&#39;].mean() sigma_NOA = train_df[&#39;Number of Open Accounts&#39;].std() train_df.loc[train_df[&#39;Number of Open Accounts&#39;] &gt; (mean_NOA + 3 * sigma_NOA), &#39;Number of Open Accounts&#39;] = (mean_NOA + 3 * sigma_NOA) test_df.loc[test_df[&#39;Number of Open Accounts&#39;] &gt; (mean_NOA + 3 * sigma_NOA), &#39;Number of Open Accounts&#39;] = (mean_NOA + 3 * sigma_NOA) . mean_MOC = train_df[&#39;Maximum Open Credit&#39;].mean() sigma_MOC = train_df[&#39;Maximum Open Credit&#39;].std() train_df.loc[train_df[&#39;Maximum Open Credit&#39;] &gt; (mean_MOC + 3 * sigma_MOC), &#39;Maximum Open Credit&#39;] = (mean_MOC + 3 * sigma_MOC) test_df.loc[test_df[&#39;Maximum Open Credit&#39;] &gt; (mean_MOC + 3 * sigma_MOC), &#39;Maximum Open Credit&#39;] = (mean_MOC + 3 * sigma_MOC) . mean_YCH = train_df[&#39;Years of Credit History&#39;].mean() sigma_YCH = train_df[&#39;Years of Credit History&#39;].std() train_df.loc[train_df[&#39;Years of Credit History&#39;] &gt; (mean_YCH + 3 * sigma_YCH), &#39;Years of Credit History&#39;] = (mean_YCH + 3 * sigma_YCH) test_df.loc[test_df[&#39;Years of Credit History&#39;] &gt; (mean_YCH + 3 * sigma_YCH), &#39;Years of Credit History&#39;] = (mean_YCH + 3 * sigma_YCH) . mean_MSLD = train_df[&#39;Months since last delinquent&#39;].mean() sigma_MSLD = train_df[&#39;Months since last delinquent&#39;].std() train_df.loc[train_df[&#39;Months since last delinquent&#39;] &gt; (mean_MSLD + 3 * sigma_MSLD), &#39;Months since last delinquent&#39;] = (mean_MSLD + 3 * sigma_MSLD) test_df.loc[test_df[&#39;Months since last delinquent&#39;] &gt; (mean_MSLD + 3 * sigma_MSLD), &#39;Months since last delinquent&#39;] = (mean_MSLD + 3 * sigma_MSLD) . mean_CCB = train_df[&#39;Current Credit Balance&#39;].mean() sigma_CCB = train_df[&#39;Current Credit Balance&#39;].std() train_df.loc[train_df[&#39;Current Credit Balance&#39;] &gt; (mean_CCB + 3 * sigma_CCB), &#39;Current Credit Balance&#39;] = (mean_CCB + 3 * sigma_CCB) test_df.loc[test_df[&#39;Current Credit Balance&#39;] &gt; (mean_CCB + 3 * sigma_CCB), &#39;Current Credit Balance&#39;] = (mean_CCB + 3 * sigma_CCB) . mean_MD = train_df[&#39;Monthly Debt&#39;].mean() sigma_MD = train_df[&#39;Monthly Debt&#39;].std() train_df.loc[train_df[&#39;Monthly Debt&#39;] &gt; (mean_MD + 3 * sigma_MD), &#39;Monthly Debt&#39;] = (mean_MD + 3 * sigma_MD) test_df.loc[test_df[&#39;Monthly Debt&#39;] &gt; (mean_MD + 3 * sigma_MD), &#39;Monthly Debt&#39;] = (mean_MD + 3 * sigma_MD) . mean_CS = train_df[&#39;Credit Score&#39;].mean() sigma_CS = train_df[&#39;Credit Score&#39;].std() train_df.loc[train_df[&#39;Credit Score&#39;] &gt; (mean_CS + 3 * sigma_CS), &#39;Credit Score&#39;] = (mean_CS + 3 * sigma_CS) test_df.loc[test_df[&#39;Credit Score&#39;] &gt; (mean_CS + 3 * sigma_CS), &#39;Credit Score&#39;] = (mean_CS + 3 * sigma_CS) . 4. &#1055;&#1086;&#1076;&#1075;&#1086;&#1090;&#1086;&#1074;&#1082;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1082; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079;&#1091; . Первым шагом сделаю таблицу с фиктивными значениями категориальных данных и нормализую числовые значения. После чего сохраню полученный DataFrame для дальнейшего анализа . cat_dum_train = pd.get_dummies(train_df[cat_col]) cat_dum_test = pd.get_dummies(test_df[cat_col]) . scaler = StandardScaler() num_norm_train = pd.DataFrame(scaler.fit_transform(train_df[num_col]), columns = num_col) num_norm_test = pd.DataFrame(scaler.transform(test_df[num_col]), columns = num_col) . train_new = pd.concat([cat_dum_train, num_norm_train], axis=1) test_new = pd.concat([cat_dum_test, num_norm_test], axis=1) . X_train, X_test, y_train, y_test = train_test_split(train_new, y, shuffle=True, test_size=0.2, random_state=56) . 5. &#1041;&#1072;&#1083;&#1072;&#1085;&#1089;&#1080;&#1088;&#1086;&#1074;&#1082;&#1072; &#1094;&#1077;&#1083;&#1077;&#1074;&#1086;&#1081; &#1087;&#1077;&#1088;&#1077;&#1084;&#1077;&#1085;&#1085;&#1086;&#1081; . y.value_counts() . 0 5387 1 2113 Name: Credit Default, dtype: int64 . df_for_balancing = pd.concat([X_train, y_train], axis=1) df_balanced = balance_df_by_target(df_for_balancing, TARGET_NAME) df_balanced[TARGET_NAME].value_counts() . 0 4292 1 3416 Name: Credit Default, dtype: int64 . X_train = df_balanced.drop(columns=TARGET_NAME) y_train = df_balanced[TARGET_NAME] . 6. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1080; &#1086;&#1094;&#1077;&#1085;&#1082;&#1072; &#1073;&#1072;&#1079;&#1086;&#1074;&#1099;&#1093; &#1084;&#1086;&#1076;&#1077;&#1083;&#1077;&#1081; . Логистическая регрессия . model_lr = LogisticRegression() model_lr.fit(X_train, y_train) y_train_pred = model_lr.predict(X_train) y_test_pred = model_lr.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.69 0.83 0.75 4292 1 0.71 0.54 0.61 3416 accuracy 0.70 7708 macro avg 0.70 0.68 0.68 7708 weighted avg 0.70 0.70 0.69 7708 TEST precision recall f1-score support 0 0.82 0.80 0.81 1095 1 0.50 0.53 0.51 405 accuracy 0.73 1500 macro avg 0.66 0.66 0.66 1500 weighted avg 0.73 0.73 0.73 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 879 216 1 192 213 . k ближайших соседей . model_knn = KNeighborsClassifier() model_knn.fit(X_train, y_train) y_train_pred = model_knn.predict(X_train) y_test_pred = model_knn.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.79 0.85 0.82 4292 1 0.80 0.72 0.76 3416 accuracy 0.80 7708 macro avg 0.80 0.79 0.79 7708 weighted avg 0.80 0.80 0.79 7708 TEST precision recall f1-score support 0 0.81 0.77 0.79 1095 1 0.44 0.50 0.47 405 accuracy 0.70 1500 macro avg 0.63 0.63 0.63 1500 weighted avg 0.71 0.70 0.70 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 839 256 1 201 204 . Бустинговые алгоритмы . XGBoost . model_xgb = xgb.XGBClassifier(random_state=56) model_xgb.fit(X_train, y_train) y_train_pred = model_xgb.predict(X_train) y_test_pred = model_xgb.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . [20:38:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. TRAIN precision recall f1-score support 0 0.97 0.97 0.97 4292 1 0.97 0.96 0.96 3416 accuracy 0.97 7708 macro avg 0.97 0.97 0.97 7708 weighted avg 0.97 0.97 0.97 7708 TEST precision recall f1-score support 0 0.81 0.81 0.81 1095 1 0.48 0.47 0.48 405 accuracy 0.72 1500 macro avg 0.64 0.64 0.64 1500 weighted avg 0.72 0.72 0.72 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 886 209 1 213 192 . LightGBM . model_lgbm = lgbm.LGBMClassifier(random_state=56) model_lgbm.fit(X_train, y_train) y_train_pred = model_lgbm.predict(X_train) y_test_pred = model_lgbm.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.91 0.95 0.93 4292 1 0.93 0.88 0.90 3416 accuracy 0.92 7708 macro avg 0.92 0.91 0.92 7708 weighted avg 0.92 0.92 0.92 7708 TEST precision recall f1-score support 0 0.81 0.84 0.83 1095 1 0.52 0.48 0.50 405 accuracy 0.74 1500 macro avg 0.67 0.66 0.66 1500 weighted avg 0.73 0.74 0.74 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 917 178 1 211 194 . CatBoost . model_catb = catb.CatBoostClassifier(silent=True, random_state=56) model_catb.fit(X_train, y_train) y_train_pred = model_catb.predict(X_train) y_test_pred = model_catb.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.89 0.95 0.92 4292 1 0.93 0.86 0.89 3416 accuracy 0.91 7708 macro avg 0.91 0.90 0.90 7708 weighted avg 0.91 0.91 0.91 7708 TEST precision recall f1-score support 0 0.81 0.83 0.82 1095 1 0.51 0.49 0.50 405 accuracy 0.74 1500 macro avg 0.66 0.66 0.66 1500 weighted avg 0.73 0.74 0.73 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 907 188 1 208 197 . На тесте у меня лучше результаты для классификатора CatBoost, для него и буду подбирать параметрры . 7. &#1042;&#1099;&#1073;&#1086;&#1088; &#1083;&#1091;&#1095;&#1096;&#1077;&#1081; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; &#1080; &#1087;&#1086;&#1076;&#1073;&#1086;&#1088; &#1075;&#1080;&#1087;&#1077;&#1088;&#1087;&#1072;&#1088;&#1072;&#1084;&#1077;&#1090;&#1088;&#1086;&#1074; . model_catb = catb.CatBoostClassifier(class_weights=[1, 3.5], silent=True, random_state=56) . params_1 = {&#39;n_estimators&#39;:[1500, 1800, 2100], &#39;max_depth&#39;:[1, 2, 3]} . %%time rs = RandomizedSearchCV(model_catb, params_1, scoring=&#39;f1&#39;, cv=cv, n_jobs=-1) rs.fit(train_new, y) . CPU times: user 17.6 s, sys: 4.03 s, total: 21.6 s Wall time: 1min 56s . RandomizedSearchCV(cv=KFold(n_splits=3, random_state=56, shuffle=True), estimator=&lt;catboost.core.CatBoostClassifier object at 0x7f8b78854dc0&gt;, n_jobs=-1, param_distributions={&#39;max_depth&#39;: [1, 2, 3], &#39;n_estimators&#39;: [1500, 1800, 2100]}, scoring=&#39;f1&#39;) . rs.best_params_ . {&#39;n_estimators&#39;: 1500, &#39;max_depth&#39;: 3} . rs.best_score_ . 0.5418973339546257 . Обучение и оценка финальной модели . %%time final_model = catb.CatBoostClassifier(n_estimators=1500, max_depth=3, silent=True, random_state=56) final_model.fit(X_train, y_train) y_train_pred = final_model.predict(X_train) y_test_pred = final_model.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.76 0.87 0.81 4292 1 0.80 0.66 0.73 3416 accuracy 0.78 7708 macro avg 0.78 0.77 0.77 7708 weighted avg 0.78 0.78 0.78 7708 TEST precision recall f1-score support 0 0.82 0.82 0.82 1095 1 0.52 0.52 0.52 405 accuracy 0.74 1500 macro avg 0.67 0.67 0.67 1500 weighted avg 0.74 0.74 0.74 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 899 196 1 194 211 CPU times: user 16.2 s, sys: 4.13 s, total: 20.4 s Wall time: 3.53 s . Получение результата . %%time final_model = catb.CatBoostClassifier(n_estimators=1500, max_depth=3, silent=True, random_state=56) final_model.fit(train_new, y) . CPU times: user 15.6 s, sys: 4.25 s, total: 19.8 s Wall time: 3.43 s . &lt;catboost.core.CatBoostClassifier at 0x7f8b795233a0&gt; . y_pred = final_model.predict(test_new) . result=pd.DataFrame({&#39;Id&#39;:np.arange(2500), &#39;Credit Default&#39;: y_pred}) . RESULT_PATH=&#39;solutions.csv&#39; result.to_csv(RESULT_PATH, index=False) . result.to_csv(&#39;solutions.csv&#39;, index=False) . В общей таблице рейтинга я не выбился далеко и получил результат 0.45695 .",
            "url": "https://zmey56.github.io/blog//course%20project/machine%20learning/classification/geekbrain/2021/11/10/course-project-classification.html",
            "relUrl": "/course%20project/machine%20learning/classification/geekbrain/2021/11/10/course-project-classification.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Курсовой проект по стоимости квартиры",
            "content": "Текста будет немного, в основном решениие. Несмотря на негативный кай я считаю отзыв преподавателя - среди 60 однокурсников занял 2 место, а на kaggle 67 место из 487. Так что уважаемый преподаватель наверное что-то я улучшил удалось, а вы просто видимо загружены большим количеством контрольных и не смогли подробно разобрать. Но цитату Вашу привожу без изменений - может кому понадобиться: . Вы добавили графики, не стали отбирать лучшие признаки и проверяли точность модели только на кагле. ник AlexTUT56? ваших новых признаков нет, схемы для валидации модели нет. блокнот, что мы разбирали на вебинаре, был дан как основа. либо вы предлагаете свое оригинальное решение, либо улучшаете существующее. улучшить не удалось. . import numpy as np import pandas as pd import random from sklearn.model_selection import cross_val_score, train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_squared_error from sklearn.model_selection import KFold, GridSearchCV import matplotlib import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . import warnings warnings.filterwarnings(&#39;ignore&#39;) sns.set(style=&#39;whitegrid&#39;) sns.set_context(&quot;paper&quot;, font_scale=1.5) pd.options.display.float_format = &#39;{:,.2f}&#39;.format pd.set_option(&#39;display.max_rows&#39;, 50) . def adjustedR2(r2,n,k): return r2-(k-1)/(n-k)*(1-r2) . TRAIN_DATASET_PATH = &#39;project/train.csv&#39; TEST_DATASET_PATH = &#39;project/test.csv&#39; . 1. &#1047;&#1072;&#1075;&#1088;&#1091;&#1078;&#1072;&#1102; &#1076;&#1072;&#1085;&#1085;&#1099;&#1077; . Описание датасета . Id - идентификационный номер квартиры | DistrictId - идентификационный номер района | Rooms - количество комнат | Square - площадь | LifeSquare - жилая площадь | KitchenSquare - площадь кухни | Floor - этаж | HouseFloor - количество этажей в доме | HouseYear - год постройки дома | Ecology_1, Ecology_2, Ecology_3 - экологические показатели местности | Social_1, Social_2, Social_3 - социальные показатели местности | Healthcare_1, Helthcare_2 - показатели местности, связанные с охраной здоровья | Shops_1, Shops_2 - показатели, связанные с наличием магазинов, торговых центров | Price - цена квартиры | . train_df = pd.read_csv(TRAIN_DATASET_PATH, index_col=&#39;Id&#39;) train_df.head() . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 Price . Id . 14038 35 | 2.00 | 47.98 | 29.44 | 6.00 | 7 | 9.00 | 1969 | 0.09 | B | B | 33 | 7976 | 5 | NaN | 0 | 11 | B | 184,966.93 | . 15053 41 | 3.00 | 65.68 | 40.05 | 8.00 | 7 | 9.00 | 1978 | 0.00 | B | B | 46 | 10309 | 1 | 240.00 | 1 | 16 | B | 300,009.45 | . 4765 53 | 2.00 | 44.95 | 29.20 | 0.00 | 8 | 12.00 | 1968 | 0.05 | B | B | 34 | 7759 | 0 | 229.00 | 1 | 3 | B | 220,925.91 | . 5809 58 | 2.00 | 53.35 | 52.73 | 9.00 | 8 | 17.00 | 1977 | 0.44 | B | B | 23 | 5735 | 3 | 1,084.00 | 0 | 5 | B | 175,616.23 | . 10783 99 | 1.00 | 39.65 | 23.78 | 7.00 | 11 | 12.00 | 1976 | 0.01 | B | B | 35 | 5776 | 1 | 2,078.00 | 2 | 4 | B | 150,226.53 | . test_df = pd.read_csv(TEST_DATASET_PATH, index_col=&#39;Id&#39;) test_df.head() . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 . Id . 725 58 | 2.00 | 49.88 | 33.43 | 6.00 | 6 | 14.00 | 1972 | 0.31 | B | B | 11 | 2748 | 1 | NaN | 0 | 0 | B | . 15856 74 | 2.00 | 69.26 | NaN | 1.00 | 6 | 1.00 | 1977 | 0.08 | B | B | 6 | 1437 | 3 | NaN | 0 | 2 | B | . 5480 190 | 1.00 | 13.60 | 15.95 | 12.00 | 2 | 5.00 | 1909 | 0.00 | B | B | 30 | 7538 | 87 | 4,702.00 | 5 | 5 | B | . 15664 47 | 2.00 | 73.05 | 51.94 | 9.00 | 22 | 22.00 | 2007 | 0.10 | B | B | 23 | 4583 | 3 | NaN | 3 | 3 | B | . 14275 27 | 1.00 | 47.53 | 43.39 | 1.00 | 17 | 17.00 | 2017 | 0.07 | B | B | 2 | 629 | 1 | NaN | 0 | 0 | A | . train_df.shape, test_df.shape . ((10000, 19), (5000, 18)) . 2. &#1042;&#1080;&#1079;&#1091;&#1072;&#1083;&#1100;&#1085;&#1099;&#1081; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Воспользуюсь вариантом предложенным на уроке и собственными мыслями. Первым шагом познакомимся с данными. Построим их гистограммы, а тех, которых не много - box-plot. . train_df.columns . Index([&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;], dtype=&#39;object&#39;) . df1=train_df[[&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;]] h = df1.hist(bins=25,figsize=(16,16),xlabelsize=&#39;10&#39;,ylabelsize=&#39;10&#39;,xrot=-15) sns.despine(left=True, bottom=True) [x.title.set_size(12) for x in h.ravel()]; [x.yaxis.tick_left() for x in h.ravel()]; . sns.set(style=&quot;whitegrid&quot;, font_scale=1) . f, axes = plt.subplots(1, 2,figsize=(15,5)) sns.boxplot(x=train_df[&#39;Rooms&#39;],y=train_df[&#39;Price&#39;], ax=axes[0]) sns.boxplot(x=train_df[&#39;Helthcare_2&#39;],y=train_df[&#39;Price&#39;], ax=axes[1]) sns.despine(left=True, bottom=True) axes[0].set(xlabel=&#39;Rooms&#39;, ylabel=&#39;Price&#39;) axes[0].yaxis.tick_left() axes[1].yaxis.set_label_position(&quot;right&quot;) axes[1].yaxis.tick_right() axes[1].set(xlabel=&#39;Helthcare_2&#39;, ylabel=&#39;Price&#39;) f, axe = plt.subplots(1, 1,figsize=(12.18,5)) sns.despine(left=True, bottom=True) sns.boxplot(x=train_df[&#39;Floor&#39;],y=train_df[&#39;Price&#39;], ax=axe) axe.yaxis.tick_left() axe.set(xlabel=&#39;Floor&#39;, ylabel=&#39;Price&#39;); . Можно конечно еще долго играться с графиками, но визуально выявить влияние вряд-ли удасться. Посмотрю на корреляцию между данными. . features = [&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;] mask = np.zeros_like(train_df[features].corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(16, 12)) plt.title(&#39;Pearson Correlation Matrix&#39;,fontsize=25) sns.heatmap(train_df[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=&quot;BuGn&quot;, #&quot;BuGn_r&quot; to reverse linecolor=&#39;w&#39;,annot=True,annot_kws={&quot;size&quot;:8},mask=mask,cbar_kws={&quot;shrink&quot;: .9}); . 3. &#1055;&#1086;&#1076;&#1075;&#1086;&#1090;&#1086;&#1074;&#1082;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Для подготовки воспользуюсь примером из урока . 3.1 &#1055;&#1088;&#1080;&#1074;&#1077;&#1076;&#1077;&#1085;&#1080;&#1077; &#1090;&#1080;&#1087;&#1086;&#1074; . train_df.dtypes . DistrictId int64 Rooms float64 Square float64 LifeSquare float64 KitchenSquare float64 Floor int64 HouseFloor float64 HouseYear int64 Ecology_1 float64 Ecology_2 object Ecology_3 object Social_1 int64 Social_2 int64 Social_3 int64 Healthcare_1 float64 Helthcare_2 int64 Shops_1 int64 Shops_2 object Price float64 dtype: object . train_df[&#39;DistrictId&#39;] = train_df[&#39;DistrictId&#39;].astype(str) test_df[&#39;DistrictId&#39;] = test_df[&#39;DistrictId&#39;].astype(str) . train_df.describe() . Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Price . count 10,000.00 | 10,000.00 | 7,887.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 5,202.00 | 10,000.00 | 10,000.00 | 10,000.00 | . mean 1.89 | 56.32 | 37.20 | 6.27 | 8.53 | 12.61 | 3,990.17 | 0.12 | 24.69 | 5,352.16 | 8.04 | 1,142.90 | 1.32 | 4.23 | 214,138.86 | . std 0.84 | 21.06 | 86.24 | 28.56 | 5.24 | 6.78 | 200,500.26 | 0.12 | 17.53 | 4,006.80 | 23.83 | 1,021.52 | 1.49 | 4.81 | 92,872.29 | . min 0.00 | 1.14 | 0.37 | 0.00 | 1.00 | 0.00 | 1,910.00 | 0.00 | 0.00 | 168.00 | 0.00 | 0.00 | 0.00 | 0.00 | 59,174.78 | . 25% 1.00 | 41.77 | 22.77 | 1.00 | 4.00 | 9.00 | 1,974.00 | 0.02 | 6.00 | 1,564.00 | 0.00 | 350.00 | 0.00 | 1.00 | 153,872.63 | . 50% 2.00 | 52.51 | 32.78 | 6.00 | 7.00 | 13.00 | 1,977.00 | 0.08 | 25.00 | 5,285.00 | 2.00 | 900.00 | 1.00 | 3.00 | 192,269.64 | . 75% 2.00 | 65.90 | 45.13 | 9.00 | 12.00 | 17.00 | 2,001.00 | 0.20 | 36.00 | 7,227.00 | 5.00 | 1,548.00 | 2.00 | 6.00 | 249,135.46 | . max 19.00 | 641.07 | 7,480.59 | 2,014.00 | 42.00 | 117.00 | 20,052,011.00 | 0.52 | 74.00 | 19,083.00 | 141.00 | 4,849.00 | 6.00 | 23.00 | 633,233.47 | . feature_num_names = train_df.drop(&#39;Price&#39;, axis=1).select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]). columns.tolist() feature_num_names . [&#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;] . feature_cat_names = train_df.select_dtypes(include=&#39;object&#39;).columns.tolist() feature_cat_names . [&#39;DistrictId&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Shops_2&#39;] . feature_bin_names = [&#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Shops_2&#39;] train_df[feature_bin_names] = train_df[feature_bin_names].replace({&#39;A&#39;:0, &#39;B&#39;:1}) test_df[feature_bin_names] = test_df[feature_bin_names].replace({&#39;A&#39;:0, &#39;B&#39;:1}) train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10000 entries, 14038 to 6306 Data columns (total 19 columns): # Column Non-Null Count Dtype -- -- 0 DistrictId 10000 non-null object 1 Rooms 10000 non-null float64 2 Square 10000 non-null float64 3 LifeSquare 7887 non-null float64 4 KitchenSquare 10000 non-null float64 5 Floor 10000 non-null int64 6 HouseFloor 10000 non-null float64 7 HouseYear 10000 non-null int64 8 Ecology_1 10000 non-null float64 9 Ecology_2 10000 non-null int64 10 Ecology_3 10000 non-null int64 11 Social_1 10000 non-null int64 12 Social_2 10000 non-null int64 13 Social_3 10000 non-null int64 14 Healthcare_1 5202 non-null float64 15 Helthcare_2 10000 non-null int64 16 Shops_1 10000 non-null int64 17 Shops_2 10000 non-null int64 18 Price 10000 non-null float64 dtypes: float64(8), int64(10), object(1) memory usage: 1.8+ MB . 3.2. &#1055;&#1088;&#1086;&#1087;&#1091;&#1097;&#1077;&#1085;&#1085;&#1099;&#1077; &#1079;&#1085;&#1072;&#1095;&#1077;&#1085;&#1080;&#1103; . train_df.isna().sum()[train_df.isna().sum() != 0] . LifeSquare 2113 Healthcare_1 4798 dtype: int64 . train_df.loc[train_df[&#39;LifeSquare&#39;].isna(), &#39;LifeSquare&#39;] = train_df[&#39;Square&#39;] - train_df[&#39;KitchenSquare&#39;] test_df.loc[test_df[&#39;LifeSquare&#39;].isna(), &#39;LifeSquare&#39;] = test_df[&#39;Square&#39;] - test_df[&#39;KitchenSquare&#39;] . pd.concat([train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanmean(x)), train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanstd(x)), train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].count(), train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: x.isna().sum()) ], axis=1, keys=[&#39;mean&#39;, &#39;std&#39;, &#39;count&#39;, &#39;nans&#39;]).sort_values(by=&#39;nans&#39;, ascending=False).head(10) . mean std count nans . DistrictId . 27 NaN | NaN | 0 | 851 | . 1 228.77 | 17.97 | 57 | 595 | . 23 0.00 | 0.00 | 1 | 564 | . 62 2,300.00 | 0.00 | 9 | 238 | . 45 NaN | NaN | 0 | 116 | . 34 NaN | NaN | 0 | 111 | . 61 80.00 | 0.00 | 8 | 110 | . 13 1,406.00 | 0.00 | 4 | 93 | . 9 30.00 | 0.00 | 202 | 92 | . 48 2,620.00 | 0.00 | 1 | 89 | . pd.concat([train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanmean(x)), train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanstd(x)), train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].count(), train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: x.isna().sum()) ], axis=1, keys=[&#39;mean&#39;, &#39;std&#39;, &#39;count&#39;, &#39;nans&#39;]).sort_values(by=&#39;nans&#39;, ascending=False) . mean std count nans . Helthcare_2 . 0 1,007.78 | 913.61 | 1275 | 2939 | . 1 811.36 | 696.54 | 925 | 1258 | . 3 1,363.38 | 1,005.68 | 1347 | 323 | . 5 1,824.09 | 1,574.23 | 212 | 176 | . 2 1,010.71 | 1,045.44 | 1056 | 102 | . 4 1,929.23 | 1,121.87 | 288 | 0 | . 6 645.00 | 0.00 | 99 | 0 | . pd.concat([test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanmean(x)), test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanstd(x)), test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].count(), test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: x.isna().sum()) ], axis=1, keys=[&#39;mean&#39;, &#39;std&#39;, &#39;count&#39;, &#39;nans&#39;]).sort_values(by=&#39;nans&#39;, ascending=False) . mean std count nans . Helthcare_2 . 0 976.07 | 905.42 | 667 | 1429 | . 1 836.12 | 738.36 | 442 | 654 | . 3 1,353.30 | 1,064.11 | 700 | 155 | . 5 1,606.40 | 1,528.20 | 97 | 89 | . 2 1,095.59 | 1,135.98 | 520 | 50 | . 4 1,829.66 | 1,086.82 | 158 | 0 | . 6 645.00 | 0.00 | 39 | 0 | . train_df.loc[train_df[&#39;Healthcare_1&#39;].isna(), &#39;Healthcare_1&#39;] = train_df.groupby([&#39;Helthcare_2&#39;])[&#39;Healthcare_1&#39;].transform(lambda x: x.mean()) . test_df.loc[test_df[&#39;Healthcare_1&#39;].isna(), &#39;Healthcare_1&#39;] = train_df.groupby([&#39;Helthcare_2&#39;])[&#39;Healthcare_1&#39;].transform(lambda x: x.mean()) . test_df[&#39;Healthcare_1&#39;] = test_df[&#39;Healthcare_1&#39;].fillna(test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].transform(&#39;mean&#39;)) . 3.3. &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1072;&#1085;&#1086;&#1084;&#1072;&#1083;&#1100;&#1085;&#1099;&#1093; &#1079;&#1085;&#1072;&#1095;&#1077;&#1085;&#1080;&#1081; . train_df[&#39;Rooms&#39;].value_counts() . 2.00 3880 1.00 3705 3.00 2235 4.00 150 5.00 18 0.00 8 10.00 2 6.00 1 19.00 1 Name: Rooms, dtype: int64 . train_df.loc[(train_df[&#39;Rooms&#39;] &gt; 5)|(train_df[&#39;Rooms&#39;] == 0), &#39;Rooms&#39;] = train_df[&#39;Rooms&#39;].mode()[0] test_df.loc[(test_df[&#39;Rooms&#39;] &gt; 5)|(test_df[&#39;Rooms&#39;] == 0), &#39;Rooms&#39;] = train_df[&#39;Rooms&#39;].mode()[0] # !! . train_df.loc[(train_df[&#39;KitchenSquare&#39;] &gt; 150) | (train_df[&#39;KitchenSquare&#39;] &gt; train_df[&#39;Square&#39;]), :] . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 Price . Id . 14656 62 | 1.00 | 47.10 | 46.45 | 2,014.00 | 4 | 1.00 | 2014 | 0.07 | 1 | 1 | 2 | 629 | 1 | 1,007.78 | 0 | 0 | 0 | 108,337.48 | . 13703 42 | 1.00 | 38.07 | 19.72 | 73.00 | 9 | 10.00 | 2006 | 0.16 | 1 | 1 | 21 | 5731 | 0 | 811.36 | 1 | 0 | 1 | 160,488.03 | . 6569 27 | 1.00 | 38.22 | 18.72 | 84.00 | 4 | 17.00 | 2018 | 0.01 | 1 | 1 | 4 | 915 | 0 | 1,007.78 | 0 | 0 | 1 | 99,079.96 | . 14679 81 | 1.00 | 32.28 | 19.28 | 1,970.00 | 6 | 1.00 | 1977 | 0.01 | 1 | 1 | 30 | 5285 | 0 | 645.00 | 6 | 6 | 1 | 105,539.56 | . train_df.loc[(train_df[&#39;KitchenSquare&#39;] &gt; 150) | (train_df[&#39;KitchenSquare&#39;] &gt; train_df[&#39;Square&#39;]), &#39;KitchenSquare&#39;] = train_df[&#39;KitchenSquare&#39;].median() test_df.loc[(test_df[&#39;KitchenSquare&#39;] &gt; 150) | (test_df[&#39;KitchenSquare&#39;] &gt; test_df[&#39;Square&#39;]), &#39;KitchenSquare&#39;] = train_df[&#39;KitchenSquare&#39;].median() . train_df.loc[(train_df[&#39;LifeSquare&#39;] &gt; 400), &#39;LifeSquare&#39;] = train_df[&#39;LifeSquare&#39;].median() test_df.loc[(test_df[&#39;LifeSquare&#39;] &gt; 400), &#39;LifeSquare&#39;] = train_df[&#39;LifeSquare&#39;].median() . train_df.loc[(train_df[&#39;Square&#39;] &gt; 400) | (train_df[&#39;Square&#39;] &lt; 10), &#39;Square&#39;] = train_df[&#39;Square&#39;].median() test_df.loc[(test_df[&#39;Square&#39;] &gt; 400) | (test_df[&#39;Square&#39;] &lt; 10), &#39;Square&#39;] = train_df[&#39;Square&#39;].median() . train_df[[&#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;]].describe() . Square LifeSquare KitchenSquare . count 10,000.00 | 10,000.00 | 10,000.00 | . mean 56.22 | 40.86 | 5.86 | . std 19.06 | 20.57 | 5.07 | . min 13.78 | 0.37 | 0.00 | . 25% 41.79 | 25.49 | 1.00 | . 50% 52.51 | 37.04 | 6.00 | . 75% 65.89 | 49.77 | 9.00 | . max 275.65 | 263.54 | 123.00 | . train_df.loc[train_df[&#39;HouseFloor&#39;] == 0, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] test_df.loc[test_df[&#39;HouseFloor&#39;] == 0, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] . train_df.loc[train_df[&#39;HouseFloor&#39;] &gt; 98, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] test_df.loc[test_df[&#39;HouseFloor&#39;] &gt; 98, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] . floor_outliers_train = train_df.loc[train_df[&#39;Floor&#39;] &gt; train_df[&#39;HouseFloor&#39;]].index floor_outliers_test = test_df.loc[test_df[&#39;Floor&#39;] &gt; test_df[&#39;HouseFloor&#39;]].index . train_df.loc[floor_outliers_train, &#39;Floor&#39;] = train_df.loc[floor_outliers_train, &#39;HouseFloor&#39;].apply(lambda x: random.randint(1, x)) test_df.loc[floor_outliers_test, &#39;Floor&#39;] = test_df.loc[floor_outliers_test, &#39;HouseFloor&#39;].apply(lambda x: random.randint(1, x)) . train_df.loc[train_df[&#39;HouseYear&#39;] &gt; 2020, &#39;HouseYear&#39;] = 2011 . 3.4. &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1091;&#1085;&#1080;&#1082;&#1072;&#1083;&#1100;&#1085;&#1099;&#1093; &#1079;&#1085;&#1072;&#1095;&#1077;&#1085;&#1080;&#1081; . print(train_df[&#39;DistrictId&#39;].nunique(), &#39; уникальных значений в train&#39;) print(test_df[&#39;DistrictId&#39;].nunique(), &#39; уникальных значений в test&#39;) . 205 уникальных значений в train 201 уникальных значений в test . district_size = train_df[&#39;DistrictId&#39;].value_counts().reset_index() .rename(columns={&#39;index&#39;:&#39;DistrictId&#39;, &#39;DistrictId&#39;:&#39;DistrictSize&#39;}) district_size . DistrictId DistrictSize . 0 27 | 851 | . 1 1 | 652 | . 2 23 | 565 | . 3 6 | 511 | . 4 9 | 294 | . ... ... | ... | . 200 117 | 1 | . 201 207 | 1 | . 202 209 | 1 | . 203 196 | 1 | . 204 205 | 1 | . 205 rows × 2 columns . districts_popular = district_size.loc[district_size[&#39;DistrictSize&#39;] &gt; 100, &#39;DistrictId&#39;].tolist() district_size.loc[~district_size[&#39;DistrictId&#39;].isin(districts_popular), &#39;DistrictId&#39;] = 999 district_size = district_size.groupby(&#39;DistrictId&#39;)[&#39;DistrictSize&#39;].agg( DistrictSize=&#39;median&#39;) district_size.reset_index(level=&#39;DistrictId&#39;, inplace=True) district_size . DistrictId DistrictSize . 0 999 | 15 | . 1 1 | 652 | . 2 11 | 129 | . 3 21 | 165 | . 4 23 | 565 | . 5 27 | 851 | . 6 30 | 228 | . 7 34 | 111 | . 8 44 | 104 | . 9 45 | 116 | . 10 46 | 119 | . 11 52 | 143 | . 12 53 | 174 | . 13 57 | 107 | . 14 58 | 179 | . 15 6 | 511 | . 16 61 | 118 | . 17 62 | 247 | . 18 74 | 114 | . 19 8 | 142 | . 20 9 | 294 | . train_df.loc[~train_df[&#39;DistrictId&#39;].isin(districts_popular), &#39;DistrictId&#39;] = 999 test_df.loc[~test_df[&#39;DistrictId&#39;].isin(districts_popular), &#39;DistrictId&#39;] = 999 . 3.5. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1085;&#1086;&#1074;&#1099;&#1093; &#1087;&#1088;&#1080;&#1079;&#1085;&#1072;&#1082;&#1086;&#1074; . train_df = train_df.merge(district_size, on=&#39;DistrictId&#39;, how=&#39;left&#39;).set_index(train_df.index) test_df = test_df.merge(district_size, on=&#39;DistrictId&#39;, how=&#39;left&#39;).set_index(test_df.index) train_df.head() . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 Price DistrictSize . Id . 14038 999 | 2.00 | 47.98 | 29.44 | 6.00 | 7 | 9.00 | 1969 | 0.09 | 1 | 1 | 33 | 7976 | 5 | 1,007.78 | 0 | 11 | 1 | 184,966.93 | 15 | . 15053 999 | 3.00 | 65.68 | 40.05 | 8.00 | 7 | 9.00 | 1978 | 0.00 | 1 | 1 | 46 | 10309 | 1 | 240.00 | 1 | 16 | 1 | 300,009.45 | 15 | . 4765 53 | 2.00 | 44.95 | 29.20 | 0.00 | 8 | 12.00 | 1968 | 0.05 | 1 | 1 | 34 | 7759 | 0 | 229.00 | 1 | 3 | 1 | 220,925.91 | 174 | . 5809 58 | 2.00 | 53.35 | 52.73 | 9.00 | 8 | 17.00 | 1977 | 0.44 | 1 | 1 | 23 | 5735 | 3 | 1,084.00 | 0 | 5 | 1 | 175,616.23 | 179 | . 10783 999 | 1.00 | 39.65 | 23.78 | 7.00 | 11 | 12.00 | 1976 | 0.01 | 1 | 1 | 35 | 5776 | 1 | 2,078.00 | 2 | 4 | 1 | 150,226.53 | 15 | . train_df[&#39;PriceOneRoom&#39;] = train_df[&#39;Price&#39;] / train_df[&#39;Rooms&#39;] . price_by_district = train_df.groupby([&#39;DistrictId&#39;], as_index=False) .agg({&#39;PriceOneRoom&#39;:&#39;median&#39;}) .rename(columns={&#39;PriceOneRoom&#39;:&#39;PriceOneRoomByDistrict&#39;}) price_by_district . DistrictId PriceOneRoomByDistrict . 0 999 | 129,339.27 | . 1 1 | 106,080.90 | . 2 11 | 100,098.98 | . 3 21 | 104,448.25 | . 4 23 | 76,232.42 | . 5 27 | 94,738.78 | . 6 30 | 93,960.73 | . 7 34 | 129,738.65 | . 8 44 | 125,925.66 | . 9 45 | 164,907.84 | . 10 46 | 117,867.67 | . 11 52 | 97,309.46 | . 12 53 | 138,669.20 | . 13 57 | 121,811.56 | . 14 58 | 105,638.52 | . 15 6 | 94,232.67 | . 16 61 | 119,679.38 | . 17 62 | 102,579.54 | . 18 74 | 127,646.56 | . 19 8 | 120,139.85 | . 20 9 | 96,749.26 | . train_df = train_df.merge(price_by_district, on=[&#39;DistrictId&#39;], how=&#39;left&#39;).set_index(train_df.index) test_df = test_df.merge(price_by_district, on=[&#39;DistrictId&#39;], how=&#39;left&#39;).set_index(test_df.index) . test_df.columns . Index([&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoomByDistrict&#39;], dtype=&#39;object&#39;) . train_df.columns . Index([&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoom&#39;, &#39;PriceOneRoomByDistrict&#39;], dtype=&#39;object&#39;) . train_df = train_df.drop([&#39;PriceOneRoom&#39;], axis=1) . train_df[&#39;RoomSquare&#39;] = train_df[&#39;Square&#39;] / train_df[&#39;Rooms&#39;] test_df[&#39;RoomSquare&#39;] = test_df[&#39;Square&#39;] / test_df[&#39;Rooms&#39;] . train_df.loc[train_df[&#39;Floor&#39;] &lt;= 5, &#39;Floor_cat&#39;] = 1 train_df.loc[(train_df[&#39;Floor&#39;] &gt; 5) &amp; (train_df[&#39;Floor&#39;] &lt;= 17), &#39;Floor_cat&#39;] = 2 train_df.loc[train_df[&#39;Floor&#39;] &gt; 17, &#39;Floor_cat&#39;] = 3 test_df.loc[test_df[&#39;Floor&#39;] &lt;= 5, &#39;Floor_cat&#39;] = 1 test_df.loc[(test_df[&#39;Floor&#39;] &gt; 5) &amp; (test_df[&#39;Floor&#39;] &lt;= 17), &#39;Floor_cat&#39;] = 2 test_df.loc[test_df[&#39;Floor&#39;] &gt; 17, &#39;Floor_cat&#39;] = 3 train_df[&#39;Floor_cat&#39;].value_counts() . 2.00 5232 1.00 4424 3.00 344 Name: Floor_cat, dtype: int64 . labels = [1, 2, 3] train_df[&#39;Floor_cat_qcut&#39;] = pd.qcut(train_df[&#39;Floor&#39;], q=3, labels=labels) test_df[&#39;Floor_cat_qcut&#39;] = pd.qcut(test_df[&#39;Floor&#39;], q=3, labels=labels) train_df[&#39;Floor_cat_qcut&#39;].value_counts() . 1 3521 2 3519 3 2960 Name: Floor_cat_qcut, dtype: int64 . train_df.loc[train_df[&#39;HouseFloor&#39;] &lt;= 5, &#39;HouseFloor_cat&#39;] = 1 train_df.loc[(train_df[&#39;HouseFloor&#39;] &gt; 5) &amp; (train_df[&#39;HouseFloor&#39;] &lt;= 17), &#39;HouseFloor_cat&#39;] = 2 train_df.loc[train_df[&#39;HouseFloor&#39;] &gt; 17, &#39;HouseFloor_cat&#39;] = 3 test_df.loc[test_df[&#39;HouseFloor&#39;] &lt;= 5, &#39;HouseFloor_cat&#39;] = 1 test_df.loc[(test_df[&#39;HouseFloor&#39;] &gt; 5) &amp; (test_df[&#39;HouseFloor&#39;] &lt;= 17), &#39;HouseFloor_cat&#39;] = 2 test_df.loc[test_df[&#39;HouseFloor&#39;] &gt; 17, &#39;HouseFloor_cat&#39;] = 3 train_df[&#39;HouseFloor_cat&#39;].value_counts() . 2.00 6838 1.00 1837 3.00 1325 Name: HouseFloor_cat, dtype: int64 . train_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]] = train_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]].astype(int) test_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]] = test_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]].astype(int) . 4. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; . 4.1. &#1054;&#1090;&#1073;&#1086;&#1088; &#1087;&#1088;&#1080;&#1079;&#1085;&#1072;&#1082;&#1086;&#1074; . feature_names = train_df.columns feature_names.tolist() . [&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoomByDistrict&#39;, &#39;RoomSquare&#39;, &#39;Floor_cat&#39;, &#39;Floor_cat_qcut&#39;, &#39;HouseFloor_cat&#39;] . target_names = [&#39;Price&#39;] feature_names = [&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, #&#39;Price&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoomByDistrict&#39;, &#39;RoomSquare&#39;, &#39;Floor_cat&#39;, &#39;Floor_cat_qcut&#39;, &#39;HouseFloor_cat&#39;] . X = train_df[feature_names] y = train_df[target_names] . forest = RandomForestRegressor(random_state=56) . forest_best = RandomForestRegressor(max_depth=15, max_features=5, random_state=56) forest_best.fit(X, y) y_pred = forest_best.predict(test_df[feature_names]) y_pred . array([162556.371278 , 231974.01815621, 218945.2288756 , ..., 328836.90951946, 198611.16589857, 176794.34957387]) . preds = pd.DataFrame() preds[&#39;Id&#39;] = test_df.index preds[&#39;Price&#39;] = y_pred preds.head() . Id Price . 0 725 | 162,556.37 | . 1 15856 | 231,974.02 | . 2 5480 | 218,945.23 | . 3 15664 | 343,927.38 | . 4 14275 | 144,108.09 | . preds.to_csv(&#39;Gladkikh_predictions_1.csv&#39;, index=False) .",
            "url": "https://zmey56.github.io/blog//course%20project/machine%20learning/classification/geekbrain/2021/10/10/course-project-regression-flat-price.html",
            "relUrl": "/course%20project/machine%20learning/classification/geekbrain/2021/10/10/course-project-regression-flat-price.html",
            "date": " • Oct 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Анализ ETF с использованием Python",
            "content": "Как использовать библиотеки Python, такие как Pandas, Matplotlib и Seaborn, для получения информации из ежедневных данных о ценах и объемах c фондового рынка. . С проникновением аналитики во многие сферы нашей жизни она не могла обойти стороной финансы. В этой статье рассмотрим ее применение для анализа ETF с целью их анализа, в том числе и с применением визуализиции. . 1. &#1054; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Для анализа будем использовать данные ETF с валютным хейджом: FXCN, FXRL, FXIT, FXUS и FXRU. Временной ряд рассмотрим за три года с 2018 по 2020 года. Само исследование проведем в Google Colaboratory. . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import pandas as pd import numpy as np import matplotlib.pyplot as plt from google.colab import files import warnings warnings.filterwarnings(&quot;ignore&quot;) . Сначало необходимо загрузить данные, которые представлены в формате CSV. . uploaded = files.upload() for fn in uploaded.keys(): print(&#39;User uploaded file «{name}» with length {length} bytes&#39;.format(name=fn, length=len(uploaded[fn]))) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving FXGD.csv to FXGD (1).csv Saving FXRU.csv to FXRU (1).csv Saving FXUS.csv to FXUS (1).csv Saving FXIT.csv to FXIT (1).csv Saving FXRL.csv to FXRL (1).csv Saving FXCN.csv to FXCN (1).csv User uploaded file «FXGD.csv» with length 53856 bytes User uploaded file «FXRU.csv» with length 32600 bytes User uploaded file «FXUS.csv» with length 56015 bytes User uploaded file «FXIT.csv» with length 56137 bytes User uploaded file «FXRL.csv» with length 55985 bytes User uploaded file «FXCN.csv» with length 56038 bytes . После этого прочтем данные с диска. Дальше необходимо создать два двадатафрейма - один с ценами закрытия, а другой с объемами торговли: . fxgd =pd.read_csv(&#39;/content/FXGD.csv&#39;) fxrl =pd.read_csv(&#39;/content/FXRL.csv&#39;) fxit =pd.read_csv(&#39;/content/FXIT.csv&#39;) fxus =pd.read_csv(&#39;/content/FXUS.csv&#39;) fxru =pd.read_csv(&#39;/content/FXRU.csv&#39;, sep=&#39;;&#39;) fxcn =pd.read_csv(&#39;/content/FXCN.csv&#39;) . def changeDF(df): df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) name =[x for x in globals() if globals()[x] is df][0] df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;HIGH&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) df = df.set_index([&#39;date&#39;]) df.columns = [name+&#39;_cl&#39;, name + &#39;_vol&#39;] return df . # df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) # name =[x for x in globals() if globals()[x] is df][0] # df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;CLOSE&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) # df = df.set_index([&#39;date&#39;]) # df.columns = [name] # return df . fxgd_change = changeDF(fxgd) fxrl_change = changeDF(fxrl) fxit_change = changeDF(fxit) fxus_change = changeDF(fxus) fxru_change = changeDF(fxru) fxcn_change = changeDF(fxcn) . etf = pd.concat([fxgd_change, fxrl_change, fxit_change, fxus_change, fxru_change, fxcn_change], axis=1) . etf.head() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . date . 2018-01-03 529.0 | 4340 | 1950.5 | 443 | 3612.0 | 581 | 2738.0 | 1049 | 641.0 | 139.0 | 2635.0 | 2098 | . 2018-01-04 527.0 | 1489 | 1992.0 | 659 | 3641.0 | 647 | 2745.0 | 586 | 639.0 | 128.0 | 2655.0 | 1331 | . 2018-01-05 526.0 | 1911 | 2004.5 | 846 | 3646.0 | 876 | 2744.0 | 322 | 637.0 | 306.0 | 2640.0 | 1664 | . 2018-01-09 525.5 | 5044 | 2024.0 | 2570 | 3673.0 | 1833 | 2766.0 | 653 | 638.0 | 448.0 | 2670.0 | 2304 | . 2018-01-10 527.5 | 9808 | 2030.0 | 765 | 3660.0 | 2485 | 2758.0 | 407 | 637.0 | 369.0 | 2665.0 | 1910 | . C FXRU пришлось немного поработать в EXCEL, так как скачанные данные прибивили лишний ноль к значению. По этому при загрузке пришлось указывать явный разделитель. . Дальше проверим наш датасет на предмет наличия значений NULL . print(etf.isnull().sum()) . fxgd_cl 0 fxgd_vol 0 fxrl_cl 0 fxrl_vol 0 fxit_cl 0 fxit_vol 0 fxus_cl 0 fxus_vol 0 fxru_cl 4 fxru_vol 4 fxcn_cl 0 fxcn_vol 0 dtype: int64 . Выбросим их, чтоб не мешали в дальнейшем расчете: . etf.dropna(inplace=True, axis=0) . Дальше имеет смысл посмотреть тип значений: . etf.dtypes . fxgd_cl float64 fxgd_vol int64 fxrl_cl float64 fxrl_vol int64 fxit_cl float64 fxit_vol int64 fxus_cl float64 fxus_vol int64 fxru_cl float64 fxru_vol float64 fxcn_cl float64 fxcn_vol int64 dtype: object . И посмотрим размер датасета: . etf.shape . (752, 12) . Так же дальше интересно посмотреть как вели себя ETF в последние полгода. Это можно сделать при помощи функции describe: . etf[-120:].describe() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . count 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | . mean 973.690000 | 148832.116667 | 3121.208333 | 8971.225000 | 8860.925000 | 14784.708333 | 4758.458333 | 12685.116667 | 950.133333 | 32841.958333 | 3878.233333 | 17515.733333 | . std 36.967338 | 94656.673543 | 169.290817 | 6816.481138 | 572.529639 | 8391.294562 | 267.491739 | 7730.347512 | 28.046367 | 14373.882822 | 219.697987 | 14471.016798 | . min 878.000000 | 34678.000000 | 2848.500000 | 2907.000000 | 7513.000000 | 4769.000000 | 4140.000000 | 4392.000000 | 880.800000 | 11069.000000 | 3422.000000 | 4448.000000 | . 25% 946.100000 | 87124.250000 | 2998.000000 | 5388.500000 | 8466.500000 | 9388.750000 | 4578.000000 | 8902.000000 | 935.900000 | 21355.000000 | 3721.000000 | 8495.250000 | . 50% 985.900000 | 127780.500000 | 3083.000000 | 7584.500000 | 9051.500000 | 12681.500000 | 4807.500000 | 11277.000000 | 951.450000 | 29360.000000 | 3898.000000 | 12329.500000 | . 75% 1001.650000 | 175438.250000 | 3219.500000 | 10754.250000 | 9306.250000 | 17640.000000 | 4982.000000 | 13963.750000 | 971.050000 | 42583.750000 | 4064.500000 | 22652.250000 | . max 1033.600000 | 666819.000000 | 3488.500000 | 67809.000000 | 9776.000000 | 63506.000000 | 5157.000000 | 73672.000000 | 1012.800000 | 91275.000000 | 4312.000000 | 101084.000000 | . pct_chg_etf[:50].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | . mean 0.018909 | 0.128734 | 0.141815 | 0.037679 | 0.010791 | 0.088726 | . std 0.634123 | 0.840398 | 1.222263 | 1.014099 | 0.544054 | 1.462912 | . min -1.291513 | -1.680871 | -4.318305 | -3.804348 | -1.550388 | -4.403670 | . 25% -0.381599 | -0.359217 | -0.432242 | -0.326851 | -0.312745 | -0.562852 | . 50% -0.094162 | 0.266967 | 0.175959 | 0.073884 | 0.000000 | 0.190041 | . 75% 0.379604 | 0.689444 | 0.847795 | 0.543863 | 0.319361 | 1.135292 | . max 1.826923 | 2.127660 | 3.065569 | 2.749529 | 1.107595 | 2.909091 | . В результате видно в каких пределах в последние полгода ETF провели большую часть аремени с вероятностью 75%. . После построим графики движения цены во времени. . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf.index, etf[&#39;fxgd_cl&#39;], &#39;tab:blue&#39; ) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf.index, etf[&#39;fxrl_cl&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf.index, etf[&#39;fxit_cl&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf.index, etf[&#39;fxus_cl&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf.index, etf[&#39;fxru_cl&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf.index, etf[&#39;fxcn_cl&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . Ежедневное процентное изменение цены etf вычисляется на основе процентного изменения между ценами закрытия 2 последовательных дней. Предположим, что цена закрытия вчера составляла 500 рублей, а сегодня она закрылась по 550 рублей. Таким образом, процентное изменение составляет 10%. т. е. ((550-500) / 500)*100. Здесь нет никакой тайны! . Далее, мы введем новый столбец, обозначающий дневную доходность в цене etf. Вычислить можно с помощью встроенной функции pct_change() в python. Так же немного переставлю колонки, чтоб визуально лучше воспринималось. . etf.columns . Index([&#39;fxgd_cl&#39;, &#39;fxgd_vol&#39;, &#39;fxrl_cl&#39;, &#39;fxrl_vol&#39;, &#39;fxit_cl&#39;, &#39;fxit_vol&#39;, &#39;fxus_cl&#39;, &#39;fxus_vol&#39;, &#39;fxru_cl&#39;, &#39;fxru_vol&#39;, &#39;fxcn_cl&#39;, &#39;fxcn_vol&#39;], dtype=&#39;object&#39;) . etf_cl = etf[[&#39;fxgd_cl&#39;, &#39;fxrl_cl&#39;, &#39;fxit_cl&#39;, &#39;fxus_cl&#39;, &#39;fxru_cl&#39;, &#39;fxcn_cl&#39;]] etf_cl_pct = etf_cl.pct_change()*100 etf_cl_pct.columns = [&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;] etf_vol = etf[[&#39;fxgd_vol&#39;, &#39;fxrl_vol&#39;, &#39;fxit_vol&#39;, &#39;fxus_vol&#39;, &#39;fxru_vol&#39;, &#39;fxcn_vol&#39;]] etf_new = pd.concat([etf_cl, etf_vol, etf_cl_pct], axis = 1) . etf_new.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-03 529.0 | 1950.5 | 3612.0 | 2738.0 | 641.0 | 2635.0 | 4340 | 443 | 581 | 1049 | 139.0 | 2098 | NaN | NaN | NaN | NaN | NaN | NaN | . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . etf_new = etf_new.dropna() . Представим изменение ежедневной доходности в виде графика во времени: . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf_new.index, etf_new[&#39;fxgd_cl_pct&#39;], &#39;tab:blue&#39;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf_new.index, etf_new[&#39;fxrl_cl_pct&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf_new.index, etf_new[&#39;fxit_cl_pct&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf_new.index, etf_new[&#39;fxus_cl_pct&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf_new.index, etf_new[&#39;fxru_cl_pct&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf_new.index, etf_new[&#39;fxcn_cl_pct&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . В течение большей части времени доходность составляет от -2% до 2% со скачками без пересечения отметки в 6% с обеих сторон. Наиболее шумной выглядит ETF FXCN. . Так же можно проверить новостные статьи за те дни, когда наблюдался резкий рост/падение цен на etf и понять чем было обусловлено. . Построим гистограмму распределения ежедневных доходов: . import seaborn as sns sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(15,15)) sns.histplot(data=etf_new[&#39;fxgd_cl_pct&#39;], kde=True, color=&quot;orange&quot;, ax=axs[0, 0]) axs[0,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxrl_cl_pct&#39;], kde=True, color=&quot;olive&quot;, ax=axs[0, 1]) axs[0,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxit_cl_pct&#39;], kde=True, color=&quot;gold&quot;, ax=axs[1, 0]) axs[1,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxus_cl_pct&#39;], kde=True, color=&quot;grey&quot;, ax=axs[1, 1]) axs[1,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxru_cl_pct&#39;], kde=True, color=&quot;teal&quot;, ax=axs[2, 0]) axs[2,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxcn_cl_pct&#39;], kde=True, color=&quot;brown&quot;, ax=axs[2, 1]) axs[2,1].set_xlim(-10,10) plt.show() . etf_new[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | . mean 0.084329 | 0.084240 | 0.140564 | 0.089850 | 0.056374 | 0.065255 | . std 1.081425 | 1.163047 | 1.398492 | 1.141144 | 0.771131 | 1.414819 | . min -5.709816 | -8.065290 | -6.874365 | -8.567335 | -5.198422 | -5.273973 | . 25% -0.456676 | -0.444714 | -0.574001 | -0.444633 | -0.346166 | -0.752409 | . 50% 0.030111 | 0.126835 | 0.206940 | 0.137979 | 0.026178 | 0.128783 | . 75% 0.622500 | 0.718721 | 0.887283 | 0.645403 | 0.445645 | 0.899653 | . max 5.619982 | 7.784431 | 8.297990 | 6.079599 | 4.604008 | 6.554307 | . Гистограммы ежедневных доходностей центрированы вокруг среднего значения, которое для всех etf было больше нуля и говорит о положительном тренде. Видно, что доходность для всех ETF большую часть времени лежала в пределах от -2,5 до 2,5%. Наибольшую доходность показали - FXIT, а наименьшую - FXRU. . 2. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1090;&#1088;&#1077;&#1085;&#1076;&#1072; . Затем мы добавляем новый столбец &quot;Тренд&quot;, значения которого основаны на ежедневном процентном изменении, которое мы рассчитали выше. Тенденция определяется отношением снизу. Скопируем датасет в новый, с которым и продолжим работу. . def trend(x): if x &gt; -0.5 and x &lt;= 0.5: return &#39;Практически или без изменений&#39; elif x &gt; 0.5 and x &lt;= 1.5: return &#39;Небольшой позитив&#39; elif x &gt; -1.5 and x &lt;= -0.5: return &#39;Небольшой негатив&#39; elif x &gt; 1.5 and x &lt;= 2.5: return &#39;Позитив&#39; elif x &gt; -2.5 and x &lt;= -1.5: return &#39;Негатив&#39; elif x &gt; 2.5 and x &lt;= 5: return &#39;Значительный позитив&#39; elif x &gt; -5 and x &lt;= -2.5: return &#39;Значительный негатив&#39; elif x &gt; 5: return &#39;Максимальный позитив&#39; elif x &lt;= -5: return &#39;Максимальный негатив&#39; . etf_trend = etf_new.copy() . etf_trend.columns[12:] . Index([&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;], dtype=&#39;object&#39;) . for stock in etf_trend.columns[12:]: etf_trend[&quot;Trend_&quot; + str(stock)] = np.zeros(etf_trend[stock].count()) etf_trend[&quot;Trend_&quot;+ str(stock)] = etf_trend[stock].apply(lambda x:trend(x)) . etf_trend.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct Trend_fxgd_cl_pct Trend_fxrl_cl_pct Trend_fxit_cl_pct Trend_fxus_cl_pct Trend_fxru_cl_pct Trend_fxcn_cl_pct . date . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | Практически или без изменений | Позитив | Небольшой позитив | Практически или без изменений | Практически или без изменений | Небольшой позитив | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | Практически или без изменений | Небольшой позитив | Небольшой позитив | Небольшой позитив | Практически или без изменений | Небольшой позитив | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | . 2018-01-11 526.0 | 2042.0 | 3673.0 | 2755.0 | 635.0 | 2650.0 | 5548 | 1220 | 1282 | 968 | 326.0 | 1722 | -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts() . Практически или без изменений 351 Небольшой позитив 166 Небольшой негатив 141 Позитив 44 Негатив 25 Значительный позитив 13 Значительный негатив 7 Максимальный негатив 2 Максимальный позитив 2 Name: Trend_fxgd_cl_pct, dtype: int64 . Дальше можно взглянуть как вели себя акции акцииETF в последние 3 года. Для этого их изменения можно визуализировать при помощи круговых диаграмм, где каждый сектор представляет процент дней, в течение которых происходил каждый тренд. Для построения будем использовать функцию groupby() со столбцом тренда. . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(20,17)) axs[0, 0].pie(etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].pie(etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].pie(etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].pie(etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].pie(etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].pie(etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 1].set_title(&#39;FXCN&#39;) plt.show() . За рассматриваемый период с 2018 года по 2020 года большую часть времени ETF практически не изменялись, или изменялись незначительно при заданных параметрах. Так же важно отметить, что при небольших изменениях они как правило были позитивными. При более больших - это соотношение сохранялось кроме FXRU. . 3. &#1045;&#1078;&#1077;&#1076;&#1085;&#1077;&#1074;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1080; &#1086;&#1073;&#1098;&#1077;&#1084;&#1099; . Следующим шагом продолжим работу с объемами: . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) axs[0].stem(etf_trend.index[-253:], etf_trend[&#39;fxgd_cl_pct&#39;][-253:]) axs[0].plot((etf_trend[&#39;fxgd_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[0].set_title(&#39;FXGD&#39;) axs[1].stem(etf_trend.index[-253:], etf_trend[&#39;fxrl_cl_pct&#39;][-253:]) axs[1].plot((etf_trend[&#39;fxrl_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[1].set_title(&#39;FXRL&#39;) axs[2].stem(etf_trend.index[-253:], etf_trend[&#39;fxit_cl_pct&#39;][-253:]) axs[2].plot((etf_trend[&#39;fxit_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[2].set_title(&#39;FXIT&#39;) axs[3].stem(etf_trend.index[-253:], etf_trend[&#39;fxus_cl_pct&#39;][-253:]) axs[3].plot((etf_trend[&#39;fxus_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[3].set_title(&#39;FXUS&#39;) axs[4].stem(etf_trend.index[-253:], etf_trend[&#39;fxru_cl_pct&#39;][-253:]) axs[4].plot((etf_trend[&#39;fxru_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[4].set_title(&#39;FXRU&#39;) axs[5].stem(etf_trend.index[-253:], etf_trend[&#39;fxcn_cl_pct&#39;][-253:]) axs[5].plot((etf_trend[&#39;fxcn_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[5].set_title(&#39;FXCN&#39;) . Text(0.5, 1.0, &#39;FXCN&#39;) . Сопоставляя ежедневный объем торговли(зеленым цветом) с ежедневной доходностью(синим цветом), было отмечено, что часто для ETF характерно, что когда объем торгов высок, наблюдается сравнительно высокий рост или падение цены. Объем торгов ETF в сочетании с ростом или падениемы на данный инструмент является показателем доверия трейдеров и инвесторов к конкретному ETF. . 4. &#1050;&#1086;&#1088;&#1088;&#1077;&#1083;&#1103;&#1094;&#1080;&#1086;&#1085;&#1085;&#1099;&#1081; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079; ETF . Основное правило диверсификации - не клади все яйца в одну корзинку. По этому если мы решили собирать портфель из ETF, то они не должны быть сильно взаимосвязаны друг с другом. Математическим языком - коэффициент корреляции Пирсона между любой парой должен быть близок к 0. Смысл - они не должны падать синхронно, чтоб инвестиции не превратились в 0. . Проанализировать корреляцию между различными ETF можно с помощью парной диаграммы Seaborn. Для удобства оставим только процентные изменения за день в отдельном новом датафрейме. . pct_chg_etf = etf_new[etf_new.columns[12:]] . sns.set(style = &#39;ticks&#39;, font_scale = 1.25) sns.pairplot(pct_chg_etf) plt.show() . pct_chg_etf.corr() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . fxgd_cl_pct 1.000000 | -0.236301 | 0.107085 | 0.119773 | 0.590029 | 0.139992 | . fxrl_cl_pct -0.236301 | 1.000000 | 0.335061 | 0.300352 | -0.384120 | 0.232063 | . fxit_cl_pct 0.107085 | 0.335061 | 1.000000 | 0.895261 | 0.138016 | 0.641551 | . fxus_cl_pct 0.119773 | 0.300352 | 0.895261 | 1.000000 | 0.202682 | 0.610576 | . fxru_cl_pct 0.590029 | -0.384120 | 0.138016 | 0.202682 | 1.000000 | 0.197225 | . fxcn_cl_pct 0.139992 | 0.232063 | 0.641551 | 0.610576 | 0.197225 | 1.000000 | . На графике визуально можно увидеть наличие корреляции между различными ETF. Обратите внимание, что корреляционный анализ выполняется для ежедневного процентного изменения(дневной доходности) цены ETF, а не для их цены. . Из полученных графиков ясно видно, что следующие FXIT и FXUS не следует класть в одну корзину, так как между нми наблюдается сильная зависимость. Остальные могут быть включены в портфель, поскольку ни одна из двух оставшихся ETF не демонстрирует какой-либо существенной корреляции. . Но у визуального анализа есть существенный недостаток - он не предоставляет подробной информации о количественной оценки взаимосвязи, таких как значение R Пирсона и p нулевой гипотезы. В связи с чем при визуальном анализе остается под вопросом FXCN - есть ли у данного ETF сильная взаимосвязь с FXUS или нет. . Один из способов решения данного вопроса - построение графиков seaborn.jointplot с подробной информацией по значению R Пирсона (коэффициент корреляции Пирсона) для каждой пары ETF. Значение R Пирсона колеблется от -1 до 1. Отрицательное значение указывает на отрицательную линейную связь, в то время как положительное значение указывает на положительную связь. Значение R Пирсона ближе к 1 (или -1) указывает на сильную корреляцию, в то время как значение ближе к 0 указывает на слабую корреляцию. . Так же чем интересны данные графики - построение гистограмм распределения по краям, а так же значение p-value. . Но если рассматривать все пары, то нам потребуется большое количество графиков. По этому остановимся только на тех, которые вызывают сомнения: . pct_chg_etf.head() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-04 -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . 2018-01-11 -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | . from scipy.stats import stats a_1 = pct_chg_etf.fxit_cl_pct b_1 = pct_chg_etf.fxus_cl_pct b_2 = pct_chg_etf.fxcn_cl_pct g_1 = sns.jointplot(&#39;fxit_cl_pct&#39;, &#39;fxcn_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_1, p_1 = stats.pearsonr(a_1, b_1) g_1.ax_joint.annotate(f&#39;$ rho = {r_1:.3f}, p = {p_1:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_1.ax_joint.scatter(a_1, b_1) g_1.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxus&#39;, size=15) g_2 = sns.jointplot(&#39;fxus_cl_pct&#39;, &#39;fxit_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_2, p_2 = stats.pearsonr(a_1, b_2) g_2.ax_joint.annotate(f&#39;$ rho = {r_2:.3f}, p = {p_2:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_2.ax_joint.scatter(a_1, b_2) g_2.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxcn&#39;, size=15) plt.tight_layout() plt.show() . Первый гррафик подтвердил наличие сильной взаимосвязи между FXIT и FXUS, что говорит о нежелательности их брать в один портфель. В свою очередь корреляция между FXCN и FXIT оказалась ниже 0,7, что говорит о возможности совместного нахождения в одной корзине. . 5. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1074;&#1086;&#1083;&#1072;&#1090;&#1080;&#1083;&#1100;&#1085;&#1086;&#1089;&#1090;&#1080; . Волатильность-один из важнейших показателей на финансовых рынках. Говорят, что ценная бумага обладает высокой волатильностью, если ее стоимость может резко измениться за короткий промежуток времени. С другой стороны, более низкая волатильность означает, что стоимость имеет тенденцию быть относительно стабильной в течение определенного периода времени. Эти изменения обусловлены несколькими факторами, включая спрос и предложение, настроения, жадность, страх и т.д. Математически волатильность измеряется с помощью статистической меры, называемой &quot;стандартным отклонением&quot;, которая измеряет отклонение актива от его средней стоимости. . Произведем рассчет 5-дневной скользящей средней дневной доходности и стандартного отклонения. После этого построим график. Все это можно выполнить при помощи функций Pandas rolling() и std(). . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) for i, etf in enumerate(pct_chg_etf.columns): axs[i].plot(pct_chg_etf[etf].rolling(5).std()*np.sqrt(5)) axs[i].plot(pct_chg_etf[etf].rolling(7).mean()) axs[i].set_title(etf[:4], size=20) . volatility = pct_chg_etf[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;,&#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].rolling(5).std()*np.sqrt(5) . volatility[:150].plot(linewidth=4, figsize = (35, 15)) plt.legend(loc=2, prop={&#39;size&#39;: 16}) . &lt;matplotlib.legend.Legend at 0x7f6bad8d31d0&gt; . Как результат вы можете заметить, что наиболее сильная низкая волатильность характерна для ETF на российские акции - FXRU. Многие трейдеры и инвесторы ищут инвестиции с более высокой волатильностью, чтобы получать более высокую прибыль. Если финансовый инструмент не движется, он не только обладает низкой волатильностью, но и имеет низкий потенциал прибыли. С другой стороны, ценные бумаги с очень высоким уровнем волатильности могут иметь огромный потенциал прибыли, но риск также высок. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/09/18/data-analysis-visualization-finance.html",
            "relUrl": "/finance/investment/python/2021/09/18/data-analysis-visualization-finance.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Курсовой проект "База данных на MySQL для рынка финансовых инструментов"",
            "content": "Согласно представленному заданию мне было необходимо придумать тему и по ней создать базу данных.Вот общее описание проекта: . Требования к курсовому проекту: . Составить общее текстовое описание БД и решаемых ею задач; | минимальное количество таблиц - 10; | скрипты создания структуры БД (с первичными ключами, индексами, внешними ключами); | создать ERDiagram для БД; | скрипты наполнения БД данными; | скрипты характерных выборок (включающие группировки, JOIN’ы, вложенные таблицы); | представления (минимум 2); | хранимые процедуры / триггеры; | Примеры: описать модель хранения данных популярного веб-сайта: кинопоиск, booking.com, wikipedia, интернет-магазин, geekbrains, госуслуги… . За основу я взял статью, размещенную на иностранном источнике. В ней показана основная схема. Осталось только ее перевести в SQL. Прежде перевод теории. . Торговля криптовалютами, покупка акций и тому подобное в наши дни чрезвычайно популярны, так как это воспринимается как легкая прибыль. В настоящее время цены растут, но мы не можем знать, когда это изменится. С другой стороны, мы знаем, что в какой-то момент это произойдет. Но мы здесь не для того, чтобы делать финансовые прогнозы. Вместо этого мы поговорим о модели данных, которую можно использовать для поддержки торговли криптовалютами и финансовыми инструментами, такими как акции или акции фондов. . 1. Общее текстовое описание базы данных и решаемых задач . Что Вам нужно знать О Торговле Валютами и Акциями . Технологические усовершенствования за последние несколько десятилетий оказали значительное влияние на торговлю. В настоящее время существует множество онлайн-торговых платформ, которые вы можете использовать. Большая часть сегодняшней торговли осуществляется виртуально – вы можете увидеть бумажные акции в музеях, но вряд ли вы увидите акции, которые вы покупаете в бумажной форме. И вам не нужно паковать чемоданы и отправляться на Уолл-стрит или любую другую фондовую биржу, чтобы совершить сделку. Не выходя из своего компьютера или мобильного устройства, вы можете покупать или продавать производные финансовые инструменты (такие как облигации, акции или товары). . Большинство сделок (продажа производных финансовых инструментов) следуют тем же правилам. Есть продавцы и покупатели. Если они договорятся о цене, сделка состоится. После сделки цена этого производного финансового инструмента будет пересчитана, и процесс продолжится с новыми трейдерами. Акции и другие производные финансовые инструменты работают точно так же. . Что такое криптовалюта? Вы, наверное, слышали о биткойне и других криптовалютах. Но что это такое? Криптовалюты похожи на виртуальные валюты, но они не привязаны к валютам реального мира (таким как евро или доллары). Вместо этого пользователи могут торговать криптовалютами между собой, как токенами. Затем они могут договориться о продаже, которая превратит их токены в реальные деньги. Эти продажи функционируют точно так же, как описанные выше сделки с акциями и акциями. . Эта тема сложна, и в нашей модели может быть много деталей (например, записи документов и транзакций). Я собираюсь сделать это просто; я не буду реализовывать какую-либо автоматическую торговлю или какие-либо формулы для создания новых цен после торгового события. . Перейдем к коду. Согласно статье база состоит из трех блоков: . CURRENCIES | TRADERS | ITEMS | . Теперь напишем код ля каждого блока: . 2. Cкрипты создания структуры БД . CURRENCIES . CREATE DATABASE IF NOT EXISTS coursework_portfolio; USE coursework_portfolio; . Сначало создаю страну из которой осуществляется торговля . DROP TABLE IF EXISTS country; CREATE TABLE country( id SERIAL PRIMARY KEY, country VARCHAR(128) ) COMMENT = &#39;Страна из которой осуществляется торговля&#39;; . Валюту которую использует пользователь для торговли . DROP TABLE IF EXISTS currency_used; CREATE TABLE currency_used( id SERIAL PRIMARY KEY, country_id BIGINT UNSIGNED NOT NULL COMMENT &#39;Ключ на другую таблицу&#39;, currency_id INT UNSIGNED COMMENT &#39;Ключ на другую таблицу&#39;, data_from DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Дата начала использования валюты&#39;, data_to DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Дата окончания использования валюты. Если NULL, то валюта до сих пор используется&#39; ) COMMENT = &#39;Валюта использованная для покупки&#39;; DESC currency_used ; . Хранятся текущие и исторические курсы между валютными парами. . DROP TABLE IF EXISTS currency_rate; CREATE TABLE currency_rate( id SERIAL PRIMARY KEY, currency_id INT UNSIGNED COMMENT &#39;Ключ на другую таблицу&#39;, base_currency_id INT UNSIGNED COMMENT &#39;Ключ на другую таблицу&#39;, rate DECIMAL(16,6) NOT NULL DEFAULT 0 COMMENT &#39;Курс валюты&#39;, ts DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Время в которое данный курс был зафиксирован&#39; ) COMMENT = &#39;Курс валюты&#39;; . Хранить все валюты, которые мы когда-либо использовали для торговли. . DROP TABLE IF EXISTS currency; CREATE TABLE currency( id INT UNSIGNED PRIMARY KEY, code VARCHAR(8) NOT NULL UNIQUE COMMENT &#39;Код используемый для уникального обозначения валюты&#39;, name VARCHAR(128) NOT NULL UNIQUE COMMENT &#39;Уникальное название этой валюты&#39;, is_active BOOL DEFAULT FALSE COMMENT &#39;Если валюта в настоящее время активна в нашей системе&#39;, is_base_currency BOOL DEFAULT FALSE COMMENT &#39;Если эта валюта является базовой валютой нашей системы.&#39; ) COMMENT = &#39;Курс валюты&#39;; . ITEMS . Таблицы item определяют все товары, доступные для торговли, и их текущий статус. Также здесь записываются все изменения, произошедшие с этими товарами с течением времени. . DROP TABLE IF EXISTS item; CREATE TABLE item( id SERIAL PRIMARY KEY, code VARCHAR(64) NOT NULL UNIQUE COMMENT &#39;Код используемый для уникального обозначения товара(акции, ПИФы и т.д.)&#39;, name VARCHAR(255) NOT NULL UNIQUE COMMENT &#39;Полное имя&#39;, is_active BOOL DEFAULT FALSE COMMENT &#39;Доступен ли этот товар для торговли или нет&#39;, currency_id INT UNSIGNED COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, details TEXT COMMENT &#39;Все дополнительные сведения (например, количество выпущенных акций) в текстовом формате.&#39; ) COMMENT = &#39;Доступные товары&#39;; . Таблица цен отслеживает все изменения цен во времени. . DROP TABLE IF EXISTS price; CREATE TABLE price( id SERIAL PRIMARY KEY, item_id BIGINT UNSIGNED COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, currency_id INT UNSIGNED COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, buy DECIMAL(16,6) NOT NULL DEFAULT 0 COMMENT &#39;Курс покупки&#39;, sell DECIMAL(16,6) NOT NULL DEFAULT 0 COMMENT &#39;Курс продажи&#39;, ts DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Время в которое сделка по последней цене была зафиксирована&#39; ) COMMENT = &#39;Изменение цены&#39;; . Таблица отчета . DROP TABLE IF EXISTS report; CREATE TABLE report( id SERIAL PRIMARY KEY, trading_data DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Дата отчета&#39;, item_id BIGINT UNSIGNED COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, currency_id INT UNSIGNED COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, first_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Начальная цена&#39;, last_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Последняя цена&#39;, min_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Минимальная цена&#39;, max_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Максимальная цена&#39;, avg_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Средняя цена&#39;, total_amount DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Общая сумма, уплаченная за этот товар в течение отчетного периода.&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Количество товаров, проданных в течение данного отчетного периода.&#39; ) COMMENT = &#39;Отчет&#39;; . TRADERS . Таблица трейдеров . DROP TABLE IF EXISTS trader; CREATE TABLE trader ( id SERIAL PRIMARY KEY, firstname VARCHAR(50) COMMENT &#39;Имя&#39;, lastname VARCHAR(50) COMMENT &#39;Фамилия&#39;, user_name VARCHAR(50) NOT NULL UNIQUE COMMENT &#39;Логин у всех уникальный&#39;, email VARCHAR(120) NOT NULL UNIQUE, confirmation_code VARCHAR(120) NOT NULL COMMENT &#39;Код, отправленный пользователю для завершения процесса регистрации.&#39;, time_registered DATETIME DEFAULT CURRENT_TIMESTAMP, time_confirmed DATETIME DEFAULT CURRENT_TIMESTAMP, country_id BIGINT UNSIGNED COMMENT &#39;Страна, в которой живет.&#39;, preffered_currency_id BIGINT UNSIGNED COMMENT &#39;Валюта, которую трейдер предпочитает&#39; ) COMMENT &#39;юзеры&#39;; . Список всех товаров, которыми в настоящее время владеет трейдер . DROP TABLE IF EXISTS current_inventory; CREATE TABLE current_inventory ( id SERIAL PRIMARY KEY, trader_id BIGINT UNSIGNED COMMENT &#39;Ссылка на трейдера&#39;, item_id BIGINT UNSIGNED COMMENT &#39;Ссылка на товар&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Количество товаров&#39; ) COMMENT &#39;Список товаров&#39;; . Торговое событие . DROP TABLE IF EXISTS trade; CREATE TABLE trade ( id SERIAL PRIMARY KEY, item_id BIGINT UNSIGNED COMMENT &#39;Ссылка на товар&#39;, seller_id BIGINT UNSIGNED DEFAULT NULL COMMENT &#39;Ссылка на трейдера&#39;, buyer_id BIGINT UNSIGNED COMMENT &#39;Ссылка на трейдера&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Количество товаров&#39;, unit_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Цена за единицу&#39;, description TEXT COMMENT &#39;Все дополнительные сведения (например, количество выпущенных акций) в текстовом формате.&#39;, offer_id BIGINT UNSIGNED COMMENT &#39;Индификатор сделки&#39; ) COMMENT &#39;Сделки&#39;; . Учет всех предложений . DROP TABLE IF EXISTS offer; CREATE TABLE offer ( id SERIAL PRIMARY KEY, item_id BIGINT UNSIGNED COMMENT &#39;Ссылка на товар&#39;, trader_id BIGINT UNSIGNED DEFAULT NULL COMMENT &#39;Ссылка на трейдера&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Количество товаров&#39;, buy BOOL DEFAULT FALSE, sell BOOL DEFAULT FALSE, price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Желаемая цена за единицу&#39;, ts DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Когда была выставлена&#39;, is_active BOOL DEFAULT FALSE COMMENT &#39;Действует ли еще это предложение&#39; ) COMMENT &#39;Сделки&#39;; . так же решил добавить в traders колонку дней рождений birthday . ALTER TABLE trader DROP COLUMN birthday; ALTER TABLE trader ADD COLUMN birthday DATE ; . 3. Скрипты создания отношений . USE coursework_portfolio; . связь один ко многим courtry - currency_used . ALTER TABLE currency_used ADD CONSTRAINT currency_used_country_fk FOREIGN KEY(country_id) REFERENCES country(id); . связь один ко многим currency - currency_used . ALTER TABLE currency_used ADD CONSTRAINT currency_used_currency_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . связь один ко многим currency - currency_rate . ALTER TABLE currency_rate ADD CONSTRAINT currency_currency_rate_fk FOREIGN KEY(currency_id) REFERENCES currency(id); ALTER TABLE currency_rate ADD CONSTRAINT currency_currency_rate_base_fk FOREIGN KEY(base_currency_id) REFERENCES currency(id); . связь один ко многим currency - price . ALTER TABLE price ADD CONSTRAINT currency_price_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . связь один ко многим currency - item . ALTER TABLE item ADD CONSTRAINT currency_item_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . связь один ко многим price - item . ALTER TABLE price ADD CONSTRAINT item_price_fk FOREIGN KEY(item_id) REFERENCES item(id); . связь один ко многим report - item . ALTER TABLE report ADD CONSTRAINT report_item_fk FOREIGN KEY(item_id) REFERENCES item(id); . связь один ко многим report - currency . ALTER TABLE report ADD CONSTRAINT currency_report_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . связь один ко многим trader - currency . ALTER TABLE trader ADD CONSTRAINT country_trader_fk FOREIGN KEY(country_id) REFERENCES country(id); . связь один ко многим trade - trader . ALTER TABLE trade ADD CONSTRAINT trader_trade_sell_fk FOREIGN KEY(seller_id) REFERENCES trader(id); . связь один ко многим trade - trader . ALTER TABLE trade ADD CONSTRAINT trader_trade_buy_fk FOREIGN KEY(buyer_id) REFERENCES trader(id); . связь один ко многим trade - offer . ALTER TABLE trade ADD CONSTRAINT offer_trade_fk FOREIGN KEY(offer_id) REFERENCES offer(id); . связь один ко многим trade - item . ALTER TABLE trade ADD CONSTRAINT item_trade_fk FOREIGN KEY(item_id) REFERENCES item(id); . связь один ко многим offer - trader . ALTER TABLE offer ADD CONSTRAINT trader_offer_fk FOREIGN KEY(trader_id) REFERENCES trader(id); . связь один ко многим offer - item . ALTER TABLE offer ADD CONSTRAINT item_offer_fk FOREIGN KEY(item_id) REFERENCES item(id); . связь один ко многим Current_inventory - trader . ALTER TABLE current_inventory ADD CONSTRAINT trader_current_inventory_fk FOREIGN KEY(trader_id) REFERENCES trader(id); . связь один ко многим Current_inventory - item . ALTER TABLE current_inventory ADD CONSTRAINT item_current_inventory_fk FOREIGN KEY(item_id) REFERENCES item(id); . 4. Создание ERDiagram для БД . В результате получилось следующая схема: . 5. Cкрипты наполнения БД данными . Наполнение сделал через популярный сайт для генерации фейковых данных Dummy Data for MYSQL Database . CREATE DATABASE coursework_portfolio; -- MariaDB dump 10.17 Distrib 10.4.15-MariaDB, for Linux (x86_64) -- -- Host: mysql.hostinger.ro Database: u574849695_20 -- -- Server version 10.4.15-MariaDB-cll-lve /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE=&#39;+00:00&#39; */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&#39;NO_AUTO_VALUE_ON_ZERO&#39; */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `country` -- USE coursework_portfolio; DROP TABLE IF EXISTS `country`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `country` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `country` varchar(128) COLLATE utf8mb4_unicode_ci DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Страна из которой осуществляется торговля&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `country` -- LOCK TABLES `country` WRITE; /*!40000 ALTER TABLE `country` DISABLE KEYS */; INSERT INTO `country` VALUES (1,&#39;Bermuda&#39;),(2,&#39;Christmas Island&#39;),(3,&#39;Holy See (Vatican City State)&#39;),(4,&#39;Macedonia&#39;),(5,&#39;Qatar&#39;),(6,&#39;Tokelau&#39;),(7,&#39;Romania&#39;),(8,&#39;Central African Republic&#39;),(9,&#39;Uganda&#39;),(10,&#39;Saint Pierre and Miquelon&#39;); /*!40000 ALTER TABLE `country` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `currency` -- DROP TABLE IF EXISTS `currency`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `currency` ( `id` int(10) unsigned NOT NULL, `code` varchar(8) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Код используемый для уникального обозначения валюты&#39;, `name` varchar(128) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Уникальное название этой валюты&#39;, `is_active` tinyint(1) DEFAULT 0 COMMENT &#39;Если валюта в настоящее время активна в нашей системе&#39;, `is_base_currency` tinyint(1) DEFAULT 0 COMMENT &#39;Если эта валюта является базовой валютой нашей системы.&#39;, PRIMARY KEY (`id`), UNIQUE KEY `code` (`code`), UNIQUE KEY `name` (`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Курс валюты&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `currency` -- LOCK TABLES `currency` WRITE; /*!40000 ALTER TABLE `currency` DISABLE KEYS */; INSERT INTO `currency` VALUES (0,&#39;htxs&#39;,&#39;eaque&#39;,0,0),(1,&#39;nnlp&#39;,&#39;soluta&#39;,1,0),(2,&#39;dtlh&#39;,&#39;quis&#39;,0,0),(3,&#39;wimu&#39;,&#39;animi&#39;,0,0),(4,&#39;lnag&#39;,&#39;earum&#39;,0,0),(5,&#39;ekbd&#39;,&#39;molestiae&#39;,1,0),(6,&#39;nivw&#39;,&#39;nam&#39;,1,0),(7,&#39;zldi&#39;,&#39;consequuntur&#39;,1,1),(8,&#39;zyrs&#39;,&#39;qui&#39;,1,1),(9,&#39;uvfn&#39;,&#39;blanditiis&#39;,1,1); /*!40000 ALTER TABLE `currency` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `currency_rate` -- DROP TABLE IF EXISTS `currency_rate`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `currency_rate` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Ключ на другую таблицу&#39;, `base_currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Ключ на другую таблицу&#39;, `rate` decimal(16,6) NOT NULL DEFAULT 0.000000 COMMENT &#39;Курс валюты&#39;, `ts` datetime DEFAULT current_timestamp() COMMENT &#39;Время в которое данный курс был зафиксирован&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Курс валюты&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `currency_rate` -- LOCK TABLES `currency_rate` WRITE; /*!40000 ALTER TABLE `currency_rate` DISABLE KEYS */; INSERT INTO `currency_rate` VALUES (1,0,0,248964.149700,&#39;1992-06-09 14:41:36&#39;),(2,1,1,390368.888800,&#39;2017-09-18 05:43:25&#39;),(3,2,2,0.000000,&#39;2001-11-16 22:01:24&#39;),(4,3,3,50.506542,&#39;1979-06-15 23:49:48&#39;),(5,4,4,81.498779,&#39;1984-06-23 02:36:52&#39;),(6,5,5,584941.290982,&#39;1970-04-04 16:47:28&#39;),(7,6,6,6265204.580000,&#39;1979-09-29 09:25:05&#39;),(8,7,7,16.154948,&#39;2010-12-21 00:43:41&#39;),(9,8,8,8.630000,&#39;1986-04-16 05:48:55&#39;),(10,9,9,2.800000,&#39;1984-10-06 10:07:42&#39;); /*!40000 ALTER TABLE `currency_rate` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `currency_used` -- DROP TABLE IF EXISTS `currency_used`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `currency_used` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `country_id` bigint(20) unsigned NOT NULL COMMENT &#39;Ключ на другую таблицу&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Ключ на другую таблицу&#39;, `data_from` datetime DEFAULT current_timestamp() COMMENT &#39;Дата начала использования валюты&#39;, `data_to` datetime DEFAULT current_timestamp() COMMENT &#39;Дата окончания использования валюты. Если NULL, то валюта до сих пор используется&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Валюта использованная для покупки&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `currency_used` -- LOCK TABLES `currency_used` WRITE; /*!40000 ALTER TABLE `currency_used` DISABLE KEYS */; INSERT INTO `currency_used` VALUES (1,1,0,&#39;1972-07-14 21:53:10&#39;,&#39;1989-06-25 02:18:23&#39;),(2,2,1,&#39;1970-03-19 12:37:26&#39;,&#39;2018-12-11 16:34:23&#39;),(3,3,2,&#39;1989-06-15 13:50:05&#39;,&#39;1985-06-10 17:07:32&#39;),(4,4,3,&#39;2008-09-26 16:36:58&#39;,&#39;1992-01-21 09:35:20&#39;),(5,5,4,&#39;1984-09-02 16:32:24&#39;,&#39;2019-07-25 03:33:31&#39;),(6,6,5,&#39;2018-10-02 04:32:51&#39;,&#39;1973-08-23 18:43:00&#39;),(7,7,6,&#39;2004-07-04 22:17:20&#39;,&#39;1990-11-19 16:48:34&#39;),(8,8,7,&#39;2007-06-25 10:34:33&#39;,&#39;1997-09-16 08:51:40&#39;),(9,9,8,&#39;1977-10-01 11:13:18&#39;,&#39;1985-03-05 19:28:04&#39;),(10,10,9,&#39;1987-03-28 10:28:24&#39;,&#39;2003-02-05 18:20:56&#39;); /*!40000 ALTER TABLE `currency_used` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `current_inventory` -- DROP TABLE IF EXISTS `current_inventory`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `current_inventory` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `trader_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на трейдера&#39;, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на товар&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Количество товаров&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Список товаров&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `current_inventory` -- LOCK TABLES `current_inventory` WRITE; /*!40000 ALTER TABLE `current_inventory` DISABLE KEYS */; INSERT INTO `current_inventory` VALUES (1,1,1,8421.000000),(2,2,2,969106.000000),(3,3,3,9235206.000000),(4,4,4,92961.000000),(5,5,5,145700.000000),(6,6,6,0.000000),(7,7,7,19133.000000),(8,8,8,0.000000),(9,9,9,26582.000000),(10,10,10,78483052.000000); /*!40000 ALTER TABLE `current_inventory` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `item` -- DROP TABLE IF EXISTS `item`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `item` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `code` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Код используемый для уникального обозначения товара(акции, ПИФы и т.д.)&#39;, `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Полное имя&#39;, `is_active` tinyint(1) DEFAULT 0 COMMENT &#39;Доступен ли этот товар для торговли или нет&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, `details` text COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;Все дополнительные сведения (например, количество выпущенных акций) в текстовом формате.&#39;, PRIMARY KEY (`id`), UNIQUE KEY `code` (`code`), UNIQUE KEY `name` (`name`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Доступные товары&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `item` -- LOCK TABLES `item` WRITE; /*!40000 ALTER TABLE `item` DISABLE KEYS */; INSERT INTO `item` VALUES (1,&#39;kzgb&#39;,&#39;doloremque&#39;,1,0,NULL),(2,&#39;krcn&#39;,&#39;omnis&#39;,1,1,NULL),(3,&#39;ywfp&#39;,&#39;rerum&#39;,0,2,NULL),(4,&#39;bbjq&#39;,&#39;eaque&#39;,0,3,NULL),(5,&#39;hsib&#39;,&#39;quis&#39;,1,4,NULL),(6,&#39;dnuf&#39;,&#39;quia&#39;,1,5,NULL),(7,&#39;wnfb&#39;,&#39;numquam&#39;,0,6,NULL),(8,&#39;pefi&#39;,&#39;quos&#39;,0,7,NULL),(9,&#39;vbrv&#39;,&#39;expedita&#39;,1,8,NULL),(10,&#39;afem&#39;,&#39;esse&#39;,0,9,NULL); /*!40000 ALTER TABLE `item` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `offer` -- DROP TABLE IF EXISTS `offer`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `offer` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на товар&#39;, `trader_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на трейдера&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Количество товаров&#39;, `buy` tinyint(1) DEFAULT 0, `sell` tinyint(1) DEFAULT 0, `price` decimal(16,6) DEFAULT NULL COMMENT &#39;Желаемая цена за единицу&#39;, `ts` datetime DEFAULT current_timestamp() COMMENT &#39;Когда была выставлена&#39;, `is_active` tinyint(1) DEFAULT 0 COMMENT &#39;Действует ли еще это предложение&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Сделки&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `offer` -- LOCK TABLES `offer` WRITE; /*!40000 ALTER TABLE `offer` DISABLE KEYS */; INSERT INTO `offer` VALUES (1,1,1,4492564.690000,0,0,3560.226016,&#39;1978-04-16 05:11:17&#39;,0),(2,2,2,88065.115876,0,1,25673.801940,&#39;2012-01-27 00:48:33&#39;,0),(3,3,3,57342.670793,1,1,5968955.094700,&#39;1994-04-03 09:01:57&#39;,0),(4,4,4,0.841573,0,1,43.341753,&#39;2016-01-27 13:50:13&#39;,1),(5,5,5,21273859.908703,0,1,1003.024800,&#39;1976-04-11 21:39:31&#39;,0),(6,6,6,330151.950000,0,0,0.908693,&#39;2005-12-14 22:57:53&#39;,1),(7,7,7,270663059.774970,1,0,105.000000,&#39;1988-11-26 14:32:08&#39;,0),(8,8,8,242300815.081650,1,1,418.591556,&#39;2009-01-04 11:59:22&#39;,0),(9,9,9,43.108410,1,0,175276231.966730,&#39;1980-11-30 22:09:40&#39;,1),(10,10,10,35074063.000000,0,0,3151.711086,&#39;2007-11-07 11:28:43&#39;,1); /*!40000 ALTER TABLE `offer` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `price` -- DROP TABLE IF EXISTS `price`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `price` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, `buy` decimal(16,6) NOT NULL DEFAULT 0.000000 COMMENT &#39;Курс покупки&#39;, `sell` decimal(16,6) NOT NULL DEFAULT 0.000000 COMMENT &#39;Курс продажи&#39;, `ts` datetime DEFAULT current_timestamp() COMMENT &#39;Время в которое сделка по последней цене была зафиксирована&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Изменение цены&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `price` -- LOCK TABLES `price` WRITE; /*!40000 ALTER TABLE `price` DISABLE KEYS */; INSERT INTO `price` VALUES (1,1,0,833143.663441,7083.640000,&#39;1974-02-28 12:14:53&#39;),(2,2,1,506349.000000,13.956034,&#39;1989-08-25 08:13:42&#39;),(3,3,2,0.000000,3.100000,&#39;1977-01-24 01:27:13&#39;),(4,4,3,30754203.895000,22564.976107,&#39;2005-03-20 21:58:47&#39;),(5,5,4,607602067.694800,692.700000,&#39;1995-05-31 22:30:59&#39;),(6,6,5,55.747000,131.327840,&#39;2011-06-04 13:19:14&#39;),(7,7,6,4047695.239467,9294.278000,&#39;1991-08-02 18:28:19&#39;),(8,8,7,2053.700000,42688565.419750,&#39;1995-09-23 02:06:23&#39;),(9,9,8,29.743971,5.057628,&#39;2002-05-03 05:17:56&#39;),(10,10,9,56795556.100000,3272.168110,&#39;1994-03-20 04:59:16&#39;); /*!40000 ALTER TABLE `price` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `report` -- DROP TABLE IF EXISTS `report`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `report` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `trading_data` datetime DEFAULT current_timestamp() COMMENT &#39;Дата отчета&#39;, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Ссылается на валюту, используемую в качестве базовой валюты для данного товара&#39;, `first_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Начальная цена&#39;, `last_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Последняя цена&#39;, `min_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Минимальная цена&#39;, `max_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Максимальная цена&#39;, `avg_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Средняя цена&#39;, `total_amount` decimal(16,6) DEFAULT NULL COMMENT &#39;Общая сумма, уплаченная за этот товар в течение отчетного периода.&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Количество товаров, проданных в течение данного отчетного периода.&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Отчет&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `report` -- LOCK TABLES `report` WRITE; /*!40000 ALTER TABLE `report` DISABLE KEYS */; INSERT INTO `report` VALUES (1,&#39;2008-09-12 05:40:01&#39;,1,0,0.000000,0.000000,11284.100000,0.000000,2113910.285725,33.000000,0.000000),(2,&#39;2005-08-05 11:49:42&#39;,2,1,7.124000,17.382430,2.353250,2569858.500000,15130.120000,0.000000,0.000000),(3,&#39;1985-04-14 09:07:10&#39;,3,2,43490527.885671,1452.364336,26509.073543,135.213446,341326328.324550,33705.000000,45819518.090000),(4,&#39;2018-02-14 00:52:19&#39;,4,3,649342.981000,3.424990,991.284828,4063.439875,1324038.780000,9267588.571890,133722.790635),(5,&#39;2003-12-24 09:46:13&#39;,5,4,2413.800000,3.000000,170787253.391930,221895.722237,2782544.825000,196.304264,85386572.462400),(6,&#39;2014-11-14 05:28:06&#39;,6,5,4.320848,0.000000,120899.185986,351820.630000,44.353438,0.000000,2229.320343),(7,&#39;2000-11-12 21:49:52&#39;,7,6,69682369.400000,211162.000000,119454.460000,37212568.372855,2133133.264035,153722219.685720,5557523.960000),(8,&#39;2021-07-08 17:06:56&#39;,8,7,341.300000,1861.743223,1.337996,49.590397,8522.873563,2269.709000,16999.627000),(9,&#39;2003-12-11 22:56:40&#39;,9,8,20354369.167190,3144.140230,0.800314,305.165000,1.136590,215.070000,0.000000),(10,&#39;1999-04-10 21:50:09&#39;,10,9,4736112.251411,1569.689600,52377.000310,492851.324047,0.000000,33864985.940000,68705694.321598); /*!40000 ALTER TABLE `report` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `trade` -- DROP TABLE IF EXISTS `trade`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `trade` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на товар&#39;, `seller_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на трейдера&#39;, `buyer_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Ссылка на трейдера&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Количество товаров&#39;, `unit_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Цена за единицу&#39;, `description` text COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;Все дополнительные сведения (например, количество выпущенных акций) в текстовом формате.&#39;, `offer_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Индификатор сделки&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Сделки&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `trade` -- LOCK TABLES `trade` WRITE; /*!40000 ALTER TABLE `trade` DISABLE KEYS */; INSERT INTO `trade` VALUES (1,1,1,1,2.460000,58902033.493045,&#39;Dolores officiis quas necessitatibus qui amet. Id error atque laborum ea maiores rerum voluptatem eum. Et tempora pariatur quaerat laborum. Placeat ipsa tenetur maiores architecto.&#39;,1),(2,2,2,2,28389.028300,64.144119,&#39;Dolorem perspiciatis consequatur eaque est corporis adipisci. Quae numquam quo provident alias natus eligendi. Voluptas ratione velit eveniet aperiam.&#39;,2),(3,3,3,3,267929.007257,3002.000000,&#39;Distinctio facere ullam nostrum commodi. Mollitia ea quo aut labore eaque. Aliquam dolores porro ut magni vitae dolore a nostrum.&#39;,3),(4,4,4,4,9942452.040000,572582523.427500,&#39;Cumque fugit similique culpa et ad. Temporibus dolores ut sint ipsum voluptas a et. Dolore et totam tempora dolorem ea. Unde dolore velit perspiciatis exercitationem.&#39;,4),(5,5,5,5,679040271.000000,73901.498440,&#39;Totam quisquam unde possimus voluptatibus quos praesentium provident. Et magnam aspernatur aut cupiditate eaque et non. Minima ex ut unde ut magni.&#39;,5),(6,6,6,6,11415.933040,34761.863528,&#39;Necessitatibus laudantium fugit porro. A et reprehenderit iusto ut deleniti ut. Et inventore aut culpa quibusdam eveniet aut nulla.&#39;,6),(7,7,7,7,4.820000,77.000000,&#39;Corporis provident natus neque et. Libero nostrum aut minima impedit. Libero quos et beatae eos. Facere et officia quis et quia.&#39;,7),(8,8,8,8,667543.999204,422918.783910,&#39;Autem rerum est nostrum placeat nulla. Velit cumque eius a enim. Dolor harum dignissimos sunt ea. Consectetur rerum eius ratione rerum nisi ipsam perspiciatis.&#39;,8),(9,9,9,9,280068485.000000,0.000000,&#39;Voluptatem veritatis consectetur libero autem reiciendis. Animi qui aut animi sed perspiciatis. Reprehenderit officia iure occaecati molestiae ipsam ipsa. Quo qui laudantium provident veniam aut.&#39;,9),(10,10,10,10,0.000000,107806.940000,&#39;Ipsa ut veniam deserunt quis. Ipsam labore aspernatur cupiditate et molestiae. Animi voluptatem vel reprehenderit tempora. At doloribus facere voluptatem dignissimos dolorem.&#39;,10); /*!40000 ALTER TABLE `trade` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `trader` -- DROP TABLE IF EXISTS `trader`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `trader` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `firstname` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;Имя&#39;, `lastname` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;Фамилия&#39;, `user_name` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Логин у всех уникальный&#39;, `email` varchar(120) COLLATE utf8mb4_unicode_ci NOT NULL, `confirmation_code` varchar(120) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Код, отправленный пользователю для завершения процесса регистрации.&#39;, `time_registered` datetime DEFAULT current_timestamp(), `time_confirmed` datetime DEFAULT current_timestamp(), `country_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Страна, в которой живет.&#39;, `preffered_currency_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Валюта, которую трейдер предпочитает&#39;, PRIMARY KEY (`id`), UNIQUE KEY `user_name` (`user_name`), UNIQUE KEY `email` (`email`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;юзеры&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `trader` -- LOCK TABLES `trader` WRITE; /*!40000 ALTER TABLE `trader` DISABLE KEYS */; INSERT INTO `trader` VALUES (1,&#39;Gideon&#39;,&#39;Hettinger&#39;,&#39;nweissnat&#39;,&#39;hilda.schulist@example.com&#39;,&#39;sdaj&#39;,&#39;1994-07-02 14:22:41&#39;,&#39;1973-09-17 17:51:44&#39;,1,0),(2,&#39;Antwon&#39;,&#39;Rogahn&#39;,&#39;eva.stoltenberg&#39;,&#39;grussel@example.org&#39;,&#39;cddo&#39;,&#39;1979-12-08 07:45:35&#39;,&#39;1990-02-28 17:41:55&#39;,2,1),(3,&#39;Letitia&#39;,&#39;Cremin&#39;,&#39;bergnaum.tyra&#39;,&#39;berniece.feeney@example.com&#39;,&#39;jrip&#39;,&#39;2005-05-27 20:49:06&#39;,&#39;1990-05-07 05:33:38&#39;,3,2),(4,&#39;Braulio&#39;,&#39;Hessel&#39;,&#39;andreanne97&#39;,&#39;margarita71@example.net&#39;,&#39;zqnt&#39;,&#39;2006-01-12 00:24:22&#39;,&#39;2021-04-16 22:24:14&#39;,4,3),(5,&#39;Clementine&#39;,&#39;Zboncak&#39;,&#39;orion53&#39;,&#39;sawayn.keara@example.net&#39;,&#39;tkwo&#39;,&#39;2004-09-08 13:05:57&#39;,&#39;2009-04-21 11:36:19&#39;,5,4),(6,&#39;Alvis&#39;,&#39;Gutkowski&#39;,&#39;ikovacek&#39;,&#39;ikovacek@example.net&#39;,&#39;supc&#39;,&#39;2009-10-11 20:46:23&#39;,&#39;2003-04-14 08:48:12&#39;,6,5),(7,&#39;Raphael&#39;,&#39;Sanford&#39;,&#39;mcdermott.providenci&#39;,&#39;leonie94@example.org&#39;,&#39;ibjn&#39;,&#39;1975-03-27 19:01:54&#39;,&#39;2000-12-20 15:01:59&#39;,7,6),(8,&#39;Ruthie&#39;,&#39;Dietrich&#39;,&#39;lyost&#39;,&#39;powlowski.hillary@example.org&#39;,&#39;mdzm&#39;,&#39;2005-02-19 04:07:58&#39;,&#39;1993-08-30 12:18:06&#39;,8,7),(9,&#39;Keagan&#39;,&#39;Gutmann&#39;,&#39;kpredovic&#39;,&#39;kody.gibson@example.com&#39;,&#39;ejxu&#39;,&#39;2001-09-05 00:29:11&#39;,&#39;2016-07-10 15:52:51&#39;,9,8),(10,&#39;Marianne&#39;,&#39;Ziemann&#39;,&#39;jace.kunde&#39;,&#39;qbergnaum@example.com&#39;,&#39;qozb&#39;,&#39;1991-04-13 01:13:57&#39;,&#39;1987-11-14 13:35:07&#39;,10,9); /*!40000 ALTER TABLE `trader` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2021-09-02 19:36:59 UPDATE trader SET country_id = (FLOOR(1 + RAND() * 3)); UPDATE offer SET item_id = (FLOOR(1 + RAND() * 3)); UPDATE offer SET trader_id = (FLOOR(1 + RAND() * 3)); UPDATE offer SET price = (FLOOR(1 + RAND() * 100000)); UPDATE offer SET buy = 0 WHERE sell = 1; UPDATE offer SET buy = 1 WHERE sell = 0; UPDATE trade SET seller_id = (FLOOR(1 + RAND() * 3)); UPDATE trade SET buyer_id = (FLOOR(1 + RAND() * 3)); UPDATE trade SET buyer_id = (FLOOR(1 + RAND() * 3)) WHERE buyer_id = seller_id ; UPDATE trade SET quantity = (FLOOR(1 + RAND() * 1000000)); UPDATE trade SET unit_price = (FLOOR(1 + RAND() * 1000)); UPDATE trade SET item_id = (FLOOR(1 + RAND() * 3)); UPDATE trade SET offer_id = (FLOOR(1 + RAND() * 3)); UPDATE report SET currency_id = (FLOOR(1 + RAND() * 3)); UPDATE trader SET birthday = CURRENT_DATE() - INTERVAL (FLOOR(20 + RAND() * 60)) YEAR; . 6. Cкрипты характерных выборок . Я конечно создал первые выборки из моей логики, но выборок конечно надо намного больше . Распределение трейдеров по странам . CREATE VIEW trader_country AS SELECT c.country, COUNT(*) AS number_people FROM trader t JOIN country c ON t.country_id = c.id GROUP BY c.country ; . Распределение количества предложений на покупку по трейдерам . DROP VIEW trader_offer_buy; CREATE VIEW trader_offer_buy AS SELECT user_name, COUNT(o.buy) AS sum_offer_buy FROM trader t JOIN offer o ON t.id = o.trader_id WHERE o.buy = 1 GROUP BY t.user_name ORDER BY sum_offer_buy DESC; . Распределение количества предложений на продажу по трейдерам . DROP VIEW trader_offer_sell; CREATE VIEW trader_offer_sell AS SELECT user_name, COUNT(o.sell) AS sum_offer_sell FROM trader t JOIN offer o ON t.id = o.trader_id WHERE o.sell = 1 GROUP BY t.user_name ORDER BY sum_offer_sell DESC; SELECT * FROM trader_country tc ; SELECT * FROM trader_offer_sell ; SELECT * FROM trader_offer_buy tob ; . 7. Создание триггеров . Триггер для проверки что возраст не пустое значение . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.birthday IS NULL THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;Необходимо добавить дату рождения&#39;; END IF; END// DELIMITER ; . Триггер для проверки что трейдер совершенолетний . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.birthday &gt;= CURRENT_DATE() - INTERVAL 18 YEAR THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;Подождите когда вам исполниться 18&#39;; END IF; END// DELIMITER ; . Триггер для проверки что трейдер одновременно не выставляет одну и ту-же позицию сразу на продажу и покупку . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.sell = 1 AND NEW.buy = 1 THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;Нельзя совершать одновременно покупку и продажу&#39;; END IF; END// DELIMITER ; . Триггер для проверки что трейдер выбрал или покупку или продажу . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.sell = 0 AND NEW.buy = 0 THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;Надо выбрать одну из операций&#39;; END IF; END// DELIMITER ; . Вот и весь код. Оценили на отлично. В конце цитата преподавателя: . Хорошо, что позаботились об индексах. ENGINE = InnoDB - значение по умолчанию. его писать необязательно. но хорошо, что Вы знаете про эту опцию. В SQL в общем случае принято именовать поля таблиц в единственном числе, а таблицы можно называть во множественном числе. важно придерживаться выбранного стиля (все в ед.ч. либо все в мн.ч.). Наиболее популярные запросы (часто исполняемые) есть смысл сохранить в виде представлений. Хорошо, что реализовали представления, триггеры, не все до этого доходят. Успехов в дальнейшем обучении! .",
            "url": "https://zmey56.github.io/blog//course%20project/mysql/geekbrain/2021/09/04/course-project-classification.html",
            "relUrl": "/course%20project/mysql/geekbrain/2021/09/04/course-project-classification.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Оптимизации портфеля с помощью Python и PyPortfolioOpt",
            "content": "В данной статье приводится пример вычисления оптимизированных весов активов в портфеле используя Портфельную теорию Марковица при помощи Python . &#1055;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1100;&#1085;&#1072;&#1103; &#1090;&#1077;&#1086;&#1088;&#1080;&#1103; &#1052;&#1072;&#1088;&#1082;&#1086;&#1074;&#1080;&#1094;&#1072; . Портфельная теория Марковица(далее ПТМ) (Modern portfolio theory) — разработанная Гарри Марковицем методика формирования инвестиционного портфеля, направленная на оптимальный выбор активов, исходя из требуемого соотношения доходность/риск. Сформулированные им в 1950-х годах идеи составляют основу современной портфельной теории. . Основные положения портфельной теории были сформулированы Гарри Марковицем при подготовке им докторской диссертации в 1950—1951 годах. . Рождением же портфельной теории Марковица считается опубликованная в «Финансовом журнале» в 1952 году статья «Выбор портфеля». В ней он впервые предложил математическую модель формирования оптимального портфеля и привёл методы построения портфелей при определённых условиях. Основная заслуга Марковица состояла в предложении вероятностной формализации понятий «доходность» и «риск», что позволило перевести задачу выбора оптимального портфеля на формальный математический язык. Надо отметить, что в годы создания теории Марковиц работал в RAND Corp., вместе с одним из основателей линейной и нелинейной оптимизации — Джорджем Данцигом и сам участвовал в решении указанных задач. Поэтому собственная теория, после необходимой формализации, хорошо ложилась в указанное русло. . Марковиц постоянно занимается усовершенствованием своей теории и в 1959 году выпускает первую посвящённую ей монографию «Выбор портфеля: эффективная диверсификация инвестиций». . В 1990 году, когда Марковицу вручают Нобелевскую премию, выходит книга «Средне-дисперсионный анализ при выборе портфеля и рынка капитала» ссылка. . 1. &#1054;&#1078;&#1080;&#1076;&#1072;&#1077;&#1084;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103;(Portfolio Expected Return) . Ожидаемая доходность портфеля будет зависеть от ожидаемой доходности каждого из активов, входящих в него. Такой подход позволяет снизить риск за счет диверсификации и одновременно максимизировать доход инвестора, поскольку убытки по одним инвестициям будут компенсированы доходом по другим. . Ожидаемая доходность портфеля представляет собой суммарную ожидаемую доходность входящих в него ценных бумаг, взвешенную с учетом их доли в портфеле. . . 2. &#1044;&#1080;&#1089;&#1087;&#1077;&#1088;&#1089;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; (Portfolio Variance ) . Дисперсия портфеля - это процесс, который определяет степень риска или волатильности, связанной с инвестиционным портфелем. Основная формула для расчета этой дисперсии фокусируется на взаимосвязи между так называемой дисперсией доходности и ковариацией, связанной с каждой из ценных бумаг, найденных в портфеле, а также с процентом или частью портфеля, который представляет каждая ценная бумага. . . 3. &#1050;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; &#1064;&#1072;&#1088;&#1087;&#1072; (Sharpe Ratio) . Коэффициент Шарпа измеряет доходность инвестиций по отношению к безрисковой ставке (казначейской ставке) и степени риска. В целом, более высокое значение коэффициента Шарпа указывает на лучшие и более прибыльные инвестиции. Таким образом, если сравнивать два портфеля с одинаковыми рисками, то при прочих равных условиях было бы лучше инвестировать в портфель с более высоким коэффициентом Шарпа. . . 4. &#1069;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1072;&#1103; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094;&#1072; (The Efficient Frontier ) . Определение и рисунок из Википедии: . Граница эффективности (англ. Efficient frontier) в портфельной теории Марковица — инвестиционный портфель, оптимизированный в отношении риска и доходности. Формально границей эффективности является набор портфелей, удовлетворяющих такому условию, что не существует другого портфеля с более высокой ожидаемой доходностью, но с таким же стандартным отклонением доходности. Понятие границы эффективности было впервые сформулировано Гарри Марковицем в 1952 году в модели Марковица. . Портфель может быть охарактеризован как «эффективный», если он имеет максимально возможный ожидаемый уровень доходности для своего уровня риска (который представлен стандартным отклонением доходности портфеля). Так, на график соотношения риска и доходности может быть нанесена любая возможная комбинация рискованных активов, и совокупность всех таких возможных портфелей определяет регион в этом пространстве. При отсутствии в портфеле безрискового актива граница эффективности определяется верхней (восходящей) частью гиперболы, ограничивающей область допустимых решений для всех соотношений активов в портфеле. . В случае же, если в портфель может быть включён безрисковый актив, граница эффективности вырождается в отрезок прямой линии, исходящий от значения доходности безрискового актива на оси ординат (ожидаемая доходность портфеля) и проходящий по касательной к границе области допустимых решений. Все портфели на отрезке между собственно безрисковым активом и точкой касания состоят из комбинации безрискового актива и рисковых активов, в то время как все портфели на линии выше и справа от точки касания образуются короткой позицией в безрисковом активе и инвестированием в рисковые активы. . . . &#1054;&#1087;&#1090;&#1080;&#1084;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; &#1085;&#1072; Python . 1. &#1048;&#1084;&#1087;&#1086;&#1088;&#1090; &#1085;&#1077;&#1086;&#1073;&#1093;&#1086;&#1076;&#1080;&#1084;&#1099;&#1093; &#1073;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1082; . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import matplotlib.pyplot as plt import numpy as np import pandas as pd import pandas_datareader as web from matplotlib.ticker import FuncFormatter . Непосредственно для анализа и оптимизации портфеля существует библиотека PyPortfolioOpt. Так как она не входит в стандартный набор, то ее необходимо установить. . !pip install PyPortfolioOpt #Installing the Portfolio Optimzation Library . Collecting PyPortfolioOpt Downloading https://files.pythonhosted.org/packages/46/55/7d39d78d554ee33a7317e345caf01339da11406c28f18bc48794fe967935/PyPortfolioOpt-1.4.1-py3-none-any.whl (56kB) |████████████████████████████████| 61kB 3.2MB/s Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.1.5) Requirement already satisfied: scipy&lt;2.0,&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.4.1) Collecting cvxpy&lt;2.0.0,&gt;=1.1.10 Downloading https://files.pythonhosted.org/packages/83/47/fd1e818b8da30ef18695a0fbf9b66611ab18506f0a44fc69480a75f4db1b/cvxpy-1.1.12.tar.gz (1.3MB) |████████████████████████████████| 1.3MB 7.9MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: numpy&lt;2.0,&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2018.9) Requirement already satisfied: ecos&gt;=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.0.7.post1) Requirement already satisfied: scs&gt;=1.1.6 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.1.3) Requirement already satisfied: osqp&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.6.2.post0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.19-&gt;PyPortfolioOpt) (1.15.0) Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp&gt;=0.4.1-&gt;cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.1.5.post0) Building wheels for collected packages: cvxpy Building wheel for cvxpy (PEP 517) ... done Created wheel for cvxpy: filename=cvxpy-1.1.12-cp37-cp37m-linux_x86_64.whl size=2731641 sha256=2c888a76787438c69d6a1dce26762a30ead689ee8b9da895efc81ad29620fbdf Stored in directory: /root/.cache/pip/wheels/9b/62/55/1da181c05c710c5d99bd560edebec3bd6a61cb69acef9dc00e Successfully built cvxpy Installing collected packages: cvxpy, PyPortfolioOpt Found existing installation: cvxpy 1.0.31 Uninstalling cvxpy-1.0.31: Successfully uninstalled cvxpy-1.0.31 Successfully installed PyPortfolioOpt-1.4.1 cvxpy-1.1.12 . Импортируем функции для дальнейшей работы: . from pypfopt.efficient_frontier import EfficientFrontier from pypfopt import risk_models from pypfopt import expected_returns from pypfopt.cla import CLA import pypfopt.plotting as pplt from matplotlib.ticker import FuncFormatter from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices . 2. &#1055;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1087;&#1086; &#1072;&#1082;&#1094;&#1080;&#1103;&#1084; &#1080;&#1079; &#1080;&#1085;&#1090;&#1077;&#1088;&#1085;&#1077;&#1090;&#1072; . Сначало установим опять пакет, который не входит в стандартный набор. Он позволяет получить данные по акциям с сайтя yahoo. . Тикеры, которые будут использоваться для анализа - одна из компаний входящих в лидеры в своем секторе. . !pip install yfinance --upgrade --no-cache-dir . Collecting yfinance Downloading https://files.pythonhosted.org/packages/a7/ee/315752b9ef281ba83c62aa7ec2e2074f85223da6e7e74efb4d3e11c0f510/yfinance-0.1.59.tar.gz Requirement already satisfied, skipping upgrade: pandas&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0) Requirement already satisfied, skipping upgrade: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB) |████████████████████████████████| 6.3MB 6.4MB/s Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... done Created wheel for yfinance: filename=yfinance-0.1.59-py2.py3-none-any.whl size=23442 sha256=519c6bb89355fc0fab0d0a1c7f12df703543e4aadbde996b33dcf9592621bb6e Stored in directory: /tmp/pip-ephem-wheel-cache-weglluyo/wheels/f8/2a/0f/4b5a86e1d52e451757eb6bc17fd899629f0925c777741b6d04 Successfully built yfinance Installing collected packages: lxml, yfinance Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.3 yfinance-0.1.59 . import yfinance as yf tickers = [&#39;LKOH.ME&#39;,&#39;GMKN.ME&#39;, &#39;DSKY.ME&#39;, &#39;NKNC.ME&#39;, &#39;MTSS.ME&#39;, &#39;IRAO.ME&#39;, &#39;SBER.ME&#39;, &#39;AFLT.ME&#39;] df_stocks= yf.download(tickers, start=&#39;2018-01-01&#39;, end=&#39;2020-12-31&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . df_stocks.head() . AFLT.ME DSKY.ME GMKN.ME IRAO.ME LKOH.ME MTSS.ME NKNC.ME SBER.ME . Date . 2018-01-03 127.199066 | 70.177948 | 8249.352539 | 3.025520 | 2844.152100 | 197.995163 | 33.301483 | 145.441605 | . 2018-01-04 134.899857 | 71.621933 | 8455.913086 | 3.181567 | 2910.237305 | 202.738434 | 33.173889 | 149.769119 | . 2018-01-05 133.450317 | 71.621933 | 8441.316406 | 3.160166 | 2967.178467 | 202.199417 | 33.237682 | 149.643677 | . 2018-01-09 136.349426 | 71.116539 | 8521.605469 | 3.103989 | 3016.222900 | 203.421158 | 33.556660 | 150.772598 | . 2018-01-10 135.262268 | 71.658035 | 8507.006836 | 3.087939 | 3026.613525 | 204.427307 | 33.811848 | 149.116821 | . Дальше необходимо проверить есть ли среди полученных значений NaN. В случае их наличия они будут мешать дальнейшему исследованию. Для того, чтобы это решить, необходимо рассмотреть или иную акцию, или заменить их для примера средней ценой между днем до и после значения NaN. . nullin_df = pd.DataFrame(df_stocks,columns=tickers) print(nullin_df.isnull().sum()) . LKOH.ME 0 GMKN.ME 0 DSKY.ME 0 NKNC.ME 0 MTSS.ME 0 IRAO.ME 0 SBER.ME 0 AFLT.ME 0 dtype: int64 . 3. &#1056;&#1072;&#1089;&#1095;&#1077;&#1090;&#1099; . Перейдем к расчетам по оптимизации портфеля и начнем с определения ожидаемой доходности и дисперсии портфеля. Далее сохраним значения весов портфеля с максимальным коэффициентом Шарпа и минимальной диспрсией. . # mu = expected_returns.mean_historical_return(df_stocks) # #Sample Variance of Portfolio # Sigma = risk_models.sample_cov(df_stocks) # #Max Sharpe Ratio - Tangent to the EF # ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks # sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks # sharpe_pwt=ef.clean_weights() # print(sharpe_pwt) #Годовая доходность mu = expected_returns.mean_historical_return(df_stocks) #Дисперсия портфеля Sigma = risk_models.sample_cov(df_stocks) . ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks sharpe_pwt=ef.clean_weights() print(sharpe_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.0), (&#39;DSKY.ME&#39;, 0.22606), (&#39;GMKN.ME&#39;, 0.48796), (&#39;IRAO.ME&#39;, 0.0), (&#39;LKOH.ME&#39;, 0.0), (&#39;MTSS.ME&#39;, 0.02953), (&#39;NKNC.ME&#39;, 0.25645), (&#39;SBER.ME&#39;, 0.0)]) . Необходимо обратить внимание, что если изменить weight_bounds=(0,1) на weight_bounds=(-1,1), то в портфеле будут учитываться и короткие позиции по акциям. . Дальше посмотрим общие характеристики по портфелю. . ef.portfolio_performance(verbose=True) . Expected annual return: 37.1% Annual volatility: 20.7% Sharpe Ratio: 1.70 . (0.37123023494063007, 0.20717177784552962, 1.695357536597058) . Теперь посмотрим портфель с минимальной волатильностью: . ef1 = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks minvol=ef1.min_volatility() minvol_pwt=ef1.clean_weights() print(minvol_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.02876), (&#39;DSKY.ME&#39;, 0.24503), (&#39;GMKN.ME&#39;, 0.10403), (&#39;IRAO.ME&#39;, 0.0938), (&#39;LKOH.ME&#39;, 0.01168), (&#39;MTSS.ME&#39;, 0.41967), (&#39;NKNC.ME&#39;, 0.09704), (&#39;SBER.ME&#39;, 0.0)]) . ef1.portfolio_performance(verbose=True, risk_free_rate = 0.27) . Expected annual return: 24.0% Annual volatility: 16.9% Sharpe Ratio: -0.18 . (0.239915644698749, 0.16885732511472468, -0.17816434839774456) . 4. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1075;&#1088;&#1072;&#1092;&#1080;&#1082;&#1072; &#1101;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1099;&#1093; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094; . Заключительным шагом является построение эффективной границы для визуального представления и расчет распределения активов. Тут встречается одна сложность, решить которую пока что мне не удалось - пакет создан для анализа в долларах и в результате в выводе присутствует их обозначение. Но с другой стороны наличия значка &quot;$&quot; не должно сильно мешать. . Анализ произведем для суммы в 100 000 рублей. . cl_obj = CLA(mu, Sigma) ax = pplt.plot_efficient_frontier(cl_obj, showfig = False) ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: &#39;{:.0%}&#39;.format(x))) ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: &#39;{:.0%}&#39;.format(y))) . Первым этапом посчитаем портфель с минимальной волатильностью: . latest_prices = get_latest_prices(df_stocks) # Allocate Portfolio Value in $ as required to show number of shares/stocks to buy, also bounds for shorting will affect allocation #Min Volatility Portfolio Allocation $10000 allocation_minv, rem_minv = DiscreteAllocation(minvol_pwt, latest_prices, total_portfolio_value=100000).lp_portfolio() print(allocation_minv) print(&quot;Leftover Fund value in$ after building minimum volatility portfolio is ${:.2f}&quot;.format(rem_minv)) print(&quot;Осталось денежных средств после построения портфеля с минимальной волатильностью составляет {:.2f} рублей&quot;.format(rem_minv)) print() . {&#39;AFLT.ME&#39;: 41, &#39;DSKY.ME&#39;: 181, &#39;IRAO.ME&#39;: 1765, &#39;LKOH.ME&#39;: 1, &#39;MTSS.ME&#39;: 127, &#39;NKNC.ME&#39;: 107} Leftover Fund value in$ after building minimum volatility portfolio is $6152.03 Осталось денежных средств после построения портфеля с минимальной волатильностью составляет 6152.03 рублей . Вторым шагом портфель с максимальным коэффициентом Шарпа: . latest_prices1 = get_latest_prices(df_stocks) allocation_shp, rem_shp = DiscreteAllocation(sharpe_pwt, latest_prices1, total_portfolio_value=100000).lp_portfolio() print(allocation_shp) print(&quot;Leftover Fund value in$ after building Max Sharpe ratio portfolio is ${:.2f}&quot;.format(rem_shp)) print(&quot;Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа {:.2f} рублей&quot;.format(rem_shp)) #allocation using integer programming via PyPortfolioOpt User Guide #Alex Putkov code used for guidance and reference in applying integer programming . {&#39;DSKY.ME&#39;: 167, &#39;GMKN.ME&#39;: 2, &#39;MTSS.ME&#39;: 9, &#39;NKNC.ME&#39;: 283} Leftover Fund value in$ after building Max Sharpe ratio portfolio is $1319.05 Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа 1319.05 рублей . В результтате нам предлагается купить для оптимального портфеля 167 акций Детского мира, 2 акции Норильского никеля, 9 акций МТС и 283 акцию Нижнекамскнефтехим. В результате у нас еще останется 1319 рублей. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/05/16/automating-portfolio-optimization.html",
            "relUrl": "/finance/investment/python/2021/05/16/automating-portfolio-optimization.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Использование API Fmp Cloud для отбора акций по дивидендам на Nasdaq с помощью Python",
            "content": "Акции с высокой дивидентной доходностью часто являются отличной инвестиционной стратегией для инвесторов, стремящихся получать приток денежных средств каждый год. В данной статье буден создан скрипт на Python для отбора их на бирже NASDAQ. . &#1063;&#1090;&#1086; &#1090;&#1072;&#1082;&#1086;&#1077; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1090;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100;? . Возьму определение из Википедии. Дивиде́ндная дохо́дность (англ. dividend yield) — это отношение величины годового дивиденда на акцию к цене акции. Данная величина выражается чаще всего в процентах. . Пример . При цене акции ОАО «Лукойл» 1124,37 рублей и дивиденде 28 рублей на акцию дивидендная доходность будет равна: . . Так же необходимо обратить внимание, что многие растущие компании, такие как для примера Amazon и Yandex, не выплачивают дивиденды, поскольку они реинвестируют всю прибыль в развитие бизнеса. Поэтому дивидендная доходность для этих фирм будет равна нулю. . &#1056;&#1072;&#1089;&#1095;&#1077;&#1090; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1076;&#1085;&#1086;&#1081; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1080; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Расчет дивидендной доходности является простой задачей, которую можно выполнить с помощью финансового API под названием fmpcloud и Python. Этот API предлагает несколько бесплатных запросов в день после регистрации. . Первым делом нужно извлечь список тикеров для всех акций, торгующихся на Nasdaq, по которым собираемся рассчитать дивидендную доходность. | import requests demo = &#39;ВАШ API КОД&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] for ticker in tickers: symbols.append(ticker[&#39;symbol&#39;]) . len(symbols) . 5500 . После необходимо пройтись по полученому списку акций и получить финансовую информацию по компании. Так же необходимо понимать, что получаем только последние данные, а не за все время существование компании. | DivYield = {} for company in symbols: try: companydata = requests.get(f&#39;https://fmpcloud.io/api/v3/profile/{company}?apikey={demo}&#39;) companydata = companydata.json() latest_Annual_Dividend = companydata[0][&#39;lastDiv&#39;] price = companydata[0][&#39;price&#39;] market_Capitalization = companydata[0][&#39;mktCap&#39;] name = companydata[0][&#39;companyName&#39;] exchange = companydata[0][&#39;exchange&#39;] dividend_Yield= latest_Annual_Dividend/price DivYield[company] = {} DivYield[company][&#39;Dividend_Yield&#39;] = dividend_Yield DivYield[company][&#39;latest_Price&#39;] = price DivYield[company][&#39;latest_Dividend&#39;] = latest_Annual_Dividend DivYield[company][&#39;market_Capit_in_M&#39;] = market_Capitalization/1000000 DivYield[company][&#39;company_Name&#39;] = name DivYield[company][&#39;exchange&#39;] = exchange except: pass . Сбор данных может занять значительное по продолжительности время. После их можно представить в виде отсортированного DataFrame, где сверху будут акций с высокой дивидендной доходностью. . import pandas as pd DivYield_dataframe = pd.DataFrame.from_dict(DivYield, orient=&#39;index&#39;) DivYield_dataframe = DivYield_dataframe.sort_values([&#39;Dividend_Yield&#39;], ascending=[False]) DivYield_dataframe.head(15) . Dividend_Yield latest_Price latest_Dividend market_Capit_in_M company_Name exchange . SNT 0.682238 | 4.1100 | 2.8040 | 95.258704 | Senstar Technologies Ltd. | Nasdaq Global Market | . TBK 0.151587 | 117.4900 | 17.8100 | 2951.818752 | Triumph Bancorp, Inc. | Nasdaq Global Select | . MGOAX 0.100728 | 16.4900 | 1.6610 | 1625.336832 | Victory Munder Mid-Cap Core Growth Fund Class A | Nasdaq Capital Market | . GLAD 0.067450 | 11.5641 | 0.7800 | 396.699520 | Gladstone Capital Corporation | Nasdaq Global Select | . NVEC 0.057954 | 69.0200 | 4.0000 | 333.589504 | NVE Corporation | Nasdaq Capital Market | . QYLG 0.054108 | 32.6199 | 1.7650 | 0.000000 | Global X NASDAQ 100 Covered Call &amp; Growth ETF | Nasdaq Global Market | . FUND 0.049811 | 8.7410 | 0.4354 | 256.320208 | Sprott Focus Trust, Inc. | Nasdaq Global Select | . SQQQ 0.044732 | 7.2655 | 0.3250 | 130697.036000 | ProShares UltraPro Short QQQ | Nasdaq Global Market | . MTEX 0.042901 | 39.1600 | 1.6800 | 74.116176 | Mannatech, Incorporated | Nasdaq Global Select | . ESHY 0.040237 | 21.0750 | 0.8480 | 0.000000 | Xtrackers J.P. Morgan ESG USD High Yield Corpo... | Nasdaq Global Select | . PBCT 0.038865 | 18.6800 | 0.7260 | 7994.199552 | People&#39;s United Financial, Inc. | Nasdaq Global Select | . NYMTP 0.038217 | 25.3290 | 0.9680 | 1337.495296 | New York Mortgage Trust, Inc. | Nasdaq Global Select | . MNSBP 0.037127 | 27.5000 | 1.0210 | 0.000000 | MainStreet Bancshares, Inc. | Nasdaq Global Select | . REG 0.033759 | 70.5000 | 2.3800 | 11976.117248 | Regency Centers Corporation | Nasdaq Global Select | . TTEC 0.032629 | 93.1700 | 3.0400 | 4377.620480 | TTEC Holdings, Inc. | Nasdaq Global Select | . &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1087;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1088;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1072; &#1080; &#1079;&#1072;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1077; . Предварительно проведем расчет средней дивидендной доходности по акциям которые платят дивиденды: . meanDivNasdaq = DivYield_dataframe[DivYield_dataframe[&#39;Dividend_Yield&#39;]&gt;0][&#39;Dividend_Yield&#39;].mean() print(&quot;Средняя дивидендная доходность по рынку Nasdaq равна &quot;, &quot;{:.2%}&quot;.format(meanDivNasdaq)) . Средняя дивидендная доходность по рынку Nasdaq равна 3.48% . Самой высокой дивидендной доходностью в полученных результатах у акций компании Senstar Technologies Ltd. — 68.22%. Так же замечено, что в системе похоже сидит баг - может выдавать акции по которым никогда не платили дивиденты. Так же по другим рынкам заметил, что в список могут включаться акции по которым перестали платить дивиденды давно. А так, как подписка Free ограничена по количеству запросов, то подстроить не удалось. Так же в том случае, если при проверке выясняется, что дивиденды платили недавно, то все равно необходимо быть осторожным при выборе компаний по данному показателю, так как он может являться результатом падения цены акций и как следствия ростом дивидендной доходности. Так же выплата высоких дивидендов может не сохраниться в будущем, тем более если у компании возникнут финансовые проблемы. . Основной смысл в следующем - анализ дивидендной доходности не должен быть единственным критерием. Я для одного из своих портфелей так же смотрю: EPS, EBITDA, FCF, срок выплаты дивидендов, капитализация компании, чистая рентабельность (отношение выручки к прибыли) и коэффициент Net Debt/EBITDA. . Но как говориться - все вышеприведенное не является инвестиционной рекомендацией и выбор остается за каждым самостоятельно. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/04/10/high-divident-stocks.html",
            "relUrl": "/finance/investment/python/2021/04/10/high-divident-stocks.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Using machine learning to predict gold mining stock prices",
            "content": "As a basis, I took a notebook published on colab for oil. This notebook examines the analysis of gold prices and shares of gold mining companies using machine analysis methods: linear regression, cluster analysis, and random forest. I immediately warn you that this post does not attempt to show the current situation and predict the future direction. Just like the author for oil, this article does not aim to raise or refute the possibilities of machine learning for analyzing stock prices or other tools. I upgraded the code for gold research in order to encourage those who are interested in further reflection and listen to constructive criticism in their address. . import yfinance as yf import pandas as pd import numpy as np import seaborn as sns from sklearn import metrics import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression . 1. Loading data . For the price of gold, take the value of the exchange-traded investment Fund SPDR Gold Trust, whose shares are 100% backed by precious metal. The quotes will be compared with the prices of gold mining companies &#39; shares: . Newmont Goldcorp (NMM) | Barrick Gold (GOLD) | AngloGold Ashanti (AU) | Kinross Gold (KGC) | Newcrest Mining (ENC) | Polyus (PLZL) | Polymetal (POLY) | Seligdar (SELG) | . gold = pd.DataFrame(yf.download(&quot;GLD&quot;, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed . gold = gold.reset_index() gold.columns = [&quot;Date&quot;,&quot;gold_price&quot;] gold[&#39;Date&#39;] = pd.to_datetime(gold[&#39;Date&#39;]) gold.head() . Date gold_price . 0 2010-01-04 | 109.800003 | . 1 2010-01-05 | 109.699997 | . 2 2010-01-06 | 111.510002 | . 3 2010-01-07 | 110.820000 | . 4 2010-01-08 | 111.370003 | . It is necessary to move the price of gold, as we will be interested in how yesterday&#39;s price affected today&#39;s stock price. . gold[&quot;gold_price&quot;] = gold[&quot;gold_price&quot;].shift(1) . shares=[&quot;NMM.SG&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,&quot;PLZL.ME&quot;,&quot;POLY.ME&quot;,&quot;SELG.ME&quot;] data= yf.download(shares, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . data = data.reset_index() data.head() . Date AU GOLD KGC NCM.AX NMM.SG PLZL.ME POLY.ME SELG.ME . 0 2010-01-04 | 39.698944 | 34.561649 | 18.105721 | 33.237167 | 26.924570 | NaN | NaN | NaN | . 1 2010-01-05 | 40.320408 | 34.989510 | 18.594805 | 33.901924 | 27.116940 | NaN | NaN | NaN | . 2 2010-01-06 | 41.601028 | 35.733963 | 19.256504 | 33.901924 | 27.289278 | NaN | NaN | NaN | . 3 2010-01-07 | 41.130215 | 35.229092 | 19.352404 | 34.298923 | NaN | NaN | NaN | NaN | . 4 2010-01-08 | 41.601028 | 35.451572 | 19.601744 | 33.421829 | 27.702093 | NaN | NaN | NaN | . data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) . all_data=pd.DataFrame() . for index in range(len(shares)): stock=pd.DataFrame() # transform the data stock=data.loc[:, (&quot;Date&quot;,shares[index])] stock[&quot;Date&quot;]=stock[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) stock.columns=[&quot;Date&quot;,&quot;share_price&quot;] test=pd.DataFrame(gold) output=stock.merge(test,on=&quot;Date&quot;,how=&quot;left&quot;) #combining two data sets stock[&quot;gold_price&quot;]=output[&quot;gold_price&quot;] stock[&#39;share_price&#39;]=pd.to_numeric(stock[&#39;share_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&#39;gold_price&#39;]=pd.to_numeric(stock[&#39;gold_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&quot;year&quot;]=pd.to_datetime(stock[&quot;Date&quot;]).dt.year #Create a column with years for subsequent filtering stock[&quot;name&quot;]=shares[index] stock = stock.dropna() #delete all NAN lines #creating a column with a scaled share price scaler=MinMaxScaler() stock[&quot;share_price_scaled&quot;]=scaler.fit_transform(stock[&quot;share_price&quot;].to_frame()) #add data to the main dataframe all_data=all_data.append(stock) #add the data . all_data_15 = all_data[(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)] all_data_15.head() . Date share_price gold_price year name share_price_scaled . 1301 2015-01-02 | 14.269927 | 113.580002 | 2015 | NMM.SG | 0.052072 | . 1302 2015-01-05 | 14.845476 | 114.080002 | 2015 | NMM.SG | 0.071190 | . 1303 2015-01-06 | 15.601913 | 115.800003 | 2015 | NMM.SG | 0.096317 | . 1304 2015-01-07 | 15.645762 | 117.120003 | 2015 | NMM.SG | 0.097773 | . 1305 2015-01-08 | 15.517859 | 116.430000 | 2015 | NMM.SG | 0.093525 | . 2. Data analysis . It is best to start analyzing data by presenting it visually, which will help you understand it better. . 2.1 Chart of gold price changes . gold[[&#39;Date&#39;,&#39;gold_price&#39;]].set_index(&#39;Date&#39;).plot(color=&quot;green&quot;, linewidth=1.0) plt.show() . 2.2. Plotting the pairplot chart for the price of Polyus and Barrick Gold shares over the past five years . palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) g = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;POLY.ME&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) g.fig.suptitle(&quot;Polyuse&quot;, y=1.08) palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) f = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;GOLD&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) f.fig.suptitle(&#39;Barrick Gold&#39;, y=1.08) plt.show() . A paired graph allows you to see the distribution of data by showing the paired relationships in the data set and the univariate distribution of data for each variable. You can also use the palette to see how this data changed in different years. . The chart is particularly interesting for 2016 and 2019, as it looks like the price of the Pole stock, Barrick Gold and the price of gold are lined up along the same line. We can also conclude from the distribution charts that the price of gold and stocks moved gradually towards higher values. . 2.3 Violinplot for the gold price . plt.figure(figsize=(10,10)) sns.set_style(&quot;whitegrid&quot;) palette=sns.cubehelix_palette(5, start=2.8, rot=0, dark=0.2, light=0.8, reverse=False) sns.violinplot(x=&quot;year&quot;, y=&quot;gold_price&quot;, data=all_data_15[[&quot;gold_price&quot;,&quot;year&quot;]], inner=&quot;quart&quot;, palette=palette, trim=True) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Price gold&quot;) plt.show() . 2.4 Violinplot for multiple shares . sns.catplot(x=&quot;year&quot;, y=&quot;share_price_scaled&quot;, col=&#39;name&#39;, col_wrap=3,kind=&quot;violin&quot;, split=True, data=all_data_15,inner=&quot;quart&quot;, palette=palette, trim=True, height=4, aspect=1.2) sns.despine(left=True) . A large fluctuation in gold prices was noted according to the charts in 2016 and 2019. As you can see from the graphs in the following figure, some companies such as Newmont Mining, Barrick Gold, AngloGold Ashanti, Newcrest Mining and Polymetal were also affected. It should also be noted that all prices are marked in the range from 0 to 1 and this may lead to inaccuracies in the interpretation. . Next, we will build distribution charts for one Russian company - Polymetal and one foreign company - Barrick Gold . sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;POLY.ME&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . It is necessary to pay attention to the distribution of the share price for the two companies and it will become clear that the shape of the density graph is the same for them. . 2.5 Charts of the dependence of the share price of various companies on the price of gold . sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,line_kws={&#39;color&#39;: &#39;blue&#39;},scatter_kws={&#39;color&#39;: &#39;grey&#39;}).set(ylim=(0, 1)) plt.show() . In fact, you won&#39;t be able to see much on these charts, although some stocks seem to have a relationship. . The next step is to try to color the charts depending on the years. . palette=sns.cubehelix_palette(5, start=2, rot=0, dark=0, light=.95, reverse=False) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,hue=&quot;year&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,palette=palette,height=4).set(ylim=(0, 1)) plt.show() . Here the picture is a little better in the sense that some companies have a data cloud stretching along a straight line in some years, which may indicate the existence of a dependency. . 3 Machine learning and prediction . I will give a definition for machine learning from Wikipedia: Machine learning is a class of artificial intelligence methods that are characterized not by direct problem solving, but by learning in the process of applying solutions to many similar problems. To build such methods, we use mathematical statistics, numerical methods, optimization methods, probability theory, graph theory, and various techniques for working with data in digital form. . Usually, machine learning algorithms can be classified into the following categories: learning with a teacher and learning without a teacher. Here is their definition from one of the sites: . Supervised learning is one of the sections of machine learning dedicated to solving the following problem. There is a set of objects (situations) and the set of possible answers (responses, reactions). There is some relationship between responses and objects, but it is unknown. Only a finite set of use cases is known — the &quot;object, response&quot; pairs, called the training sample. Based on this data, you need to restore the dependency, that is, build an algorithm that can give a fairly accurate answer for any object. To measure the accuracy of responses, a quality functional is introduced in a certain way. see the Links) . Unsupervised learning is one of the sections of machine learning. Studies a wide class of data processing problems in which only descriptions of a set of objects (training sample) are known, and it is required to detect internal relationships, dependencies, and patterns that exist between objects. Learning without a teacher is often contrasted with learning with a teacher, when each training object is given a &quot;correct answer&quot;, and you need to find the relationship between the objects and the answers. see links) . The following machine learning methods will be discussed later: . Cluster analysis | Linear regression | Random forest | . Using these algorithms, you can evaluate overvalued or undervalued stocks relative to the price of gold and possible movement on the next day. I remind you that you must be very careful and use the conclusions from this post at your own risk. I also remind you that my main goal is to show the potential of machine learning for stock valuation. . 3.1. Cluster analysis for Barrick Gold stock . Clustering is the task of dividing a set of objects into groups called clusters. Each group should contain &quot;similar&quot; objects, and objects from different groups should be as different as possible. . from sklearn.cluster import KMeans poly=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;] # We need to scale also gold price, so clustering is not influenced by the relative size of one axis. poly=pd.DataFrame(poly) poly[&#39;gold_price_scaled&#39;] = scaler.fit_transform(poly[&quot;gold_price&quot;].to_frame()) poly[&quot;cluster&quot;] = KMeans(n_clusters=5, random_state=1).fit_predict(poly[[&quot;share_price_scaled&quot;,&quot;gold_price_scaled&quot;]]) # The 954 most common RGB monitor colors https://xkcd.com/color/rgb/ colors = [&quot;baby blue&quot;, &quot;amber&quot;, &quot;scarlet&quot;, &quot;grey&quot;,&quot;milk chocolate&quot;, &quot;windows blue&quot;] palette=sns.xkcd_palette(colors) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,ci=None,palette=palette, hue=&quot;cluster&quot;,fit_reg=0 ,data=poly) plt.show() . Cluster analysis is used in a large number of machine learning tasks. But I have given it only for informational purposes, since in this form it does not bring much benefit to our analysis. . 3.2. Linear regression between Barrick Gold shares and the gold price . Next, we will build a regular linear regression using training with a teacher. The goal is to estimate the forecast of data for the last 100 days of 2019 based on data from 2018/2019 (excluding estimated ones). Training data is the data used to build the model, and test data is the data that we will try to predict. . for sh in shares: print(sh) #Data Preparation share_18=pd.DataFrame() share_18=all_data_15[(all_data_15[&#39;name&#39;]==sh)] # Get data 2018/19 share_18=share_18[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Just using 1 variable for linear regression. Split the data into training/testing sets train = share_18[:-100] test = share_18[-100:] x_train=train[&quot;gold_price&quot;].to_frame() y_train=train[&#39;share_price&#39;].to_frame() x_test=test[&quot;gold_price&quot;].to_frame() y_test=test[&#39;share_price&#39;].to_frame() regr = LinearRegression() #Create linear regression object regr.fit(x_train,y_train) #Train the model using the training sets print(&quot;Coefficients: &quot;, float(regr.coef_)) print(np.corrcoef(x_train,y_train, rowvar=False)) y_pred = regr.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # Plot outputs using matplotlib plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . NMM.SG Coefficients: 0.6629423053739908 [[1. 0.790953] [0.790953 1. ]] Mean Absolute Error: 6.063058573972694 Mean Squared Error: 39.21188296210148 Root Mean Squared Error: 6.261939233344689 . GOLD Coefficients: 0.3355465472461071 [[1. 0.67139243] [0.67139243 1. ]] Mean Absolute Error: 3.3769293704374657 Mean Squared Error: 11.756813554455096 Root Mean Squared Error: 3.4288210152259473 . AU Coefficients: 0.31252669952857776 [[1. 0.67830589] [0.67830589 1. ]] Mean Absolute Error: 2.2471377544809683 Mean Squared Error: 5.789211153877581 Root Mean Squared Error: 2.4060779608893768 . KGC Coefficients: 0.10461302060876282 [[1. 0.78266367] [0.78266367 1. ]] Mean Absolute Error: 1.0583009847297946 Mean Squared Error: 1.1523726951635975 Root Mean Squared Error: 1.073486234268329 . NCM.AX Coefficients: 0.5623005799590818 [[1. 0.79891272] [0.79891272 1. ]] Mean Absolute Error: 2.0335289996635937 Mean Squared Error: 5.836462091267656 Root Mean Squared Error: 2.415877085297937 . PLZL.ME Coefficients: 103.84435014609612 [[1. 0.60373084] [0.60373084 1. ]] Mean Absolute Error: 1315.093426667142 Mean Squared Error: 1776892.2964767825 Root Mean Squared Error: 1333.0012364873419 . POLY.ME Coefficients: 10.772023429299809 [[1. 0.63694034] [0.63694034 1. ]] Mean Absolute Error: 69.33753863275061 Mean Squared Error: 6800.525447108329 Root Mean Squared Error: 82.46529844187995 . SELG.ME Coefficients: 0.15570348678870732 [[1. 0.51630147] [0.51630147 1. ]] Mean Absolute Error: 1.8096071903165585 Mean Squared Error: 4.039450515732427 Root Mean Squared Error: 2.009838430255633 . From the above charts, we can conclude that the price of gold predicts the price of shares of foreign companies on the next day quite well. In Russian companies, this picture looks much worse. Of course, there may be a false impression about Seligdar shares. But visual analysis of the chart allows you to discard this assumption. . 3.3 Random forest on Newmont Goldcorp shares against the price of gold and shares of gold companies . Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . The random forest algorithm accepts more than one variable in the input data to predict the output data. It works very efficiently on large amounts of data, can handle many input variables, has efficient methods for estimating missing data, and many other advantages. The main disadvantages are: . Random forests is slow to generate forecasts because it has many decision trees. Whenever it makes a forecast, all the trees in the forest must make a forecast for the same given input and then vote on it. This whole process takes a long time. | the Model is difficult to interpret compared to the decision tree, where you can easily make a decision by following the path in the tree. | One of the great advantages of a random forest is that it can be used for both classification and regression problems, which make up most of today&#39;s machine learning systems. I will talk about random forests in classification, since classification is sometimes considered a building block of machine learning. Below you can see what a random forest with two trees looks like: . In addition to the gold price, we will use other variables to forecast the Newmont Goldcorp share price. This will be the share prices of other foreign gold mining companies. I know it doesn&#39;t make a lot of sense, but we just want to see how to build this type of model. This will allow us to see the impact of each of them on the final forecast.Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . from sklearn.ensemble import RandomForestRegressor # 1.- Data Preparation nmm15=pd.DataFrame() nmm15=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NMM.SG&quot;) &amp; (all_data_15[&#39;year&#39;]&gt;2016 )] nmm15=nmm15[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Load share price of other variables nmm15[&#39;GOLD&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;GOLD&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;GOLD&#39;] = nmm15[&#39;GOLD&#39;].shift(1) nmm15[&#39;AU&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;AU&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;AU&#39;] = nmm15[&#39;AU&#39;].shift(1) nmm15[&#39;KGC&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;KGC&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;KGC&#39;] = nmm15[&#39;KGC&#39;].shift(1) nmm15[&#39;NCM.AX&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NCM.AX&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;NCM.AX&#39;] = nmm15[&#39;NCM.AX&#39;].shift(1) nmm15 = nmm15.drop(nmm15.index[0]) train = nmm15[:-100] test = nmm15[-100:] x_train=train[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]] y_train=train[&#39;share_price&#39;] x_test=test[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,]] y_test=test[&#39;share_price&#39;].to_frame() # 2.- Create Randomforest object usinig a max depth=5 regressor = RandomForestRegressor(n_estimators=200, max_depth=5 ) # 3.- Train data clf=regressor.fit(x_train, y_train) # 4.- Predict! y_pred=regressor.predict(x_test) y_pred_list = list(y_pred) y_pred=pd.DataFrame(y_pred) . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_pred=plt.scatter(nmm15[&quot;gold_price&quot;], regressor.predict(nmm15[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]]), color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train,plt_pred),(&quot;train data&quot;,&quot;prediction&quot;)) plt.show() . The resulting model looks really good in addition, we must remember that Random Forest has many more parameters to configure, but the key one is the maximum depth, which is unlimited by default. Next, we&#39;ll check how this model predicts or tests data. . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . y_pred = clf.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 1.410409517520304 Mean Squared Error: 3.0995744019029483 Root Mean Squared Error: 1.7605608202794212 . importances=regressor.feature_importances_ indices=list(x_train) print(&quot;Feature ranking:&quot;) for f in range(x_train.shape[1]): print(&quot;Feature %s (%f)&quot; % (indices[f], importances[f])) f, (ax1) = plt.subplots(1, 1, figsize=(8, 6), sharex=True) sns.barplot(indices, importances, palette=&quot;BrBG&quot;, ax=ax1) ax1.set_ylabel(&quot;Importance&quot;) . Feature ranking: Feature gold_price (0.627703) Feature GOLD (0.045197) Feature AU (0.040957) Feature KGC (0.038973) Feature NCM.AX (0.247171) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0, 0.5, &#39;Importance&#39;) . By the importance of the signs, it immediately becomes clear how strong the value of gold is. . In short, I hope I was able to reveal to you the beginnings of a project on using machine learning to study stock prices, and I hope to hear your comments. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "relUrl": "/finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Использование метода Монте-Карло для создания портфеля",
            "content": "Начинающие (да и не только) инвесторы часто задаются вопросом о том, как отобрать для себя идеальное соотношение активов входящих в портфель. Часто (или не очень, но знаю про двух точно) у некоторых брокеров эту функцию выполняет торговый робот. Но заложенные в них алгоритмы не раскрываются. . В этом посте будет рассмотрено то, как оптимизировать портфель при помощи Python и симуляции Монте Карло. Под оптимизацией портфеля понимается такое соотношение весов, которое будет удовлетворять одному из условий: . Портфель с минимальным уровнем риском при желаемой доходности; | Портфель с максимальной доходностью при установленном риске; | Портфель с максимальным значением доходности | . Для расчета возьмем девять акций, которые рекомендовал торговый робот одного из брокеров на начало января 2020 года и так же он устанавливал по ним оптимальные веса в портфеле: &#39;ATVI&#39;,&#39;BA&#39;,&#39;CNP&#39;,&#39;CMA&#39;, &#39;STZ&#39;,&#39;GPN&#39;,&#39;MPC&#39;,&#39;NEM&#39; и &#39;PKI&#39;. Для анализа будет взяты данные по акциям за последние три года. . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt # Получаем данные по акциям ticker = [&#39;ATVI&#39;,&#39;BA&#39;,&#39;CNP&#39;,&#39;CMA&#39;, &#39;STZ&#39;,&#39;GPN&#39;,&#39;MPC&#39;,&#39;NEM&#39;, &#39;PKI&#39;] stock = yf.download(ticker,&#39;2017-01-01&#39;, &#39;2019-01-31&#39;) . [*********************100%***********************] 9 of 9 completed . Если сложить долю всех акций, входящих в портфель, то сумма должна стремиться к единице (а лучше быть равна). Дальше как обычно проведем подготовку данных для расчетов: . all_adj_close = stock[[&#39;Adj Close&#39;]] # ежедневная доходность all_returns = all_adj_close.pct_change() # узнаем среднюю доходность и получаем ковариационную матрицу mean_returns = all_returns.mean() cov_matrix = all_returns.cov() . Теперь можно провести расчет для весов предложенных торговым роботом и узнать доходность данного портфеля за последник три года и стандартное отклонение. . robot = np.array([0.0441, 0.1030, 0.1086, 0.2070, 0.1525, 0.0714, 0.0647, 0.1828, 0.0661]) # доходность, стандартное отклонение и коэффициент Шарпо portfolio_return_robot = np.sum(mean_returns * robot) portfolio_std_dev_robot = np.sqrt(np.dot(robot.T,np.dot(cov_matrix, robot))) sharpo_robot = portfolio_return_robot/portfolio_std_dev_robot # объединим полученные значения в таблицу и выведем ее robot_result = np.array([portfolio_return_robot, portfolio_std_dev_robot, sharpo_robot]) robot_result = np.array([portfolio_return_robot, portfolio_std_dev_robot, sharpo_robot]) robot_result = np.concatenate((robot_result, robot), axis=0) robot_sim_result = pd.DataFrame(robot_result, columns=[&#39;Robot&#39;], index=[&#39;ret&#39;,&#39;stdev&#39;,&#39;sharpe&#39;,ticker[0],ticker[1],ticker[2],ticker[3],ticker[4],ticker[5],ticker[6],ticker[7],ticker[8]]) print(robot_sim_result) . Robot ret 0.000852 stdev 0.008635 sharpe 0.098683 ATVI 0.044100 BA 0.103000 CNP 0.108600 CMA 0.207000 STZ 0.152500 GPN 0.071400 MPC 0.064700 NEM 0.182800 PKI 0.066100 . Симуляция Монте-Карло . Первоначально небольшое вступительное слово о том, как используется метод Монте-Карла для оптимизации портфеля . Сначала акциям задаются случайные веса, после чего производится расчет доходности и стандартного отклонения. Полученные значения сохраняются. Следующим шагом случайным образом меняются веса (главное не забывать, что их сумма должна составлять единицу) и все повторяется — расчет и сохранение полученного значения. Количество итераций зависит от времени, мощностей компьютера для расчета и рисков, который готов принять инвестор. В этот раз попробуем провести 10000 расчетов для выявления портфеля с минимальным убытком и максимальным значением коэффициента Шарпа. . num_iterations = 10000 simulation_res = np.zeros((4+len(ticker)-1,num_iterations)) # сама итерация for i in range(num_iterations): #Выбрать случайные веса и нормализовать, чтоб сумма равнялась 1 weights = np.array(np.random.random(9)) weights /= np.sum(weights) #Вычислить доходность и стандартное отклонение portfolio_return = np.sum(mean_returns * weights) portfolio_std_dev = np.sqrt(np.dot(weights.T,np.dot(cov_matrix, weights))) #Сохранить все полученные значения в массив simulation_res[0,i] = portfolio_return simulation_res[1,i] = portfolio_std_dev #Вычислить коэффициент Шарпа и сохранить simulation_res[2,i] = simulation_res[0,i] / simulation_res[1,i] #Сохранить веса for j in range(len(weights)): simulation_res[j+3,i] = weights[j] # сохраняем полученный массив в DataFrame для построения данных и анализа. sim_frame = pd.DataFrame(simulation_res.T,columns=[&#39;ret&#39;,&#39;stdev&#39;,&#39;sharpe&#39;,ticker[0],ticker[1],ticker[2],ticker[3],ticker[4],ticker[5],ticker[6],ticker[7],ticker[8]]) . Теперь можно рассчитать портфель с максимальным коэффициентом Шарпа или минимальным риска. . max_sharpe = sim_frame.iloc[sim_frame[&#39;sharpe&#39;].idxmax()] # узнать минимальное стандартное отклонение min_std = sim_frame.iloc[sim_frame[&#39;stdev&#39;].idxmin()] print (&quot;The portfolio for max Sharpe Ratio: n&quot;, max_sharpe) print (&quot;The portfolio for min risk: n&quot;, min_std) . The portfolio for max Sharpe Ratio: ret 0.001258 stdev 0.010977 sharpe 0.114567 ATVI 0.065653 BA 0.377049 CNP 0.028034 CMA 0.102441 STZ 0.149974 GPN 0.091727 MPC 0.020134 NEM 0.155008 PKI 0.009980 Name: 9001, dtype: float64 The portfolio for min risk: ret 0.000546 stdev 0.007489 sharpe 0.072923 ATVI 0.020115 BA 0.021464 CNP 0.179417 CMA 0.256539 STZ 0.035221 GPN 0.100015 MPC 0.209400 NEM 0.076111 PKI 0.101716 Name: 9578, dtype: float64 . Ну а самое важное представление можно получить, когда данные визуализируешь: . fig, ax = plt.subplots(figsize=(10, 10)) #Создать разноцветный график scatter plot для различных значений коэффициента Шарпо по оси x и стандартного отклонения по оси y plt.scatter(sim_frame.stdev,sim_frame.ret,c=sim_frame.sharpe,cmap=&#39;RdYlBu&#39;) plt.xlabel(&#39;Standard Deviation&#39;) plt.ylabel(&#39;Returns&#39;) plt.ylim(0,.0015) plt.xlim(0.007,0.012) plt.scatter(max_sharpe[1],max_sharpe[0],marker=(5,1,0),color=&#39;r&#39;,s=600) plt.scatter(min_std[1],min_std[0],marker=(5,1,0),color=&#39;b&#39;,s=600) plt.scatter(portfolio_std_dev_robot, portfolio_return_robot,marker=(5,1,0),color=&#39;g&#39;,s=600) plt.show() . Портфель с максимальным коэффициентом Шарпа показан красной звездой, синей — с минимальным стандартным отклонением и зеленой — предложенный роботом. Как видно — портфель, предложенный роботом, не совпадает с этими показателями, но на каком остановиться портфеле — выбор остается за инвестором. А я постараюсь в конце года вернуться к сравнению портфелей. А сейчас все три портфеля находятся в просадке. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/05/03/using-monte-carlo-method-create-portfolio.html",
            "relUrl": "/finance/investment/python/2020/05/03/using-monte-carlo-method-create-portfolio.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Общий финансовый анализ на Python (Часть 3)",
            "content": "После всех вычислений, приведенных в прошлых двух публикациях, можно углубиться в статистический анализ и рассмотреть метод наименьших квадратов. Для этой цели используется библиотека statsmodels, которая позволяет пользователям исследовать данные, оценивать статистические модели и выполнять статистические тесты. За основу были взяты эта статья и эта статья. Само описание используемой функции на английском доступно по следующей ссылке. . Сначала немного теории: . О линейной регрессии . Линейная регрессия используется в качестве прогнозирующей модели, когда предполагается линейная зависимость между зависимой переменной (переменная, которую мы пытаемся предсказать) и независимой переменной (переменная и/или переменные, используемые для предсказания). . В самом простой случае при рассмотрении используется одна переменная на основании которой мы пытаемся предсказать другую. Формула в этом случае имеет следующий вид: . Y = C + M*X . Y - зависимая переменная (результат / прогноз / оценка) | C - Константа (Y-Intercept) | M - Наклон линии регрессии (угловой коэффициент или градиент оценённой линии; она представляет собой величину, на которую Y увеличивается в среднем, если мы увеличиваем X на одну единицу) | X - независимая переменная (предиктор, используемый в прогнозе Y) | . В действительности так же может существовать связь между зависимой переменной и несколькими независимыми переменными. Для этих типов моделей (при условии линейности) мы можем использовать множественную линейную регрессию следующего вида: . Y = C + M1X1 + M2X2 + … . Бета — коэффициент . Про данный коэффициент написано уже много, для примера на этой странице . Коротко, если не вдаваться в подробности, то можно его охарактеризовать следующим образом: . Акции c бета-коэффициентом: . ноль указывает на отсутствие корреляции между акцией и индексом | единица указывает на то, что акция имеет ту же волатильность, что и индекс | больше одного — указывает на большую доходность (а следовательно и риски) акции, чем индекс | менее единицы — менее волатильная акция, чем индекса | . Другими словами, если акция увеличится на 14%, в то время как рынок вырос всего на 10%, то бета-коэффициент акции составит 1,4. Как правило на рынках с более высоким бета-коэффициентом можно предположить лучшие условия для вознаграждения (а следовательно и для риска). . . Практика . Следующий код Python включает в себя пример линейной регрессии, где входной переменной является доходность по Индексу МосБиржи, а оцениваемая переменная — доходность по акциям Аэрофлот. . Для того, чтобы отсутствовала необходимость вспоминать как загружать данные и приводить данные к форме, необходимой для расчета — код приводиться с момента загрузки данных и до получения результатов. Вот полный синтаксис для выполнения линейной регрессии в Python с использованием statsmodels: . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt import statsmodels.api as sm #Загружаю данные ticker = [&#39;AFLT.ME&#39;,&#39;IMOEX.ME&#39;] stock = yf.download(ticker) # Выделение скорректированой цены закрытия all_adj_close = stock[[&#39;Adj Close&#39;]] # Вычисление доходности all_returns = np.log(all_adj_close / all_adj_close.shift(1)) # Выделение доходности по акциям aflt_returns = all_returns[&#39;Adj Close&#39;][[&#39;AFLT.ME&#39;]].fillna(0) # Выделение доходности по индексу МосБиржи moex_returns = all_returns[&#39;Adj Close&#39;][[&#39;IMOEX.ME&#39;]].fillna(0) # Создание нового DataFrame return_data = pd.concat([aflt_returns, moex_returns], axis=1)[1:] return_data.columns = [&#39;AFLT.ME&#39;, &#39;IMOEX.ME&#39;] # Добавляем столбец единиц и определяем X и y X = sm.add_constant(return_data[&#39;IMOEX.ME&#39;]) y = return_data[&#39;AFLT.ME&#39;] # Создание модели model_moex = sm.OLS(y,X).fit() # Вывод результатов print(model_moex.summary()) . [*********************100%***********************] 2 of 2 completed OLS Regression Results ============================================================================== Dep. Variable: AFLT.ME R-squared: 0.135 Model: OLS Adj. R-squared: 0.135 Method: Least Squares F-statistic: 454.5 Date: Mon, 18 Oct 2021 Prob (F-statistic): 7.89e-94 Time: 19:33:22 Log-Likelihood: 7260.3 No. Observations: 2909 AIC: -1.452e+04 Df Residuals: 2907 BIC: -1.450e+04 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] const -7.886e-05 0.000 -0.213 0.831 -0.001 0.001 IMOEX.ME 0.8101 0.038 21.320 0.000 0.736 0.885 ============================================================================== Omnibus: 651.369 Durbin-Watson: 1.857 Prob(Omnibus): 0.000 Jarque-Bera (JB): 22810.853 Skew: -0.295 Prob(JB): 0.00 Kurtosis: 16.706 Cond. No. 103. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . На сайте yahoo и Мосбиржи бета коэффициент отличается незначительно в большую сторону. Но надо честно признаться, что расчет для некоторых других акций с российской биржи показал более значительные отличия, но в пределах интервала. . . Тот же анализ для акции FB и индекса SP500. Здесь вычисление, как в оригинале, проводится через месячную доходность. . sp_500 = yf.download(&#39;^GSPC&#39;, start=&quot;2012-05-01&quot;) fb = yf.download(&#39;FB&#39;) # Пересчет в месячную доходность fb = fb.resample(&#39;BM&#39;).apply(lambda x: x[-1]) sp_500 = sp_500.resample(&#39;BM&#39;).apply(lambda x: x[-1]) monthly_prices = pd.concat([fb[&#39;Close&#39;], sp_500[&#39;Close&#39;]], axis=1) monthly_prices.columns = [&#39;FB&#39;, &#39;^GSPC&#39;] monthly_returns = monthly_prices.pct_change(1) clean_monthly_returns = monthly_returns.dropna(axis=0) X = clean_monthly_returns[&#39;^GSPC&#39;] y = clean_monthly_returns[&#39;FB&#39;] X1 = sm.add_constant(X) model_fb_sp_500 = sm.OLS(y, X1) results_fb_sp_500 = model_fb_sp_500.fit() print(results_fb_sp_500.summary()) . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed OLS Regression Results ============================================================================== Dep. Variable: FB R-squared: 0.181 Model: OLS Adj. R-squared: 0.174 Method: Least Squares F-statistic: 24.59 Date: Mon, 18 Oct 2021 Prob (F-statistic): 2.56e-06 Time: 19:47:24 Log-Likelihood: 110.11 No. Observations: 113 AIC: -216.2 Df Residuals: 111 BIC: -210.8 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] const 0.0131 0.009 1.446 0.151 -0.005 0.031 ^GSPC 1.1515 0.232 4.959 0.000 0.691 1.612 ============================================================================== Omnibus: 26.778 Durbin-Watson: 1.820 Prob(Omnibus): 0.000 Jarque-Bera (JB): 104.750 Skew: 0.671 Prob(JB): 1.79e-23 Kurtosis: 7.522 Cond. No. 26.8 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . . В этом случае все совпало и подтвердило возможность использование statsmodels для определения коэффициента бета. . Ну и в качестве бонуса — если Вы хотите получить только бета — коэффициент и остальную статистику вы хотите оставить в стороне, то предлагается еще один код для его расчета: . from scipy import stats slope, intercept, r_value, p_value, std_err = stats.linregress(X, y) print(slope) . 1.1515416131106546 . Правда это не означает, что всю остальные получаемые значения надо игнорировать, но для их понимания понадобятся знание статистики. Приведу небольшую выдержку из получаемых значений: . R-squared, который является коэффициентом детерминации и принимает значения от 0 до 1. Чем ближе значение коэффициента к 1, тем сильнее зависимость; | Adj. R-squared — скорректированный R-squared на основании числа наблюдений и числа степеней свободы; | std err — стандартная ошибка оценки коэффициента; | P&gt;|t| — р-значение Величина менее 0,05 считается статистически значимой; | 0.025 и 0.975 — нижнее и верхнее значения доверительного интервала. | и т.д. | . На этом пока что все. Конечно, представляет интерес поискать зависимость между различными величинами для того, чтобы через одну предсказать другую и получить профит. В одном из иностранных источников встретилось предсказание индекса через процентную ставку и уровень безработицы. Но если изменение процентной ставки в России можно взять с сайта Центробанка, то другие пока продолжаю искать. К сожалению, на сайте Росстата не удалось найти актуальные. Это заключительная публикация в рамках статей общего финансового анализа. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/04/12/general-financial-analysis-python-part-3.html",
            "relUrl": "/finance/investment/python/2020/04/12/general-financial-analysis-python-part-3.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Общий финансовый анализ на Python (Часть 2)",
            "content": "Скользящее окно (Moving Windows) . В заголовке я привел дословный перевод. Если кто меня поправит, и другой термин более применим — то спасибо. . Смысл скользящего окна– с каждым новым значением функция пересчитывается за заданный период времени. Этих функций большое количество. Для примера: rolling.mean(), rolling.std(), которые чаще всего и используют при анализе движения акций. rolling.mean() — это обычная скользящая средняя, которая сглаживает краткосрочные колебания и позволяет визуализировать общую тенденцию. . adj_close_px = sber[&#39;Adj Close&#39;] # Вычисляю скользящую среднию moving_avg = adj_close_px.rolling(window=40).mean() # Вывожу результат print(moving_avg[-10:]) . Date 2021-10-04 329.640254 2021-10-05 330.483754 2021-10-06 331.295254 2021-10-07 332.290504 2021-10-08 333.397253 2021-10-11 334.841753 2021-10-12 335.932753 2021-10-13 336.992754 2021-10-14 338.135253 2021-10-15 339.242503 Name: Adj Close, dtype: float64 . График, который позволит понять то, что получается в результате работы данной функции: . sber[&#39;40&#39;] = adj_close_px.rolling(window=40).mean() # Вычисление длинной скользящей средней sber[&#39;252&#39;] = adj_close_px.rolling(window=252).mean() # Построение полученных значений sber[[&#39;Adj Close&#39;, &#39;40&#39;, &#39;252&#39;]].plot(figsize=(20,20)) plt.show() . Как видно rolling.mean() справляется с поставленной задачей. Функция сглаживает краткосрочные колебания и позволяет увидеть долгосрочный тренд на основании которого можно принять решение: цена выше рассматриваемой скользящей средней — берем акцию, ниже — продаем акцию — если просто и я бы не советовал следовать этому методу. Как правило помимо скользящих средних используются и другие индикаторы, которые могут подтвердить правильность принимаемого решения. Каждый должен самостоятельно принять решение в зависимости от стиля торговли. . Волатильность . Волатильность акций — это изменение дисперсии доходности акций в течение определенного периода времени. Обычно сравнивают волатильность одной акции с другой, чтобы получить представление о том, какая может иметь больший риск, или с рыночным индексом, чтобы понять волатильность акций относительно рынка. Как правило, чем выше волатильность, тем рискованнее инвестиции в эту акцию. Необходимо отметить, что она не является постоянной и изменяется с течением времени. Это можно увидеть опять же при помощи функции rolling.std(), входящей в пакет pandas. Пример расчета изменения волатильности: . min_periods = 60 # Вычисляю волатильность vol = daily_pct_change.rolling(min_periods).std() * np.sqrt(min_periods) # Строю график vol.plot(figsize=(15, 5)) plt.show() . Прошу обратить внимание, что в отличие от прошлой недели у меня появилось еще два значения — индекс московской биржи (IMOEX.ME) и РБК (RBCM.ME). Их значения мне потребуются в следующей публикации про метод наименьших квадратов. А на сегодня все. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/03/29/general-financial-analysis-python-part-2.html",
            "relUrl": "/finance/investment/python/2020/03/29/general-financial-analysis-python-part-2.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Общий финансовый анализ на Python (Часть 1)",
            "content": "В прошлой статье рассмотрено как можно получить информацию по финансовым инструментам. Дальше будет опубликовано несколько статей о том, что первоначально можно делать с полученными данными, как проводить анализ и составлять стратегию. Материалы составлены на основании публикаций в иностранных источниках и курсах на одной из онлайн платформ. . В этой статье будет рассмотрено, как рассчитывать доходность, волатильность и построить один из основных индикаторов. . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt sber = yf.download(&#39;SBER.ME&#39;,&#39;2016-01-01&#39;) . [*********************100%***********************] 1 of 1 completed . Доходность . Данная величина представляет собой процентное изменение стоимости акции за один торговый день. Оно не учитывает дивиденды и комиссии. Его легко рассчитать используя функцию pct_change () из пакета Pandas. . Как правило используют лог доходность, так как она позволяет лучше понять и исследовать изменения с течением времени. . daily_close = sber[[&#39;Adj Close&#39;]] # Дневная доходность daily_pct_change = daily_close.pct_change() # Заменить NA значения на 0 daily_pct_change.fillna(0, inplace=True) print(daily_pct_change.head()) # Дневная лог доходность daily_log_returns = np.log(daily_close.pct_change()+1) print(daily_log_returns.head()) . Adj Close Date 2016-01-04 0.000000 2016-01-05 0.008979 2016-01-06 -0.020629 2016-01-11 -0.060093 2016-01-12 0.007470 Adj Close Date 2016-01-04 NaN 2016-01-05 0.008939 2016-01-06 -0.020845 2016-01-11 -0.061974 2016-01-12 0.007442 . Чтобы из полученных данных узнать недельную и/или месячную доходность, используют функцию resample(). . monthly = sber.resample(&#39;BM&#39;).apply(lambda x: x[-1]) # Месячная доходность print(monthly.pct_change().tail()) # Пересчитать `sber` по кварталам и взять среднее значение за квартал quarter = sber.resample(&quot;4M&quot;).mean() # Квартальную доходность print(quarter.pct_change().tail()) . Open High Low Close Adj Close Volume Date 2021-06-30 -0.010055 -0.018202 -0.017745 -0.015593 -0.015593 0.363875 2021-07-30 -0.012117 -0.000782 -0.007253 0.001604 0.001604 0.589072 2021-08-31 0.086022 0.077640 0.086870 0.072213 0.072213 -0.344803 2021-09-30 0.004505 0.035464 0.006936 0.033522 0.033522 1.449116 2021-10-29 0.145282 0.105815 0.123980 0.093531 0.093531 -0.536555 Open High Low Close Adj Close Volume Date 2020-09-30 0.056380 0.052326 0.063663 0.059189 0.231932 -0.288569 2021-01-31 0.127438 0.128327 0.125520 0.126951 0.263348 0.268118 2021-05-31 0.159379 0.156331 0.162285 0.159680 0.174569 -0.295427 2021-09-30 0.101638 0.099245 0.104338 0.101912 0.156688 -0.429262 2022-01-31 0.156123 0.168087 0.150327 0.160703 0.160703 0.702144 . Функция pct_change () удобна для использования, но в свою очередь скрывает то, как получается значение. Схожее вычисление, которое поможет понять механизм, можно выполнить при помощи shift() из пакета из пакета Pandas. Дневная цена закрытия делится на прошлую (сдвинутую на один) цену и из полученного значения вычитается единица. Но есть один незначительный минус – первое значение в результате получается NA. . Расчет доходности основан на формуле: . . Дальше строится диаграмма распределения доходности и рассчитывается основная статистика: . Для значений по российским акциям есть небольшая тонкость. К названию акцию добавляется точка и заглавными буквами ME. Спасибо знатоки на смартлабе подсказали. . daily_pct_change = daily_close / daily_close.shift(1) - 1 print(daily_pct_change.head()) . Adj Close Date 2016-01-04 NaN 2016-01-05 0.008979 2016-01-06 -0.020629 2016-01-11 -0.060093 2016-01-12 0.007470 . daily_pct_change.hist(bins=50) plt.show() # Общая статистика print(daily_pct_change.describe()) . Adj Close count 1454.000000 mean 0.001442 std 0.018295 min -0.161417 25% -0.008009 50% 0.000708 75% 0.011137 max 0.128987 . Распределение выглядит очень симметрично и нормально распределённым вокруг значения 0,00. Для получения других значений статистики используется функция description (). В результате видно, что среднее значение немного больше нуля, а стандартное отклонение составляет практически 0,02. . Кумулятивная доходность . Кумулятивная дневная прибыль полезна для определения стоимости инвестиций через определенные промежуток времени. Ее можно рассчитать, как приводится в коде ниже. . cum_daily_return = (1 + daily_pct_change).cumprod() print(cum_daily_return.tail()) . Adj Close Date 2021-10-11 6.651933 2021-10-12 6.504360 2021-10-13 6.531676 2021-10-14 6.467939 2021-10-15 6.371218 . cum_daily_return.plot(figsize=(8,5)) plt.show() . Можно пересчитать доходность в месячном периоде: . cum_monthly_return = cum_daily_return.resample(&quot;M&quot;).mean() print(cum_monthly_return.tail()) . Adj Close Date 2021-06-30 5.354601 2021-07-31 5.182009 2021-08-31 5.584808 2021-09-30 5.653339 2021-10-31 6.318509 . Знание того, как рассчитать доходность, является ценным при анализе акции. Но еще большую ценность оно представляет при сравнении с другими акциями. . Возьмем некоторые акции (выбор их совершенно случайный) и построим их диаграмму. . ticker = [&#39;AFLT.ME&#39;,&#39;DSKY.ME&#39;,&#39;IRAO.ME&#39;,&#39;PIKK.ME&#39;, &#39;PLZL.ME&#39;,&#39;SBER.ME&#39;,&#39;ENRU.ME&#39;] stock = yf.download(ticker,&#39;2018-01-01&#39;) # Дневная доходность в `daily_close_px` daily_pct_change = stock[&#39;Adj Close&#39;].pct_change() # Распределение daily_pct_change.hist(bins=50, sharex=True, figsize=(20,8)) plt.show() . [*********************100%***********************] 7 of 7 completed . Еще один полезный график —матрица рассеяния. Ее можно легко построить при помощи функции scatter_matrix (), входящей в библиотеку pandas. В качестве аргументов используется daily_pct_change и устанавливается параметр Ядерной оценки плотности — Kernel Density Estimation. Кроме того, можно установить прозрачность с помощью параметра alpha и размер графика с помощью параметра figsize. . from pandas.plotting import scatter_matrix # Матрица рассеивания `daily_pct_change` scatter_matrix(daily_pct_change, diagonal=&#39;kde&#39;, alpha=0.1,figsize=(20,20)) plt.show() . На этом пока все. В следующей статье будет рассмотрено вычисление волатильности, средней и использование метода наименьших квадратов. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/03/14/general-financial-analysis-python-part-1.html",
            "relUrl": "/finance/investment/python/2020/03/14/general-financial-analysis-python-part-1.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Получение котировок акций при помощи Python (перевод)",
            "content": "Представляю вашему вниманию перевод статьи «Historical Stock Price Data in Python» автора Ishan Shah. . Статья о том, как получить ежедневные исторические данные по акциям, используя yfinance, и минутные данные, используя alpha vantage. . Как вы знаете, акции относятся к очень волатильному инструменту и очень важно тщательно анализировать поведение цены, прежде чем принимать какие-либо торговые решения. Ну а сначала надо получить данные и python может помочь в этом. . Биржевые данные могут быть загружены при помощи различных пакетов. В этой статье будут рассмотрены yahoo finance. . Его можно установить при помощи команды pip install yfinance. Приведенный ниже код показывает, как получить данные для AAPL с 2016 по 2019 год и построить скорректированную цену закрытия (скорректированная цена закрытия на дивиденды и сплиты) на графике . import yfinance as yf # Get the data for the stock AAPL data = yf.download(&#39;AAPL&#39;,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;) # Import the plotting library import matplotlib.pyplot as plt %matplotlib inline # Plot the close price of the AAPL data[&#39;Adj Close&#39;].plot() plt.show() . [*********************100%***********************] 1 of 1 completed . Ну а если необходимо получить по нескольким акциям, то необходимо внести небольшое дополнение в код. Для хранения значений используется DataFrame. При помощи пакета matplotlib и полученных данных можно построить график дневной доходности. . import pandas as pd tickers_list = [&#39;AAPL&#39;, &#39;WMT&#39;, &#39;IBM&#39;, &#39;MU&#39;, &#39;BA&#39;, &#39;AXP&#39;] # Import pandas data = pd.DataFrame(columns=tickers_list) # Fetch the data for ticker in tickers_list: data[ticker] = yf.download(ticker,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;)[&#39;Adj Close&#39;] # Print first 5 rows of the data data.head() . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . AAPL WMT IBM MU BA AXP . Date . 2015-12-31 24.266079 | 54.044613 | 106.805664 | 14.140234 | 129.673157 | 63.341213 | . 2016-01-04 24.286833 | 54.185669 | 105.509621 | 14.309997 | 126.005119 | 61.556175 | . 2016-01-05 23.678221 | 55.472874 | 105.431984 | 14.799313 | 126.516304 | 60.609035 | . 2016-01-06 23.214846 | 56.028313 | 104.904243 | 14.200150 | 124.507416 | 58.925949 | . 2016-01-07 22.235073 | 57.333141 | 103.111473 | 13.640932 | 119.287819 | 58.395412 | . # Plot all the close prices ((data.pct_change()+1).cumprod()).plot(figsize=(10, 7)) # Show the legend plt.legend() # Define the label for the title of the figure plt.title(&quot;Adjusted Close Price&quot;, fontsize=16) # Define the labels for x-axis and y-axis plt.ylabel(&#39;Price&#39;, fontsize=14) plt.xlabel(&#39;Year&#39;, fontsize=14) # Plot the grid lines plt.grid(which=&quot;major&quot;, color=&#39;k&#39;, linestyle=&#39;-.&#39;, linewidth=0.5) plt.show() . Для значений по российским акциям есть небольшая тонкость. К названию акцию добавляется точка и заглавными буквами ME. Спасибо знатоки на смартлабе подсказали. . tickers_list_rus = [&#39;TTLK.ME&#39;, &#39;GMKN.ME&#39;, &#39;LSRG.ME&#39;, &#39;TATNP.ME&#39;, &#39;MSTT.ME&#39;,&#39;YNDX.ME&#39;] data_rus = pd.DataFrame(columns=tickers_list_rus) . for ticker in tickers_list_rus: data_rus[ticker] = yf.download(ticker,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . data_rus.head() . TTLK.ME GMKN.ME LSRG.ME TATNP.ME MSTT.ME YNDX.ME . Date . 2016-01-04 0.091873 | 5647.466797 | 410.957520 | 112.794388 | 52.599274 | 1064.099976 | . 2016-01-05 0.092520 | 5750.643066 | 422.438416 | 112.513115 | 52.870754 | 1133.900024 | . 2016-01-06 0.092844 | 5690.508789 | 430.975464 | 111.613007 | 52.802883 | 1112.000000 | . 2016-01-11 0.092844 | 5443.646973 | 409.485657 | 110.150345 | 52.056313 | 987.000000 | . 2016-01-12 0.095755 | 5413.896484 | 400.948547 | 108.575157 | 51.581223 | 999.000000 | . ((data_rus.pct_change()+1).cumprod()).plot(figsize=(10, 7)) # Show the legend plt.legend() # Define the label for the title of the figure plt.title(&quot;Adjusted Close Price&quot;, fontsize=16) # Define the labels for x-axis and y-axis plt.ylabel(&#39;Price&#39;, fontsize=14) plt.xlabel(&#39;Year&#39;, fontsize=14) # Plot the grid lines plt.grid(which=&quot;major&quot;, color=&#39;k&#39;, linestyle=&#39;-.&#39;, linewidth=0.5) plt.show() . В дальнейшем эти данные можно проанализировать, создать торговую стратегию и оценить эффективность при помощи пакета pyfolio. В нем можно оценить коэффициент Шарпа, коэффициент Сортино, максимальную просадку и многие другие необходимые показатели. . Надеюсь, что мой перевод оригинальной статьи будет для Вас полезен. Код был проверен и все работает. Но пока для меня остался вопрос в возможности использования Alpha vantage для российского рынка. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/02/09/getting-stock-quotes-using-python.html",
            "relUrl": "/finance/investment/python/2020/02/09/getting-stock-quotes-using-python.html",
            "date": " • Feb 9, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Выпускной проект "Идентификация интернет-пользователей" Week 1",
            "content": "Данную дипломную работу делали 7 недель. Это задание на первую неделю. . Идентификация интернет-пользователей . ﻿В этом проекте мы будем решать задачу идентификации пользователя по его поведению в сети Интернет. Это сложная и интересная задача на стыке анализа данных и поведенческой психологии. В качестве примера, компания Яндекс решает задачу идентификации взломщика почтового ящика по его поведению. В двух словах, взломщик будет себя вести не так, как владелец ящика: он может не удалять сообщения сразу по прочтении, как это делал хозяин, он будет по-другому ставить флажки сообщениям и даже по-своему двигать мышкой. Тогда такого злоумышленника можно идентифицировать и &quot;выкинуть&quot; из почтового ящика, предложив хозяину войти по SMS-коду. Этот пилотный проект описан в статье на Хабрахабре. Похожие вещи делаются, например, в Google Analytics и описываются в научных статьях, найти можно многое по фразам &quot;Traversal Pattern Mining&quot; и &quot;Sequential Pattern Mining&quot;. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . Мы будем решать похожую задачу: по последовательности из нескольких веб-сайтов, посещенных подряд одним и тем же человеком, мы будем идентифицировать этого человека. Идея такая: пользователи Интернета по-разному переходят по ссылкам, и это может помогать их идентифицировать (кто-то сначала в почту, потом про футбол почитать, затем новости, контакт, потом наконец – работать, кто-то – сразу работать, если это возможно). . Будем использовать данные из статьи &quot;A Tool for Classification of Sequential Data&quot;. И хотя мы не можем рекомендовать эту статью (описанные методы далеки от state-of-the-art, лучше обращаться к книге &quot;Frequent Pattern Mining&quot; и последним статьям с ICDM), но данные там собраны аккуратно и представляют интерес. . Имеются данные с прокси-серверов Университета Блеза Паскаля, их вид очень простой: ID пользователя, timestamp, посещенный веб-сайт. . Скачать исходные данные можно по ссылке в статье (там же описание), для этого задания хватит данных не по всем 3000 пользователям, а по 10 и 150. Ссылка на архив capstone_user_identification.zip (~7 Mb, в развернутом виде ~60 Mb). . В ходе выполнения проекта вас ожидает 4 задания типа Programming Assignment, посвященных предобработке данных, первичному анализу, визуальному анализу данных, сравнению моделей классификации и настройке выбранной модели и изучению ее переобучения. Также у вас будет 3 взаимно оцениваемых задания (Peer Review) – по визуализации данных (в том числе со свеже созданными признаками), по оценке результатов участия в соревновании Kaggle Inclass и по всему проекту в целом. . В ходе проекта мы будем работать с библиотекой Vowpal Wabbit. Если будут проблемы с ее установкой, можно воспользоваться Docker-образом, например, тем, что описан в Wiki репозитория открытого курса OpenDataScience по машинному обучению. . План проекта такой: . 1 неделя. Подготовка данных к анализу и построению моделей. Programming Assignment . Первая часть проекта посвящена подготовке данных для дальнейшего описательного анализа и построения прогнозных моделей. Надо будет написать код для предобработки данных (исходно посещенные веб-сайты указаны для каждого пользователя в отдельном файле) и формирования единой обучающей выборки. Также в этой части мы познакомимся с разреженным форматом данных (матрицы Scipy.sparse), который хорошо подходит для данной задачи. . Подготовка обучающей выборки | Работа с разреженным форматом данных | . 2 неделя. Подготовка и первичный анализ данных. Programming Assignment . На второй неделе мы продолжим подготовливать данные для дальнейшего анализа и построения прогнозных моделей. Конкретно, раньше мы определили что сессия – это последовательность из 10 посещенных пользователем сайтов, теперь сделаем длину сессии параметром, и потом при обучении прогнозных моделей выберем лучшую длину сессии. Также мы познакомимся с предобработанными данными и статистически проверим первые гипотезы, связанные с нашими наблюдениями. . Подготовка нескольких обучающих выборок для сравнения | Первичный анализ данных, проверка гипотез | . 3 неделя. Визуальный анализ данных построение признаков. Peer-Review . На 3 неделе мы займемся визуальным анализом данных и построением признаков. Сначала мы вместе построим и проанализируем несколько признаков, потом Вы сможете сами придумать и описать различные признаки. Задание имеет вид Peer-Review, так что творчество здесь активно приветствуется. Если задействуете IPython-виджеты, библиотеку Plotly, анимации и прочий интерактив, всем от этого будет только лучше. . Визуальный анализ данных | Построение признаков | . 4 неделя. Сравнение алгоритмов классификации. Programming Assignment . Тут мы наконец подойдем к обучению моделей классификации, сравним на кросс-валидации несколько алгоритмов, разберемся, какие параметры длины сессии (session_length и window_size) лучше использовать. Также для выбранного алгоритма построим кривые валидации (как качество классификации зависит от одного из гиперпараметров алгоритма) и кривые обучения (как качество классификации зависит от объема выборки). . Сравнение нескольких алгоритмов на сессиях из 10 сайтов | Выбор параметров – длины сессии и ширины окна | Идентификация конкретного пользователя и кривые обучения | . 5 неделя. Соревнование Kaggle Inclass по идентификации пользователей. Peer-Review . Здесь мы вспомним про концепцию стохастического градиентного спуска и попробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными соревнования Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто побьет указанные в соревновании бенчмарки. . 6 неделя. Vowpal Wabbit. Tutorial + Programming Assignment . На этой неделе мы познакомимся с популярной библиотекой Vowpal Wabbit и попробуем ее на данных по веб-сессиям. Знакомиться будем на данных Scikit-learn по новостям, сначала в режиме бинарной классификации, затем – в многоклассовом режиме. Затем будем классифицировать рецензии к фильмам с сайта IMDB. Наконец, применим Vowpal Wabbit к данным по веб-сессиям. Материала немало, но Vowpal Wabbit того стоит! . Статья про Vowpal Wabbit | Применение Vowpal Wabbit к данным по посещению сайтов | . 7 неделя. Оформление финального проекта. Peer-Review . В самом конце Вас ожидает взаимная проверка финальных версий проекта. Здесь можно будет разгуляться, поскольку свобода творчества есть на каждом этапе проекта: можно использовать все исходные данные по 3000 пользователям, можно создавать свои интересные признаки, строить красивые картинки, использовать свои модели или ансамбли моделей и делать выводы. Поэтому совет такой: по мере выполнения заданий параллельно копируйте код и описание в .ipynb-файл проекта или описывайте результаты по ходу в текстовом редакторе. . Подготовка данных к анализу и построению моделей . Первая часть проекта посвящена подготовке данных для дальнейшего описательного анализа и построения прогнозных моделей. Надо будет написать код для предобработки данных (исходно посещенные веб-сайты указаны для каждого пользователя в отдельном файле) и формирования единой обучающей выборки. Также в этой части мы познакомимся с разреженным форматом данных (матрицы Scipy.sparse), который хорошо подходит для данной задачи. . . user0031.csv user0039.csv user0100.csv user0128.csv user0237.csv user0033.csv user0050.csv user0127.csv user0207.csv user0241.csv . . import os import math import collections import time import pandas as pd from tqdm.auto import tqdm from glob import glob from collections import Counter . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/capstone_user_identification&#39; . Реализуйте функцию prepare_train_set, которая принимает на вход путь к каталогу с csv-файлами path_to_csv_files и параметр session_length – длину сессии, а возвращает 2 объекта: . DataFrame, в котором строки соответствуют уникальным сессиям из session_length сайтов, session_length столбцов – индексам этих session_length сайтов и последний столбец – ID пользователя | частотный словарь сайтов вида {&#39;site_string&#39;: [site_id, site_freq]}, например для недавнего игрушечного примера это будет {&#39;vk.com&#39;: (1, 2), &#39;google.com&#39;: (2, 2), &#39;yandex.ru&#39;: (3, 3), &#39;facebook.com&#39;: (4, 1)} | . Детали: . Смотрите чуть ниже пример вывода, что должна возвращать функция | Используйте glob (или аналоги) для обхода файлов в каталоге. Для определенности, отсортируйте список файлов лексикографически. Удобно использовать tqdm для отслеживания числа выполненных итераций цикла | Создайте частотный словарь уникальных сайтов (вида {&#39;site_string&#39;: (site_id, site_freq)}) и заполняйте его по ходу чтения файлов. Начните с 1 | Рекомендуется меньшие индексы давать более часто попадающимся сайтам (приницип наименьшего описания) | Не делайте entity recognition, считайте google.com, http://www.google.com и www.google.com разными сайтами (подключить entity recognition можно уже в рамках индивидуальной работы над проектом) | Скорее всего в файле число записей не кратно числу session_length. Тогда последняя сессия будет короче. Остаток заполняйте нулями. То есть если в файле 24 записи и сессии длины 10, то 3 сессия будет состоять из 4 сайтов, и ей мы сопоставим вектор [site1_id, site2_id, site3_id, site4_id, 0, 0, 0, 0, 0, 0, user_id] | В итоге некоторые сессии могут повторяться – оставьте как есть, не удаляйте дубликаты. Если в двух сессиях все сайты одинаковы, но сессии принадлежат разным пользователям, то тоже оставляйте как есть, это естественная неопределенность в данных Не оставляйте в частотном словаре сайт 0 (уже в конце, когда функция возвращает этот словарь) 150 файлов из capstone_websites_data/150users/ у меня обработались за 1.7 секунды, но многое, конечно, зависит от реализации функции и от используемого железа. И вообще, первая реализация скорее всего будет не самой эффективной, дальше можно заняться профилированием (особенно если планируете запускать этот код для 3000 пользователей). Также эффективная реализация этой функции поможет нам на следующей неделе. | . def prepare_train_set(path_to_csv_files, session_length=10): stock_files = sorted(glob(path_to_csv_files)) #создать общий датафрейм со всеми пользователями и сайтами df = pd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) #создать словарь с частотой сайтов и отсортировать его sorted_site = dict(collections.OrderedDict(sorted(Counter(df.site).items(), key=lambda kv: kv[1], reverse = True))) #print(sorted_site) #определить site_id и добавить его в в кортеж, который добавить в словарь sorted_site_list = list(sorted_site.keys()) df_site_dict_2 = {} for i, site in enumerate(sorted_site, 1): df_site_dict_2[site] = (i, sorted_site.get(site)) #создаю список сайтов из site_id list_all_site = [] user = 1 for filename in tqdm((stock_files), desc=&#39;Loop2&#39;): tmp_df = pd.read_csv(filename) list_site = [] #прохожу по сайтам в каждом файле и преобразую их в site_id for site in tqdm(tmp_df.site, desc = &#39;Loop3&#39;): list_site.append(df_site_dict_2.get(site)[0]) #добавляю нули до сессии из 10 сайтов multiple_len = (math.ceil(len(list_site)/session_length))*session_length tmp = [0] * (multiple_len - len(list_site)) list_site.extend(tmp) count = 0 #объединение всех списков в один основной while (count &lt; (len(list_site)/session_length)): ind_1 = count * session_length count = count + 1 ind_2 = count * session_length sess = list_site[ind_1 : ind_2] sess.append(user) list_all_site.append(sess) user = user + 1 #создание датафрейма из основного name_site = [] for i in range(session_length): name_site.append(&#39;site&#39;+str(i+1)) name_site.append(&#39;user_id&#39;) dt_tmp = pd.DataFrame(list_all_site, columns=name_site) return(dt_tmp, df_site_dict_2) . Проведу тест на трех юзерах и сверюсь с заданием . path = os.path.join(PATH_TO_DATA, &#39;3users/*.csv&#39;) . start = time.time() train_data_toy, site_freq_3users = prepare_train_set(path) end = time.time() print(end - start) . 0.269303560256958 . site_freq_3users . {&#39;accounts.google.com&#39;: (8, 1), &#39;apis.google.com&#39;: (9, 1), &#39;football.kulichki.ru&#39;: (6, 2), &#39;geo.mozilla.org&#39;: (7, 1), &#39;google.com&#39;: (1, 9), &#39;mail.google.com&#39;: (5, 2), &#39;meduza.io&#39;: (4, 3), &#39;oracle.com&#39;: (2, 8), &#39;plus.google.com&#39;: (10, 1), &#39;vk.com&#39;: (3, 3), &#39;yandex.ru&#39;: (11, 1)} . train_data_toy . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 user_id . 0 3 | 2 | 2 | 7 | 2 | 1 | 8 | 5 | 9 | 10 | 1 | . 1 3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 2 3 | 2 | 6 | 6 | 2 | 0 | 0 | 0 | 0 | 0 | 2 | . 3 4 | 1 | 2 | 1 | 2 | 1 | 1 | 5 | 11 | 4 | 3 | . 4 4 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . Все работает, перехожу на 10 пользователей . Часть 1. Сколько уникальных сессий из 10 сайтов в выборке с 10 пользователями? . path = os.path.join(PATH_TO_DATA, &#39;10users/*.csv&#39;) . start = time.time() train_data_toy, site_freq_10users = prepare_train_set(path) end = time.time() print(end - start) . 1.2535443305969238 . len(train_data_toy) . 14061 . В результате получил 14061 уникальных сессий из 10 сайтов у 10 пользователей. . Часть 2. Сколько всего уникальных сайтов в выборке из 10 пользователей? . len(site_freq_10users) . 4913 . Всего получил 4913 уникальных сайта . Часть 3. Сколько уникальных сессий из 10 сайтов в выборке со 150 пользователями? . path = os.path.join(PATH_TO_DATA, &#39;150users/*.csv&#39;) . start = time.time() train_data_toy, site_freq_150users = prepare_train_set(path) end = time.time() print(end - start) . 108.86379528045654 . len(train_data_toy) . 137019 . Похоже не самая эффективная реализация функции. Отработала у меня за 1,5 минуты, а у преподавателя за 1,7 секунды. Результат получил - 137019 . Часть 4. Сколько всего уникальных сайтов в выборке из 150 пользователей? . len(site_freq_150users) . 27797 . Уникальных сайтов в выборке из 150 пользователей - 27797. . Часть 5. Каковы топ-10 самых популярных сайтов среди посещенных 150 пользователями . list(site_freq_150users.keys())[:10] . [&#39;www.google.fr&#39;, &#39;www.google.com&#39;, &#39;www.facebook.com&#39;, &#39;apis.google.com&#39;, &#39;s.youtube.com&#39;, &#39;clients1.google.com&#39;, &#39;mail.google.com&#39;, &#39;plus.google.com&#39;, &#39;safebrowsing-cache.google.com&#39;, &#39;www.youtube.com&#39;] .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/2019/07/10/project-identification-of-internet-users.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/2019/07/10/project-identification-of-internet-users.html",
            "date": " • Jul 10, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alexander Gladkikh, and I made my own website on GitHub dedicated to my Hobbies: Machine learning, Deep Learning, and algorithmic trading. . I take part in kaggle competitions, have knowledge of R and Python (Pandas, NumPy, Scipy, Scikit-learn, XGBoost), Java . At the main work I participate in projects on the use of new technologies in the field of labor protection and ecology. . I have been engaged in technical analysis of financial markets for a long time. Familiar with software Amibroker, and Metatrader Quik (scripting). . At work I had to deal with the analysis of data in the performance of research in biology at the Institute and writing projects on environmental protection. . My degrees . Corporate Energy University, 2020 . Digital production technologies in the power industry . YANDEX, MIPT, 2019 . Machine learning and data analysis . City Business School, 2019 . MINI-MBA Professional .",
          "url": "https://zmey56.github.io/blog//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zmey56.github.io/blog//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}