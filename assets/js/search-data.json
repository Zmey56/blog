{
  
    
        "post0": {
            "title": "Course project for classification by loan approval",
            "content": "Problem statement . Task . It is required, based on the available data on the bank&#39;s customers, to build a model using a training dataset to predict the non-fulfillment of debt obligations on a current loan. Make a forecast for the examples from the test dataset. . Name of data files . course_project_train.csv - training dataset course_project_test.csv - test dataset . Target variable . Credit Default - the fact of non-fulfillment of credit obligations . Quality metric . F1-score (sklearn.metrics.f1_score) . Solution requirements . Target metric . F1 &gt; 0.5 | The metric is evaluated by the quality of the forecast for the main class (1 - loan delinquency) | . The solution must contain . Jupyter Notebook with the code of your solution, named after the pattern {FULL name}_solution.ipynb, example SShirkin_solution.ipynb | CSV file with forecasts of the target variable for the test dataset, named after the sample {FULL name}_predictions.csv, example SShirkin_predictions.csv | Recommendations for the code file (ipynb) . The file must contain headers and comments (markdown) | It is better to design repetitive operations in the form of functions | Do not output a large number of rows of tables (5-10 is enough) | If possible, add graphs describing the data (about 3-5) | Add only the best model, that is, do not include in the code all the options for solving the project | The project script should work from the beginning to the end (from loading data to unloading predictions) | The whole project should be in one script (ipynb file). | It is allowed to use Python libraries and machine learning models that were in this course. | Deadlines for delivery . You need to submit the project within 5 days after the end of the last webinar. Estimates of works submitted before the deadline will be presented in the form of a rating ranked according to a given quality metric. Projects submitted after the deadline or submitted again do not get into the rating, but you will be able to find out the result. . An approximate description of the stages of the course project . Building a classification model . Overview of the training dataset | Handling emissions | Processing of omissions | Data analysis | Selection of features | Balancing classes | Selection of models, obtaining a baseline | Choosing the best model, setting hyperparameters | Quality control, fight against retraining | Interpretation of results | Forecasting on a test dataset . Perform the same stages of processing and building features for the test dataset | Predict the target variable using a model based on a training dataset | Forecasts should be for all examples from the test dataset (for all rows) | Observe the original order of the examples from the test dataset | &#1054;&#1073;&#1079;&#1086;&#1088; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Description of the dataset . Home Ownership - Home ownership | Annual Income - annual income | Years in current job - the number of years at the current job | Tax Liens - tax encumbrances | Number of Open Accounts - number of open accounts | Years of Credit History - number of years of credit history | Maximum Open Credit - the largest open credit | Number of Credit Problems - number of credit problems | Months since last delinquent - the number of months since the last payment delay | Bankruptcies - bankruptcies | Purpose - purpose of the loan | Term - loan term | Current Loan Amount - current loan amount | Current Credit Balance - current credit balance | Monthly Debt - monthly debt | Credit Default - the fact of non-fulfillment of credit obligations (0 - repaid on time, 1 - overdue) | . Connecting the script library . import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, learning_curve from sklearn.metrics import classification_report, f1_score, precision_score, recall_score from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import RandomizedSearchCV, KFold import xgboost as xgb, lightgbm as lgbm, catboost as catb from scipy.stats import uniform as sp_randFloat from scipy.stats import randint as sp_randInt . import warnings warnings.filterwarnings(&#39;ignore&#39;) sns.set(style=&#39;whitegrid&#39;) sns.set_context(&quot;paper&quot;, font_scale=1.5) pd.options.display.float_format = &#39;{:,.2f}&#39;.format pd.set_option(&#39;display.max_rows&#39;, 50) . def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred): print(&#39;TRAIN n n&#39; + classification_report(y_train_true, y_train_pred)) print(&#39;TEST n n&#39; + classification_report(y_test_true, y_test_pred)) print(&#39;CONFUSION MATRIX n&#39;) print(pd.crosstab(y_test_true, y_test_pred)) . def balance_df_by_target(df, target_name): target_counts = df[target_name].value_counts() major_class_name = target_counts.argmax() minor_class_name = target_counts.argmin() disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1 for i in range(disbalance_coeff): sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name]) df = df.append(sample, ignore_index=True) return df.sample(frac=1) . def show_learning_curve_plot(estimator, X, y, cv=3, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)): train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=&#39;f1&#39;, train_sizes=train_sizes, n_jobs=n_jobs) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(15,8)) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;) plt.plot(train_sizes, train_scores_mean, &#39;o-&#39;, color=&quot;r&quot;, label=&quot;Training score&quot;) plt.plot(train_sizes, test_scores_mean, &#39;o-&#39;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;) plt.title(f&quot;Learning curves ({type(estimator).__name__})&quot;) plt.xlabel(&quot;Training examples&quot;) plt.ylabel(&quot;Score&quot;) plt.legend(loc=&quot;best&quot;) plt.grid() plt.show() . def show_proba_calibration_plots(y_predicted_probs, y_true_labels): preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels))) thresholds = [] precisions = [] recalls = [] f1_scores = [] for threshold in np.linspace(0.1, 0.9, 9): thresholds.append(threshold) precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) scores_table = pd.DataFrame({&#39;f1&#39;:f1_scores, &#39;precision&#39;:precisions, &#39;recall&#39;:recalls, &#39;probability&#39;:thresholds}).sort_values(&#39;f1&#39;, ascending=False).round(3) figure = plt.figure(figsize = (15, 5)) plt1 = figure.add_subplot(121) plt1.plot(thresholds, precisions, label=&#39;Precision&#39;, linewidth=4) plt1.plot(thresholds, recalls, label=&#39;Recall&#39;, linewidth=4) plt1.plot(thresholds, f1_scores, label=&#39;F1&#39;, linewidth=4) plt1.set_ylabel(&#39;Scores&#39;) plt1.set_xlabel(&#39;Probability threshold&#39;) plt1.set_title(&#39;Probabilities threshold calibration&#39;) plt1.legend(bbox_to_anchor=(0.25, 0.25)) plt1.table(cellText = scores_table.values, colLabels = scores_table.columns, colLoc = &#39;center&#39;, cellLoc = &#39;center&#39;, loc = &#39;bottom&#39;, bbox = [0, -1.3, 1, 1]) plt2 = figure.add_subplot(122) plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], label=&#39;Another class&#39;, color=&#39;royalblue&#39;, alpha=1) plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], label=&#39;Main class&#39;, color=&#39;darkcyan&#39;, alpha=0.8) plt2.set_ylabel(&#39;Number of examples&#39;) plt2.set_xlabel(&#39;Probabilities&#39;) plt2.set_title(&#39;Probability histogram&#39;) plt2.legend(bbox_to_anchor=(1, 1)) plt.show() . def show_feature_importances(feature_names, feature_importances, get_top=None): feature_importances = pd.DataFrame({&#39;feature&#39;: feature_names, &#39;importance&#39;: feature_importances}) feature_importances = feature_importances.sort_values(&#39;importance&#39;, ascending=False) plt.figure(figsize = (20, len(feature_importances) * 0.355)) sns.barplot(feature_importances[&#39;importance&#39;], feature_importances[&#39;feature&#39;]) plt.xlabel(&#39;Importance&#39;) plt.title(&#39;Importance of features&#39;) plt.show() if get_top is not None: return feature_importances[&#39;feature&#39;][:get_top].tolist() . def plot_feature_importance(importance,names,model_type): #Create arrays from feature importance and feature names feature_importance = np.array(importance) feature_names = np.array(names) #Create a DataFrame using a Dictionary data={&#39;feature_names&#39;:feature_names,&#39;feature_importance&#39;:feature_importance} fi_df = pd.DataFrame(data) #Sort the DataFrame in order decreasing feature importance fi_df.sort_values(by=[&#39;feature_importance&#39;], ascending=False,inplace=True) #Define size of bar plot plt.figure(figsize=(10,8)) #Plot Searborn bar chart sns.barplot(x=fi_df[&#39;feature_importance&#39;], y=fi_df[&#39;feature_names&#39;]) #Add chart labels plt.title(model_type + &#39;FEATURE IMPORTANCE&#39;) plt.xlabel(&#39;FEATURE IMPORTANCE&#39;) plt.ylabel(&#39;FEATURE NAMES&#39;) . Paths to directories and files . TARGET_NAME = &#39;Credit Default&#39; TRAIN_DATASET_PATH = &#39;data/train.csv&#39; TEST_DATASET_PATH = &#39;data/test.csv&#39; SCALER_FILE_PATH = &#39;data/scaler.pkl&#39; TRAIN_PART_PATH = &#39;data/training_project_train_part.csv&#39; TEST_PART_PATH = &#39;data/training_project_test_part.csv&#39; . Loading data . train_df = pd.read_csv(TRAIN_DATASET_PATH) train_df.head() . Home Ownership Annual Income Years in current job Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Purpose Term Current Loan Amount Current Credit Balance Monthly Debt Credit Score Credit Default . 0 Own Home | 482,087.00 | NaN | 0.00 | 11.00 | 26.30 | 685,960.00 | 1.00 | NaN | 1.00 | debt consolidation | Short Term | 99,999,999.00 | 47,386.00 | 7,914.00 | 749.00 | 0 | . 1 Own Home | 1,025,487.00 | 10+ years | 0.00 | 15.00 | 15.30 | 1,181,730.00 | 0.00 | NaN | 0.00 | debt consolidation | Long Term | 264,968.00 | 394,972.00 | 18,373.00 | 737.00 | 1 | . 2 Home Mortgage | 751,412.00 | 8 years | 0.00 | 11.00 | 35.00 | 1,182,434.00 | 0.00 | NaN | 0.00 | debt consolidation | Short Term | 99,999,999.00 | 308,389.00 | 13,651.00 | 742.00 | 0 | . 3 Own Home | 805,068.00 | 6 years | 0.00 | 8.00 | 22.50 | 147,400.00 | 1.00 | NaN | 1.00 | debt consolidation | Short Term | 121,396.00 | 95,855.00 | 11,338.00 | 694.00 | 0 | . 4 Rent | 776,264.00 | 8 years | 0.00 | 13.00 | 13.60 | 385,836.00 | 1.00 | NaN | 0.00 | debt consolidation | Short Term | 125,840.00 | 93,309.00 | 7,180.00 | 719.00 | 0 | . test_df = pd.read_csv(TEST_DATASET_PATH) test_df.head() . Home Ownership Annual Income Years in current job Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Purpose Term Current Loan Amount Current Credit Balance Monthly Debt Credit Score . 0 Rent | NaN | 4 years | 0.00 | 9.00 | 12.50 | 220,968.00 | 0.00 | 70.00 | 0.00 | debt consolidation | Short Term | 162,470.00 | 105,906.00 | 6,813.00 | NaN | . 1 Rent | 231,838.00 | 1 year | 0.00 | 6.00 | 32.70 | 55,946.00 | 0.00 | 8.00 | 0.00 | educational expenses | Short Term | 78,298.00 | 46,037.00 | 2,318.00 | 699.00 | . 2 Home Mortgage | 1,152,540.00 | 3 years | 0.00 | 10.00 | 13.70 | 204,600.00 | 0.00 | NaN | 0.00 | debt consolidation | Short Term | 200,178.00 | 146,490.00 | 18,729.00 | 7,260.00 | . 3 Home Mortgage | 1,220,313.00 | 10+ years | 0.00 | 16.00 | 17.00 | 456,302.00 | 0.00 | 70.00 | 0.00 | debt consolidation | Short Term | 217,382.00 | 213,199.00 | 27,559.00 | 739.00 | . 4 Home Mortgage | 2,340,952.00 | 6 years | 0.00 | 11.00 | 23.60 | 1,207,272.00 | 0.00 | NaN | 0.00 | debt consolidation | Long Term | 777,634.00 | 425,391.00 | 42,605.00 | 706.00 | . 1. Overview of the training dataset . print(train_df.shape, test_df.shape) . (7500, 17) (2500, 16) . train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7500 entries, 0 to 7499 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 Home Ownership 7500 non-null object 1 Annual Income 5943 non-null float64 2 Years in current job 7129 non-null object 3 Tax Liens 7500 non-null float64 4 Number of Open Accounts 7500 non-null float64 5 Years of Credit History 7500 non-null float64 6 Maximum Open Credit 7500 non-null float64 7 Number of Credit Problems 7500 non-null float64 8 Months since last delinquent 3419 non-null float64 9 Bankruptcies 7486 non-null float64 10 Purpose 7500 non-null object 11 Term 7500 non-null object 12 Current Loan Amount 7500 non-null float64 13 Current Credit Balance 7500 non-null float64 14 Monthly Debt 7500 non-null float64 15 Credit Score 5943 non-null float64 16 Credit Default 7500 non-null int64 dtypes: float64(12), int64(1), object(4) memory usage: 996.2+ KB . columns_name = train_df.columns . train_df.nunique(dropna=False) . Home Ownership 4 Annual Income 5479 Years in current job 12 Tax Liens 8 Number of Open Accounts 39 Years of Credit History 408 Maximum Open Credit 6963 Number of Credit Problems 8 Months since last delinquent 90 Bankruptcies 6 Purpose 15 Term 2 Current Loan Amount 5386 Current Credit Balance 6592 Monthly Debt 6716 Credit Score 269 Credit Default 2 dtype: int64 . columns_name . Index([&#39;Home Ownership&#39;, &#39;Annual Income&#39;, &#39;Years in current job&#39;, &#39;Tax Liens&#39;, &#39;Number of Open Accounts&#39;, &#39;Years of Credit History&#39;, &#39;Maximum Open Credit&#39;, &#39;Number of Credit Problems&#39;, &#39;Months since last delinquent&#39;, &#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;, &#39;Current Loan Amount&#39;, &#39;Current Credit Balance&#39;, &#39;Monthly Debt&#39;, &#39;Credit Score&#39;, &#39;Credit Default&#39;], dtype=&#39;object&#39;) . # I will also highlight the analyzed variable y = train_df[&#39;Credit Default&#39;] cat_col = [&#39;Home Ownership&#39;, &#39;Years in current job&#39;, &#39;Tax Liens&#39;, &#39;Number of Credit Problems&#39;,&#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;] num_col = [&#39;Annual Income&#39;, &#39;Number of Open Accounts&#39;, &#39;Maximum Open Credit&#39;, &#39;Years of Credit History&#39;, &#39;Months since last delinquent&#39;, &#39;Current Loan Amount&#39;, &#39;Current Credit Balance&#39;, &#39;Monthly Debt&#39;, &#39;Credit Score&#39;,] . cat_df = train_df[cat_col] cat_df = cat_df.astype(str) num_df = train_df[num_col] num_df = num_df.astype(float) . fig, ax = plt.subplots(4,2, figsize=(40,35)) sns.countplot(train_df[&#39;Home Ownership&#39;], ax=ax[0,0]) sns.countplot(train_df[&#39;Years in current job&#39;], ax=ax[0,1]) sns.countplot(train_df[&#39;Tax Liens&#39;], ax=ax[1,0]) sns.countplot(train_df[&#39;Number of Credit Problems&#39;], ax=ax[1,1]) sns.countplot(train_df[&#39;Bankruptcies&#39;], ax=ax[2,0]) sns.countplot(train_df[&#39;Purpose&#39;], ax=ax[2,1]) sns.countplot(train_df[&#39;Term&#39;], ax=ax[3,0]) fig.show() . for c in cat_df.columns: print (&quot;- %s &quot; % c) print (cat_df[c].value_counts()) . - Home Ownership Home Mortgage 3637 Rent 3204 Own Home 647 Have Mortgage 12 Name: Home Ownership, dtype: int64 - Years in current job 10+ years 2332 2 years 705 3 years 620 &lt; 1 year 563 5 years 516 1 year 504 4 years 469 6 years 426 7 years 396 nan 371 8 years 339 9 years 259 Name: Years in current job, dtype: int64 - Tax Liens 0.0 7366 1.0 83 2.0 30 3.0 10 4.0 6 6.0 2 5.0 2 7.0 1 Name: Tax Liens, dtype: int64 - Number of Credit Problems 0.0 6469 1.0 882 2.0 93 3.0 35 4.0 9 5.0 7 6.0 4 7.0 1 Name: Number of Credit Problems, dtype: int64 - Bankruptcies 0.0 6660 1.0 786 2.0 31 nan 14 3.0 7 4.0 2 Name: Bankruptcies, dtype: int64 - Purpose debt consolidation 5944 other 665 home improvements 412 business loan 129 buy a car 96 medical bills 71 major purchase 40 take a trip 37 buy house 34 small business 26 wedding 15 moving 11 educational expenses 10 vacation 8 renewable energy 2 Name: Purpose, dtype: int64 - Term Short Term 5556 Long Term 1944 Name: Term, dtype: int64 . h = num_df.hist(bins=25,figsize=(16,16),xlabelsize=&#39;10&#39;,ylabelsize=&#39;10&#39;,xrot=-15) sns.despine(left=True, bottom=True) [x.title.set_size(12) for x in h.ravel()]; [x.yaxis.tick_left() for x in h.ravel()]; . mask = np.zeros_like(num_df.corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(16, 12)) plt.title(&#39;Pearson Correlation Matrix&#39;,fontsize=25) sns.heatmap(num_df.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=&quot;BuGn&quot;, #&quot;BuGn_r&quot; to reverse linecolor=&#39;w&#39;,annot=True,annot_kws={&quot;size&quot;:8},mask=mask,cbar_kws={&quot;shrink&quot;: .9}); . 2. Processing of omissions . train_df.isna().sum() . Home Ownership 0 Annual Income 1557 Years in current job 371 Tax Liens 0 Number of Open Accounts 0 Years of Credit History 0 Maximum Open Credit 0 Number of Credit Problems 0 Months since last delinquent 4081 Bankruptcies 14 Purpose 0 Term 0 Current Loan Amount 0 Current Credit Balance 0 Monthly Debt 0 Credit Score 1557 Credit Default 0 dtype: int64 . train_df.describe() . Annual Income Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Current Loan Amount Current Credit Balance Monthly Debt Credit Score Credit Default . count 5,943.00 | 7,500.00 | 7,500.00 | 7,500.00 | 7,500.00 | 7,500.00 | 3,419.00 | 7,486.00 | 7,500.00 | 7,500.00 | 7,500.00 | 5,943.00 | 7,500.00 | . mean 1,366,391.72 | 0.03 | 11.13 | 18.32 | 945,153.73 | 0.17 | 34.69 | 0.12 | 11,873,177.45 | 289,833.24 | 18,314.45 | 1,151.09 | 0.28 | . std 845,339.20 | 0.27 | 4.91 | 7.04 | 16,026,216.67 | 0.50 | 21.69 | 0.35 | 31,926,122.97 | 317,871.38 | 11,926.76 | 1,604.45 | 0.45 | . min 164,597.00 | 0.00 | 2.00 | 4.00 | 0.00 | 0.00 | 0.00 | 0.00 | 11,242.00 | 0.00 | 0.00 | 585.00 | 0.00 | . 25% 844,341.00 | 0.00 | 8.00 | 13.50 | 279,229.50 | 0.00 | 16.00 | 0.00 | 180,169.00 | 114,256.50 | 10,067.50 | 711.00 | 0.00 | . 50% 1,168,386.00 | 0.00 | 10.00 | 17.00 | 478,159.00 | 0.00 | 32.00 | 0.00 | 309,573.00 | 209,323.00 | 16,076.50 | 731.00 | 0.00 | . 75% 1,640,137.00 | 0.00 | 14.00 | 21.80 | 793,501.50 | 0.00 | 50.00 | 0.00 | 519,882.00 | 360,406.25 | 23,818.00 | 743.00 | 1.00 | . max 10,149,344.00 | 7.00 | 43.00 | 57.70 | 1,304,726,170.00 | 7.00 | 118.00 | 4.00 | 99,999,999.00 | 6,506,797.00 | 136,679.00 | 7,510.00 | 1.00 | . First, I will fill in the Annual Income using median values . median_income = train_df[&#39;Annual Income&#39;].median() train_df[&#39;Annual Income&#39;] = train_df[&#39;Annual Income&#39;].fillna(median_income) test_df[&#39;Annual Income&#39;] = test_df[&#39;Annual Income&#39;].fillna(median_income) . Then I&#39;m doing Years in current job. At the moment they have a textual meaning. So I will select the most common ones by frequency and replace the missing value with them . max_YCJ = train_df[&#39;Years in current job&#39;].value_counts().index[0] train_df[&#39;Years in current job&#39;] = train_df[&#39;Years in current job&#39;].fillna(max_YCJ) test_df[&#39;Years in current job&#39;] = test_df[&#39;Years in current job&#39;].fillna(max_YCJ) . I will also replace the values with figures that are more acceptable for further use in calculations. . train_df[&#39;Years in current job&#39;] = train_df[&#39;Years in current job&#39;].replace({&#39;10+ years&#39;:10,&#39;2 years&#39;:2, &#39;3 years&#39;:3, &#39;&lt; 1 year&#39;:0, &#39;5 years&#39;:5, &#39;1 year&#39;:1, &#39;4 years&#39;:4, &#39;6 years&#39;:6,&#39;7 years&#39;:7, &#39;8 years&#39;:8, &#39;9 years&#39;:9}) test_df[&#39;Years in current job&#39;] = test_df[&#39;Years in current job&#39;].replace({&#39;10+ years&#39;:10,&#39;2 years&#39;:2, &#39;3 years&#39;:3, &#39;&lt; 1 year&#39;:0, &#39;5 years&#39;:5, &#39;1 year&#39;:1, &#39;4 years&#39;:4, &#39;6 years&#39;:6,&#39;7 years&#39;:7, &#39;8 years&#39;:8, &#39;9 years&#39;:9}) . With Months since last delinquent, I will do the same as Annual Income - I use the median. But maybe it makes sense to delete this column altogether, since most of the values are missing . median_delinquent = train_df[&#39;Months since last delinquent&#39;].median() train_df[&#39;Months since last delinquent&#39;] = train_df[&#39;Months since last delinquent&#39;].fillna(median_delinquent) test_df[&#39;Months since last delinquent&#39;] = test_df[&#39;Months since last delinquent&#39;].fillna(median_delinquent) . For bankrupts, when checking for frequency, it turns out that most were not bankrupts. Accordingly, I will replace the missing values with 0. . train_df[&#39;Bankruptcies&#39;].value_counts() . 0.00 6660 1.00 786 2.00 31 3.00 7 4.00 2 Name: Bankruptcies, dtype: int64 . train_df[&#39;Bankruptcies&#39;] = train_df[&#39;Bankruptcies&#39;].fillna(0.00) test_df[&#39;Bankruptcies&#39;] = test_df[&#39;Bankruptcies&#39;].fillna(0.00) . The last column with NaN values remains - Credit Score. I will fill it with the already familiar method - medians . median_CS = train_df[&#39;Credit Score&#39;].median() train_df[&#39;Credit Score&#39;] = train_df[&#39;Credit Score&#39;].fillna(median_CS) test_df[&#39;Credit Score&#39;] = test_df[&#39;Credit Score&#39;].fillna(median_CS) . 3. Handling emissions . I look at emissions by graphs and frequency. . I leave the categorical columns unchanged - &#39;Years in current job&#39;, &#39;Tax License&#39;,&#39;Number of CreditProblems&#39;,&#39;Bankruptcy&#39;, &#39;Purpose&#39;, &#39;Term&#39;. . From the frequency calculation in HomeOwnership, the value for Have Mortgage is knocked out as too low. Most likely, this is an erroneous entry for Home Mortgage. . test_df[&#39;Home Ownership&#39;].value_counts() . Home Mortgage 1225 Rent 1020 Own Home 248 Have Mortgage 7 Name: Home Ownership, dtype: int64 . train_df.loc[train_df[&#39;Home Ownership&#39;] == &#39;Have Mortgage&#39;, &#39;Home Ownership&#39;] = &#39;Home Mortgage&#39; test_df.loc[test_df[&#39;Home Ownership&#39;] == &#39;Have Mortgage&#39;, &#39;Home Ownership&#39;] = &#39;Home Mortgage&#39; . Since there is no such goal as renewable energy in the test DataFrame, I combine them with travel and call it something else . train_df[&#39;Purpose&#39;] = train_df[&#39;Purpose&#39;].replace({&#39;vacation&#39;:&#39;other&#39;,&#39;renewable energy&#39;:&#39;other&#39;}) test_df[&#39;Purpose&#39;] = test_df[&#39;Purpose&#39;].replace({&#39;vacation&#39;:&#39;other&#39;,&#39;renewable energy&#39;:&#39;other&#39;}) . Next, I remove strong outliers by annual income, loans issued, the largest open loan, the number of years of credit history, the number of months since the last overdue payment, the current credit balance, monthly debt and credit score. If the value goes out for three sigma, then I assign it an average plus three sigma. . There are no exceedances of more than three sigma in the current loan amount, so I leave it unchanged. . mean_AI = train_df[&#39;Annual Income&#39;].mean() sigma_AI = train_df[&#39;Annual Income&#39;].std() train_df.loc[train_df[&#39;Annual Income&#39;] &gt; (mean_AI + 3 * sigma_AI), &#39;Annual Income&#39;] = (mean_AI + 3 * sigma_AI) test_df.loc[test_df[&#39;Annual Income&#39;] &gt; (mean_AI + 3 * sigma_AI), &#39;Annual Income&#39;] = (mean_AI + 3 * sigma_AI) . mean_NOA = train_df[&#39;Number of Open Accounts&#39;].mean() sigma_NOA = train_df[&#39;Number of Open Accounts&#39;].std() train_df.loc[train_df[&#39;Number of Open Accounts&#39;] &gt; (mean_NOA + 3 * sigma_NOA), &#39;Number of Open Accounts&#39;] = (mean_NOA + 3 * sigma_NOA) test_df.loc[test_df[&#39;Number of Open Accounts&#39;] &gt; (mean_NOA + 3 * sigma_NOA), &#39;Number of Open Accounts&#39;] = (mean_NOA + 3 * sigma_NOA) . mean_MOC = train_df[&#39;Maximum Open Credit&#39;].mean() sigma_MOC = train_df[&#39;Maximum Open Credit&#39;].std() train_df.loc[train_df[&#39;Maximum Open Credit&#39;] &gt; (mean_MOC + 3 * sigma_MOC), &#39;Maximum Open Credit&#39;] = (mean_MOC + 3 * sigma_MOC) test_df.loc[test_df[&#39;Maximum Open Credit&#39;] &gt; (mean_MOC + 3 * sigma_MOC), &#39;Maximum Open Credit&#39;] = (mean_MOC + 3 * sigma_MOC) . mean_YCH = train_df[&#39;Years of Credit History&#39;].mean() sigma_YCH = train_df[&#39;Years of Credit History&#39;].std() train_df.loc[train_df[&#39;Years of Credit History&#39;] &gt; (mean_YCH + 3 * sigma_YCH), &#39;Years of Credit History&#39;] = (mean_YCH + 3 * sigma_YCH) test_df.loc[test_df[&#39;Years of Credit History&#39;] &gt; (mean_YCH + 3 * sigma_YCH), &#39;Years of Credit History&#39;] = (mean_YCH + 3 * sigma_YCH) . mean_MSLD = train_df[&#39;Months since last delinquent&#39;].mean() sigma_MSLD = train_df[&#39;Months since last delinquent&#39;].std() train_df.loc[train_df[&#39;Months since last delinquent&#39;] &gt; (mean_MSLD + 3 * sigma_MSLD), &#39;Months since last delinquent&#39;] = (mean_MSLD + 3 * sigma_MSLD) test_df.loc[test_df[&#39;Months since last delinquent&#39;] &gt; (mean_MSLD + 3 * sigma_MSLD), &#39;Months since last delinquent&#39;] = (mean_MSLD + 3 * sigma_MSLD) . mean_CCB = train_df[&#39;Current Credit Balance&#39;].mean() sigma_CCB = train_df[&#39;Current Credit Balance&#39;].std() train_df.loc[train_df[&#39;Current Credit Balance&#39;] &gt; (mean_CCB + 3 * sigma_CCB), &#39;Current Credit Balance&#39;] = (mean_CCB + 3 * sigma_CCB) test_df.loc[test_df[&#39;Current Credit Balance&#39;] &gt; (mean_CCB + 3 * sigma_CCB), &#39;Current Credit Balance&#39;] = (mean_CCB + 3 * sigma_CCB) . mean_MD = train_df[&#39;Monthly Debt&#39;].mean() sigma_MD = train_df[&#39;Monthly Debt&#39;].std() train_df.loc[train_df[&#39;Monthly Debt&#39;] &gt; (mean_MD + 3 * sigma_MD), &#39;Monthly Debt&#39;] = (mean_MD + 3 * sigma_MD) test_df.loc[test_df[&#39;Monthly Debt&#39;] &gt; (mean_MD + 3 * sigma_MD), &#39;Monthly Debt&#39;] = (mean_MD + 3 * sigma_MD) . mean_CS = train_df[&#39;Credit Score&#39;].mean() sigma_CS = train_df[&#39;Credit Score&#39;].std() train_df.loc[train_df[&#39;Credit Score&#39;] &gt; (mean_CS + 3 * sigma_CS), &#39;Credit Score&#39;] = (mean_CS + 3 * sigma_CS) test_df.loc[test_df[&#39;Credit Score&#39;] &gt; (mean_CS + 3 * sigma_CS), &#39;Credit Score&#39;] = (mean_CS + 3 * sigma_CS) . 4. Preparing data for analysis . The first step is to make a table with fictitious values of categorical data and normalize numeric values. After that, I save the received Data Frame for further analysis . cat_dum_train = pd.get_dummies(train_df[cat_col]) cat_dum_test = pd.get_dummies(test_df[cat_col]) . scaler = StandardScaler() num_norm_train = pd.DataFrame(scaler.fit_transform(train_df[num_col]), columns = num_col) num_norm_test = pd.DataFrame(scaler.transform(test_df[num_col]), columns = num_col) . train_new = pd.concat([cat_dum_train, num_norm_train], axis=1) test_new = pd.concat([cat_dum_test, num_norm_test], axis=1) . X_train, X_test, y_train, y_test = train_test_split(train_new, y, shuffle=True, test_size=0.2, random_state=56) . 5. Balancing the target variable . y.value_counts() . 0 5387 1 2113 Name: Credit Default, dtype: int64 . df_for_balancing = pd.concat([X_train, y_train], axis=1) df_balanced = balance_df_by_target(df_for_balancing, TARGET_NAME) df_balanced[TARGET_NAME].value_counts() . 0 4292 1 3416 Name: Credit Default, dtype: int64 . X_train = df_balanced.drop(columns=TARGET_NAME) y_train = df_balanced[TARGET_NAME] . 6. Construction and evaluation of basic models . Logistic regression . model_lr = LogisticRegression() model_lr.fit(X_train, y_train) y_train_pred = model_lr.predict(X_train) y_test_pred = model_lr.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.69 0.83 0.75 4292 1 0.71 0.54 0.61 3416 accuracy 0.70 7708 macro avg 0.70 0.68 0.68 7708 weighted avg 0.70 0.70 0.69 7708 TEST precision recall f1-score support 0 0.82 0.80 0.81 1095 1 0.50 0.53 0.51 405 accuracy 0.73 1500 macro avg 0.66 0.66 0.66 1500 weighted avg 0.73 0.73 0.73 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 879 216 1 192 213 . k nearest neighbors . model_knn = KNeighborsClassifier() model_knn.fit(X_train, y_train) y_train_pred = model_knn.predict(X_train) y_test_pred = model_knn.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.79 0.85 0.82 4292 1 0.80 0.72 0.76 3416 accuracy 0.80 7708 macro avg 0.80 0.79 0.79 7708 weighted avg 0.80 0.80 0.79 7708 TEST precision recall f1-score support 0 0.81 0.77 0.79 1095 1 0.44 0.50 0.47 405 accuracy 0.70 1500 macro avg 0.63 0.63 0.63 1500 weighted avg 0.71 0.70 0.70 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 839 256 1 201 204 . Boosting algorithms . XGBoost . model_xgb = xgb.XGBClassifier(random_state=56) model_xgb.fit(X_train, y_train) y_train_pred = model_xgb.predict(X_train) y_test_pred = model_xgb.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . [20:38:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. TRAIN precision recall f1-score support 0 0.97 0.97 0.97 4292 1 0.97 0.96 0.96 3416 accuracy 0.97 7708 macro avg 0.97 0.97 0.97 7708 weighted avg 0.97 0.97 0.97 7708 TEST precision recall f1-score support 0 0.81 0.81 0.81 1095 1 0.48 0.47 0.48 405 accuracy 0.72 1500 macro avg 0.64 0.64 0.64 1500 weighted avg 0.72 0.72 0.72 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 886 209 1 213 192 . LightGBM . model_lgbm = lgbm.LGBMClassifier(random_state=56) model_lgbm.fit(X_train, y_train) y_train_pred = model_lgbm.predict(X_train) y_test_pred = model_lgbm.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.91 0.95 0.93 4292 1 0.93 0.88 0.90 3416 accuracy 0.92 7708 macro avg 0.92 0.91 0.92 7708 weighted avg 0.92 0.92 0.92 7708 TEST precision recall f1-score support 0 0.81 0.84 0.83 1095 1 0.52 0.48 0.50 405 accuracy 0.74 1500 macro avg 0.67 0.66 0.66 1500 weighted avg 0.73 0.74 0.74 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 917 178 1 211 194 . CatBoost . model_catb = catb.CatBoostClassifier(silent=True, random_state=56) model_catb.fit(X_train, y_train) y_train_pred = model_catb.predict(X_train) y_test_pred = model_catb.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.89 0.95 0.92 4292 1 0.93 0.86 0.89 3416 accuracy 0.91 7708 macro avg 0.91 0.90 0.90 7708 weighted avg 0.91 0.91 0.91 7708 TEST precision recall f1-score support 0 0.81 0.83 0.82 1095 1 0.51 0.49 0.50 405 accuracy 0.74 1500 macro avg 0.66 0.66 0.66 1500 weighted avg 0.73 0.74 0.73 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 907 188 1 208 197 . On the test, I have the best results for the Adaboost classifier, and I will select the parameters for it . 7. Choosing the best model and selecting hyperparameters . model_catb = catb.CatBoostClassifier(class_weights=[1, 3.5], silent=True, random_state=56) . params_1 = {&#39;n_estimators&#39;:[1500, 1800, 2100], &#39;max_depth&#39;:[1, 2, 3]} . %%time rs = RandomizedSearchCV(model_catb, params_1, scoring=&#39;f1&#39;, cv=cv, n_jobs=-1) rs.fit(train_new, y) . CPU times: user 17.6 s, sys: 4.03 s, total: 21.6 s Wall time: 1min 56s . RandomizedSearchCV(cv=KFold(n_splits=3, random_state=56, shuffle=True), estimator=&lt;catboost.core.CatBoostClassifier object at 0x7f8b78854dc0&gt;, n_jobs=-1, param_distributions={&#39;max_depth&#39;: [1, 2, 3], &#39;n_estimators&#39;: [1500, 1800, 2100]}, scoring=&#39;f1&#39;) . rs.best_params_ . {&#39;n_estimators&#39;: 1500, &#39;max_depth&#39;: 3} . rs.best_score_ . 0.5418973339546257 . Training and evaluation of the final model . %%time final_model = catb.CatBoostClassifier(n_estimators=1500, max_depth=3, silent=True, random_state=56) final_model.fit(X_train, y_train) y_train_pred = final_model.predict(X_train) y_test_pred = final_model.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.76 0.87 0.81 4292 1 0.80 0.66 0.73 3416 accuracy 0.78 7708 macro avg 0.78 0.77 0.77 7708 weighted avg 0.78 0.78 0.78 7708 TEST precision recall f1-score support 0 0.82 0.82 0.82 1095 1 0.52 0.52 0.52 405 accuracy 0.74 1500 macro avg 0.67 0.67 0.67 1500 weighted avg 0.74 0.74 0.74 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 899 196 1 194 211 CPU times: user 16.2 s, sys: 4.13 s, total: 20.4 s Wall time: 3.53 s . Getting the result . %%time final_model = catb.CatBoostClassifier(n_estimators=1500, max_depth=3, silent=True, random_state=56) final_model.fit(train_new, y) . CPU times: user 15.6 s, sys: 4.25 s, total: 19.8 s Wall time: 3.43 s . &lt;catboost.core.CatBoostClassifier at 0x7f8b795233a0&gt; . y_pred = final_model.predict(test_new) . result=pd.DataFrame({&#39;Id&#39;:np.arange(2500), &#39;Credit Default&#39;: y_pred}) . RESULT_PATH=&#39;solutions.csv&#39; result.to_csv(RESULT_PATH, index=False) . result.to_csv(&#39;solutions.csv&#39;, index=False) . In the overall rating table, I didn&#39;t get far and got a result of 0.45695 .",
            "url": "https://zmey56.github.io/blog//course%20project/machine%20learning/classification/geekbrain/2021/11/10/course-project-classification.html",
            "relUrl": "/course%20project/machine%20learning/classification/geekbrain/2021/11/10/course-project-classification.html",
            "date": " â€¢ Nov 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Course project on the cost of an apartment",
            "content": "There won&#39;t be much text, mostly a solution. I took 2nd place among 60 classmates, and 67th place out of 487 on kaggle. . import numpy as np import pandas as pd import random from sklearn.model_selection import cross_val_score, train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_squared_error from sklearn.model_selection import KFold, GridSearchCV import matplotlib import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline . import warnings warnings.filterwarnings(&#39;ignore&#39;) sns.set(style=&#39;whitegrid&#39;) sns.set_context(&quot;paper&quot;, font_scale=1.5) pd.options.display.float_format = &#39;{:,.2f}&#39;.format pd.set_option(&#39;display.max_rows&#39;, 50) . def adjustedR2(r2,n,k): return r2-(k-1)/(n-k)*(1-r2) . TRAIN_DATASET_PATH = &#39;project/train.csv&#39; TEST_DATASET_PATH = &#39;project/test.csv&#39; . 1. Uploading data . Description of the dataset . Id - apartment identification number | DistrictId - identification number of the district | Rooms - number of rooms | Square - Square | Life Square - living area | KitchenSquare - kitchen area | Floor - floor | House Floor - number of floors in the house | House Year - the year the house was built | Ecology_1, Ecology_2, Ecology_3 - environmental indicators of the area | Social_1, Social_2, Social_3 - social indicators of the area | Healthcare_1, Healthcare_2 - terrain indicators related to health protection | Shops_1, Shops_2 - indicators related to the availability of stores, shopping centers | Price - apartment price | . train_df = pd.read_csv(TRAIN_DATASET_PATH, index_col=&#39;Id&#39;) train_df.head() . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 Price . Id . 14038 35 | 2.00 | 47.98 | 29.44 | 6.00 | 7 | 9.00 | 1969 | 0.09 | B | B | 33 | 7976 | 5 | NaN | 0 | 11 | B | 184,966.93 | . 15053 41 | 3.00 | 65.68 | 40.05 | 8.00 | 7 | 9.00 | 1978 | 0.00 | B | B | 46 | 10309 | 1 | 240.00 | 1 | 16 | B | 300,009.45 | . 4765 53 | 2.00 | 44.95 | 29.20 | 0.00 | 8 | 12.00 | 1968 | 0.05 | B | B | 34 | 7759 | 0 | 229.00 | 1 | 3 | B | 220,925.91 | . 5809 58 | 2.00 | 53.35 | 52.73 | 9.00 | 8 | 17.00 | 1977 | 0.44 | B | B | 23 | 5735 | 3 | 1,084.00 | 0 | 5 | B | 175,616.23 | . 10783 99 | 1.00 | 39.65 | 23.78 | 7.00 | 11 | 12.00 | 1976 | 0.01 | B | B | 35 | 5776 | 1 | 2,078.00 | 2 | 4 | B | 150,226.53 | . test_df = pd.read_csv(TEST_DATASET_PATH, index_col=&#39;Id&#39;) test_df.head() . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 . Id . 725 58 | 2.00 | 49.88 | 33.43 | 6.00 | 6 | 14.00 | 1972 | 0.31 | B | B | 11 | 2748 | 1 | NaN | 0 | 0 | B | . 15856 74 | 2.00 | 69.26 | NaN | 1.00 | 6 | 1.00 | 1977 | 0.08 | B | B | 6 | 1437 | 3 | NaN | 0 | 2 | B | . 5480 190 | 1.00 | 13.60 | 15.95 | 12.00 | 2 | 5.00 | 1909 | 0.00 | B | B | 30 | 7538 | 87 | 4,702.00 | 5 | 5 | B | . 15664 47 | 2.00 | 73.05 | 51.94 | 9.00 | 22 | 22.00 | 2007 | 0.10 | B | B | 23 | 4583 | 3 | NaN | 3 | 3 | B | . 14275 27 | 1.00 | 47.53 | 43.39 | 1.00 | 17 | 17.00 | 2017 | 0.07 | B | B | 2 | 629 | 1 | NaN | 0 | 0 | A | . train_df.shape, test_df.shape . ((10000, 19), (5000, 18)) . 2. Visual data analysis . I will use the option offered in the lesson and my own thoughts. The first step is to get acquainted with the data. Let&#39;s build their histograms, and those that are not many - boxplot. . train_df.columns . Index([&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;], dtype=&#39;object&#39;) . df1=train_df[[&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;]] h = df1.hist(bins=25,figsize=(16,16),xlabelsize=&#39;10&#39;,ylabelsize=&#39;10&#39;,xrot=-15) sns.despine(left=True, bottom=True) [x.title.set_size(12) for x in h.ravel()]; [x.yaxis.tick_left() for x in h.ravel()]; . sns.set(style=&quot;whitegrid&quot;, font_scale=1) . f, axes = plt.subplots(1, 2,figsize=(15,5)) sns.boxplot(x=train_df[&#39;Rooms&#39;],y=train_df[&#39;Price&#39;], ax=axes[0]) sns.boxplot(x=train_df[&#39;Helthcare_2&#39;],y=train_df[&#39;Price&#39;], ax=axes[1]) sns.despine(left=True, bottom=True) axes[0].set(xlabel=&#39;Rooms&#39;, ylabel=&#39;Price&#39;) axes[0].yaxis.tick_left() axes[1].yaxis.set_label_position(&quot;right&quot;) axes[1].yaxis.tick_right() axes[1].set(xlabel=&#39;Helthcare_2&#39;, ylabel=&#39;Price&#39;) f, axe = plt.subplots(1, 1,figsize=(12.18,5)) sns.despine(left=True, bottom=True) sns.boxplot(x=train_df[&#39;Floor&#39;],y=train_df[&#39;Price&#39;], ax=axe) axe.yaxis.tick_left() axe.set(xlabel=&#39;Floor&#39;, ylabel=&#39;Price&#39;); . Of course, you can still play with charts for a long time, but it is unlikely that you will be able to visually identify the impact. I&#39;ll look at the correlation between the data. . features = [&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;] mask = np.zeros_like(train_df[features].corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(16, 12)) plt.title(&#39;Pearson Correlation Matrix&#39;,fontsize=25) sns.heatmap(train_df[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=&quot;BuGn&quot;, #&quot;BuGn_r&quot; to reverse linecolor=&#39;w&#39;,annot=True,annot_kws={&quot;size&quot;:8},mask=mask,cbar_kws={&quot;shrink&quot;: .9}); . 3. Data preparation . To prepare, I will use an example from the lesson . 3.1 Type conversion . train_df.dtypes . DistrictId int64 Rooms float64 Square float64 LifeSquare float64 KitchenSquare float64 Floor int64 HouseFloor float64 HouseYear int64 Ecology_1 float64 Ecology_2 object Ecology_3 object Social_1 int64 Social_2 int64 Social_3 int64 Healthcare_1 float64 Helthcare_2 int64 Shops_1 int64 Shops_2 object Price float64 dtype: object . train_df[&#39;DistrictId&#39;] = train_df[&#39;DistrictId&#39;].astype(str) test_df[&#39;DistrictId&#39;] = test_df[&#39;DistrictId&#39;].astype(str) . train_df.describe() . Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Price . count 10,000.00 | 10,000.00 | 7,887.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 10,000.00 | 5,202.00 | 10,000.00 | 10,000.00 | 10,000.00 | . mean 1.89 | 56.32 | 37.20 | 6.27 | 8.53 | 12.61 | 3,990.17 | 0.12 | 24.69 | 5,352.16 | 8.04 | 1,142.90 | 1.32 | 4.23 | 214,138.86 | . std 0.84 | 21.06 | 86.24 | 28.56 | 5.24 | 6.78 | 200,500.26 | 0.12 | 17.53 | 4,006.80 | 23.83 | 1,021.52 | 1.49 | 4.81 | 92,872.29 | . min 0.00 | 1.14 | 0.37 | 0.00 | 1.00 | 0.00 | 1,910.00 | 0.00 | 0.00 | 168.00 | 0.00 | 0.00 | 0.00 | 0.00 | 59,174.78 | . 25% 1.00 | 41.77 | 22.77 | 1.00 | 4.00 | 9.00 | 1,974.00 | 0.02 | 6.00 | 1,564.00 | 0.00 | 350.00 | 0.00 | 1.00 | 153,872.63 | . 50% 2.00 | 52.51 | 32.78 | 6.00 | 7.00 | 13.00 | 1,977.00 | 0.08 | 25.00 | 5,285.00 | 2.00 | 900.00 | 1.00 | 3.00 | 192,269.64 | . 75% 2.00 | 65.90 | 45.13 | 9.00 | 12.00 | 17.00 | 2,001.00 | 0.20 | 36.00 | 7,227.00 | 5.00 | 1,548.00 | 2.00 | 6.00 | 249,135.46 | . max 19.00 | 641.07 | 7,480.59 | 2,014.00 | 42.00 | 117.00 | 20,052,011.00 | 0.52 | 74.00 | 19,083.00 | 141.00 | 4,849.00 | 6.00 | 23.00 | 633,233.47 | . feature_num_names = train_df.drop(&#39;Price&#39;, axis=1).select_dtypes(include=[&#39;float64&#39;, &#39;int64&#39;]). columns.tolist() feature_num_names . [&#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;] . feature_cat_names = train_df.select_dtypes(include=&#39;object&#39;).columns.tolist() feature_cat_names . [&#39;DistrictId&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Shops_2&#39;] . feature_bin_names = [&#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Shops_2&#39;] train_df[feature_bin_names] = train_df[feature_bin_names].replace({&#39;A&#39;:0, &#39;B&#39;:1}) test_df[feature_bin_names] = test_df[feature_bin_names].replace({&#39;A&#39;:0, &#39;B&#39;:1}) train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 10000 entries, 14038 to 6306 Data columns (total 19 columns): # Column Non-Null Count Dtype -- -- 0 DistrictId 10000 non-null object 1 Rooms 10000 non-null float64 2 Square 10000 non-null float64 3 LifeSquare 7887 non-null float64 4 KitchenSquare 10000 non-null float64 5 Floor 10000 non-null int64 6 HouseFloor 10000 non-null float64 7 HouseYear 10000 non-null int64 8 Ecology_1 10000 non-null float64 9 Ecology_2 10000 non-null int64 10 Ecology_3 10000 non-null int64 11 Social_1 10000 non-null int64 12 Social_2 10000 non-null int64 13 Social_3 10000 non-null int64 14 Healthcare_1 5202 non-null float64 15 Helthcare_2 10000 non-null int64 16 Shops_1 10000 non-null int64 17 Shops_2 10000 non-null int64 18 Price 10000 non-null float64 dtypes: float64(8), int64(10), object(1) memory usage: 1.8+ MB . 3.2. Missing values . train_df.isna().sum()[train_df.isna().sum() != 0] . LifeSquare 2113 Healthcare_1 4798 dtype: int64 . train_df.loc[train_df[&#39;LifeSquare&#39;].isna(), &#39;LifeSquare&#39;] = train_df[&#39;Square&#39;] - train_df[&#39;KitchenSquare&#39;] test_df.loc[test_df[&#39;LifeSquare&#39;].isna(), &#39;LifeSquare&#39;] = test_df[&#39;Square&#39;] - test_df[&#39;KitchenSquare&#39;] . pd.concat([train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanmean(x)), train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanstd(x)), train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].count(), train_df.groupby(&#39;DistrictId&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: x.isna().sum()) ], axis=1, keys=[&#39;mean&#39;, &#39;std&#39;, &#39;count&#39;, &#39;nans&#39;]).sort_values(by=&#39;nans&#39;, ascending=False).head(10) . mean std count nans . DistrictId . 27 NaN | NaN | 0 | 851 | . 1 228.77 | 17.97 | 57 | 595 | . 23 0.00 | 0.00 | 1 | 564 | . 62 2,300.00 | 0.00 | 9 | 238 | . 45 NaN | NaN | 0 | 116 | . 34 NaN | NaN | 0 | 111 | . 61 80.00 | 0.00 | 8 | 110 | . 13 1,406.00 | 0.00 | 4 | 93 | . 9 30.00 | 0.00 | 202 | 92 | . 48 2,620.00 | 0.00 | 1 | 89 | . pd.concat([train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanmean(x)), train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanstd(x)), train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].count(), train_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: x.isna().sum()) ], axis=1, keys=[&#39;mean&#39;, &#39;std&#39;, &#39;count&#39;, &#39;nans&#39;]).sort_values(by=&#39;nans&#39;, ascending=False) . mean std count nans . Helthcare_2 . 0 1,007.78 | 913.61 | 1275 | 2939 | . 1 811.36 | 696.54 | 925 | 1258 | . 3 1,363.38 | 1,005.68 | 1347 | 323 | . 5 1,824.09 | 1,574.23 | 212 | 176 | . 2 1,010.71 | 1,045.44 | 1056 | 102 | . 4 1,929.23 | 1,121.87 | 288 | 0 | . 6 645.00 | 0.00 | 99 | 0 | . pd.concat([test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanmean(x)), test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: np.nanstd(x)), test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].count(), test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].apply(lambda x: x.isna().sum()) ], axis=1, keys=[&#39;mean&#39;, &#39;std&#39;, &#39;count&#39;, &#39;nans&#39;]).sort_values(by=&#39;nans&#39;, ascending=False) . mean std count nans . Helthcare_2 . 0 976.07 | 905.42 | 667 | 1429 | . 1 836.12 | 738.36 | 442 | 654 | . 3 1,353.30 | 1,064.11 | 700 | 155 | . 5 1,606.40 | 1,528.20 | 97 | 89 | . 2 1,095.59 | 1,135.98 | 520 | 50 | . 4 1,829.66 | 1,086.82 | 158 | 0 | . 6 645.00 | 0.00 | 39 | 0 | . train_df.loc[train_df[&#39;Healthcare_1&#39;].isna(), &#39;Healthcare_1&#39;] = train_df.groupby([&#39;Helthcare_2&#39;])[&#39;Healthcare_1&#39;].transform(lambda x: x.mean()) . test_df.loc[test_df[&#39;Healthcare_1&#39;].isna(), &#39;Healthcare_1&#39;] = train_df.groupby([&#39;Helthcare_2&#39;])[&#39;Healthcare_1&#39;].transform(lambda x: x.mean()) . test_df[&#39;Healthcare_1&#39;] = test_df[&#39;Healthcare_1&#39;].fillna(test_df.groupby(&#39;Helthcare_2&#39;)[&#39;Healthcare_1&#39;].transform(&#39;mean&#39;)) . 3.3. Processing of abnormal values . train_df[&#39;Rooms&#39;].value_counts() . 2.00 3880 1.00 3705 3.00 2235 4.00 150 5.00 18 0.00 8 10.00 2 6.00 1 19.00 1 Name: Rooms, dtype: int64 . train_df.loc[(train_df[&#39;Rooms&#39;] &gt; 5)|(train_df[&#39;Rooms&#39;] == 0), &#39;Rooms&#39;] = train_df[&#39;Rooms&#39;].mode()[0] test_df.loc[(test_df[&#39;Rooms&#39;] &gt; 5)|(test_df[&#39;Rooms&#39;] == 0), &#39;Rooms&#39;] = train_df[&#39;Rooms&#39;].mode()[0] # !! . train_df.loc[(train_df[&#39;KitchenSquare&#39;] &gt; 150) | (train_df[&#39;KitchenSquare&#39;] &gt; train_df[&#39;Square&#39;]), :] . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 Price . Id . 14656 62 | 1.00 | 47.10 | 46.45 | 2,014.00 | 4 | 1.00 | 2014 | 0.07 | 1 | 1 | 2 | 629 | 1 | 1,007.78 | 0 | 0 | 0 | 108,337.48 | . 13703 42 | 1.00 | 38.07 | 19.72 | 73.00 | 9 | 10.00 | 2006 | 0.16 | 1 | 1 | 21 | 5731 | 0 | 811.36 | 1 | 0 | 1 | 160,488.03 | . 6569 27 | 1.00 | 38.22 | 18.72 | 84.00 | 4 | 17.00 | 2018 | 0.01 | 1 | 1 | 4 | 915 | 0 | 1,007.78 | 0 | 0 | 1 | 99,079.96 | . 14679 81 | 1.00 | 32.28 | 19.28 | 1,970.00 | 6 | 1.00 | 1977 | 0.01 | 1 | 1 | 30 | 5285 | 0 | 645.00 | 6 | 6 | 1 | 105,539.56 | . train_df.loc[(train_df[&#39;KitchenSquare&#39;] &gt; 150) | (train_df[&#39;KitchenSquare&#39;] &gt; train_df[&#39;Square&#39;]), &#39;KitchenSquare&#39;] = train_df[&#39;KitchenSquare&#39;].median() test_df.loc[(test_df[&#39;KitchenSquare&#39;] &gt; 150) | (test_df[&#39;KitchenSquare&#39;] &gt; test_df[&#39;Square&#39;]), &#39;KitchenSquare&#39;] = train_df[&#39;KitchenSquare&#39;].median() . train_df.loc[(train_df[&#39;LifeSquare&#39;] &gt; 400), &#39;LifeSquare&#39;] = train_df[&#39;LifeSquare&#39;].median() test_df.loc[(test_df[&#39;LifeSquare&#39;] &gt; 400), &#39;LifeSquare&#39;] = train_df[&#39;LifeSquare&#39;].median() . train_df.loc[(train_df[&#39;Square&#39;] &gt; 400) | (train_df[&#39;Square&#39;] &lt; 10), &#39;Square&#39;] = train_df[&#39;Square&#39;].median() test_df.loc[(test_df[&#39;Square&#39;] &gt; 400) | (test_df[&#39;Square&#39;] &lt; 10), &#39;Square&#39;] = train_df[&#39;Square&#39;].median() . train_df[[&#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;]].describe() . Square LifeSquare KitchenSquare . count 10,000.00 | 10,000.00 | 10,000.00 | . mean 56.22 | 40.86 | 5.86 | . std 19.06 | 20.57 | 5.07 | . min 13.78 | 0.37 | 0.00 | . 25% 41.79 | 25.49 | 1.00 | . 50% 52.51 | 37.04 | 6.00 | . 75% 65.89 | 49.77 | 9.00 | . max 275.65 | 263.54 | 123.00 | . train_df.loc[train_df[&#39;HouseFloor&#39;] == 0, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] test_df.loc[test_df[&#39;HouseFloor&#39;] == 0, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] . train_df.loc[train_df[&#39;HouseFloor&#39;] &gt; 98, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] test_df.loc[test_df[&#39;HouseFloor&#39;] &gt; 98, &#39;HouseFloor&#39;] = train_df[&#39;HouseFloor&#39;].mode()[0] . floor_outliers_train = train_df.loc[train_df[&#39;Floor&#39;] &gt; train_df[&#39;HouseFloor&#39;]].index floor_outliers_test = test_df.loc[test_df[&#39;Floor&#39;] &gt; test_df[&#39;HouseFloor&#39;]].index . train_df.loc[floor_outliers_train, &#39;Floor&#39;] = train_df.loc[floor_outliers_train, &#39;HouseFloor&#39;].apply(lambda x: random.randint(1, x)) test_df.loc[floor_outliers_test, &#39;Floor&#39;] = test_df.loc[floor_outliers_test, &#39;HouseFloor&#39;].apply(lambda x: random.randint(1, x)) . train_df.loc[train_df[&#39;HouseYear&#39;] &gt; 2020, &#39;HouseYear&#39;] = 2011 . 3.4. Processing unique values . print(train_df[&#39;DistrictId&#39;].nunique(), &#39; ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð² train&#39;) print(test_df[&#39;DistrictId&#39;].nunique(), &#39; ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð² test&#39;) . 205 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð² train 201 ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð² test . district_size = train_df[&#39;DistrictId&#39;].value_counts().reset_index() .rename(columns={&#39;index&#39;:&#39;DistrictId&#39;, &#39;DistrictId&#39;:&#39;DistrictSize&#39;}) district_size . DistrictId DistrictSize . 0 27 | 851 | . 1 1 | 652 | . 2 23 | 565 | . 3 6 | 511 | . 4 9 | 294 | . ... ... | ... | . 200 117 | 1 | . 201 207 | 1 | . 202 209 | 1 | . 203 196 | 1 | . 204 205 | 1 | . 205 rows Ã— 2 columns . districts_popular = district_size.loc[district_size[&#39;DistrictSize&#39;] &gt; 100, &#39;DistrictId&#39;].tolist() district_size.loc[~district_size[&#39;DistrictId&#39;].isin(districts_popular), &#39;DistrictId&#39;] = 999 district_size = district_size.groupby(&#39;DistrictId&#39;)[&#39;DistrictSize&#39;].agg( DistrictSize=&#39;median&#39;) district_size.reset_index(level=&#39;DistrictId&#39;, inplace=True) district_size . DistrictId DistrictSize . 0 999 | 15 | . 1 1 | 652 | . 2 11 | 129 | . 3 21 | 165 | . 4 23 | 565 | . 5 27 | 851 | . 6 30 | 228 | . 7 34 | 111 | . 8 44 | 104 | . 9 45 | 116 | . 10 46 | 119 | . 11 52 | 143 | . 12 53 | 174 | . 13 57 | 107 | . 14 58 | 179 | . 15 6 | 511 | . 16 61 | 118 | . 17 62 | 247 | . 18 74 | 114 | . 19 8 | 142 | . 20 9 | 294 | . train_df.loc[~train_df[&#39;DistrictId&#39;].isin(districts_popular), &#39;DistrictId&#39;] = 999 test_df.loc[~test_df[&#39;DistrictId&#39;].isin(districts_popular), &#39;DistrictId&#39;] = 999 . 3.5. Building new features . train_df = train_df.merge(district_size, on=&#39;DistrictId&#39;, how=&#39;left&#39;).set_index(train_df.index) test_df = test_df.merge(district_size, on=&#39;DistrictId&#39;, how=&#39;left&#39;).set_index(test_df.index) train_df.head() . DistrictId Rooms Square LifeSquare KitchenSquare Floor HouseFloor HouseYear Ecology_1 Ecology_2 Ecology_3 Social_1 Social_2 Social_3 Healthcare_1 Helthcare_2 Shops_1 Shops_2 Price DistrictSize . Id . 14038 999 | 2.00 | 47.98 | 29.44 | 6.00 | 7 | 9.00 | 1969 | 0.09 | 1 | 1 | 33 | 7976 | 5 | 1,007.78 | 0 | 11 | 1 | 184,966.93 | 15 | . 15053 999 | 3.00 | 65.68 | 40.05 | 8.00 | 7 | 9.00 | 1978 | 0.00 | 1 | 1 | 46 | 10309 | 1 | 240.00 | 1 | 16 | 1 | 300,009.45 | 15 | . 4765 53 | 2.00 | 44.95 | 29.20 | 0.00 | 8 | 12.00 | 1968 | 0.05 | 1 | 1 | 34 | 7759 | 0 | 229.00 | 1 | 3 | 1 | 220,925.91 | 174 | . 5809 58 | 2.00 | 53.35 | 52.73 | 9.00 | 8 | 17.00 | 1977 | 0.44 | 1 | 1 | 23 | 5735 | 3 | 1,084.00 | 0 | 5 | 1 | 175,616.23 | 179 | . 10783 999 | 1.00 | 39.65 | 23.78 | 7.00 | 11 | 12.00 | 1976 | 0.01 | 1 | 1 | 35 | 5776 | 1 | 2,078.00 | 2 | 4 | 1 | 150,226.53 | 15 | . train_df[&#39;PriceOneRoom&#39;] = train_df[&#39;Price&#39;] / train_df[&#39;Rooms&#39;] . price_by_district = train_df.groupby([&#39;DistrictId&#39;], as_index=False) .agg({&#39;PriceOneRoom&#39;:&#39;median&#39;}) .rename(columns={&#39;PriceOneRoom&#39;:&#39;PriceOneRoomByDistrict&#39;}) price_by_district . DistrictId PriceOneRoomByDistrict . 0 999 | 129,339.27 | . 1 1 | 106,080.90 | . 2 11 | 100,098.98 | . 3 21 | 104,448.25 | . 4 23 | 76,232.42 | . 5 27 | 94,738.78 | . 6 30 | 93,960.73 | . 7 34 | 129,738.65 | . 8 44 | 125,925.66 | . 9 45 | 164,907.84 | . 10 46 | 117,867.67 | . 11 52 | 97,309.46 | . 12 53 | 138,669.20 | . 13 57 | 121,811.56 | . 14 58 | 105,638.52 | . 15 6 | 94,232.67 | . 16 61 | 119,679.38 | . 17 62 | 102,579.54 | . 18 74 | 127,646.56 | . 19 8 | 120,139.85 | . 20 9 | 96,749.26 | . train_df = train_df.merge(price_by_district, on=[&#39;DistrictId&#39;], how=&#39;left&#39;).set_index(train_df.index) test_df = test_df.merge(price_by_district, on=[&#39;DistrictId&#39;], how=&#39;left&#39;).set_index(test_df.index) . test_df.columns . Index([&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoomByDistrict&#39;], dtype=&#39;object&#39;) . train_df.columns . Index([&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoom&#39;, &#39;PriceOneRoomByDistrict&#39;], dtype=&#39;object&#39;) . train_df = train_df.drop([&#39;PriceOneRoom&#39;], axis=1) . train_df[&#39;RoomSquare&#39;] = train_df[&#39;Square&#39;] / train_df[&#39;Rooms&#39;] test_df[&#39;RoomSquare&#39;] = test_df[&#39;Square&#39;] / test_df[&#39;Rooms&#39;] . train_df.loc[train_df[&#39;Floor&#39;] &lt;= 5, &#39;Floor_cat&#39;] = 1 train_df.loc[(train_df[&#39;Floor&#39;] &gt; 5) &amp; (train_df[&#39;Floor&#39;] &lt;= 17), &#39;Floor_cat&#39;] = 2 train_df.loc[train_df[&#39;Floor&#39;] &gt; 17, &#39;Floor_cat&#39;] = 3 test_df.loc[test_df[&#39;Floor&#39;] &lt;= 5, &#39;Floor_cat&#39;] = 1 test_df.loc[(test_df[&#39;Floor&#39;] &gt; 5) &amp; (test_df[&#39;Floor&#39;] &lt;= 17), &#39;Floor_cat&#39;] = 2 test_df.loc[test_df[&#39;Floor&#39;] &gt; 17, &#39;Floor_cat&#39;] = 3 train_df[&#39;Floor_cat&#39;].value_counts() . 2.00 5232 1.00 4424 3.00 344 Name: Floor_cat, dtype: int64 . labels = [1, 2, 3] train_df[&#39;Floor_cat_qcut&#39;] = pd.qcut(train_df[&#39;Floor&#39;], q=3, labels=labels) test_df[&#39;Floor_cat_qcut&#39;] = pd.qcut(test_df[&#39;Floor&#39;], q=3, labels=labels) train_df[&#39;Floor_cat_qcut&#39;].value_counts() . 1 3521 2 3519 3 2960 Name: Floor_cat_qcut, dtype: int64 . train_df.loc[train_df[&#39;HouseFloor&#39;] &lt;= 5, &#39;HouseFloor_cat&#39;] = 1 train_df.loc[(train_df[&#39;HouseFloor&#39;] &gt; 5) &amp; (train_df[&#39;HouseFloor&#39;] &lt;= 17), &#39;HouseFloor_cat&#39;] = 2 train_df.loc[train_df[&#39;HouseFloor&#39;] &gt; 17, &#39;HouseFloor_cat&#39;] = 3 test_df.loc[test_df[&#39;HouseFloor&#39;] &lt;= 5, &#39;HouseFloor_cat&#39;] = 1 test_df.loc[(test_df[&#39;HouseFloor&#39;] &gt; 5) &amp; (test_df[&#39;HouseFloor&#39;] &lt;= 17), &#39;HouseFloor_cat&#39;] = 2 test_df.loc[test_df[&#39;HouseFloor&#39;] &gt; 17, &#39;HouseFloor_cat&#39;] = 3 train_df[&#39;HouseFloor_cat&#39;].value_counts() . 2.00 6838 1.00 1837 3.00 1325 Name: HouseFloor_cat, dtype: int64 . train_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]] = train_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]].astype(int) test_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]] = test_df[[&#39;Floor_cat&#39;, &#39;HouseFloor_cat&#39;]].astype(int) . 4. Analysis . 4.1. Selection of features . feature_names = train_df.columns feature_names.tolist() . [&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;Price&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoomByDistrict&#39;, &#39;RoomSquare&#39;, &#39;Floor_cat&#39;, &#39;Floor_cat_qcut&#39;, &#39;HouseFloor_cat&#39;] . target_names = [&#39;Price&#39;] feature_names = [&#39;DistrictId&#39;, &#39;Rooms&#39;, &#39;Square&#39;, &#39;LifeSquare&#39;, &#39;KitchenSquare&#39;, &#39;Floor&#39;, &#39;HouseFloor&#39;, &#39;HouseYear&#39;, &#39;Ecology_1&#39;, &#39;Ecology_2&#39;, &#39;Ecology_3&#39;, &#39;Social_1&#39;, &#39;Social_2&#39;, &#39;Social_3&#39;, &#39;Healthcare_1&#39;, &#39;Helthcare_2&#39;, &#39;Shops_1&#39;, &#39;Shops_2&#39;, &#39;DistrictSize&#39;, &#39;PriceOneRoomByDistrict&#39;, &#39;RoomSquare&#39;, &#39;Floor_cat&#39;, &#39;Floor_cat_qcut&#39;, &#39;HouseFloor_cat&#39;] . X = train_df[feature_names] y = train_df[target_names] . forest = RandomForestRegressor(random_state=56) . forest_best = RandomForestRegressor(max_depth=15, max_features=5, random_state=56) forest_best.fit(X, y) y_pred = forest_best.predict(test_df[feature_names]) y_pred . array([162556.371278 , 231974.01815621, 218945.2288756 , ..., 328836.90951946, 198611.16589857, 176794.34957387]) . preds = pd.DataFrame() preds[&#39;Id&#39;] = test_df.index preds[&#39;Price&#39;] = y_pred preds.head() . Id Price . 0 725 | 162,556.37 | . 1 15856 | 231,974.02 | . 2 5480 | 218,945.23 | . 3 15664 | 343,927.38 | . 4 14275 | 144,108.09 | . preds.to_csv(&#39;Gladkikh_predictions_1.csv&#39;, index=False) .",
            "url": "https://zmey56.github.io/blog//course%20project/machine%20learning/classification/geekbrain/2021/10/10/course-project-regression-flat-price.html",
            "relUrl": "/course%20project/machine%20learning/classification/geekbrain/2021/10/10/course-project-regression-flat-price.html",
            "date": " â€¢ Oct 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Course project "MySQL database for the financial instruments market."",
            "content": "According to the task presented, I needed to come up with a topic and create a database based on it.Here is a general description of the project: . Requirements for the course project: . Make a general text description of the database and the tasks it solves; | the minimum number of tables is 10; | scripts for creating a database structure (with primary keys, indexes, foreign keys); | create an ER Diagram for the database; | scripts for filling the database with data; | scripts of characteristic samples (including groupings, joins, nested tables); | submissions (minimum 2); | stored procedures/triggers; | Examples: describe the data storage model of a popular website: kinopoisk, booking.com , wikipedia, online store, geekbrains, public servicesâ€¦ . I took [the article] as a basis (https://vertabelo.com/blog/a-data-model-for-trading-stocks-funds-and-cryptocurrencies /) posted on a foreign source. It shows the basic scheme. It remains only to translate it into SQL. First of all, the translation of the theory. . Trading cryptocurrencies, buying stocks and the like are extremely popular these days, as it is perceived as an easy profit. Prices are currently rising, but we canâ€™t know when that will change. On the other hand, we know that this will happen at some point. But we are not here to make financial forecasts. Instead, weâ€™ll talk about a data model that can be used to support trading in cryptocurrencies and financial instruments such as stocks or fund stocks. . 1. General text description of the database and the tasks to be solved . What You Need to Know About Trading Currencies and Stocks . Technological improvements over the past few decades have had a significant impact on trade. There are currently many online trading platforms that you can use. Most of todayâ€™s trading is done virtually â€“ you can see paper stocks in museums, but it is unlikely that you will see stocks that you buy in paper form. And you donâ€™t have to pack your bags and go to Wall Street or any other stock exchange to make a deal. From the comfort of your computer or mobile device, you can buy or sell derivative financial instruments (such as bonds, stocks or commodities). . Most transactions (sale of derivative financial instruments) follow the same rules. There are sellers and buyers. If they agree on the price, the deal will take place. After the transaction, the price of this derivative financial instrument will be recalculated, and the process will continue with new traders. Stocks and other derivative financial instruments work the same way. . What is cryptocurrency? Youâ€™ve probably heard of bitcoin and other cryptocurrencies. But what is it? Cryptocurrencies are similar to virtual currencies, but they are not tied to real world currencies (such as euros or dollars). Instead, users can trade cryptocurrencies among themselves as tokens. Then they can negotiate a sale that will turn their tokens into real money. These sales function in exactly the same way as the stock and stock transactions described above. . This topic is complex, and there may be many details in our model (for example, records of documents and transactions). Iâ€™m going to make it simple; Iâ€™m not going to implement any automated trading or any formulas to create new prices after a trading event. . Letâ€™s move on to the code. According to the article , the database consists of three blocks: . CURRENCIES | TRADERS | ITEMS | . Now letâ€™s write the code for each block: . ###2. Scripts for creating a database structure . CURRENCIES . CREATE DATABASE IF NOT EXISTS coursework_portfolio; USE coursework_portfolio; . First I create the country from which the trade is carried out . DROP TABLE IF EXISTS country; CREATE TABLE country( id SERIAL PRIMARY KEY, country VARCHAR(128) ) COMMENT = &#39;The country from which the trade is carried out&#39;; . The currency that the user uses for trading . DROP TABLE IF EXISTS currency_used; CREATE TABLE currency_used( id SERIAL PRIMARY KEY, country_id BIGINT UNSIGNED NOT NULL COMMENT &#39;Key to another table&#39;, currency_id INT UNSIGNED COMMENT &#39;Key to another table&#39;, data_from DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Start date of currency usage&#39;, data_to DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;The end date of currency usage. If NULL, then the currency is still in use&#39; ) COMMENT = &#39;Currency used for purchase&#39;; DESC currency_used ; . Current and historical exchange rates between currency pairs are stored. . DROP TABLE IF EXISTS currency_rate; CREATE TABLE currency_rate( id SERIAL PRIMARY KEY, currency_id INT UNSIGNED COMMENT &#39;Key to another table&#39;, base_currency_id INT UNSIGNED COMMENT &#39;Key to another table&#39;, rate DECIMAL(16,6) NOT NULL DEFAULT 0 COMMENT &#39;Currency exchange rate&#39;, ts DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;The time at which this course was fixed&#39; ) COMMENT = &#39;Currency exchange rate&#39;; . Store all the currencies that we have ever used for trading. . DROP TABLE IF EXISTS currency; CREATE TABLE currency( id INT UNSIGNED PRIMARY KEY, code VARCHAR(8) NOT NULL UNIQUE COMMENT &#39;The code used for the unique designation of the currency&#39;, name VARCHAR(128) NOT NULL UNIQUE COMMENT &#39;The unique name of this currency&#39;, is_active BOOL DEFAULT FALSE COMMENT &#39;If the currency is currently active in our system&#39;, is_base_currency BOOL DEFAULT FALSE COMMENT &#39;If this currency is the base currency of our system.&#39; ) COMMENT = &#39;Currency exchange rate&#39;; . ITEMS . The item tables define all the products available for trading and their current status. It also records all the changes that have occurred with these products over time. . DROP TABLE IF EXISTS item; CREATE TABLE item( id SERIAL PRIMARY KEY, code VARCHAR(64) NOT NULL UNIQUE COMMENT &#39;The code used for the unique designation of the product (shares, mutual funds, etc.)&#39;, name VARCHAR(255) NOT NULL UNIQUE COMMENT &#39;Full name&#39;, is_active BOOL DEFAULT FALSE COMMENT &#39;Is this product available for trading or not&#39;, currency_id INT UNSIGNED COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, details TEXT COMMENT &#39;All additional information (for example, the number of shares issued) in text format.&#39; ) COMMENT = &#39;Available products&#39;; . The price table tracks all price changes over time. . DROP TABLE IF EXISTS price; CREATE TABLE price( id SERIAL PRIMARY KEY, item_id BIGINT UNSIGNED COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, currency_id INT UNSIGNED COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, buy DECIMAL(16,6) NOT NULL DEFAULT 0 COMMENT &#39;Purchase rate&#39;, sell DECIMAL(16,6) NOT NULL DEFAULT 0 COMMENT &#39;Selling rate&#39;, ts DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;The time at which the transaction at the last price was fixed&#39; ) COMMENT = &#39;Price change&#39;; . Report table . DROP TABLE IF EXISTS report; CREATE TABLE report( id SERIAL PRIMARY KEY, trading_data DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;Report date&#39;, item_id BIGINT UNSIGNED COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, currency_id INT UNSIGNED COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, first_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Initial price&#39;, last_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Last price&#39;, min_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Minimum price&#39;, max_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Maximum price&#39;, avg_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Average price&#39;, total_amount DECIMAL(16,6) DEFAULT NULL COMMENT &#39;The total amount paid for this product during the reporting period.&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;The number of products sold during this reporting period.&#39; ) COMMENT = &#39;Report&#39;; . TRADERS . Table of traders . DROP TABLE IF EXISTS trader; CREATE TABLE trader ( id SERIAL PRIMARY KEY, firstname VARCHAR(50) COMMENT &#39;Name&#39;, lastname VARCHAR(50) COMMENT &#39;Surname&#39;, user_name VARCHAR(50) NOT NULL UNIQUE COMMENT &#39;Everyone&#39;s login is unique&#39;, email VARCHAR(120) NOT NULL UNIQUE, confirmation_code VARCHAR(120) NOT NULL COMMENT &#39;The code sent to the user to complete the registration process.&#39;, time_registered DATETIME DEFAULT CURRENT_TIMESTAMP, time_confirmed DATETIME DEFAULT CURRENT_TIMESTAMP, country_id BIGINT UNSIGNED COMMENT &#39;The country in which he lives.&#39;, preffered_currency_id BIGINT UNSIGNED COMMENT &#39;The currency that the trader prefers&#39; ) COMMENT &#39;users&#39;; . A list of all the products that the trader currently owns . DROP TABLE IF EXISTS current_inventory; CREATE TABLE current_inventory ( id SERIAL PRIMARY KEY, trader_id BIGINT UNSIGNED COMMENT &#39;Link to the trader&#39;, item_id BIGINT UNSIGNED COMMENT &#39;Product link&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Number of products&#39; ) COMMENT &#39;List of products&#39;; . Trading Event . DROP TABLE IF EXISTS trade; CREATE TABLE trade ( id SERIAL PRIMARY KEY, item_id BIGINT UNSIGNED COMMENT &#39;Product link&#39;, seller_id BIGINT UNSIGNED DEFAULT NULL COMMENT &#39;Link to the trader&#39;, buyer_id BIGINT UNSIGNED COMMENT &#39;Link to the trader&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Number of products&#39;, unit_price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Price per unit&#39;, description TEXT COMMENT &#39;All additional information (for example, the number of shares issued) in text format.&#39;, offer_id BIGINT UNSIGNED COMMENT &#39;Transaction Identifier&#39; ) COMMENT &#39;Transactions&#39;; . Accounting for all offers . DROP TABLE IF EXISTS offer; CREATE TABLE offer ( id SERIAL PRIMARY KEY, item_id BIGINT UNSIGNED COMMENT &#39;Product link&#39;, trader_id BIGINT UNSIGNED DEFAULT NULL COMMENT &#39;Link to the trader&#39;, quantity DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Number of products&#39;, buy BOOL DEFAULT FALSE, sell BOOL DEFAULT FALSE, price DECIMAL(16,6) DEFAULT NULL COMMENT &#39;Desired price per unit&#39;, ts DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT &#39;When was it exposed&#39;, is_active BOOL DEFAULT FALSE COMMENT &#39;Is this offer still valid&#39; ) COMMENT &#39;Transactions&#39;; . I also decided to add the birthday column to traders . ALTER TABLE trader DROP COLUMN birthday; ALTER TABLE trader ADD COLUMN birthday DATE ; . 3. Relationship creation scripts . USE coursework_portfolio; . one-to-many relationship currency - currency_used . ALTER TABLE currency_used ADD CONSTRAINT currency_used_country_fk FOREIGN KEY(country_id) REFERENCES country(id); . one-to-many relationship currency - currency_used . ALTER TABLE currency_used ADD CONSTRAINT currency_used_currency_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . one-to-many relationship currency - currency_rate . ALTER TABLE currency_rate ADD CONSTRAINT currency_currency_rate_fk FOREIGN KEY(currency_id) REFERENCES currency(id); ALTER TABLE currency_rate ADD CONSTRAINT currency_currency_rate_base_fk FOREIGN KEY(base_currency_id) REFERENCES currency(id); . one - to- many currency - price relationship . ALTER TABLE price ADD CONSTRAINT currency_price_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . one-to-many currency - item relationship . ALTER TABLE item ADD CONSTRAINT currency_item_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . one- to- many relationship price - item . ALTER TABLE price ADD CONSTRAINT item_price_fk FOREIGN KEY(item_id) REFERENCES item(id); . one-to-many relationship report - item . ALTER TABLE report ADD CONSTRAINT report_item_fk FOREIGN KEY(item_id) REFERENCES item(id); . one - to - many report - currency relationship . ALTER TABLE report ADD CONSTRAINT currency_report_fk FOREIGN KEY(currency_id) REFERENCES currency(id); . one-to-many trader - currency connection . ALTER TABLE trader ADD CONSTRAINT country_trader_fk FOREIGN KEY(country_id) REFERENCES country(id); . one-to-many relationship trade - trader . ALTER TABLE trade ADD CONSTRAINT trader_trade_sell_fk FOREIGN KEY(seller_id) REFERENCES trader(id); . one-to-many relationship trade - trader . ALTER TABLE trade ADD CONSTRAINT trader_trade_buy_fk FOREIGN KEY(buyer_id) REFERENCES trader(id); . one - to - many trade - offer relationship . ALTER TABLE trade ADD CONSTRAINT offer_trade_fk FOREIGN KEY(offer_id) REFERENCES offer(id); . one-to-many relationship trade - item . ALTER TABLE trade ADD CONSTRAINT item_trade_fk FOREIGN KEY(item_id) REFERENCES item(id); . one-to-many relationship offer - trader . ALTER TABLE offer ADD CONSTRAINT trader_offer_fk FOREIGN KEY(trader_id) REFERENCES trader(id); . one-to- many relationship offer - item . ALTER TABLE offer ADD CONSTRAINT item_offer_fk FOREIGN KEY(item_id) REFERENCES item(id); . one-to-many relationship Current_inventory - trader . ALTER TABLE current_inventory ADD CONSTRAINT trader_current_inventory_fk FOREIGN KEY(trader_id) REFERENCES trader(id); . one-to-many relationship Current_inventory - item . ALTER TABLE current_inventory ADD CONSTRAINT item_current_inventory_fk FOREIGN KEY(item_id) REFERENCES item(id); . 4. Creating an ERDiagram for a database . The result is the following scheme: . . 5. Scripts for filling the database with data . The filling was done through a popular site for generating fake data [Dummy Data for MYSQL Database] (http://filldb.info /) . CREATE DATABASE coursework_portfolio; -- MariaDB dump 10.17 Distrib 10.4.15-MariaDB, for Linux (x86_64) -- -- Host: mysql.hostinger.ro Database: u574849695_20 -- -- Server version 10.4.15-MariaDB-cll-lve /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!40101 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE=&#39;+00:00&#39; */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE=&#39;NO_AUTO_VALUE_ON_ZERO&#39; */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `country` -- USE coursework_portfolio; DROP TABLE IF EXISTS `country`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `country` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `country` varchar(128) COLLATE utf8mb4_unicode_ci DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;The country from which the trade is carried out&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `country` -- LOCK TABLES `country` WRITE; /*!40000 ALTER TABLE `country` DISABLE KEYS */; INSERT INTO `country` VALUES (1,&#39;Bermuda&#39;),(2,&#39;Christmas Island&#39;),(3,&#39;Holy See (Vatican City State)&#39;),(4,&#39;Macedonia&#39;),(5,&#39;Qatar&#39;),(6,&#39;Tokelau&#39;),(7,&#39;Romania&#39;),(8,&#39;Central African Republic&#39;),(9,&#39;Uganda&#39;),(10,&#39;Saint Pierre and Miquelon&#39;); /*!40000 ALTER TABLE `country` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `currency` -- DROP TABLE IF EXISTS `currency`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `currency` ( `id` int(10) unsigned NOT NULL, `code` varchar(8) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;The code used for the unique designation of the currency&#39;, `name` varchar(128) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;The unique name of this currency&#39;, `is_active` tinyint(1) DEFAULT 0 COMMENT &#39;If the currency is currently active in our system&#39;, `is_base_currency` tinyint(1) DEFAULT 0 COMMENT &#39;If this currency is the base currency of our system.&#39;, PRIMARY KEY (`id`), UNIQUE KEY `code` (`code`), UNIQUE KEY `name` (`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Currency exchange rate&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `currency` -- LOCK TABLES `currency` WRITE; /*!40000 ALTER TABLE `currency` DISABLE KEYS */; INSERT INTO `currency` VALUES (0,&#39;htxs&#39;,&#39;eaque&#39;,0,0),(1,&#39;nnlp&#39;,&#39;soluta&#39;,1,0),(2,&#39;dtlh&#39;,&#39;quis&#39;,0,0),(3,&#39;wimu&#39;,&#39;animi&#39;,0,0),(4,&#39;lnag&#39;,&#39;earum&#39;,0,0),(5,&#39;ekbd&#39;,&#39;molestiae&#39;,1,0),(6,&#39;nivw&#39;,&#39;nam&#39;,1,0),(7,&#39;zldi&#39;,&#39;consequuntur&#39;,1,1),(8,&#39;zyrs&#39;,&#39;qui&#39;,1,1),(9,&#39;uvfn&#39;,&#39;blanditiis&#39;,1,1); /*!40000 ALTER TABLE `currency` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `currency_rate` -- DROP TABLE IF EXISTS `currency_rate`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `currency_rate` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Key to another table&#39;, `base_currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Key to another table&#39;, `rate` decimal(16,6) NOT NULL DEFAULT 0.000000 COMMENT &#39;Currency exchange rate&#39;, `ts` datetime DEFAULT current_timestamp() COMMENT &#39;The time at which this course was fixed&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Currency exchange rate&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `currency_rate` -- LOCK TABLES `currency_rate` WRITE; /*!40000 ALTER TABLE `currency_rate` DISABLE KEYS */; INSERT INTO `currency_rate` VALUES (1,0,0,248964.149700,&#39;1992-06-09 14:41:36&#39;),(2,1,1,390368.888800,&#39;2017-09-18 05:43:25&#39;),(3,2,2,0.000000,&#39;2001-11-16 22:01:24&#39;),(4,3,3,50.506542,&#39;1979-06-15 23:49:48&#39;),(5,4,4,81.498779,&#39;1984-06-23 02:36:52&#39;),(6,5,5,584941.290982,&#39;1970-04-04 16:47:28&#39;),(7,6,6,6265204.580000,&#39;1979-09-29 09:25:05&#39;),(8,7,7,16.154948,&#39;2010-12-21 00:43:41&#39;),(9,8,8,8.630000,&#39;1986-04-16 05:48:55&#39;),(10,9,9,2.800000,&#39;1984-10-06 10:07:42&#39;); /*!40000 ALTER TABLE `currency_rate` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `currency_used` -- DROP TABLE IF EXISTS `currency_used`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `currency_used` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `country_id` bigint(20) unsigned NOT NULL COMMENT &#39;Key to another table&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Key to another table&#39;, `data_from` datetime DEFAULT current_timestamp() COMMENT &#39;Start date of currency usage&#39;, `data_to` datetime DEFAULT current_timestamp() COMMENT &#39;The end date of currency usage. If NULL, then the currency is still in use&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Currency used for purchase&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `currency_used` -- LOCK TABLES `currency_used` WRITE; /*!40000 ALTER TABLE `currency_used` DISABLE KEYS */; INSERT INTO `currency_used` VALUES (1,1,0,&#39;1972-07-14 21:53:10&#39;,&#39;1989-06-25 02:18:23&#39;),(2,2,1,&#39;1970-03-19 12:37:26&#39;,&#39;2018-12-11 16:34:23&#39;),(3,3,2,&#39;1989-06-15 13:50:05&#39;,&#39;1985-06-10 17:07:32&#39;),(4,4,3,&#39;2008-09-26 16:36:58&#39;,&#39;1992-01-21 09:35:20&#39;),(5,5,4,&#39;1984-09-02 16:32:24&#39;,&#39;2019-07-25 03:33:31&#39;),(6,6,5,&#39;2018-10-02 04:32:51&#39;,&#39;1973-08-23 18:43:00&#39;),(7,7,6,&#39;2004-07-04 22:17:20&#39;,&#39;1990-11-19 16:48:34&#39;),(8,8,7,&#39;2007-06-25 10:34:33&#39;,&#39;1997-09-16 08:51:40&#39;),(9,9,8,&#39;1977-10-01 11:13:18&#39;,&#39;1985-03-05 19:28:04&#39;),(10,10,9,&#39;1987-03-28 10:28:24&#39;,&#39;2003-02-05 18:20:56&#39;); /*!40000 ALTER TABLE `currency_used` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `current_inventory` -- DROP TABLE IF EXISTS `current_inventory`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `current_inventory` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `trader_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Link to the trader&#39;, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Product link&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Number of products&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;List of products&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `current_inventory` -- LOCK TABLES `current_inventory` WRITE; /*!40000 ALTER TABLE `current_inventory` DISABLE KEYS */; INSERT INTO `current_inventory` VALUES (1,1,1,8421.000000),(2,2,2,969106.000000),(3,3,3,9235206.000000),(4,4,4,92961.000000),(5,5,5,145700.000000),(6,6,6,0.000000),(7,7,7,19133.000000),(8,8,8,0.000000),(9,9,9,26582.000000),(10,10,10,78483052.000000); /*!40000 ALTER TABLE `current_inventory` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `item` -- DROP TABLE IF EXISTS `item`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `item` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `code` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;The code used for the unique designation of the product (shares, mutual funds, etc.)&#39;, `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Full name&#39;, `is_active` tinyint(1) DEFAULT 0 COMMENT &#39;Is this product available for trading or not&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, `details` text COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;All additional information (for example, the number of shares issued) in text format.&#39;, PRIMARY KEY (`id`), UNIQUE KEY `code` (`code`), UNIQUE KEY `name` (`name`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Available products&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `item` -- LOCK TABLES `item` WRITE; /*!40000 ALTER TABLE `item` DISABLE KEYS */; INSERT INTO `item` VALUES (1,&#39;kzgb&#39;,&#39;doloremque&#39;,1,0,NULL),(2,&#39;krcn&#39;,&#39;omnis&#39;,1,1,NULL),(3,&#39;ywfp&#39;,&#39;rerum&#39;,0,2,NULL),(4,&#39;bbjq&#39;,&#39;eaque&#39;,0,3,NULL),(5,&#39;hsib&#39;,&#39;quis&#39;,1,4,NULL),(6,&#39;dnuf&#39;,&#39;quia&#39;,1,5,NULL),(7,&#39;wnfb&#39;,&#39;numquam&#39;,0,6,NULL),(8,&#39;pefi&#39;,&#39;quos&#39;,0,7,NULL),(9,&#39;vbrv&#39;,&#39;expedita&#39;,1,8,NULL),(10,&#39;afem&#39;,&#39;esse&#39;,0,9,NULL); /*!40000 ALTER TABLE `item` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `offer` -- DROP TABLE IF EXISTS `offer`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `offer` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Product link&#39;, `trader_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Link to the trader&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Number of products&#39;, `buy` tinyint(1) DEFAULT 0, `sell` tinyint(1) DEFAULT 0, `price` decimal(16,6) DEFAULT NULL COMMENT &#39;Desired price per unit&#39;, `ts` datetime DEFAULT current_timestamp() COMMENT &#39;When was it exposed&#39;, `is_active` tinyint(1) DEFAULT 0 COMMENT &#39;Is this offer still valid&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Transactions&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `offer` -- LOCK TABLES `offer` WRITE; /*!40000 ALTER TABLE `offer` DISABLE KEYS */; INSERT INTO `offer` VALUES (1,1,1,4492564.690000,0,0,3560.226016,&#39;1978-04-16 05:11:17&#39;,0),(2,2,2,88065.115876,0,1,25673.801940,&#39;2012-01-27 00:48:33&#39;,0),(3,3,3,57342.670793,1,1,5968955.094700,&#39;1994-04-03 09:01:57&#39;,0),(4,4,4,0.841573,0,1,43.341753,&#39;2016-01-27 13:50:13&#39;,1),(5,5,5,21273859.908703,0,1,1003.024800,&#39;1976-04-11 21:39:31&#39;,0),(6,6,6,330151.950000,0,0,0.908693,&#39;2005-12-14 22:57:53&#39;,1),(7,7,7,270663059.774970,1,0,105.000000,&#39;1988-11-26 14:32:08&#39;,0),(8,8,8,242300815.081650,1,1,418.591556,&#39;2009-01-04 11:59:22&#39;,0),(9,9,9,43.108410,1,0,175276231.966730,&#39;1980-11-30 22:09:40&#39;,1),(10,10,10,35074063.000000,0,0,3151.711086,&#39;2007-11-07 11:28:43&#39;,1); /*!40000 ALTER TABLE `offer` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `price` -- DROP TABLE IF EXISTS `price`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `price` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, `buy` decimal(16,6) NOT NULL DEFAULT 0.000000 COMMENT &#39;Purchase rate&#39;, `sell` decimal(16,6) NOT NULL DEFAULT 0.000000 COMMENT &#39;Selling rate&#39;, `ts` datetime DEFAULT current_timestamp() COMMENT &#39;The time at which the transaction at the last price was fixed&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Price change&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `price` -- LOCK TABLES `price` WRITE; /*!40000 ALTER TABLE `price` DISABLE KEYS */; INSERT INTO `price` VALUES (1,1,0,833143.663441,7083.640000,&#39;1974-02-28 12:14:53&#39;),(2,2,1,506349.000000,13.956034,&#39;1989-08-25 08:13:42&#39;),(3,3,2,0.000000,3.100000,&#39;1977-01-24 01:27:13&#39;),(4,4,3,30754203.895000,22564.976107,&#39;2005-03-20 21:58:47&#39;),(5,5,4,607602067.694800,692.700000,&#39;1995-05-31 22:30:59&#39;),(6,6,5,55.747000,131.327840,&#39;2011-06-04 13:19:14&#39;),(7,7,6,4047695.239467,9294.278000,&#39;1991-08-02 18:28:19&#39;),(8,8,7,2053.700000,42688565.419750,&#39;1995-09-23 02:06:23&#39;),(9,9,8,29.743971,5.057628,&#39;2002-05-03 05:17:56&#39;),(10,10,9,56795556.100000,3272.168110,&#39;1994-03-20 04:59:16&#39;); /*!40000 ALTER TABLE `price` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `report` -- DROP TABLE IF EXISTS `report`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `report` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `trading_data` datetime DEFAULT current_timestamp() COMMENT &#39;Report date&#39;, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, `currency_id` int(10) unsigned DEFAULT NULL COMMENT &#39;Refers to the currency used as the base currency for this product&#39;, `first_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Initial price&#39;, `last_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Last price&#39;, `min_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Minimum price&#39;, `max_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Maximum price&#39;, `avg_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Average price&#39;, `total_amount` decimal(16,6) DEFAULT NULL COMMENT &#39;The total amount paid for this product during the reporting period.&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;The number of products sold during this reporting period.&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Report&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `report` -- LOCK TABLES `report` WRITE; /*!40000 ALTER TABLE `report` DISABLE KEYS */; INSERT INTO `report` VALUES (1,&#39;2008-09-12 05:40:01&#39;,1,0,0.000000,0.000000,11284.100000,0.000000,2113910.285725,33.000000,0.000000),(2,&#39;2005-08-05 11:49:42&#39;,2,1,7.124000,17.382430,2.353250,2569858.500000,15130.120000,0.000000,0.000000),(3,&#39;1985-04-14 09:07:10&#39;,3,2,43490527.885671,1452.364336,26509.073543,135.213446,341326328.324550,33705.000000,45819518.090000),(4,&#39;2018-02-14 00:52:19&#39;,4,3,649342.981000,3.424990,991.284828,4063.439875,1324038.780000,9267588.571890,133722.790635),(5,&#39;2003-12-24 09:46:13&#39;,5,4,2413.800000,3.000000,170787253.391930,221895.722237,2782544.825000,196.304264,85386572.462400),(6,&#39;2014-11-14 05:28:06&#39;,6,5,4.320848,0.000000,120899.185986,351820.630000,44.353438,0.000000,2229.320343),(7,&#39;2000-11-12 21:49:52&#39;,7,6,69682369.400000,211162.000000,119454.460000,37212568.372855,2133133.264035,153722219.685720,5557523.960000),(8,&#39;2021-07-08 17:06:56&#39;,8,7,341.300000,1861.743223,1.337996,49.590397,8522.873563,2269.709000,16999.627000),(9,&#39;2003-12-11 22:56:40&#39;,9,8,20354369.167190,3144.140230,0.800314,305.165000,1.136590,215.070000,0.000000),(10,&#39;1999-04-10 21:50:09&#39;,10,9,4736112.251411,1569.689600,52377.000310,492851.324047,0.000000,33864985.940000,68705694.321598); /*!40000 ALTER TABLE `report` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `trade` -- DROP TABLE IF EXISTS `trade`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `trade` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `item_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Product link&#39;, `seller_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Link to the trader&#39;, `buyer_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Link to the trader&#39;, `quantity` decimal(16,6) DEFAULT NULL COMMENT &#39;Number of products&#39;, `unit_price` decimal(16,6) DEFAULT NULL COMMENT &#39;Price per unit&#39;, `description` text COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;All additional information (for example, the number of shares issued) in text format.&#39;, `offer_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;Transaction Identifier&#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;Transactions&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `trade` -- LOCK TABLES `trade` WRITE; /*!40000 ALTER TABLE `trade` DISABLE KEYS */; INSERT INTO `trade` VALUES (1,1,1,1,2.460000,58902033.493045,&#39;Dolores officiis quas necessitatibus qui amet. Id error atque laborum ea maiores rerum voluptatem eum. Et tempora pariatur quaerat laborum. Placeat ipsa tenetur maiores architecto.&#39;,1),(2,2,2,2,28389.028300,64.144119,&#39;Dolorem perspiciatis consequatur eaque est corporis adipisci. Quae numquam quo provident alias natus eligendi. Voluptas ratione velit eveniet aperiam.&#39;,2),(3,3,3,3,267929.007257,3002.000000,&#39;Distinctio facere ullam nostrum commodi. Mollitia ea quo aut labore eaque. Aliquam dolores porro ut magni vitae dolore a nostrum.&#39;,3),(4,4,4,4,9942452.040000,572582523.427500,&#39;Cumque fugit similique culpa et ad. Temporibus dolores ut sint ipsum voluptas a et. Dolore et totam tempora dolorem ea. Unde dolore velit perspiciatis exercitationem.&#39;,4),(5,5,5,5,679040271.000000,73901.498440,&#39;Totam quisquam unde possimus voluptatibus quos praesentium provident. Et magnam aspernatur aut cupiditate eaque et non. Minima ex ut unde ut magni.&#39;,5),(6,6,6,6,11415.933040,34761.863528,&#39;Necessitatibus laudantium fugit porro. A et reprehenderit iusto ut deleniti ut. Et inventore aut culpa quibusdam eveniet aut nulla.&#39;,6),(7,7,7,7,4.820000,77.000000,&#39;Corporis provident natus neque et. Libero nostrum aut minima impedit. Libero quos et beatae eos. Facere et officia quis et quia.&#39;,7),(8,8,8,8,667543.999204,422918.783910,&#39;Autem rerum est nostrum placeat nulla. Velit cumque eius a enim. Dolor harum dignissimos sunt ea. Consectetur rerum eius ratione rerum nisi ipsam perspiciatis.&#39;,8),(9,9,9,9,280068485.000000,0.000000,&#39;Voluptatem veritatis consectetur libero autem reiciendis. Animi qui aut animi sed perspiciatis. Reprehenderit officia iure occaecati molestiae ipsam ipsa. Quo qui laudantium provident veniam aut.&#39;,9),(10,10,10,10,0.000000,107806.940000,&#39;Ipsa ut veniam deserunt quis. Ipsam labore aspernatur cupiditate et molestiae. Animi voluptatem vel reprehenderit tempora. At doloribus facere voluptatem dignissimos dolorem.&#39;,10); /*!40000 ALTER TABLE `trade` ENABLE KEYS */; UNLOCK TABLES; -- -- Table structure for table `trader` -- DROP TABLE IF EXISTS `trader`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!40101 SET character_set_client = utf8 */; CREATE TABLE `trader` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `firstname` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;Name&#39;, `lastname` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;Surname&#39;, `user_name` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;Everyone&#39;s login is unique&#39;, `email` varchar(120) COLLATE utf8mb4_unicode_ci NOT NULL, `confirmation_code` varchar(120) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#39;The code sent to the user to complete the registration process.&#39;, `time_registered` datetime DEFAULT current_timestamp(), `time_confirmed` datetime DEFAULT current_timestamp(), `country_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;The country in which he lives.&#39;, `preffered_currency_id` bigint(20) unsigned DEFAULT NULL COMMENT &#39;The currency that the trader prefers&#39;, PRIMARY KEY (`id`), UNIQUE KEY `user_name` (`user_name`), UNIQUE KEY `email` (`email`) ) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;users&#39;; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `trader` -- LOCK TABLES `trader` WRITE; /*!40000 ALTER TABLE `trader` DISABLE KEYS */; INSERT INTO `trader` VALUES (1,&#39;Gideon&#39;,&#39;Hettinger&#39;,&#39;nweissnat&#39;,&#39;hilda.schulist@example.com&#39;,&#39;sdaj&#39;,&#39;1994-07-02 14:22:41&#39;,&#39;1973-09-17 17:51:44&#39;,1,0),(2,&#39;Antwon&#39;,&#39;Rogahn&#39;,&#39;eva.stoltenberg&#39;,&#39;grussel@example.org&#39;,&#39;cddo&#39;,&#39;1979-12-08 07:45:35&#39;,&#39;1990-02-28 17:41:55&#39;,2,1),(3,&#39;Letitia&#39;,&#39;Cremin&#39;,&#39;bergnaum.tyra&#39;,&#39;berniece.feeney@example.com&#39;,&#39;jrip&#39;,&#39;2005-05-27 20:49:06&#39;,&#39;1990-05-07 05:33:38&#39;,3,2),(4,&#39;Braulio&#39;,&#39;Hessel&#39;,&#39;andreanne97&#39;,&#39;margarita71@example.net&#39;,&#39;zqnt&#39;,&#39;2006-01-12 00:24:22&#39;,&#39;2021-04-16 22:24:14&#39;,4,3),(5,&#39;Clementine&#39;,&#39;Zboncak&#39;,&#39;orion53&#39;,&#39;sawayn.keara@example.net&#39;,&#39;tkwo&#39;,&#39;2004-09-08 13:05:57&#39;,&#39;2009-04-21 11:36:19&#39;,5,4),(6,&#39;Alvis&#39;,&#39;Gutkowski&#39;,&#39;ikovacek&#39;,&#39;ikovacek@example.net&#39;,&#39;supc&#39;,&#39;2009-10-11 20:46:23&#39;,&#39;2003-04-14 08:48:12&#39;,6,5),(7,&#39;Raphael&#39;,&#39;Sanford&#39;,&#39;mcdermott.providenci&#39;,&#39;leonie94@example.org&#39;,&#39;ibjn&#39;,&#39;1975-03-27 19:01:54&#39;,&#39;2000-12-20 15:01:59&#39;,7,6),(8,&#39;Ruthie&#39;,&#39;Dietrich&#39;,&#39;lyost&#39;,&#39;powlowski.hillary@example.org&#39;,&#39;mdzm&#39;,&#39;2005-02-19 04:07:58&#39;,&#39;1993-08-30 12:18:06&#39;,8,7),(9,&#39;Keagan&#39;,&#39;Gutmann&#39;,&#39;kpredovic&#39;,&#39;kody.gibson@example.com&#39;,&#39;ejxu&#39;,&#39;2001-09-05 00:29:11&#39;,&#39;2016-07-10 15:52:51&#39;,9,8),(10,&#39;Marianne&#39;,&#39;Ziemann&#39;,&#39;jace.kunde&#39;,&#39;qbergnaum@example.com&#39;,&#39;qozb&#39;,&#39;1991-04-13 01:13:57&#39;,&#39;1987-11-14 13:35:07&#39;,10,9); /*!40000 ALTER TABLE `trader` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2021-09-02 19:36:59 UPDATE trader SET country_id = (FLOOR(1 + RAND() * 3)); UPDATE offer SET item_id = (FLOOR(1 + RAND() * 3)); UPDATE offer SET trader_id = (FLOOR(1 + RAND() * 3)); UPDATE offer SET price = (FLOOR(1 + RAND() * 100000)); UPDATE offer SET buy = 0 WHERE sell = 1; UPDATE offer SET buy = 1 WHERE sell = 0; UPDATE trade SET seller_id = (FLOOR(1 + RAND() * 3)); UPDATE trade SET buyer_id = (FLOOR(1 + RAND() * 3)); UPDATE trade SET buyer_id = (FLOOR(1 + RAND() * 3)) WHERE buyer_id = seller_id ; UPDATE trade SET quantity = (FLOOR(1 + RAND() * 1000000)); UPDATE trade SET unit_price = (FLOOR(1 + RAND() * 1000)); UPDATE trade SET item_id = (FLOOR(1 + RAND() * 3)); UPDATE trade SET offer_id = (FLOOR(1 + RAND() * 3)); UPDATE report SET currency_id = (FLOOR(1 + RAND() * 3)); UPDATE trader SET birthday = CURRENT_DATE() - INTERVAL (FLOOR(20 + RAND() * 60)) YEAR; . 6. Scripts of characteristic samples . Of course, I created the first samples from my logic, but of course I need a lot more samples . Distribution of traders by country . CREATE VIEW trader_country AS SELECT c.country, COUNT(*) AS number_people FROM trader t JOIN country c ON t.country_id = c.id GROUP BY c.country ; . Distribution of the number of purchase offers by traders . DROP VIEW trader_offer_buy; CREATE VIEW trader_offer_buy AS SELECT user_name, COUNT(o.buy) AS sum_offer_buy FROM trader t JOIN offer o ON t.id = o.trader_id WHERE o.buy = 1 GROUP BY t.user_name ORDER BY sum_offer_buy DESC; . Distribution of the number of offers for sale by traders . DROP VIEW trader_offer_sell; CREATE VIEW trader_offer_sell AS SELECT user_name, COUNT(o.sell) AS sum_offer_sell FROM trader t JOIN offer o ON t.id = o.trader_id WHERE o.sell = 1 GROUP BY t.user_name ORDER BY sum_offer_sell DESC; SELECT * FROM trader_country tc ; SELECT * FROM trader_offer_sell ; SELECT * FROM trader_offer_buy tob ; . 7. Creating Triggers . Trigger to check that age is not an empty value . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.birthday IS NULL THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;It is necessary to add the date of birth&#39;; END IF; END// DELIMITER ; . Trigger to verify that the trader is of legal age . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.birthday &gt;= CURRENT_DATE() - INTERVAL 18 YEAR THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;Wait until you turn 18&#39;; END IF; END// DELIMITER ; . Trigger to check that a trader does not simultaneously put the same position on sale and purchase at once . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.sell = 1 AND NEW.buy = 1 THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;You cannot make a purchase and a sale at the same time&#39;; END IF; END// DELIMITER ; . Trigger to check that the trader has chosen either buy or sell . DROP TRIGGER IF EXISTS trader_age; DELIMITER // CREATE TRIGGER trader_age BEFORE UPDATE ON trader FOR EACH ROW BEGIN IF NEW.sell = 0 AND NEW.buy = 0 THEN SIGNAL SQLSTATE &#39;45000&#39; SET MESSAGE_TEXT = &#39;It is necessary to choose one of the operations&#39;; END IF; END// DELIMITER ; . Thatâ€™s the whole code. Rated excellent. At the end, a quote from the teacher: . Itâ€™s good that the indexes were taken care of. ENGINE = InnoDB - default value. it is not necessary to write it. but itâ€™s good that you know about this option. In SQL, it is generally accepted to name the fields of tables in the singular, and tables can be called in the plural. it is important to adhere to the chosen style (all in units or all in many hours). It makes sense to save the most popular queries (often executed) in the form of representations. Itâ€™s good that views and triggers have been implemented, not everyone reaches this point. Good luck in further training! .",
            "url": "https://zmey56.github.io/blog//course%20project/mysql/geekbrain/2021/09/04/course-project-classification.html",
            "relUrl": "/course%20project/mysql/geekbrain/2021/09/04/course-project-classification.html",
            "date": " â€¢ Sep 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Using machine learning to predict gold mining stock prices",
            "content": "As a basis, I took a notebook published on colab for oil. This notebook examines the analysis of gold prices and shares of gold mining companies using machine analysis methods: linear regression, cluster analysis, and random forest. I immediately warn you that this post does not attempt to show the current situation and predict the future direction. Just like the author for oil, this article does not aim to raise or refute the possibilities of machine learning for analyzing stock prices or other tools. I upgraded the code for gold research in order to encourage those who are interested in further reflection and listen to constructive criticism in their address. . import yfinance as yf import pandas as pd import numpy as np import seaborn as sns from sklearn import metrics import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression . 1. Loading data . For the price of gold, take the value of the exchange-traded investment Fund SPDR Gold Trust, whose shares are 100% backed by precious metal. The quotes will be compared with the prices of gold mining companies &#39; shares: . Newmont Goldcorp (NMM) | Barrick Gold (GOLD) | AngloGold Ashanti (AU) | Kinross Gold (KGC) | Newcrest Mining (ENC) | Polyus (PLZL) | Polymetal (POLY) | Seligdar (SELG) | . gold = pd.DataFrame(yf.download(&quot;GLD&quot;, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed . gold = gold.reset_index() gold.columns = [&quot;Date&quot;,&quot;gold_price&quot;] gold[&#39;Date&#39;] = pd.to_datetime(gold[&#39;Date&#39;]) gold.head() . Date gold_price . 0 2010-01-04 | 109.800003 | . 1 2010-01-05 | 109.699997 | . 2 2010-01-06 | 111.510002 | . 3 2010-01-07 | 110.820000 | . 4 2010-01-08 | 111.370003 | . It is necessary to move the price of gold, as we will be interested in how yesterday&#39;s price affected today&#39;s stock price. . gold[&quot;gold_price&quot;] = gold[&quot;gold_price&quot;].shift(1) . shares=[&quot;NMM.SG&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,&quot;PLZL.ME&quot;,&quot;POLY.ME&quot;,&quot;SELG.ME&quot;] data= yf.download(shares, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . data = data.reset_index() data.head() . Date AU GOLD KGC NCM.AX NMM.SG PLZL.ME POLY.ME SELG.ME . 0 2010-01-04 | 39.698944 | 34.561649 | 18.105721 | 33.237167 | 26.924570 | NaN | NaN | NaN | . 1 2010-01-05 | 40.320408 | 34.989510 | 18.594805 | 33.901924 | 27.116940 | NaN | NaN | NaN | . 2 2010-01-06 | 41.601028 | 35.733963 | 19.256504 | 33.901924 | 27.289278 | NaN | NaN | NaN | . 3 2010-01-07 | 41.130215 | 35.229092 | 19.352404 | 34.298923 | NaN | NaN | NaN | NaN | . 4 2010-01-08 | 41.601028 | 35.451572 | 19.601744 | 33.421829 | 27.702093 | NaN | NaN | NaN | . data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) . all_data=pd.DataFrame() . for index in range(len(shares)): stock=pd.DataFrame() # transform the data stock=data.loc[:, (&quot;Date&quot;,shares[index])] stock[&quot;Date&quot;]=stock[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) stock.columns=[&quot;Date&quot;,&quot;share_price&quot;] test=pd.DataFrame(gold) output=stock.merge(test,on=&quot;Date&quot;,how=&quot;left&quot;) #combining two data sets stock[&quot;gold_price&quot;]=output[&quot;gold_price&quot;] stock[&#39;share_price&#39;]=pd.to_numeric(stock[&#39;share_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&#39;gold_price&#39;]=pd.to_numeric(stock[&#39;gold_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&quot;year&quot;]=pd.to_datetime(stock[&quot;Date&quot;]).dt.year #Create a column with years for subsequent filtering stock[&quot;name&quot;]=shares[index] stock = stock.dropna() #delete all NAN lines #creating a column with a scaled share price scaler=MinMaxScaler() stock[&quot;share_price_scaled&quot;]=scaler.fit_transform(stock[&quot;share_price&quot;].to_frame()) #add data to the main dataframe all_data=all_data.append(stock) #add the data . all_data_15 = all_data[(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)] all_data_15.head() . Date share_price gold_price year name share_price_scaled . 1301 2015-01-02 | 14.269927 | 113.580002 | 2015 | NMM.SG | 0.052072 | . 1302 2015-01-05 | 14.845476 | 114.080002 | 2015 | NMM.SG | 0.071190 | . 1303 2015-01-06 | 15.601913 | 115.800003 | 2015 | NMM.SG | 0.096317 | . 1304 2015-01-07 | 15.645762 | 117.120003 | 2015 | NMM.SG | 0.097773 | . 1305 2015-01-08 | 15.517859 | 116.430000 | 2015 | NMM.SG | 0.093525 | . 2. Data analysis . It is best to start analyzing data by presenting it visually, which will help you understand it better. . 2.1 Chart of gold price changes . gold[[&#39;Date&#39;,&#39;gold_price&#39;]].set_index(&#39;Date&#39;).plot(color=&quot;green&quot;, linewidth=1.0) plt.show() . 2.2. Plotting the pairplot chart for the price of Polyus and Barrick Gold shares over the past five years . palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) g = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;POLY.ME&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) g.fig.suptitle(&quot;Polyuse&quot;, y=1.08) palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) f = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;GOLD&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) f.fig.suptitle(&#39;Barrick Gold&#39;, y=1.08) plt.show() . A paired graph allows you to see the distribution of data by showing the paired relationships in the data set and the univariate distribution of data for each variable. You can also use the palette to see how this data changed in different years. . The chart is particularly interesting for 2016 and 2019, as it looks like the price of the Pole stock, Barrick Gold and the price of gold are lined up along the same line. We can also conclude from the distribution charts that the price of gold and stocks moved gradually towards higher values. . 2.3 Violinplot for the gold price . plt.figure(figsize=(10,10)) sns.set_style(&quot;whitegrid&quot;) palette=sns.cubehelix_palette(5, start=2.8, rot=0, dark=0.2, light=0.8, reverse=False) sns.violinplot(x=&quot;year&quot;, y=&quot;gold_price&quot;, data=all_data_15[[&quot;gold_price&quot;,&quot;year&quot;]], inner=&quot;quart&quot;, palette=palette, trim=True) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Price gold&quot;) plt.show() . 2.4 Violinplot for multiple shares . sns.catplot(x=&quot;year&quot;, y=&quot;share_price_scaled&quot;, col=&#39;name&#39;, col_wrap=3,kind=&quot;violin&quot;, split=True, data=all_data_15,inner=&quot;quart&quot;, palette=palette, trim=True, height=4, aspect=1.2) sns.despine(left=True) . A large fluctuation in gold prices was noted according to the charts in 2016 and 2019. As you can see from the graphs in the following figure, some companies such as Newmont Mining, Barrick Gold, AngloGold Ashanti, Newcrest Mining and Polymetal were also affected. It should also be noted that all prices are marked in the range from 0 to 1 and this may lead to inaccuracies in the interpretation. . Next, we will build distribution charts for one Russian company - Polymetal and one foreign company - Barrick Gold . sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;POLY.ME&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . It is necessary to pay attention to the distribution of the share price for the two companies and it will become clear that the shape of the density graph is the same for them. . 2.5 Charts of the dependence of the share price of various companies on the price of gold . sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,line_kws={&#39;color&#39;: &#39;blue&#39;},scatter_kws={&#39;color&#39;: &#39;grey&#39;}).set(ylim=(0, 1)) plt.show() . In fact, you won&#39;t be able to see much on these charts, although some stocks seem to have a relationship. . The next step is to try to color the charts depending on the years. . palette=sns.cubehelix_palette(5, start=2, rot=0, dark=0, light=.95, reverse=False) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,hue=&quot;year&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,palette=palette,height=4).set(ylim=(0, 1)) plt.show() . Here the picture is a little better in the sense that some companies have a data cloud stretching along a straight line in some years, which may indicate the existence of a dependency. . 3 Machine learning and prediction . I will give a definition for machine learning from Wikipedia: Machine learning is a class of artificial intelligence methods that are characterized not by direct problem solving, but by learning in the process of applying solutions to many similar problems. To build such methods, we use mathematical statistics, numerical methods, optimization methods, probability theory, graph theory, and various techniques for working with data in digital form. . Usually, machine learning algorithms can be classified into the following categories: learning with a teacher and learning without a teacher. Here is their definition from one of the sites: . Supervised learning is one of the sections of machine learning dedicated to solving the following problem. There is a set of objects (situations) and the set of possible answers (responses, reactions). There is some relationship between responses and objects, but it is unknown. Only a finite set of use cases is known â€” the &quot;object, response&quot; pairs, called the training sample. Based on this data, you need to restore the dependency, that is, build an algorithm that can give a fairly accurate answer for any object. To measure the accuracy of responses, a quality functional is introduced in a certain way. see the Links) . Unsupervised learning is one of the sections of machine learning. Studies a wide class of data processing problems in which only descriptions of a set of objects (training sample) are known, and it is required to detect internal relationships, dependencies, and patterns that exist between objects. Learning without a teacher is often contrasted with learning with a teacher, when each training object is given a &quot;correct answer&quot;, and you need to find the relationship between the objects and the answers. see links) . The following machine learning methods will be discussed later: . Cluster analysis | Linear regression | Random forest | . Using these algorithms, you can evaluate overvalued or undervalued stocks relative to the price of gold and possible movement on the next day. I remind you that you must be very careful and use the conclusions from this post at your own risk. I also remind you that my main goal is to show the potential of machine learning for stock valuation. . 3.1. Cluster analysis for Barrick Gold stock . Clustering is the task of dividing a set of objects into groups called clusters. Each group should contain &quot;similar&quot; objects, and objects from different groups should be as different as possible. . from sklearn.cluster import KMeans poly=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;] # We need to scale also gold price, so clustering is not influenced by the relative size of one axis. poly=pd.DataFrame(poly) poly[&#39;gold_price_scaled&#39;] = scaler.fit_transform(poly[&quot;gold_price&quot;].to_frame()) poly[&quot;cluster&quot;] = KMeans(n_clusters=5, random_state=1).fit_predict(poly[[&quot;share_price_scaled&quot;,&quot;gold_price_scaled&quot;]]) # The 954 most common RGB monitor colors https://xkcd.com/color/rgb/ colors = [&quot;baby blue&quot;, &quot;amber&quot;, &quot;scarlet&quot;, &quot;grey&quot;,&quot;milk chocolate&quot;, &quot;windows blue&quot;] palette=sns.xkcd_palette(colors) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,ci=None,palette=palette, hue=&quot;cluster&quot;,fit_reg=0 ,data=poly) plt.show() . Cluster analysis is used in a large number of machine learning tasks. But I have given it only for informational purposes, since in this form it does not bring much benefit to our analysis. . 3.2. Linear regression between Barrick Gold shares and the gold price . Next, we will build a regular linear regression using training with a teacher. The goal is to estimate the forecast of data for the last 100 days of 2019 based on data from 2018/2019 (excluding estimated ones). Training data is the data used to build the model, and test data is the data that we will try to predict. . for sh in shares: print(sh) #Data Preparation share_18=pd.DataFrame() share_18=all_data_15[(all_data_15[&#39;name&#39;]==sh)] # Get data 2018/19 share_18=share_18[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Just using 1 variable for linear regression. Split the data into training/testing sets train = share_18[:-100] test = share_18[-100:] x_train=train[&quot;gold_price&quot;].to_frame() y_train=train[&#39;share_price&#39;].to_frame() x_test=test[&quot;gold_price&quot;].to_frame() y_test=test[&#39;share_price&#39;].to_frame() regr = LinearRegression() #Create linear regression object regr.fit(x_train,y_train) #Train the model using the training sets print(&quot;Coefficients: &quot;, float(regr.coef_)) print(np.corrcoef(x_train,y_train, rowvar=False)) y_pred = regr.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # Plot outputs using matplotlib plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . NMM.SG Coefficients: 0.6629423053739908 [[1. 0.790953] [0.790953 1. ]] Mean Absolute Error: 6.063058573972694 Mean Squared Error: 39.21188296210148 Root Mean Squared Error: 6.261939233344689 . GOLD Coefficients: 0.3355465472461071 [[1. 0.67139243] [0.67139243 1. ]] Mean Absolute Error: 3.3769293704374657 Mean Squared Error: 11.756813554455096 Root Mean Squared Error: 3.4288210152259473 . AU Coefficients: 0.31252669952857776 [[1. 0.67830589] [0.67830589 1. ]] Mean Absolute Error: 2.2471377544809683 Mean Squared Error: 5.789211153877581 Root Mean Squared Error: 2.4060779608893768 . KGC Coefficients: 0.10461302060876282 [[1. 0.78266367] [0.78266367 1. ]] Mean Absolute Error: 1.0583009847297946 Mean Squared Error: 1.1523726951635975 Root Mean Squared Error: 1.073486234268329 . NCM.AX Coefficients: 0.5623005799590818 [[1. 0.79891272] [0.79891272 1. ]] Mean Absolute Error: 2.0335289996635937 Mean Squared Error: 5.836462091267656 Root Mean Squared Error: 2.415877085297937 . PLZL.ME Coefficients: 103.84435014609612 [[1. 0.60373084] [0.60373084 1. ]] Mean Absolute Error: 1315.093426667142 Mean Squared Error: 1776892.2964767825 Root Mean Squared Error: 1333.0012364873419 . POLY.ME Coefficients: 10.772023429299809 [[1. 0.63694034] [0.63694034 1. ]] Mean Absolute Error: 69.33753863275061 Mean Squared Error: 6800.525447108329 Root Mean Squared Error: 82.46529844187995 . SELG.ME Coefficients: 0.15570348678870732 [[1. 0.51630147] [0.51630147 1. ]] Mean Absolute Error: 1.8096071903165585 Mean Squared Error: 4.039450515732427 Root Mean Squared Error: 2.009838430255633 . From the above charts, we can conclude that the price of gold predicts the price of shares of foreign companies on the next day quite well. In Russian companies, this picture looks much worse. Of course, there may be a false impression about Seligdar shares. But visual analysis of the chart allows you to discard this assumption. . 3.3 Random forest on Newmont Goldcorp shares against the price of gold and shares of gold companies . Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . The random forest algorithm accepts more than one variable in the input data to predict the output data. It works very efficiently on large amounts of data, can handle many input variables, has efficient methods for estimating missing data, and many other advantages. The main disadvantages are: . Random forests is slow to generate forecasts because it has many decision trees. Whenever it makes a forecast, all the trees in the forest must make a forecast for the same given input and then vote on it. This whole process takes a long time. | the Model is difficult to interpret compared to the decision tree, where you can easily make a decision by following the path in the tree. | One of the great advantages of a random forest is that it can be used for both classification and regression problems, which make up most of today&#39;s machine learning systems. I will talk about random forests in classification, since classification is sometimes considered a building block of machine learning. Below you can see what a random forest with two trees looks like: . In addition to the gold price, we will use other variables to forecast the Newmont Goldcorp share price. This will be the share prices of other foreign gold mining companies. I know it doesn&#39;t make a lot of sense, but we just want to see how to build this type of model. This will allow us to see the impact of each of them on the final forecast.Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . from sklearn.ensemble import RandomForestRegressor # 1.- Data Preparation nmm15=pd.DataFrame() nmm15=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NMM.SG&quot;) &amp; (all_data_15[&#39;year&#39;]&gt;2016 )] nmm15=nmm15[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Load share price of other variables nmm15[&#39;GOLD&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;GOLD&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;GOLD&#39;] = nmm15[&#39;GOLD&#39;].shift(1) nmm15[&#39;AU&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;AU&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;AU&#39;] = nmm15[&#39;AU&#39;].shift(1) nmm15[&#39;KGC&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;KGC&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;KGC&#39;] = nmm15[&#39;KGC&#39;].shift(1) nmm15[&#39;NCM.AX&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NCM.AX&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;NCM.AX&#39;] = nmm15[&#39;NCM.AX&#39;].shift(1) nmm15 = nmm15.drop(nmm15.index[0]) train = nmm15[:-100] test = nmm15[-100:] x_train=train[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]] y_train=train[&#39;share_price&#39;] x_test=test[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,]] y_test=test[&#39;share_price&#39;].to_frame() # 2.- Create Randomforest object usinig a max depth=5 regressor = RandomForestRegressor(n_estimators=200, max_depth=5 ) # 3.- Train data clf=regressor.fit(x_train, y_train) # 4.- Predict! y_pred=regressor.predict(x_test) y_pred_list = list(y_pred) y_pred=pd.DataFrame(y_pred) . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_pred=plt.scatter(nmm15[&quot;gold_price&quot;], regressor.predict(nmm15[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]]), color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train,plt_pred),(&quot;train data&quot;,&quot;prediction&quot;)) plt.show() . The resulting model looks really good in addition, we must remember that Random Forest has many more parameters to configure, but the key one is the maximum depth, which is unlimited by default. Next, we&#39;ll check how this model predicts or tests data. . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . y_pred = clf.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 1.410409517520304 Mean Squared Error: 3.0995744019029483 Root Mean Squared Error: 1.7605608202794212 . importances=regressor.feature_importances_ indices=list(x_train) print(&quot;Feature ranking:&quot;) for f in range(x_train.shape[1]): print(&quot;Feature %s (%f)&quot; % (indices[f], importances[f])) f, (ax1) = plt.subplots(1, 1, figsize=(8, 6), sharex=True) sns.barplot(indices, importances, palette=&quot;BrBG&quot;, ax=ax1) ax1.set_ylabel(&quot;Importance&quot;) . Feature ranking: Feature gold_price (0.627703) Feature GOLD (0.045197) Feature AU (0.040957) Feature KGC (0.038973) Feature NCM.AX (0.247171) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0, 0.5, &#39;Importance&#39;) . By the importance of the signs, it immediately becomes clear how strong the value of gold is. . In short, I hope I was able to reveal to you the beginnings of a project on using machine learning to study stock prices, and I hope to hear your comments. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "relUrl": "/finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "date": " â€¢ Nov 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Graduation project "Identification of Internet users" - Vowpal Wabbit. Tutorial + Programming Assignment",
            "content": "Week 6. Vowpal Wabbit. Tutorial + Programming Assignment . This week we will get acquainted with the popular Vowpal Wabbit library and try it on site visit data. . 6 week plan: . Part 1. Article on Vowpal Wabbit | Part 2. Application of Vowpal Wabbit to Site Visit data | 2.1. Data Preparation | 2.2. Validation by Deferred Sampling | 2.3. Validation by test Sampling (Public Leaderboard) | . In this part of the project, videos of the following lectures of the course &quot;Learning from marked data&quot; may be useful to us: . Stochastic gradient ÑÐ¿ÑƒÑÐº | Linear models. sklearn.linear_model. ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ | . [Presentation] will also be useful(https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem08_vw.pdf ) lecturer of specialization Evgeny Sokolov. And, of course, documentation Vowpal Wabbit. . Part 1. Article about Vowpal Wabbit . Let&#39;s read the article about Vowpal Wabbit on Habra from the OpenDataScience open course series on machine learning. We can download notebook, attached to the article, view the code, study it and change it. This is the only way to deal with Vowpal Wabbit. . Part 2. Applying Vowpal Wabbit to Site Visit Data . 2.1. Data preparation . Next, let&#39;s look at Vowpal Wabbit in action. However, in the task of our competition for binary classification of web sessions, we will not notice a difference - both in quality and in speed (although you can check). Therefore, we will demonstrate all the agility of VW in the task of classification into 400 classes. The initial data is still the same, but 400 users have been allocated, and the task of identifying them is being solved. Download the data from here, and here we will fill in the result - files train_sessions_400users.csv and test_sessions_400users.csv. . import os import pandas as pd import numpy as np import scipy.sparse as sps from scipy.sparse import csr_matrix from sklearn.linear_model import LogisticRegression, SGDClassifier from sklearn import preprocessing from sklearn.metrics import accuracy_score . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/Kaggle&#39; . Let&#39;s upload the training and test samples. It can be noticed that the test sessions here are clearly separated in time from the sessions in the training sample. . train_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,&#39;train_sessions_400users.csv&#39;), index_col=&#39;session_id&#39;) . test_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,&#39;test_sessions_400users.csv&#39;), index_col=&#39;session_id&#39;) . test_df_400.shape . (46473, 20) . train_df_400.head() . site1 time1 site2 time2 site3 time3 site4 time4 site5 time5 site6 time6 site7 time7 site8 time8 site9 time9 site10 time10 user_id . session_id . 1 23713 | 2014-03-24 15:22:40 | 23720.0 | 2014-03-24 15:22:48 | 23713.0 | 2014-03-24 15:22:48 | 23713.0 | 2014-03-24 15:22:54 | 23720.0 | 2014-03-24 15:22:54 | 23713.0 | 2014-03-24 15:22:55 | 23713.0 | 2014-03-24 15:23:01 | 23713.0 | 2014-03-24 15:23:03 | 23713.0 | 2014-03-24 15:23:04 | 23713.0 | 2014-03-24 15:23:05 | 653 | . 2 8726 | 2014-04-17 14:25:58 | 8725.0 | 2014-04-17 14:25:59 | 665.0 | 2014-04-17 14:25:59 | 8727.0 | 2014-04-17 14:25:59 | 45.0 | 2014-04-17 14:25:59 | 8725.0 | 2014-04-17 14:26:01 | 45.0 | 2014-04-17 14:26:01 | 5320.0 | 2014-04-17 14:26:18 | 5320.0 | 2014-04-17 14:26:47 | 5320.0 | 2014-04-17 14:26:48 | 198 | . 3 303 | 2014-03-21 10:12:24 | 19.0 | 2014-03-21 10:12:36 | 303.0 | 2014-03-21 10:12:54 | 303.0 | 2014-03-21 10:13:01 | 303.0 | 2014-03-21 10:13:24 | 303.0 | 2014-03-21 10:13:36 | 303.0 | 2014-03-21 10:13:54 | 309.0 | 2014-03-21 10:14:01 | 303.0 | 2014-03-21 10:14:06 | 303.0 | 2014-03-21 10:14:24 | 34 | . 4 1359 | 2013-12-13 09:52:28 | 925.0 | 2013-12-13 09:54:34 | 1240.0 | 2013-12-13 09:54:34 | 1360.0 | 2013-12-13 09:54:34 | 1344.0 | 2013-12-13 09:54:34 | 1359.0 | 2013-12-13 09:54:34 | 1346.0 | 2013-12-13 09:54:34 | 1345.0 | 2013-12-13 09:54:34 | 1344.0 | 2013-12-13 09:58:19 | 1345.0 | 2013-12-13 09:58:19 | 601 | . 5 11 | 2013-11-26 12:35:29 | 85.0 | 2013-11-26 12:35:31 | 52.0 | 2013-11-26 12:35:31 | 85.0 | 2013-11-26 12:35:32 | 11.0 | 2013-11-26 12:35:32 | 52.0 | 2013-11-26 12:35:32 | 11.0 | 2013-11-26 12:37:03 | 85.0 | 2013-11-26 12:37:03 | 10.0 | 2013-11-26 12:37:03 | 85.0 | 2013-11-26 12:37:04 | 273 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We see that there are 182793 sessions in the training sample, 46473 in the test sample, and the sessions really belong to 400 different users. . train_df_400.shape, test_df_400.shape, train_df_400[&#39;user_id&#39;].nunique() . ((182793, 21), (46473, 20), 400) . Vowpal Wabbit likes class labels to be distributed from 1 to K, where K is the number of classes in the classification problem (in our case, 400). Therefore, we will have to use LabelEncoder, and then add +1 (Label Encoder translates labels into the range from 0 to K-1). Then it will be necessary to apply the reverse transformation. . y = train_df_400.user_id class_encoder = preprocessing.LabelEncoder() y_for_vw = class_encoder.fit_transform(y)+1 . Next, we will compare VW with SGDClassifier and with logistic regression. All these models need input data preprocessing. Let&#39;s prepare sparse matrices for sklearn models, as we did in part 5: . combine training and test samples | we will select only sites (signs from &#39;site1&#39; to &#39;site10&#39;) | replace the omissions with zeros (our sites were numbered from 0) | we will translate into a sparse csr_matrix format | let&#39;s break back into the training and test parts | . train_test_df = pd.concat([train_df_400, test_df_400]) . sites = [&#39;site&#39; + str(i) for i in range(1, 11)] train_test_df_sites = train_test_df[sites] . train_test_df_sites.isnull().sum().sum() train_test_df_sites = train_test_df_sites.fillna(0) . idx_split = train_df_400.shape[0] train_test_sparse = csr_matrix((np.ones(train_test_df_sites.values.size, dtype=np.uint8), train_test_df_sites.values.reshape(-1), np.arange(train_test_df_sites.values.shape[0] + 1) * train_test_df_sites.values.shape[1]))[:, 1:] X_train_sparse = train_test_sparse[:idx_split, :] X_test_sparse = train_test_sparse[idx_split:, :] y = train_df_400[&#39;user_id&#39;].values . 2.2. Validation by deferred sampling . Let&#39;s select the training (70%) and deferred (30%) parts of the original training sample. We do not mix the data, we take into account that the sessions are sorted by time. . train_share = int(.7 * train_df_400.shape[0]) train_df_part = train_df_400[sites].iloc[:train_share, :] valid_df = train_df_400[sites].iloc[train_share:, :] X_train_part_sparse = X_train_sparse[:train_share, :] X_valid_sparse = X_train_sparse[train_share:, :] . y_train_part = y[:train_share] y_valid = y[train_share:] y_train_part_for_vw = y_for_vw[:train_share] y_valid_for_vw = y_for_vw[train_share:] . We implement a function, arrays_to_vw, which translates the training sample into the Vowpal Wabbit format. . Entrance: . X - matrix `NumPy&#39; (training sample) | y (optional) - response vector (NumPy). Optional, since we will process the test matrix with the same function | train - flag, True in the case of a training sample, False in the case of a test sample | out_file â€“ the path to the file .vw to which the recording will be made | . Details: . it is necessary to go through all the rows of the matrix X and write down all the values separated by a space, first adding the necessary class label from the vector y and the separator sign | | in the test sample, in place of the labels of the target class, you can write arbitrary, for example, 1 | . def arrays_to_vw(X, y=None, train=True, out_file=&#39;tmp.vw&#39;): X = np.nan_to_num(X) X = X.astype(int) with open(out_file, &#39;w&#39;) as f: print(X.shape) for i in range(X.shape[0]): string = &#39; &#39;.join([str(x) for x in X[i]]) if y is None: f.write(str(1) + &quot; | &quot; + string + &quot; n&quot;) else: f.write(str(y[i]) + &quot; | &quot; + string + &quot; n&quot;) . Let&#39;s apply the written function to the part of the training sample (train_df_part, y_train_part_for_vw), to the deferred sample (valid_df, y_valid_for_vw), to the entire training sample and to the entire test sample. It should be noted that our method accepts matrices and vectors NumPy as input . %%time arrays_to_vw(train_df_part.values, y_train_part_for_vw, True, os.path.join(PATH_TO_DATA,&#39;train_part.vw&#39;)) arrays_to_vw(valid_df.values, y_valid_for_vw, False, os.path.join(PATH_TO_DATA,&#39;valid.vw&#39;)) arrays_to_vw(train_df_400[sites].values, y_for_vw, True, os.path.join(PATH_TO_DATA,&#39;train.vw&#39;)) arrays_to_vw(test_df_400[sites].values, None, False, os.path.join(PATH_TO_DATA,&#39;test.vw&#39;)) . (127955, 10) (54838, 10) (182793, 10) (46473, 10) CPU times: user 4.03 s, sys: 22.6 ms, total: 4.05 s Wall time: 4.16 s . Let&#39;s check the result . !head -3 $PATH_TO_DATA/train_part.vw . 262 | 23713 23720 23713 23713 23720 23713 23713 23713 23713 23713 82 | 8726 8725 665 8727 45 8725 45 5320 5320 5320 16 | 303 19 303 303 303 303 303 309 303 303 . !head -3 $PATH_TO_DATA/valid.vw . 4 | 7 923 923 923 11 924 7 924 838 7 160 | 91 198 11 11 302 91 668 311 310 91 312 | 27085 848 118 118 118 118 11 118 118 118 . !head -3 $PATH_TO_DATA/test.vw . 1 | 9 304 308 307 91 308 312 300 305 309 1 | 838 504 68 11 838 11 838 886 27 305 1 | 190 192 8 189 191 189 190 2375 192 8 . Let&#39;s train the Vowpal Wabbit model on a sample of train_part.vw. We indicate that the classification problem with 400 classes (--oaa) is being solved, we will make 3 passes through the sample (--passes). Let&#39;s set some cache file (--cache_file, you can just specify the -c flag), so VW will be faster to do all the next passes after the first one (the last cache file is deleted using the -k argument). We also specify the value of the parameter b=26. This is the number of bits used for hashing, in this case you need more than 18 by default. Finally, specify random_seed=17. We are not changing the other parameters yet. . train_part_vw = os.path.join(PATH_TO_DATA, &#39;train_part.vw&#39;) valid_vw = os.path.join(PATH_TO_DATA, &#39;valid.vw&#39;) train_vw = os.path.join(PATH_TO_DATA, &#39;train.vw&#39;) test_vw = os.path.join(PATH_TO_DATA, &#39;test.vw&#39;) model = os.path.join(PATH_TO_DATA, &#39;vw_model.vw&#39;) pred = os.path.join(PATH_TO_DATA, &#39;vw_pred.csv&#39;) . %%time !vw --oaa 400 /content/drive/MyDrive/DATA/Stepik/Kaggle/train_part.vw --passes 3 -c -k -b 26 --random_seed 17 -f /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_model.vw . final_regressor = /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_model.vw Num weight bits = 26 learning rate = 0.5 initial_t = 0 power_t = 0.5 decay_learning_rate = 1 tcmalloc: large alloc 1073741824 bytes == 0x5596c0ee8000 @ 0x7f0c99005001 0x7f0c98ba1b5f 0x7f0c98bafa21 0x7f0c98c52e00 0x7f0c98c40be3 0x7f0c98c48395 0x7f0c98c48c44 0x5596be56c237 0x5596be56ba8b 0x7f0c981c0bf7 0x5596be56c05a creating cache_file = /content/drive/MyDrive/DATA/Stepik/Kaggle/train_part.vw.cache Reading datafile = /content/drive/MyDrive/DATA/Stepik/Kaggle/train_part.vw num sources = 1 average since example example current current current loss last counter weight label predict features 1.000000 1.000000 1 1.0 262 1 11 1.000000 1.000000 2 2.0 82 262 11 1.000000 1.000000 4 4.0 241 262 11 1.000000 1.000000 8 8.0 352 262 11 1.000000 1.000000 16 16.0 135 16 11 1.000000 1.000000 32 32.0 71 112 11 0.968750 0.937500 64 64.0 358 231 11 0.976562 0.984375 128 128.0 348 346 11 0.941406 0.906250 256 256.0 202 202 11 0.947266 0.953125 512 512.0 30 1 11 0.925781 0.904297 1024 1024.0 36 290 11 0.908203 0.890625 2048 2048.0 21 128 11 0.880127 0.852051 4096 4096.0 80 229 11 0.856323 0.832520 8192 8192.0 307 356 11 0.828003 0.799683 16384 16384.0 59 193 11 0.795441 0.762878 32768 32768.0 262 30 11 0.760468 0.725494 65536 65536.0 171 238 11 0.724008 0.724008 131072 131072.0 6 6 11 h 0.697339 0.670672 262144 262144.0 12 12 11 h finished run number of examples per pass = 115160 passes used = 3 weighted example sum = 345480.000000 weighted label sum = 0.000000 average loss = 0.661352 h total feature number = 3800280 CPU times: user 249 ms, sys: 48.4 ms, total: 297 ms Wall time: 34 s . Let&#39;s write down the forecasts on the valid sample.vw in vw_valid_phead.csv. . %%time !vw -i /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_model.vw -t -d /content/drive/MyDrive/DATA/Stepik/Kaggle/valid.vw -p /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_valid_pred.csv . only testing predictions = /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_valid_pred.csv Num weight bits = 26 learning rate = 0.5 initial_t = 0 power_t = 0.5 using no cache Reading datafile = /content/drive/MyDrive/DATA/Stepik/Kaggle/valid.vw num sources = 1 average since example example current current current loss last counter weight label predict features 1.000000 1.000000 1 1.0 4 188 11 1.000000 1.000000 2 2.0 160 220 11 0.750000 0.500000 4 4.0 143 143 11 0.750000 0.750000 8 8.0 247 247 11 0.687500 0.625000 16 16.0 341 30 11 0.593750 0.500000 32 32.0 237 237 11 0.609375 0.625000 64 64.0 178 178 11 0.640625 0.671875 128 128.0 132 228 11 0.656250 0.671875 256 256.0 14 14 11 0.646484 0.636719 512 512.0 370 370 11 0.663086 0.679688 1024 1024.0 189 189 11 0.655762 0.648438 2048 2048.0 311 311 11 0.657227 0.658691 4096 4096.0 195 318 11 0.660156 0.663086 8192 8192.0 171 195 11 0.657654 0.655151 16384 16384.0 362 51 11 0.655121 0.652588 32768 32768.0 248 248 11 finished run number of examples per pass = 54838 passes used = 1 weighted example sum = 54838.000000 weighted label sum = 0.000000 average loss = 0.654583 total feature number = 603218 CPU times: user 87 ms, sys: 31.6 ms, total: 119 ms Wall time: 11.3 s . We count the forecasts of kaggle_data/vw_valid_phead.csv from the file and look at the proportion of correct answers on the deferred part. . vw_valid = pd.read_csv( os.path.join(PATH_TO_DATA, &#39;vw_valid_pred.csv&#39;), header=None) . print(&#39;The percentage of correct responses on the deferred sample for Vowpal Wabbit: %f&#39; % accuracy_score(y_valid_for_vw, vw_valid)) . The percentage of correct responses on the deferred sample for Vowpal Wabbit: 0.345417 . Now we will train SGDClassifier (3 sample passes, logistic loss function) and LogisticRegression on 70% of the sparse training sample â€“ (X_train_part_sparse, y_train_part), make a forecast for the delayed sample (X_valid_sparse, y_valid) and calculate the proportion of correct answers. Logistic regression will not be trained quickly â€“ this is normal. We will specify random_state=17, n_jobs=-1 everywhere. For SGDClassifier, we will also specify max_iter=3. . logit = LogisticRegression(random_state=17, n_jobs=-1) sgd_logit = SGDClassifier(loss=&#39;log&#39;, random_state=17, max_iter=3) . %%time logit.fit(X_train_part_sparse, y_train_part) . CPU times: user 1.86 s, sys: 289 ms, total: 2.14 s Wall time: 5min 57s . LogisticRegression(n_jobs=-1, random_state=17) . %%time sgd_logit.fit(X_train_part_sparse, y_train_part) . CPU times: user 24.6 s, sys: 6.5 ms, total: 24.6 s Wall time: 24.5 s . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, . SGDClassifier(loss=&#39;log&#39;, max_iter=3, random_state=17) . Question 1. Calculate the proportion of correct answers on the deferred sample for Vowpal Wabbit, round to 3 decimal places. . Question 2. Calculate the proportion of correct answers on the deferred sample for SGD, round to 3 decimal places. . Question 3. Calculate the proportion of correct answers on the deferred sample for logistic regression, round to 3 decimal places. . vw_valid_acc = accuracy_score(y_valid_for_vw, vw_valid) sgd_valid_acc = accuracy_score(y_valid, sgd_logit.predict(X_valid_sparse)) logit_valid_acc = accuracy_score(y_valid, logit.predict(X_valid_sparse)) . def write_answer_to_file(answer, file_address): with open(file_address, &#39;w&#39;) as out_f: out_f.write(str(answer)) . write_answer_to_file(round(vw_valid_acc, 3), os.path.join(PATH_TO_DATA, &#39;answer6_1.txt&#39;)) write_answer_to_file(round(sgd_valid_acc, 3), os.path.join(PATH_TO_DATA, &#39;answer6_2.txt&#39;)) write_answer_to_file(round(logit_valid_acc, 3), os.path.join(PATH_TO_DATA, &#39;answer6_3.txt&#39;)) . 2.3. &#1042;&#1072;&#1083;&#1080;&#1076;&#1072;&#1094;&#1080;&#1103; &#1087;&#1086; &#1090;&#1077;&#1089;&#1090;&#1086;&#1074;&#1086;&#1081; &#1074;&#1099;&#1073;&#1086;&#1088;&#1082;&#1077; (Public Leaderboard) . Let&#39;s train a VW model with the same parameters on the entire training sample - train.vw. . %%time !vw --oaa 400 /content/drive/MyDrive/DATA/Stepik/Kaggle/train.vw --passes 3 -c -k -b 26 --random_seed 17 -f /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_model.vw . final_regressor = /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_model.vw Num weight bits = 26 learning rate = 0.5 initial_t = 0 power_t = 0.5 decay_learning_rate = 1 tcmalloc: large alloc 1073741824 bytes == 0x56465f178000 @ 0x7f5207ec5001 0x7f5207a61b5f 0x7f5207a6fa21 0x7f5207b12e00 0x7f5207b00be3 0x7f5207b08395 0x7f5207b08c44 0x56465e302237 0x56465e301a8b 0x7f5207080bf7 0x56465e30205a creating cache_file = /content/drive/MyDrive/DATA/Stepik/Kaggle/train.vw.cache Reading datafile = /content/drive/MyDrive/DATA/Stepik/Kaggle/train.vw num sources = 1 average since example example current current current loss last counter weight label predict features 1.000000 1.000000 1 1.0 262 1 11 1.000000 1.000000 2 2.0 82 262 11 1.000000 1.000000 4 4.0 241 262 11 1.000000 1.000000 8 8.0 352 262 11 1.000000 1.000000 16 16.0 135 16 11 1.000000 1.000000 32 32.0 71 112 11 0.968750 0.937500 64 64.0 358 231 11 0.976562 0.984375 128 128.0 348 346 11 0.941406 0.906250 256 256.0 202 202 11 0.947266 0.953125 512 512.0 30 1 11 0.925781 0.904297 1024 1024.0 36 290 11 0.908203 0.890625 2048 2048.0 21 128 11 0.880127 0.852051 4096 4096.0 80 229 11 0.856323 0.832520 8192 8192.0 307 356 11 0.828003 0.799683 16384 16384.0 59 193 11 0.795441 0.762878 32768 32768.0 262 30 11 0.760468 0.725494 65536 65536.0 171 238 11 0.725319 0.690170 131072 131072.0 180 159 11 0.692989 0.692989 262144 262144.0 88 221 11 h finished run number of examples per pass = 164514 passes used = 3 weighted example sum = 493542.000000 weighted label sum = 0.000000 average loss = 0.642595 h total feature number = 5428962 CPU times: user 368 ms, sys: 49.4 ms, total: 417 ms Wall time: 44.5 s . Let&#39;s make a forecast for the test sample. . %%time !vw -t -d /content/drive/MyDrive/DATA/Stepik/Kaggle/test.vw -i /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_model.vw -p /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_test_pred.csv . only testing predictions = /content/drive/MyDrive/DATA/Stepik/Kaggle/vw_test_pred.csv Num weight bits = 26 learning rate = 0.5 initial_t = 0 power_t = 0.5 using no cache Reading datafile = /content/drive/MyDrive/DATA/Stepik/Kaggle/test.vw num sources = 1 average since example example current current current loss last counter weight label predict features 1.000000 1.000000 1 1.0 1 90 11 1.000000 1.000000 2 2.0 1 21 11 1.000000 1.000000 4 4.0 1 265 11 1.000000 1.000000 8 8.0 1 137 11 1.000000 1.000000 16 16.0 1 273 11 1.000000 1.000000 32 32.0 1 384 11 1.000000 1.000000 64 64.0 1 139 11 1.000000 1.000000 128 128.0 1 85 11 1.000000 1.000000 256 256.0 1 25 11 0.994141 0.988281 512 512.0 1 364 11 0.990234 0.986328 1024 1024.0 1 202 11 0.992188 0.994141 2048 2048.0 1 181 11 0.993652 0.995117 4096 4096.0 1 21 11 0.994629 0.995605 8192 8192.0 1 137 11 0.995300 0.995972 16384 16384.0 1 326 11 0.994568 0.993835 32768 32768.0 1 10 11 finished run number of examples per pass = 46473 passes used = 1 weighted example sum = 46473.000000 weighted label sum = 0.000000 average loss = 0.994642 total feature number = 511203 CPU times: user 109 ms, sys: 24.4 ms, total: 134 ms Wall time: 10.8 s . Let&#39;s write the forecast to a file, apply the reverse conversion of labels (there was a LabelEncoder and then +1 in the label) and send the solution to Kaggle. . def write_to_submission_file(predicted_labels, out_file, target=&#39;user_id&#39;, index_label=&quot;session_id&quot;): # turn predictions into data frame and save as csv file predicted_df = pd.DataFrame(predicted_labels, index = np.arange(1, predicted_labels.shape[0] + 1), columns=[target]) predicted_df.to_csv(out_file, index_label=index_label) . vw_pred = pd.read_csv(&#39;/content/drive/MyDrive/DATA/Stepik/Kaggle/vw_test_pred.csv&#39;, header=None) . vw_subm = class_encoder.inverse_transform(np.ravel(vw_pred) - 1) . write_to_submission_file(vw_subm, os.path.join(PATH_TO_DATA, &#39;/content/drive/MyDrive/DATA/Stepik/Kaggle/vw_pred_kaggle.csv&#39;)) . Let&#39;s do the same for SGD and logistic regression. . sgd_logit = SGDClassifier(loss=&#39;log&#39;, random_state=17, max_iter=3, n_jobs=-1) sgd_logit.fit(X_train_part_sparse, y_train_part) sgd_logit_test_pred = sgd_logit.predict(X_test_sparse) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, . logit = LogisticRegression(random_state=17, n_jobs=-1, solver = &#39;lbfgs&#39;) logit.fit(X_train_sparse, y) logit_test_pred = logit.predict(X_test_sparse) . write_to_submission_file(sgd_logit_test_pred, os.path.join(PATH_TO_DATA, &#39;/content/drive/MyDrive/DATA/Stepik/Kaggle/sgd_pred.csv&#39;)) write_to_submission_file(logit_test_pred, os.path.join(PATH_TO_DATA, &#39;/content/drive/MyDrive/DATA/Stepik/Kaggle/logit_pred.csv&#39;)) . Let&#39;s look at the proportion of correct answers on the public part (public leaderboard) of the test sample this competitions. . Question 4. What is the proportion of correct answers on the public part of the test sample (public leaderboard) for Vowpal Wabbit? . Question 5. What is the proportion of correct answers on the public part of the test sample (public leaderboard) for SGD? . Question 6. What is the proportion of correct answers on the public part of the test sample (public leaderboard) for logistic regression? . vw_lb_score, sgd_lb_score, logit_lb_score = 0.18164, 0.16994, 0.19060 write_answer_to_file(round(vw_lb_score, 3), os.path.join(PATH_TO_DATA,&#39;answer6_4.txt&#39;)) write_answer_to_file(round(sgd_lb_score, 3), os.path.join(PATH_TO_DATA,&#39;answer6_5.txt&#39;)) write_answer_to_file(round(logit_lb_score, 3), os.path.join(PATH_TO_DATA,&#39;answer6_6.txt&#39;)) . Logistic regression showed the best result among the other two algorithms Vowpal Wabbit and SGD, but more time is spent on its training. SGD showed the worst result, but nevertheless he learns quickly. Vowpal Wabbit showed higher quality than SGD. .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/vowpalwabbit/english/2019/09/15/project-identification-of-internet-users-task-week6.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/vowpalwabbit/english/2019/09/15/project-identification-of-internet-users-task-week6.html",
            "date": " â€¢ Sep 15, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Graduation project "Identification of Internet users" - Kaggle Inclass User Identification competition. Peer-Review",
            "content": "Week 5. Kaggle &quot;Catch Me If You Can&quot; Competition . This week we will recall the concept of stochastic gradient descent and try the Scikit-learn SGDClassifier classifier, which works much faster on large samples than the algorithms we tested in week 4. We will also get acquainted with the data competition Kaggle for user identification and we will make the first parcels in it. . In this part of the project, videos of the following lectures of the course &quot;Learning from marked data&quot; may be useful to us: . Stochastic gradient descent | Linear models. Sklearn.linear_model. Classification | . We can also go back and view the task &quot;Linear regression and stochastic gradient descent&quot; of the 1st week of the 2nd course of specialization. . from __future__ import division, print_function # disable any Anaconda warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) import os import pickle import numpy as np import pandas as pd import scipy.sparse as sps from time import time from scipy.sparse import csr_matrix, hstack from sklearn.model_selection import train_test_split from sklearn.linear_model import SGDClassifier from sklearn.metrics import roc_auc_score . Counting the data competitions in the DataFrame train_df and test_df (training and test samples). . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/catch_me_if_you_can&#39; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . train_df = pd.read_csv(os.path.join(PATH_TO_DATA, &#39;train_sessions.csv&#39;), index_col=&#39;session_id&#39;) test_df = pd.read_csv(os.path.join(PATH_TO_DATA, &#39;test_sessions.csv&#39;), index_col=&#39;session_id&#39;) . train_df.head() . site1 time1 site2 time2 site3 time3 site4 time4 site5 time5 site6 time6 site7 time7 site8 time8 site9 time9 site10 time10 target . session_id . 1 718 | 2014-02-20 10:02:45 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 0 | . 2 890 | 2014-02-22 11:19:50 | 941.0 | 2014-02-22 11:19:50 | 3847.0 | 2014-02-22 11:19:51 | 941.0 | 2014-02-22 11:19:51 | 942.0 | 2014-02-22 11:19:51 | 3846.0 | 2014-02-22 11:19:51 | 3847.0 | 2014-02-22 11:19:52 | 3846.0 | 2014-02-22 11:19:52 | 1516.0 | 2014-02-22 11:20:15 | 1518.0 | 2014-02-22 11:20:16 | 0 | . 3 14769 | 2013-12-16 16:40:17 | 39.0 | 2013-12-16 16:40:18 | 14768.0 | 2013-12-16 16:40:19 | 14769.0 | 2013-12-16 16:40:19 | 37.0 | 2013-12-16 16:40:19 | 39.0 | 2013-12-16 16:40:19 | 14768.0 | 2013-12-16 16:40:20 | 14768.0 | 2013-12-16 16:40:21 | 14768.0 | 2013-12-16 16:40:22 | 14768.0 | 2013-12-16 16:40:24 | 0 | . 4 782 | 2014-03-28 10:52:12 | 782.0 | 2014-03-28 10:52:42 | 782.0 | 2014-03-28 10:53:12 | 782.0 | 2014-03-28 10:53:42 | 782.0 | 2014-03-28 10:54:12 | 782.0 | 2014-03-28 10:54:42 | 782.0 | 2014-03-28 10:55:12 | 782.0 | 2014-03-28 10:55:42 | 782.0 | 2014-03-28 10:56:12 | 782.0 | 2014-03-28 10:56:42 | 0 | . 5 22 | 2014-02-28 10:53:05 | 177.0 | 2014-02-28 10:55:22 | 175.0 | 2014-02-28 10:55:22 | 178.0 | 2014-02-28 10:55:23 | 177.0 | 2014-02-28 10:55:23 | 178.0 | 2014-02-28 10:55:59 | 175.0 | 2014-02-28 10:55:59 | 177.0 | 2014-02-28 10:55:59 | 177.0 | 2014-02-28 10:57:06 | 178.0 | 2014-02-28 10:57:11 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Let&#39;s combine the training and test samples â€“ this will be needed to bring them together later to a sparse format. . train_test_df = pd.concat([train_df, test_df]) . In the training sample we see the following signs: . site1 - index of the first visited site in the session | time1 â€“ time of visiting the first site in the session | ... | site10 - index of the 10th visited site in the session | time10 â€“ time of visiting the 10th site in the session | user_id - user ID | . User sessions are allocated in such a way that they cannot be longer than half an hour or 10 sites. That is, the session is considered over either when the user visited 10 sites in a row, or when the session took more than 30 minutes. . Let&#39;s look at the statistics of the signs. . Skips occur where sessions are short (less than 10 sites). For example, if a person visited on January 1, 2015 vk.com at 20:01, then yandex.ru at 20:29, then google.com at 20:33, then its first session will consist of only two sites (site1 - site ID vk.com , time1 - 2015-01-01 20:01:00, site2 - site ID yandex.ru , time2 â€“ 2015-01-01 20:29:00, other signs are NaN), and starting from google.com a new session will start because more than 30 minutes have already passed since the visit vk.com. . train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 253561 entries, 1 to 253561 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 site1 253561 non-null int64 1 time1 253561 non-null object 2 site2 250098 non-null float64 3 time2 250098 non-null object 4 site3 246919 non-null float64 5 time3 246919 non-null object 6 site4 244321 non-null float64 7 time4 244321 non-null object 8 site5 241829 non-null float64 9 time5 241829 non-null object 10 site6 239495 non-null float64 11 time6 239495 non-null object 12 site7 237297 non-null float64 13 time7 237297 non-null object 14 site8 235224 non-null float64 15 time8 235224 non-null object 16 site9 233084 non-null float64 17 time9 233084 non-null object 18 site10 231052 non-null float64 19 time10 231052 non-null object 20 target 253561 non-null int64 dtypes: float64(9), int64(2), object(10) memory usage: 42.6+ MB . test_df.head() . site1 time1 site2 time2 site3 time3 site4 time4 site5 time5 site6 time6 site7 time7 site8 time8 site9 time9 site10 time10 . session_id . 1 29 | 2014-10-04 11:19:53 | 35.0 | 2014-10-04 11:19:53 | 22.0 | 2014-10-04 11:19:54 | 321.0 | 2014-10-04 11:19:54 | 23.0 | 2014-10-04 11:19:54 | 2211.0 | 2014-10-04 11:19:54 | 6730.0 | 2014-10-04 11:19:54 | 21.0 | 2014-10-04 11:19:54 | 44582.0 | 2014-10-04 11:20:00 | 15336.0 | 2014-10-04 11:20:00 | . 2 782 | 2014-07-03 11:00:28 | 782.0 | 2014-07-03 11:00:53 | 782.0 | 2014-07-03 11:00:58 | 782.0 | 2014-07-03 11:01:06 | 782.0 | 2014-07-03 11:01:09 | 782.0 | 2014-07-03 11:01:10 | 782.0 | 2014-07-03 11:01:23 | 782.0 | 2014-07-03 11:01:29 | 782.0 | 2014-07-03 11:01:30 | 782.0 | 2014-07-03 11:01:53 | . 3 55 | 2014-12-05 15:55:12 | 55.0 | 2014-12-05 15:55:13 | 55.0 | 2014-12-05 15:55:14 | 55.0 | 2014-12-05 15:56:15 | 55.0 | 2014-12-05 15:56:16 | 55.0 | 2014-12-05 15:56:17 | 55.0 | 2014-12-05 15:56:18 | 55.0 | 2014-12-05 15:56:19 | 1445.0 | 2014-12-05 15:56:33 | 1445.0 | 2014-12-05 15:56:36 | . 4 1023 | 2014-11-04 10:03:19 | 1022.0 | 2014-11-04 10:03:19 | 50.0 | 2014-11-04 10:03:20 | 222.0 | 2014-11-04 10:03:21 | 202.0 | 2014-11-04 10:03:21 | 3374.0 | 2014-11-04 10:03:22 | 50.0 | 2014-11-04 10:03:22 | 48.0 | 2014-11-04 10:03:22 | 48.0 | 2014-11-04 10:03:23 | 3374.0 | 2014-11-04 10:03:23 | . 5 301 | 2014-05-16 15:05:31 | 301.0 | 2014-05-16 15:05:32 | 301.0 | 2014-05-16 15:05:33 | 66.0 | 2014-05-16 15:05:39 | 67.0 | 2014-05-16 15:05:40 | 69.0 | 2014-05-16 15:05:40 | 70.0 | 2014-05-16 15:05:40 | 68.0 | 2014-05-16 15:05:40 | 71.0 | 2014-05-16 15:05:40 | 167.0 | 2014-05-16 15:05:44 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; test_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 82797 entries, 1 to 82797 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 site1 82797 non-null int64 1 time1 82797 non-null object 2 site2 81308 non-null float64 3 time2 81308 non-null object 4 site3 80075 non-null float64 5 time3 80075 non-null object 6 site4 79182 non-null float64 7 time4 79182 non-null object 8 site5 78341 non-null float64 9 time5 78341 non-null object 10 site6 77566 non-null float64 11 time6 77566 non-null object 12 site7 76840 non-null float64 13 time7 76840 non-null object 14 site8 76151 non-null float64 15 time8 76151 non-null object 16 site9 75484 non-null float64 17 time9 75484 non-null object 18 site10 74806 non-null float64 19 time10 74806 non-null object dtypes: float64(9), int64(1), object(10) memory usage: 13.3+ MB . In the training sample there are 2297 sessions of one user (Alice) and 251264 sessions of other users, not Alice. The class imbalance is very strong, and looking at the proportion of correct answers (accuracy) is not indicative. . train_df[&#39;target&#39;].value_counts() . 0 251264 1 2297 Name: target, dtype: int64 . So far, we will use only the indexes of visited sites for the forecast. The indexes were numbered from 1, so we&#39;ll replace the omissions with zeros. . train_test_df_sites = train_test_df[[&#39;site%d&#39; % i for i in range(1, 11)]].fillna(0).astype(&#39;int&#39;) . train_test_df_sites.head(10) . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 . session_id . 1 718 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 890 | 941 | 3847 | 941 | 942 | 3846 | 3847 | 3846 | 1516 | 1518 | . 3 14769 | 39 | 14768 | 14769 | 37 | 39 | 14768 | 14768 | 14768 | 14768 | . 4 782 | 782 | 782 | 782 | 782 | 782 | 782 | 782 | 782 | 782 | . 5 22 | 177 | 175 | 178 | 177 | 178 | 175 | 177 | 177 | 178 | . 6 570 | 21 | 570 | 21 | 21 | 0 | 0 | 0 | 0 | 0 | . 7 803 | 23 | 5956 | 17513 | 37 | 21 | 803 | 17514 | 17514 | 17514 | . 8 22 | 21 | 29 | 5041 | 14422 | 23 | 21 | 5041 | 14421 | 14421 | . 9 668 | 940 | 942 | 941 | 941 | 942 | 940 | 23 | 21 | 22 | . 10 3700 | 229 | 570 | 21 | 229 | 21 | 21 | 21 | 2336 | 2044 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Let&#39;s create sparse matrices X_train_sparse and X_test_sparse in the same way as we did earlier. We use the combined matrix train_test_df_sites, then divide it back into the training and test parts. . It should be noted that we have zeros in less than 10 sites in sessions, so the first sign (how many times 0 was caught) is different in meaning from the rest (how many times a site with the index $i$ was caught). Therefore, we will delete the first column of the sparse matrix. . Let&#39;s separate the answers in the training sample into a separate vector y. . def to_sparse(X): &quot;&quot;&quot;Transformation of the matrix from dense to sparse. Args: X (numpy.ndarray): The original (dense) matrix. Returns: scipy.sparse.csr.csr_matrix: Sparse matrix. &quot;&quot;&quot; return csr_matrix((np.ones(X.size, dtype=int), X.reshape(-1), np.arange(X.shape[0] + 1) * X.shape[1]))[:, 1:] train_test_sparse = to_sparse(train_test_df_sites.values) X_train_sparse = train_test_sparse[:train_df.shape[0]] X_test_sparse = train_test_sparse[train_df.shape[0]:] y = train_df.target . Question 1. Output the dimensions of the matrices X_train_sparse and X_test_sparse â€“ 4 numbers on one line separated by a space: the number of rows and columns of the matrix X_train_sparse, then the number of rows and columns of the matrix X_test_sparse. . print(X_train_sparse.shape[0], &quot; - &quot;, X_train_sparse.shape[1],&quot; - &quot;, X_test_sparse.shape[0], &quot; - &quot;, X_test_sparse.shape[1]) . 253561 - 48371 - 82797 - 48371 . Save the objects X_train_sparse, X_test_sparse and y to the pickle files (the latter to the file kaggle_data/train_target.pkl). . with open(os.path.join(PATH_TO_DATA, &#39;X_train_sparse.pkl&#39;), &#39;wb&#39;) as X_train_sparse_pkl: pickle.dump(X_train_sparse, X_train_sparse_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;X_test_sparse.pkl&#39;), &#39;wb&#39;) as X_test_sparse_pkl: pickle.dump(X_test_sparse, X_test_sparse_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;train_target.pkl&#39;), &#39;wb&#39;) as train_target_pkl: pickle.dump(y, train_target_pkl, protocol=2) . Let&#39;s divide the training sample into 2 parts in the proportion of 7/3, and without mixing. The initial data is ordered by time, the test sample is clearly separated by time from the training one, we will observe the same here. . train_share = int(.7 * X_train_sparse.shape[0]) X_train, y_train = X_train_sparse[:train_share, :], y[:train_share] X_valid, y_valid = X_train_sparse[train_share:, :], y[train_share:] . Create an object &#39;sklearn.linear_model.SGDClassifier&#39; with a logistic loss function and the parameter random_state=17. We will leave the other parameters by default. Let&#39;s train the model on the sample (X_train, y_train). . from sklearn import linear_model . sgd_logit = SGDClassifier(loss=&#39;log&#39;, random_state=17, n_jobs=-1) sgd_logit.fit(X_train, y_train) . SGDClassifier(loss=&#39;log&#39;, n_jobs=-1, random_state=17) . Let&#39;s make a forecast in the form of predicted probabilities that this is Alice&#39;s session, on a deferred sample (X_valid, y_valid). . logit_valid_pred_proba = sgd_logit.predict_proba(X_valid) . Question 2. Let&#39;s calculate the ROC AUC of a logistic regression trained using stochastic gradient descent on a deferred sample. Round it up to 3 digits after the separator. . from sklearn.metrics import roc_auc_score . round(roc_auc_score(y_valid, logit_valid_pred_proba[:, 1]), 3) . 0.934 . Let&#39;s make a forecast in the form of predicted probabilities of being assigned to class 1 for the test sample using the same sgd_logit, trained already on the entire training sample (and not by 70%). . %%time sgd_logit = SGDClassifier(loss=&#39;log&#39;, random_state=17, n_jobs=-1) sgd_logit.fit(X_train_sparse, y) logit_test_pred_proba = sgd_logit.predict_proba(X_test_sparse) . CPU times: user 775 ms, sys: 104 ms, total: 879 ms Wall time: 777 ms . We will write the answers to a file and send it to Kaggle. Let&#39;s give our team (of one person) on Kaggle a talking name - according to the template &quot;[YDF &amp; MIPT] Coursera_Username&quot;, so that we can easily identify our answer on the leaderboard. . def write_to_submission_file(predicted_labels, out_file, target=&#39;target&#39;, index_label=&quot;session_id&quot;): # turn predictions into data frame and save as csv file predicted_df = pd.DataFrame(predicted_labels, index = np.arange(1, predicted_labels.shape[0] + 1), columns=[target]) predicted_df.to_csv(out_file, index_label=index_label) . write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join(PATH_TO_DATA, &#39;prediction_2.csv&#39;)) . Improving the model, building new features . Ways to improve . Use previously constructed features to improve the model (you can check them on a smaller sample of 150 users by separating one of the users from the rest â€“ it&#39;s faster) | Adjust model parameters (for example, regularization coefficients) | If the power allows (or you have enough patience), you can try mixing (blending) the responses of the boosting and linear model. Here one of the most famous tutorials on mixing algorithm responses, also good article Alexandra Diakonova | Please note that the competition also provides the initial data on the web pages visited by Alice and the other 1557 users (train.zip ). Based on this data, you can form your own training sample. | . Let&#39;s create such a sign, which will be a number of the form YYYY MM from the date when the session took place, for example 201407 -- 2014 and 7 month. Thus, we will take into account the monthly linear trend for the entire period of the data provided. . train_test_df_ver_2 = pd.concat([train_df, test_df]) . train_test_df_sites_ver_2 = train_test_df_ver_2[[&#39;site%d&#39; % i for i in range(1, 11)]].fillna(0).astype(&#39;int&#39;) . train_test_df_time_ver_2 = train_test_df_ver_2[[&#39;time%d&#39; % i for i in range(1, 11)]].fillna(0).astype(&#39;datetime64[ns]&#39;) . new_feat = pd.DataFrame(index = train_test_df_time_ver_2.index) . def morning(hour): if hour &lt;= 11: return 1 else: return 0 . new_feat[&#39;year_month&#39;] = train_test_df_time_ver_2[&#39;time1&#39;].apply(lambda ts: 100 * ts.year + ts.month) . new_feat.head() . year_month . session_id . 1 201402 | . 2 201402 | . 3 201312 | . 4 201403 | . 5 201402 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; scaler = StandardScaler() new_feat[&#39;year_month_scaled&#39;] = scaler.fit_transform(new_feat[&#39;year_month&#39;].values.reshape(-1,1)) . Let&#39;s add two new signs: start_hour and morning. . The start_hour attribute is the hour at which the session started (from 0 to 23), and the morning binary attribute is 1 if the session started in the morning and 0 if the session started later (we will assume that morning is if start_hour is 11 or less). . new_feat[&#39;start_hour&#39;] = train_test_df_time_ver_2[&#39;time1&#39;].apply(lambda ts: ts.hour) . new_feat[&#39;morning&#39;] = new_feat[&#39;start_hour&#39;].apply(morning) . new_feat = new_feat.drop([&#39;year_month&#39;], axis=1) . year_month_scaled start_hour morning . session_id . 1 0.476232 | 10 | 1 | . 2 0.476232 | 11 | 1 | . 3 -1.800775 | 16 | 0 | . 4 0.501532 | 10 | 1 | . 5 0.476232 | 10 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_test_df_sites_new_feat = pd.concat([train_test_df_sites_ver_2, new_feat], axis=1) . train_test_df_sites_new_feat.head() . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 year_month_scaled start_hour morning . session_id . 1 718 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.476232 | 10 | 1 | . 2 890 | 941 | 3847 | 941 | 942 | 3846 | 3847 | 3846 | 1516 | 1518 | 0.476232 | 11 | 1 | . 3 14769 | 39 | 14768 | 14769 | 37 | 39 | 14768 | 14768 | 14768 | 14768 | -1.800775 | 16 | 0 | . 4 782 | 782 | 782 | 782 | 782 | 782 | 782 | 782 | 782 | 782 | 0.501532 | 10 | 1 | . 5 22 | 177 | 175 | 178 | 177 | 178 | 175 | 177 | 177 | 178 | 0.476232 | 10 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_test_sparse = to_sparse(train_test_df_sites_new_feat.values) X_train_sparse_new_feat = train_test_sparse[:train_df.shape[0]] X_test_sparse_new_feat = train_test_sparse[train_df.shape[0]:] . train_share = int(.7 * X_train_sparse.shape[0]) X_train, y_train = X_train_sparse_new_feat[:train_share, :], y[:train_share] X_valid, y_valid = X_train_sparse_new_feat[train_share:, :], y[train_share:] . sgd_logit = SGDClassifier(loss=&#39;log&#39;, random_state=17, n_jobs=-1) sgd_logit.fit(X_train, y_train) . SGDClassifier(loss=&#39;log&#39;, n_jobs=-1, random_state=17) . logit_valid_pred_proba = sgd_logit.predict_proba(X_valid) . round(roc_auc_score(y_valid, logit_valid_pred_proba[:, 1]), 3) . 0.96 . %%time sgd_logit = SGDClassifier(loss=&#39;log&#39;, random_state=17, n_jobs=-1) sgd_logit.fit(X_train_sparse_new_feat, y) logit_test_pred_proba = sgd_logit.predict_proba(X_test_sparse_new_feat) . CPU times: user 1.05 s, sys: 77.9 ms, total: 1.13 s Wall time: 1.19 s . write_to_submission_file(logit_test_pred_proba[:, 1], os.path.join(PATH_TO_DATA, &#39;prediction_new_feat_2.csv&#39;)) . My best achievement last time on the leaderboard was score 0.92159. This time we have reached the score 0.91881 .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/english/2019/08/18/project-identification-of-internet-users-week5.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/english/2019/08/18/project-identification-of-internet-users-week5.html",
            "date": " â€¢ Aug 18, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Graduation project "Identification of Internet users" - Comparison of classification algorithms. Programming Assignment",
            "content": "Week 4. Comparison of classification algorithms . Now we will finally approach the training of classification models, compare several algorithms on cross-validation, and figure out which session length parameters (session_length and window_size) are better to use. Also, for the selected algorithm, we will construct validation curves (how the classification quality depends on one of the hyperparameters of the algorithm) and learning curves (how the classification quality depends on the sample size). . 4 week plan: . Part 1. Comparison of several algorithms in sessions of 10 sites | Part 2. Selection of parameters - session length and window width | Part 3. Identification of a specific user and learning curves | . In this part of the project, video recordings of the following lectures of the course &quot;Learning from marked data&quot; may be useful: . Linear qualification | Comparison of algorithms and selection of hypermarkets | Cross-validation. Sklearn.cross_validation | Linear models. Sklearn.linear_model. ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ | and many others | . #%load_ext watermark . . from __future__ import division, print_function # disable any Anaconda warnings import warnings warnings.filterwarnings(&#39;ignore&#39;) from time import time import itertools import os import numpy as np import pandas as pd import seaborn as sns %matplotlib inline from matplotlib import pyplot as plt import pickle from scipy.sparse import csr_matrix from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV from sklearn.metrics import accuracy_score, f1_score . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/capstone_user_identification&#39; . Part 1. Comparison of several algorithms in sessions of 10 sites . Let&#39;s load the previously serialized objects X_sparse_10 users and y_10 users, corresponding to the training sample for 10 users. . with open(os.path.join(PATH_TO_DATA, &#39;X_sparse_10users.pkl&#39;), &#39;rb&#39;) as X_sparse_10users_pkl: X_sparse_10users = pickle.load(X_sparse_10users_pkl) with open(os.path.join(PATH_TO_DATA, &#39;y_10users.pkl&#39;), &#39;rb&#39;) as y_10users_pkl: y_10users = pickle.load(y_10users_pkl) . There are more than 14 thousand sessions and almost 5 thousand unique visited sites. . X_sparse_10users.shape . (14061, 4913) . Let&#39;s split the sample into 2 parts. On one we will carry out cross-validation, on the second we will evaluate the model trained after cross-validation. . # X_sparse_10users = X_sparse_10users.todense() . X_train, X_valid, y_train, y_valid = train_test_split(X_sparse_10users, y_10users, test_size=0.3, random_state=17, stratify=y_10users) . Let&#39;s set the type of cross-validation in advance: 3-fold, with mixing, the parameter random_state = 56 - for reproducibility. . skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17) . Auxiliary function for drawing validation curves after starting GridSearchCV (or RandomizedCV). . def plot_validation_curves(param_values, grid_cv_results_): train_mu, train_std = grid_cv_results_[&#39;mean_train_score&#39;], grid_cv_results_[&#39;std_train_score&#39;] valid_mu, valid_std = grid_cv_results_[&#39;mean_test_score&#39;], grid_cv_results_[&#39;std_test_score&#39;] train_line = plt.plot(param_values, train_mu, &#39;-&#39;, label=&#39;train&#39;, color=&#39;green&#39;) valid_line = plt.plot(param_values, valid_mu, &#39;-&#39;, label=&#39;test&#39;, color=&#39;red&#39;) plt.fill_between(param_values, train_mu - train_std, train_mu + train_std, edgecolor=&#39;none&#39;, facecolor=train_line[0].get_color(), alpha=0.2) plt.fill_between(param_values, valid_mu - valid_std, valid_mu + valid_std, edgecolor=&#39;none&#39;, facecolor=valid_line[0].get_color(), alpha=0.2) plt.legend() . 1. Let&#39;s train a &#39;KNeighborsClassifier&#39; with 100 nearest neighbors (we&#39;ll leave the rest of the parameters by default, only &#39;n_jobs&#39;= -1 for parallelization) and look at the proportion of correct answers on 3-fold cross-validation (for the sake of reproducibility, we use the StratifiedKFold skf&#39; object for this) on the sample(X_train, y_train)and separately on the sample(X_valid, y_valid)`. . from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score from time import time . knn = KNeighborsClassifier(n_neighbors=100) . t_start = time() knn_cv_score = cross_val_score(knn, X_train, y_train, cv=skf, n_jobs=-1) print(&quot;CV scores:&quot;, knn_cv_score) print(&quot;mean:&quot;, np.mean(knn_cv_score)) print(&quot;std:&quot;, np.std(knn_cv_score)) print(&quot;Time elapsed: &quot;, time()-t_start) . CV scores: [0.56598598 0.55409936 0.55792683] mean: 0.5593373897012363 std: 0.004954135909104914 Time elapsed: 3.559767246246338 . t_start = time() scores = cross_val_score(knn, X_valid, y_valid, cv=skf) print(&quot;CV scores:&quot;, scores) print(&quot;mean:&quot;, np.mean(scores)) print(&quot;std:&quot;, np.std(scores)) print(&quot;Time elapsed: &quot;, time()-t_start) . CV scores: [0.50817342 0.51351351 0.48435277] mean: 0.5020132353203838 std: 0.012676699846532288 Time elapsed: 0.5638549327850342 . Importernt:I had a problem with my code in this place: cross_val_score did not work and gave result NaN or, if I used the todense, code worked for too long. . Question 1 Let&#39;s calculate the proportion of correct answers for the KNeighborsClassifier on cross-validation and deferred sampling. I&#39;ll round each one up to 3 decimal places. . knn.fit(X_train, y_train) knn_pred = knn.predict(X_valid) . print(f&quot;KNN Cross-Validation Score: {knn_cv_score.mean():.3f}&quot;) print(f&quot;KNN Validation Score: {accuracy_score(y_valid, knn_pred):.3f}&quot;) . KNN Cross-Validation Score: 0.559 KNN Validation Score: 0.584 . 2. Train a random forest (RandomForestClassifier) of 100 trees (for reproducibility random_state=17). Let&#39;s look at the OOB score (for which we will immediately set oob_score=True) and the proportion of correct answers in the sample (X_valid, y_valid). For parallelization, set n_jobs= -1. . Question 2. Calculate the percentages of correct answers for RandomForestClassifier during Out-of-Bag evaluation and on deferred sampling? . from sklearn.ensemble import RandomForestClassifier . clf = RandomForestClassifier(n_estimators=100, random_state=17, oob_score=True, n_jobs=-1) . t_start = time() clf.fit(X_train, y_train) print(&#39;Score: &#39;, clf.score(X_train, y_train)) print(&quot;Time elapsed: &quot;, time()-t_start) . Score: 0.9759195285511075 Time elapsed: 11.684211492538452 . print(clf.oob_score_) . 0.7172322698638488 . t_start = time() clf_pred = clf.predict(X_valid) print(accuracy_score(y_valid, clf_pred)) print(&quot;Time elapsed: &quot;, time()-t_start) . 0.7312159279450107 Time elapsed: 0.20986366271972656 . 3. Let&#39;s train Logistic Regression with the default parameter C and random_state=17 (for reproducibility). Let&#39;s look at the proportion of correct answers on cross-validation (using the scf object created earlier) and on the sample (X_valid, y_valid). For parallelization, set n_jobs= -1. . from sklearn.linear_model import LogisticRegression, LogisticRegressionCV . logit = LogisticRegression(random_state=17, n_jobs=-1) . t_start = time() logit_cv_score = cross_val_score(logit, X_train, y_train, cv=skf) logit.fit(X_train, y_train) logit_y_pred = logit.predict(X_valid) logit_val_score = accuracy_score(y_valid, logit_y_pred) print(f&quot;LogReg Cross-Validation Score: {logit_cv_score.mean():.3f}&quot;) print(f&quot;LogReg Validation Score: {logit_val_score:.3f}&quot;) print(&quot;Time elapsed: &quot;, time()-t_start) . LogReg Cross-Validation Score: 0.761 LogReg Validation Score: 0.777 Time elapsed: 10.484177112579346 . Using LogisticRegressionCV, we will select the parameter C for Logistic Regression first in a wide range: 10 values from 1e-4 to 1e2, use logspace from NumPy. Specify the LogisticRegressionCV parameters multi_class=&#39;multinomial&#39; and random_state=17. For cross-validation, we use the skf object created earlier. For parallelization, set n_jobs= -1. . At the end, we will draw validation curves for parameter C. . t_start = time() logit_c_values1 = np.logspace(-4, 2, 10) logit_grid_searcher1 = LogisticRegressionCV(Cs=logit_c_values1, cv=skf, multi_class=&quot;multinomial&quot;, random_state=17) logit_grid_searcher1.fit(X_train, y_train) print(&quot;Time elapsed: &quot;, time()-t_start) . Time elapsed: 91.80030512809753 . The average values of the proportion of correct responses to cross-validation for each of the 10 parameters C. . logit_mean_cv_scores1 = np.array( list(logit_grid_searcher1.scores_.values())).mean(axis=(0, 1)) logit_mean_cv_scores1 . array([0.31954964, 0.47307397, 0.55202236, 0.64875035, 0.71438846, 0.75177962, 0.76092382, 0.75848551, 0.749849 , 0.74029823]) . We will output the best value of the proportion of correct answers on cross-validation and the corresponding value with. . best_score1 = np.max(logit_mean_cv_scores1) best_C1 = logit_grid_searcher1.Cs_[np.argmax(logit_mean_cv_scores1)] print(f&quot;Best Score: {best_score1}&quot;) print(f&quot;Best C: {best_C1}&quot;) . Best Score: 0.7609238210638737 Best C: 1.0 . Let&#39;s draw a graph of the dependence of the proportion of correct answers to cross-validation on `C&#39;. . plt.plot(logit_c_values1, logit_mean_cv_scores1); . Now the same thing, only the values of parameter &#39;C&#39; are iterated over in the range np.linspace(0.1, 7, 20). Let&#39;s draw validation curves again, determine the maximum value of the proportion of correct answers on cross-validation. . t_start = time() logit_c_values2 = np.linspace(0.1, 7, 20) logit_grid_searcher2 = LogisticRegressionCV(Cs=logit_c_values2, cv=skf, multi_class=&#39;multinomial&#39;, random_state=17, n_jobs=-1) logit_grid_searcher2.fit(X_train, y_train) print(&quot;Time elapsed: &quot;, time()-t_start) . Time elapsed: 105.8310170173645 . The average values of the proportion of correct responses to cross-validation for each of the 10 parameters `C&#39;. . logit_mean_cv_scores2 = np.array( list(logit_grid_searcher2.scores_.values())).mean(axis=(0, 1)) logit_mean_cv_scores2 . array([0.73481117, 0.75919655, 0.76102545, 0.76082216, 0.76133023, 0.76143192, 0.75990775, 0.75929811, 0.76000937, 0.75939977, 0.75919661, 0.75868861, 0.757571 , 0.75736787, 0.75716462, 0.75614861, 0.75553901, 0.75513262, 0.75401502, 0.75350692]) . We will output the best value of the proportion of correct answers on cross-validation and the corresponding value with. . best_score2 = np.max(logit_mean_cv_scores2) best_C2 = logit_grid_searcher2.Cs_[np.argmax(logit_mean_cv_scores2)] print(f&quot;Best Score: {best_score2}&quot;) print(f&quot;Best C: {best_C2}&quot;) . Best Score: 0.761431920171076 Best C: 1.9157894736842107 . Let&#39;s draw a graph of the dependence of the proportion of correct answers to cross-validation on `C&#39;. . plt.plot(logit_c_values2, logit_mean_cv_scores2); . We output the proportion of correct answers in the sample (X_value, y_value)&#39; for logistic regression with the best values foundC&#39;. . t_start = time() logit = LogisticRegression(C=best_C2, n_jobs=-1, random_state=17) logit_cv_score = cross_val_score(logit, X_train, y_train, cv=skf, n_jobs=-1) logit.fit(X_train, y_train) logit_y_pred = logit.predict(X_valid) logit_val_score = accuracy_score(y_valid, logit_y_pred) print(&quot;Time elapsed: &quot;, time()-t_start) . Time elapsed: 9.804409980773926 . Question 3. Let&#39;s calculate the proportions of correct answers for &#39;logit_grid_searcher 2&#39; on cross-validation for the best value of parameter &#39;C` and on deferred sampling. Round each one to 3 decimal places and print it separated by a space. . print(f&quot;LogReg Cross-Validation Score: {logit_cv_score.mean():.3f}&quot;) print(f&quot;LogReg Validation Score: {logit_val_score:.3f}&quot;) . LogReg Cross-Validation Score: 0.762 LogReg Validation Score: 0.782 . 4. Let&#39;s train a linear SVM (&#39;LinearSVC) with the parameter &#39;C&#39;=1 and &#39;random_state&#39;=17 (for reproducibility). Let&#39;s look at the proportion of correct answers on cross-validation (using theskf&#39; object created earlier) and on the sample (X_valid, y_valid). . from sklearn.svm import LinearSVC . t_start = time() svm = LinearSVC(C=1, random_state=17) scores_svm = cross_val_score(svm, X_train, y_train, cv=skf, n_jobs=-1) print(&quot;CV scores:&quot;, scores_svm) print(&quot;mean:&quot;, np.mean(scores_svm)) print(&quot;std:&quot;, np.std(scores_svm)) print(&quot;Time elapsed: &quot;, time()-t_start) . CV scores: [0.75068577 0.73270344 0.7695122 ] mean: 0.7509671352428245 std: 0.015028426724668621 Time elapsed: 3.3960418701171875 . t_start = time() svm.fit(X_train, y_train) svm_pred = svm.predict(X_valid) print(accuracy_score(y_valid, svm_pred)) print(&quot;Time elapsed: &quot;, time()-t_start) . 0.7769613652524295 Time elapsed: 2.0046310424804688 . Using GridSearchCV, we will select the parameter C for SVM first in a wide range: 10 values from 1e-4 to 1 e4, use linspace from NumPy. Let&#39;s draw validation curves. . %%time svm_params1 = { &quot;C&quot;: np.linspace(1e-4, 1e4, 10) } svm_grid_searcher1 = GridSearchCV(estimator=svm, cv=skf, param_grid=svm_params1, return_train_score=True) svm_grid_searcher1.fit(X_train, y_train) . CPU times: user 49.9 s, sys: 34.8 ms, total: 50 s Wall time: 49.7 s . t_start = time() svm_params1 = {&#39;C&#39;: np.linspace(1e-4, 1e4, 10)} svm_grid_searcher1 = GridSearchCV(svm, param_grid=svm_params1, cv=skf, return_train_score=True, n_jobs=-1) svm_grid_searcher1.fit(X_train, y_train) print(&quot;Time elapsed: &quot;, time()-t_start) . Time elapsed: 44.00320863723755 . We will output the best value of the proportion of correct answers on cross-validation and the corresponding value C. . svm_grid_searcher1.best_params_ . {&#39;C&#39;: 5555.555600000001} . Let&#39;s draw a graph of the dependence of the proportion of correct answers to cross-validation on C. . plot_validation_curves(svm_params1[&#39;C&#39;], svm_grid_searcher1.cv_results_) . But we remember that with the default regularization parameter (C=1) on cross-validation, the proportion of correct answers is higher. This is the case (not uncommon) when you can make a mistake and iterate over the parameters in the wrong range (the reason is that we took a uniform grid over a large interval and missed a really good range of values C). Here it is much more meaningful to select C in the region of 1, besides, this way the model learns faster than with large C. . Using GridSearchCV, we will select the parameter C for SVM in the range (1e-3, 1), 30 values using `linspace&#39; from NumPy. Let&#39;s draw validation curves. . %%time svm_params2 = {&#39;C&#39;: np.linspace(1e-3, 1, 30)} svm_grid_searcher2 = GridSearchCV(svm, param_grid=svm_params2, cv=skf, return_train_score=True, n_jobs=-1) svm_grid_searcher2.fit(X_train, y_train) . CPU times: user 1.77 s, sys: 150 ms, total: 1.92 s Wall time: 1min 18s . Output the best value of the proportion of correct answers on cross-validation and the corresponding value of C. . best_score = svm_grid_searcher2.best_score_ best_params = svm_grid_searcher2.best_params_ print(f&quot;Best Score: {best_score}&quot;) print(f&quot;Best Params: {best_params}&quot;) . Best Score: 0.7670206386611259 Best Params: {&#39;C&#39;: 0.10434482758620689} . Let&#39;s draw a graph of the dependence of the proportion of correct answers to cross-validation on &#39;C&#39;. . plot_validation_curves(svm_params2[&#39;C&#39;], svm_grid_searcher2.cv_results_) . Output the proportion of correct answers in the sample (X_value, y_value)&#39; for &#39;LinearSVC with the best values found `C&#39;. . %%time svm = LinearSVC(**best_params, random_state=17) svm_cv_score = cross_val_score(svm, X_train, y_train, cv=skf, n_jobs=-1) svm.fit(X_train, y_train) svm_y_pred = svm.predict(X_valid) svm_val_score = accuracy_score(y_valid, svm_y_pred) . CPU times: user 683 ms, sys: 8.26 ms, total: 691 ms Wall time: 1.74 s . Question 4. Let&#39;s calculate the proportions of correct answers for &#39;stm_grid_searcher 2&#39; on cross-validation for the best value of parameter &#39;C` and on deferred sampling. Round each one to 3 decimal places and print it separated by a space. . print(f&quot;SVC Cross-Validation Score: {svm_cv_score.mean():.3f}&quot;) print(f&quot;SVC Validation Score: {svm_val_score:.3f}&quot;) . SVC Cross-Validation Score: 0.767 SVC Validation Score: 0.781 . Part 2. Selection of parameters - session length and window width . Let&#39;s take LinearSVC, which showed the best quality on cross-validation in part 1, and check its work on 8 more samples for 10 users (with different combinations of parameters session_length and window_size). Since there are already more calculations here, we will not re-select the regularization parameter C every time. . Let&#39;s define the model_assessment function, the documentation of which is described below. The split of the sample with &#39;train_test_split&#39; should be stratified. . def model_assessment(estimator, path_to_X_pickle, path_to_y_pickle, cv, random_state=17, test_size=0.3): &#39;&#39;&#39; Estimates CV-accuracy for (1 - test_size) share of (X_sparse, y) loaded from path_to_X_pickle and path_to_y_pickle and holdout accuracy for (test_size) share of (X_sparse, y). The split is made with stratified train_test_split with params random_state and test_size. :param estimator â€“ Scikit-learn estimator (classifier or regressor) :param path_to_X_pickle â€“ path to pickled sparse X (instances and their features) :param path_to_y_pickle â€“ path to pickled y (responses) :param cv â€“ cross-validation as in cross_val_score (use StratifiedKFold here) :param random_state â€“ for train_test_split :param test_size â€“ for train_test_split :returns mean CV-accuracy for (X_train, y_train) and accuracy for (X_valid, y_valid) where (X_train, y_train) and (X_valid, y_valid) are (1 - test_size) and (testsize) shares of (X_sparse, y). &#39;&#39;&#39; with open(path_to_X_pickle, &#39;rb&#39;) as X_sparse_10users_pkl: X_sparse_10users = pickle.load(X_sparse_10users_pkl) with open(path_to_y_pickle, &#39;rb&#39;) as y_10users_pkl: y_10users = pickle.load(y_10users_pkl) X_train, X_valid, y_train, y_valid = train_test_split(X_sparse_10users, y_10users, test_size=0.3, random_state=17, stratify=y_10users) t_start = time() scores_svm = cross_val_score(estimator, X_train, y_train, cv=skf, n_jobs=-1) t_start = time() estimator.fit(X_train, y_train) svm_pred = estimator.predict(X_valid) return(np.mean(scores_svm), accuracy_score(y_valid, svm_pred), &quot; Time elapsed: &quot;, time()-t_start) . Let&#39;s make sure that the function works. . model_assessment(svm_grid_searcher2.best_estimator_, os.path.join(PATH_TO_DATA, &#39;X_sparse_10users.pkl&#39;), os.path.join(PATH_TO_DATA, &#39;y_10users.pkl&#39;), skf, random_state=17, test_size=0.3) . (0.7670206386611259, 0.7807537331121118, &#39; Time elapsed: &#39;, 0.6428191661834717) . Let&#39;s use the model_assessment function for the best algorithm from the previous part (namely, &#39;svm_grid_searcher 2.bestestimator`) and 9 samples of the form with different combinations of parameters session_length and window_size for 10 users. We will output the session_length and window_size parameters in the loop, as well as the output result of the model_assessment function. . Here, for convenience, it is worth creating copies of previously created pickle files X_sparse_10users.pkl, X_sparse_150users.pkl, y_10users.pkl and y_150users.pkl, adding s10_w10 to their names, which means the session length is 10 and the window width is 10. . !cp $PATH_TO_DATA/X_sparse_10users.pkl $PATH_TO_DATA/X_sparse_10users_s10_w10.pkl !cp $PATH_TO_DATA/X_sparse_150users.pkl $PATH_TO_DATA/X_sparse_150users_s10_w10.pkl !cp $PATH_TO_DATA/y_10users.pkl $PATH_TO_DATA/y_10users_s10_w10.pkl !cp $PATH_TO_DATA/y_150users.pkl $PATH_TO_DATA/y_150users_s10_w10.pkl . %%time estimator = svm_grid_searcher2.best_estimator_ for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]): if window_size &lt;= session_length: path_to_X_pkl = os.path.join( PATH_TO_DATA, f&quot;X_sparse_10users_s{session_length}_w{window_size}.pkl&quot;) path_to_y_pkl = os.path.join( PATH_TO_DATA, f&quot;y_10users_s{session_length}_w{window_size}.pkl&quot;) print(window_size, session_length, model_assessment(estimator=estimator, path_to_X_pickle=path_to_X_pkl, path_to_y_pickle=path_to_y_pkl, cv=skf)) . 10 15 (0.8243252292702751, 0.8404835269021095, &#39; Time elapsed: &#39;, 1.0584142208099365) 10 10 (0.7670206386611259, 0.7807537331121118, &#39; Time elapsed: &#39;, 0.6238193511962891) 7 15 (0.8495024256089474, 0.8543222166915547, &#39; Time elapsed: &#39;, 1.623363971710205) 7 10 (0.7983645917156946, 0.8073668491786958, &#39; Time elapsed: &#39;, 0.974440336227417) 7 7 (0.754765400423003, 0.7617388418782147, &#39; Time elapsed: &#39;, 0.5486903190612793) 5 15 (0.8670355547005402, 0.8752963489805595, &#39; Time elapsed: &#39;, 2.1419732570648193) 5 10 (0.8177520250854086, 0.8245614035087719, &#39; Time elapsed: &#39;, 1.218752145767212) 5 7 (0.772939529035208, 0.7853247984826932, &#39; Time elapsed: &#39;, 0.761998176574707) 5 5 (0.7254849424351582, 0.7362494073020389, &#39; Time elapsed: &#39;, 0.501572847366333) CPU times: user 11.2 s, sys: 107 ms, total: 11.3 s Wall time: 33.2 s . %%time estimator = svm_grid_searcher2.best_estimator_ for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]): if window_size &lt;= session_length: path_to_X_pkl = os.path.join( PATH_TO_DATA, f&quot;X_sparse_150users_s{session_length}_w{window_size}.pkl&quot;) path_to_y_pkl = os.path.join( PATH_TO_DATA, f&quot;y_150users_s{session_length}_w{window_size}.pkl&quot;) print(window_size, session_length, model_assessment(estimator=estimator, path_to_X_pickle=path_to_X_pkl, path_to_y_pickle=path_to_y_pkl, cv=skf)) . 10 15 (0.5488098589346596, 0.5751471804602735, &#39; Time elapsed: &#39;, 223.8541784286499) 10 10 (0.46308633866107823, 0.4836276942538802, &#39; Time elapsed: &#39;, 136.27440857887268) 7 15 (0.5828479247872232, 0.6084920121265797, &#39; Time elapsed: &#39;, 309.6582760810852) 7 10 (0.5015547672228792, 0.5239295568348264, &#39; Time elapsed: &#39;, 213.65414190292358) 7 7 (0.43694798464211154, 0.45307763054808053, &#39; Time elapsed: &#39;, 149.04103302955627) 5 15 (0.6139887051608968, 0.6360295906945053, &#39; Time elapsed: &#39;, 467.08626675605774) 5 10 (0.5265866745928696, 0.5458826106000876, &#39; Time elapsed: &#39;, 317.86096453666687) 5 7 (0.46509602699080665, 0.48189516717769015, &#39; Time elapsed: &#39;, 195.35350489616394) 5 5 (0.4084080325808655, 0.4217282328320436, &#39; Time elapsed: &#39;, 142.6359510421753) CPU times: user 36min 40s, sys: 7.97 s, total: 36min 48s Wall time: 1h 29min 29s . Question 5. Let&#39;s calculate the proportion of correct answers for LinearSVC with the configured parameter C and the selection X_sparse_10 users_s15_w5. We will indicate the proportions of correct answers on cross-validation and on deferred sampling. Round each one to 3 decimal places and print it separated by a space. . %%time estimator = svm_grid_searcher2.best_estimator_ path_to_X_pkl = os.path.join(PATH_TO_DATA, &quot;X_sparse_10users_s15_w5.pkl&quot;) path_to_y_pkl = os.path.join(PATH_TO_DATA, &quot;y_10users_s15_w5.pkl&quot;) with open(path_to_X_pkl, &#39;rb&#39;) as X_sparse_10users_pkl: X_sparse_10users = pickle.load(X_sparse_10users_pkl) with open(path_to_y_pkl, &#39;rb&#39;) as y_10users_pkl: y_10users = pickle.load(y_10users_pkl) X_train, X_valid, y_train, y_valid = train_test_split(X_sparse_10users, y_10users, test_size=0.3, random_state=17, stratify=y_10users) scores_svm = cross_val_score(estimator, X_train, y_train, cv=skf, n_jobs=-1) estimator.fit(X_train, y_train) svm_pred = estimator.predict(X_valid) print(f&quot;SVC Cross-Validation Score: {np.mean(scores_svm):.3f}&quot;) print(f&quot;SVC Validation Score: {accuracy_score(y_valid, svm_pred):.3f}&quot;) . SVC Cross-Validation Score: 0.867 SVC Validation Score: 0.875 CPU times: user 2.39 s, sys: 25.6 ms, total: 2.41 s Wall time: 6.38 s . Make a conclusion about how the quality of classification depends on the length of the session and the width of the window. . %%time estimator = svm_grid_searcher2.best_estimator_ for window_size, session_length in [(5, 5), (7, 7), (10, 10)]: path_to_X_pkl = os.path.join( PATH_TO_DATA, f&quot;X_sparse_150users_s{session_length}_w{window_size}.pkl&quot;) path_to_y_pkl = os.path.join( PATH_TO_DATA, f&quot;y_150users_s{session_length}_w{window_size}.pkl&quot;) print(window_size, session_length, model_assessment(estimator=estimator, path_to_X_pickle=path_to_X_pkl, path_to_y_pickle=path_to_y_pkl, cv=skf)) . 5 5 (0.4084080325808655, 0.4217282328320436, &#39; Time elapsed: &#39;, 133.36306834220886) 7 7 (0.43694798464211154, 0.45307763054808053, &#39; Time elapsed: &#39;, 120.4077639579773) 10 10 (0.46308633866107823, 0.4836276942538802, &#39; Time elapsed: &#39;, 114.10968589782715) CPU times: user 6min 16s, sys: 1.8 s, total: 6min 17s Wall time: 16min 33s . Question 6. Calculate the proportions of correct answers for LinearSVC with the C&#39; parameter configured and theX_sparse_150 users` selection. Specify the proportions of correct answers on cross-validation and on deferred sampling. Round each one to 3 decimal places and separate it with a space. . %%time estimator = svm_grid_searcher2.best_estimator_ path_to_X_pkl = os.path.join(PATH_TO_DATA, &quot;X_sparse_150users.pkl&quot;) path_to_y_pkl = os.path.join(PATH_TO_DATA, &quot;y_150users.pkl&quot;) with open(path_to_X_pkl, &#39;rb&#39;) as X_sparse_150users_pkl: X_sparse_150users = pickle.load(X_sparse_150users_pkl) with open(path_to_y_pkl, &#39;rb&#39;) as y_150users_pkl: y_150users = pickle.load(y_150users_pkl) X_train, X_valid, y_train, y_valid = train_test_split(X_sparse_150users, y_150users, test_size=0.3, random_state=17, stratify=y_150users) scores_svm = cross_val_score(estimator, X_train, y_train, cv=skf, n_jobs=-1) estimator.fit(X_train, y_train) svm_pred = estimator.predict(X_valid) print(f&quot;SVC Cross-Validation Score: {np.mean(scores_svm):.3f}&quot;) print(f&quot;SVC Validation Score: {accuracy_score(y_valid, svm_pred):.3f}&quot;) . SVC Cross-Validation Score: 0.463 SVC Validation Score: 0.484 CPU times: user 1min 46s, sys: 339 ms, total: 1min 47s Wall time: 4min 37s . Part 3. Identification of a specific user and learning curves . Since it may be disappointing that the multiclass share of correct answers in a sample of 150 users is small, let&#39;s be glad that a particular user can be identified well enough. . Let&#39;s load the previously serialized objects X_sparse_150users and y_150users corresponding to the training sample for 150 users with parameters (session_length, window_size) = (10.10). Just exactly break them down into 70% and 30%. . with open(os.path.join(PATH_TO_DATA, &#39;X_sparse_150users.pkl&#39;), &#39;rb&#39;) as X_sparse_150users_pkl: X_sparse_150users = pickle.load(X_sparse_150users_pkl) with open(os.path.join(PATH_TO_DATA, &#39;y_150users.pkl&#39;), &#39;rb&#39;) as y_150users_pkl: y_150users = pickle.load(y_150users_pkl) . X_train_150, X_valid_150, y_train_150, y_valid_150 = train_test_split(X_sparse_150users, y_150users, test_size=0.3, random_state=17, stratify=y_150users) . Let&#39;s train LogisticRegressionCV for one value of the parameter C (the best on cross-validation in 1 part, use the exact value, not by eye). Now we will solve 150 tasks &quot;One-against-All&quot;, so we will specify the argument multi_class=ovr. As always, where possible, specify n_jobs=-1 and random_state=17. . from sklearn.linear_model import LogisticRegression, LogisticRegressionCV best_C2_tmp = 1.9157894736842107 skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=17) . y_train_150_tmp = [] for i in y_train_150: y_train_150_tmp.append(int(i[4:])) # convert to int y_train_150_work = np.array(y_train_150_tmp, dtype=np.int) . %%time logit_cv_150users = LogisticRegressionCV(Cs=[best_C2_tmp], cv=skf, multi_class=&quot;ovr&quot;, n_jobs=-1, random_state=17) logit_cv_150users.fit(X_train_150, y_train_150_work) . CPU times: user 6min 55s, sys: 7min 30s, total: 14min 25s Wall time: 15min 25s . Look at the average proportions of correct responses to cross-validation in the task of identifying each user individually. . cv_scores_by_user = logit_cv_150users.scores_ for user_id in logit_cv_150users.scores_: print(f&quot;User {user_id}, CV score: {cv_scores_by_user[user_id].mean()}&quot;) . User 6, CV score: 0.996058928403866 User 13, CV score: 0.9963091551718745 User 15, CV score: 0.9952352652925046 User 16, CV score: 0.9918676300397236 User 28, CV score: 0.9903766955470061 User 31, CV score: 0.9943907499504759 User 33, CV score: 0.9937651830304546 User 39, CV score: 0.9858621876075193 User 46, CV score: 0.9980398903172666 User 49, CV score: 0.9952039869465036 User 50, CV score: 0.9943386193738076 User 53, CV score: 0.9937130524537862 User 65, CV score: 0.9969451482072295 User 66, CV score: 0.9948077945638234 User 82, CV score: 0.996361285748543 User 85, CV score: 0.9963717118638766 User 89, CV score: 0.9908667229676894 User 92, CV score: 0.994422028296477 User 100, CV score: 0.9944741588731455 User 102, CV score: 0.9911586541970326 User 103, CV score: 0.980565721018006 User 105, CV score: 0.9969034437458948 User 106, CV score: 0.9948494990251583 User 118, CV score: 0.990950131890359 User 119, CV score: 0.9965906602858841 User 120, CV score: 0.994286488797139 User 126, CV score: 0.995058021331832 User 127, CV score: 0.9915965510410477 User 128, CV score: 0.9846944626901463 User 138, CV score: 0.9970598354759 User 158, CV score: 0.9970598354759 User 160, CV score: 0.9968200348232252 User 165, CV score: 0.997362192820577 User 172, CV score: 0.9964863991325471 User 177, CV score: 0.9967783303618906 User 203, CV score: 0.9975707151272508 User 207, CV score: 0.9877805928289178 User 223, CV score: 0.9965489558245494 User 233, CV score: 0.9963091551718746 User 235, CV score: 0.9966636430932199 User 236, CV score: 0.9900117815103271 User 237, CV score: 0.9893862145903057 User 238, CV score: 0.9962570245952062 User 240, CV score: 0.9957461449438553 User 241, CV score: 0.9959650933658629 User 242, CV score: 0.9951414302545015 User 245, CV score: 0.9960067978271976 User 246, CV score: 0.9970181310145653 User 249, CV score: 0.9950058907551634 User 252, CV score: 0.9965072513632145 User 254, CV score: 0.9920135956543952 User 256, CV score: 0.9961006328652008 User 258, CV score: 0.9959338150198618 User 259, CV score: 0.9949120557171603 User 260, CV score: 0.9974038972819117 User 261, CV score: 0.9897511286269848 User 263, CV score: 0.9927955543044217 User 264, CV score: 0.9966010864012178 User 269, CV score: 0.9871341736782293 User 270, CV score: 0.9894279190516405 User 273, CV score: 0.9944011760658097 User 287, CV score: 0.9901681732403323 User 294, CV score: 0.9957461449438553 User 298, CV score: 0.9912941936963707 User 301, CV score: 0.9972058010905717 User 308, CV score: 0.9957044404825206 User 315, CV score: 0.9975498628965833 User 318, CV score: 0.9958921105585269 User 327, CV score: 0.9966427908625525 User 332, CV score: 0.9968096087078915 User 333, CV score: 0.9962674507105397 User 339, CV score: 0.9971223921679022 User 340, CV score: 0.9967157736698883 User 342, CV score: 0.99225339630707 User 344, CV score: 0.9966532169778862 User 351, CV score: 0.99245149249841 User 356, CV score: 0.9976019934732517 User 361, CV score: 0.9965802341705504 User 363, CV score: 0.9964863991325471 User 411, CV score: 0.9912212108890349 User 417, CV score: 0.9967261997852219 User 425, CV score: 0.9942239321051369 User 430, CV score: 0.9962674507105399 User 435, CV score: 0.9970389832452327 User 436, CV score: 0.9951831347158362 User 440, CV score: 0.9970389832452327 User 444, CV score: 0.9978105157799256 User 475, CV score: 0.9892506750909679 User 476, CV score: 0.9969868526685642 User 486, CV score: 0.9954125092531774 User 515, CV score: 0.9942135059898033 User 533, CV score: 0.9937964613764558 User 561, CV score: 0.9845276448448073 User 563, CV score: 0.9968304609385589 User 564, CV score: 0.9956835882518532 User 568, CV score: 0.991784221117054 User 569, CV score: 0.9893028056676362 User 570, CV score: 0.9982901170852752 User 573, CV score: 0.9907624618143527 User 575, CV score: 0.9900534859716618 User 576, CV score: 0.9941613754131349 User 580, CV score: 0.9867484074108828 User 583, CV score: 0.9808368000166817 User 584, CV score: 0.9811912879380271 User 600, CV score: 0.9915756988103803 User 603, CV score: 0.9957044404825206 User 605, CV score: 0.9975290106659159 User 640, CV score: 0.9972579316672402 User 647, CV score: 0.9976436979345866 User 653, CV score: 0.9973830450512443 User 664, CV score: 0.9952144130618373 User 665, CV score: 0.9969138698612284 User 677, CV score: 0.9966323647472187 User 692, CV score: 0.9969347220918957 User 697, CV score: 0.9959963717118638 User 705, CV score: 0.9964342685558788 User 722, CV score: 0.9947035334104865 User 740, CV score: 0.996694921439221 User 741, CV score: 0.9968513131692264 User 756, CV score: 0.9955793270985164 User 780, CV score: 0.9965489558245494 User 784, CV score: 0.9966532169778862 User 785, CV score: 0.9969555743225631 User 797, CV score: 0.995756571059189 User 812, CV score: 0.9949224818324941 User 844, CV score: 0.9970285571298989 User 859, CV score: 0.9981337253552699 User 868, CV score: 0.9965489558245494 User 875, CV score: 0.9957148665978544 User 932, CV score: 0.990512235046344 User 996, CV score: 0.9933168600711061 User 1014, CV score: 0.9971328182832359 User 1040, CV score: 0.9970389832452327 User 1054, CV score: 0.9964655469018799 User 1248, CV score: 0.9977375329725898 User 1267, CV score: 0.9973309144745759 User 1299, CV score: 0.996924295976562 User 1371, CV score: 0.9934419734551104 User 1797, CV score: 0.994891203486493 User 1798, CV score: 0.9966740692085535 User 1993, CV score: 0.9967991825925578 User 2118, CV score: 0.9978522202412603 User 2174, CV score: 0.995860832212526 User 2191, CV score: 0.9952665436385058 User 2250, CV score: 0.9973413405899096 User 2355, CV score: 0.995860832212526 User 2408, CV score: 0.9937547569151209 User 2493, CV score: 0.9966115125165516 User 2625, CV score: 0.9961423373265355 User 2902, CV score: 0.9971223921679022 . The results seem impressive, but perhaps we forget about the imbalance of classes, and a high proportion of correct answers can be obtained by constant prediction. Let&#39;s calculate for each user the difference between the proportion of correct answers to cross-validation (just calculated using LogisticRegressionCV) and the proportion of labels in y_train_150 other than the ID of this user (this is the proportion of correct answers that can be obtained if the classifier always &quot;says&quot; that this is not the user with the number i in the classification task i-vs-All). . class_distr = np.bincount(y_train_150_work) acc_diff_vs_constant = [] for user_id in np.unique(y_train_150_work): val = (class_distr.sum() - class_distr[user_id]) / class_distr.sum() print(user_id) diff = cv_scores_by_user[user_id].mean() - val acc_diff_vs_constant.append(diff) print(f&quot;User: {user_id} Val: {val:.3f} Diff: {diff}&quot;) . 6 User: 6 Val: 0.984 Diff: 0.011656396943062974 13 User: 13 Val: 0.996 Diff: 0.000604714689353858 15 User: 15 Val: 0.994 Diff: 0.0008340892266949229 16 User: 16 Val: 0.985 Diff: 0.007152315118909902 28 User: 28 Val: 0.988 Diff: 0.0024292848727492933 31 User: 31 Val: 0.994 Diff: -6.255669200216918e-05 33 User: 33 Val: 0.993 Diff: 0.0012198554940413553 39 User: 39 Val: 0.984 Diff: 0.0019496835673996626 46 User: 46 Val: 0.997 Diff: 0.0009174981493643708 49 User: 49 Val: 0.994 Diff: 0.0013762472240467227 50 User: 50 Val: 0.994 Diff: 0.00018767007600650754 53 User: 53 Val: 0.992 Diff: 0.0016681784533899569 65 User: 65 Val: 0.997 Diff: 2.0852230667389726e-05 66 User: 66 Val: 0.995 Diff: -5.213057666852983e-05 82 User: 82 Val: 0.996 Diff: 1.0426115333750374e-05 85 User: 85 Val: 0.996 Diff: 0.00017724396067264614 89 User: 89 Val: 0.990 Diff: 0.0007923847653602545 92 User: 92 Val: 0.994 Diff: 0.0002710789986758444 100 User: 100 Val: 0.995 Diff: -0.0002710789986758444 102 User: 102 Val: 0.990 Diff: 0.0007194019580243349 103 User: 103 Val: 0.977 Diff: 0.0035657314441213117 105 User: 105 Val: 0.996 Diff: 0.0008862198033635638 106 User: 106 Val: 0.987 Diff: 0.007631916424259533 118 User: 118 Val: 0.990 Diff: 0.0009487764953656219 119 User: 119 Val: 0.996 Diff: 0.0006464191506886374 120 User: 120 Val: 0.994 Diff: 0.0006255669200211367 126 User: 126 Val: 0.994 Diff: 0.0010009070720340407 127 User: 127 Val: 0.988 Diff: 0.004087037210805722 128 User: 128 Val: 0.980 Diff: 0.005098370398173291 138 User: 138 Val: 0.997 Diff: -5.2130576668418804e-05 158 User: 158 Val: 0.997 Diff: 0.00028150511400959477 160 User: 160 Val: 0.997 Diff: 0.00028150511400959477 165 User: 165 Val: 0.997 Diff: 0.00028150511400959477 172 User: 172 Val: 0.996 Diff: 0.00022937453734106494 177 User: 177 Val: 0.996 Diff: 0.0002815051140097058 203 User: 203 Val: 0.996 Diff: 0.0013866733393805841 207 User: 207 Val: 0.986 Diff: 0.0014179516853815022 223 User: 223 Val: 0.996 Diff: 8.34089226695589e-05 233 User: 233 Val: 0.996 Diff: 7.298280733591955e-05 235 User: 235 Val: 0.997 Diff: -8.34089226695589e-05 236 User: 236 Val: 0.989 Diff: 0.0012302816093752167 237 User: 237 Val: 0.988 Diff: 0.0013762472240467227 238 User: 238 Val: 0.996 Diff: -1.0426115333639352e-05 240 User: 240 Val: 0.996 Diff: -3.12783460011401e-05 241 User: 241 Val: 0.996 Diff: -0.0001355394993378667 242 User: 242 Val: 0.995 Diff: 0.00047960130534974166 245 User: 245 Val: 0.996 Diff: 0.00023980065267481532 246 User: 246 Val: 0.997 Diff: -0.00010426115333694863 249 User: 249 Val: 0.995 Diff: -9.383503800330928e-05 252 User: 252 Val: 0.996 Diff: 0.00020852230667367522 254 User: 254 Val: 0.990 Diff: 0.0020956491820712797 256 User: 256 Val: 0.996 Diff: 0.0005421579973517998 258 User: 258 Val: 0.996 Diff: 0.00017724396067275716 259 User: 259 Val: 0.995 Diff: -0.0002710789986758444 260 User: 260 Val: 0.997 Diff: 0.0003232095753442632 261 User: 261 Val: 0.989 Diff: 0.0006464191506885264 263 User: 263 Val: 0.992 Diff: 0.0011990032633740766 264 User: 264 Val: 0.996 Diff: 0.00039619238268018275 269 User: 269 Val: 0.986 Diff: 0.0013866733393805841 270 User: 270 Val: 0.985 Diff: 0.004305985632813036 273 User: 273 Val: 0.994 Diff: 0.0006672713813560271 287 User: 287 Val: 0.988 Diff: 0.001928831336732162 294 User: 294 Val: 0.996 Diff: -0.00020852230667389726 298 User: 298 Val: 0.990 Diff: 0.0015222128387184508 301 User: 301 Val: 0.995 Diff: 0.001730735145392126 308 User: 308 Val: 0.995 Diff: 0.0007298280733580853 315 User: 315 Val: 0.997 Diff: 0.0004378968440148512 318 User: 318 Val: 0.995 Diff: 0.0005525841126853281 327 User: 327 Val: 0.997 Diff: 1.0426115333639352e-05 332 User: 332 Val: 0.997 Diff: -1.0426115333750374e-05 333 User: 333 Val: 0.995 Diff: 0.0012928383013771638 339 User: 339 Val: 0.996 Diff: 0.0011990032633741876 340 User: 340 Val: 0.997 Diff: 0.00020852230667367522 342 User: 342 Val: 0.992 Diff: 0.00034406180601176395 344 User: 344 Val: 0.997 Diff: -0.00012511338400422733 351 User: 351 Val: 0.991 Diff: 0.0014179516853815022 356 User: 356 Val: 0.997 Diff: 0.0003857662673464324 361 User: 361 Val: 0.997 Diff: -5.2130576668418804e-05 363 User: 363 Val: 0.995 Diff: 0.0011885771480403262 411 User: 411 Val: 0.989 Diff: 0.001991388028734442 417 User: 417 Val: 0.997 Diff: 9.383503800308723e-05 425 User: 425 Val: 0.994 Diff: 6.255669200205816e-05 430 User: 430 Val: 0.995 Diff: 0.0010217593027015415 435 User: 435 Val: 0.997 Diff: -0.00011468726867058798 436 User: 436 Val: 0.995 Diff: 0.0001668178453390068 440 User: 440 Val: 0.997 Diff: -5.2130576668418804e-05 444 User: 444 Val: 0.997 Diff: 0.0007923847653603655 475 User: 475 Val: 0.988 Diff: 0.0012615599553762458 476 User: 476 Val: 0.996 Diff: 0.0008549414573624237 486 User: 486 Val: 0.995 Diff: 0.0001668178453390068 515 User: 515 Val: 0.994 Diff: 0.00022937453734117597 533 User: 533 Val: 0.993 Diff: 0.0011572988020394082 561 User: 561 Val: 0.981 Diff: 0.0035553053287875613 563 User: 563 Val: 0.996 Diff: 0.0005213057666844101 564 User: 564 Val: 0.995 Diff: 0.0005525841126854392 568 User: 568 Val: 0.992 Diff: -4.170446133477945e-05 569 User: 569 Val: 0.985 Diff: 0.0042538550561445065 570 User: 570 Val: 0.996 Diff: 0.0025856766027545497 573 User: 573 Val: 0.991 Diff: -0.00019809619133992484 575 User: 575 Val: 0.989 Diff: 0.0014492300313825313 576 User: 576 Val: 0.994 Diff: -0.00022937453734106494 580 User: 580 Val: 0.980 Diff: 0.006266095315546338 583 User: 583 Val: 0.966 Diff: 0.014794657658502963 584 User: 584 Val: 0.978 Diff: 0.0027524944480935565 600 User: 600 Val: 0.991 Diff: 0.000980054841366762 603 User: 603 Val: 0.995 Diff: 0.0006464191506886374 605 User: 605 Val: 0.995 Diff: 0.0024605632187502113 640 User: 640 Val: 0.997 Diff: 0.00021894842200753661 647 User: 647 Val: 0.996 Diff: 0.0016264739920553994 653 User: 653 Val: 0.997 Diff: 0.0001355394993378667 664 User: 664 Val: 0.995 Diff: 0.0006359930353549981 665 User: 665 Val: 0.997 Diff: 0.00023980065267481532 677 User: 677 Val: 0.996 Diff: 0.0007298280733580853 692 User: 692 Val: 0.997 Diff: 6.255669200205816e-05 697 User: 697 Val: 0.995 Diff: 0.000750680304025475 705 User: 705 Val: 0.996 Diff: 0.0002502267680085657 722 User: 722 Val: 0.994 Diff: 0.0010843159947033776 740 User: 740 Val: 0.996 Diff: 0.0008340892266950339 741 User: 741 Val: 0.997 Diff: 7.298280733591955e-05 756 User: 756 Val: 0.995 Diff: 0.0002710789986758444 780 User: 780 Val: 0.996 Diff: 0.0007819586500266151 784 User: 784 Val: 0.995 Diff: 0.001699456799391097 785 User: 785 Val: 0.997 Diff: -8.34089226695589e-05 797 User: 797 Val: 0.995 Diff: 0.0005421579973516888 812 User: 812 Val: 0.990 Diff: 0.005421579973517776 844 User: 844 Val: 0.997 Diff: 6.255669200205816e-05 859 User: 859 Val: 0.997 Diff: 0.0012198554940414663 868 User: 868 Val: 0.997 Diff: -0.00022937453734106494 875 User: 875 Val: 0.990 Diff: 0.0055779717035230325 932 User: 932 Val: 0.988 Diff: 0.0024084326420817925 996 User: 996 Val: 0.990 Diff: 0.0028359033707631154 1014 User: 1014 Val: 0.996 Diff: 0.0013449688780456936 1040 User: 1040 Val: 0.995 Diff: 0.0018662746447301037 1054 User: 1054 Val: 0.996 Diff: 0.000114687268670699 1248 User: 1248 Val: 0.997 Diff: 0.00040661849801393313 1267 User: 1267 Val: 0.997 Diff: 0.0003336356906780136 1299 User: 1299 Val: 0.997 Diff: 9.383503800308723e-05 1371 User: 1371 Val: 0.989 Diff: 0.0047126041308269695 1797 User: 1797 Val: 0.992 Diff: 0.0031486868307737392 1798 User: 1798 Val: 0.995 Diff: 0.001240707724708745 1993 User: 1993 Val: 0.996 Diff: 0.0008757936880298134 2118 User: 2118 Val: 0.996 Diff: 0.0015117867233847004 2174 User: 2174 Val: 0.995 Diff: 0.0005108796513507707 2191 User: 2191 Val: 0.995 Diff: 0.0004378968440149622 2250 User: 2250 Val: 0.997 Diff: -9.383503800319826e-05 2355 User: 2355 Val: 0.987 Diff: 0.008424301189619787 2408 User: 2408 Val: 0.992 Diff: 0.0013345427627119433 2493 User: 2493 Val: 0.996 Diff: 0.0003023573446770955 2625 User: 2625 Val: 0.995 Diff: 0.0006776974966896665 2902 User: 2902 Val: 0.995 Diff: 0.00207479695140389 . num_better_than_default = (np.array(acc_diff_vs_constant) &gt; 0).sum() num_better_than_default . 127 . Question 7. Let&#39;s calculate the proportion of users for whom the logistic regression on cross-validation gives a better forecast than the constant one. Round it up to 3 decimal places.| . better = num_better_than_default / len(acc_diff_vs_constant) print(better) . 0.8466666666666667 . Next, we will build learning curves for a specific user, for example, for the 128th. Let&#39;s make a new binary vector based on y_150 users, its values will be 1 or 0, depending on whether the user ID is 128. . y_binary_128 = y_150users == &#39;user0128&#39; y_binary_128.astype(&quot;int&quot;) . array([0, 0, 0, ..., 0, 0, 0]) . from sklearn.model_selection import learning_curve def plot_learning_curve(val_train, val_test, train_sizes, xlabel=&#39;Training Set Size&#39;, ylabel=&#39;score&#39;): def plot_with_err(x, data, **kwargs): mu, std = data.mean(1), data.std(1) lines = plt.plot(x, mu, &#39;-&#39;, **kwargs) plt.fill_between(x, mu - std, mu + std, edgecolor=&#39;none&#39;, facecolor=lines[0].get_color(), alpha=0.2) plot_with_err(train_sizes, val_train, label=&#39;train&#39;) plot_with_err(train_sizes, val_test, label=&#39;valid&#39;) plt.xlabel(xlabel); plt.ylabel(ylabel) plt.legend(loc=&#39;lower right&#39;); . Let&#39;s calculate the proportions of correct answers to cross-validation in the classification problem &quot;user128-vs-All&quot; depending on the sample size. . %%time train_sizes = np.linspace(0.25, 1, 20) estimator = svm_grid_searcher2.best_estimator_ n_train, val_train, val_test = learning_curve( estimator=estimator, X=X_sparse_150users, y=y_binary_128, train_sizes=train_sizes, cv=skf, n_jobs=-1, random_state=17) . CPU times: user 630 ms, sys: 148 ms, total: 777 ms Wall time: 20.5 s . plot_learning_curve(val_train, val_test, n_train, xlabel=&#39;train_size&#39;, ylabel=&#39;accuracy&#39;) . Ways to improve . of course, you can check a bunch of algorithms, for example, Xgboost, but in such a task it is very unlikely that something will do better than linear methods | it is interesting to check the quality of the algorithm on data where sessions were distinguished not by the number of sites visited, but by time, for example, 5, 7, 10 and 15 minutes. Separately, it is worth noting the data of our ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ñ | again, if the resources allow, you can check how well you can solve the problem for 3000 users | . Next week we will remember about linear models trained by stochastic gradient descent, and we will rejoice at how much faster they work. We will also make the first (or not the first) in the parcels [competition] (https://in class.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2 ) Kaggle class. .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/english/2019/08/09/project-identification-of-internet-users-week4.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/english/2019/08/09/project-identification-of-internet-users-week4.html",
            "date": " â€¢ Aug 9, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Graduation project "Identification of Internet users" - Visual data analysis and feature construction. Peer-Review",
            "content": "In week 3, I will do visual data analysis and feature construction. First, I will build and analyze several signs related to the time of visiting the site, then you can come up with and describe various signs. . Task plan: . Building features | Visual data analysis | Verification of constructed features | Further construction of features | . import os import time import pickle import re import pandas as pd import numpy as np import scipy.sparse as sps import seaborn as sns from glob import glob from scipy.sparse import csr_matrix from datetime import timedelta from matplotlib import pyplot as plt . import warnings warnings.filterwarnings(&quot;ignore&quot;) . Building features . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/capstone_user_identification&#39; . I will create a new one based on the functions prepare_train_set and prepare_sparse_train_set_window â€“ prepare_train_set_with_fe, (from &quot;feature engineering&quot;), I will create the following signs: . session_timespan - session duration (the difference between the maximum and minimum time of visiting sites in a session, in seconds) | unique_sites â€“ the number of unique sites in the session | start_hour - the hour of the start of the session (that is, the hour in the record of the minimum timestamp among ten) | day_of_week â€“ the day of the week (that is, the day of the week in the record of the minimum timestamp among ten) | . The function should return a new Data Frame (as the function prepare_train_set returned), only there should be 4 more signs. The order in which the signs are added: site1, ... site 10, session_timespan, unique_suites, start_hour, day_of_week and user_id (this can be seen below by the way the function is called). . def prepare_train_set_with_fe(path_to_csv_files, site_freq_path, feature_names, session_length=10, window_size=10): stock_files = sorted(glob(path_to_csv_files)) #create a shared dataframe with all users and sites df = pd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) #read the file with the site name, identification number and frequency with open(site_freq_path, &quot;rb&quot;) as fp: df_site_dict = pickle.load(fp) #create number list site data = [] list_all_site = [] #user = 0 for filename in stock_files: user = int(filename[-8:-4]) tmp_df = pd.read_csv(filename) tmp_df[&#39;timestamp&#39;] = pd.to_datetime(tmp_df[&#39;timestamp&#39;]) start_hour_site = list(tmp_df[&#39;timestamp&#39;].dt.hour) day_of_week_site = list(tmp_df[&#39;timestamp&#39;].dt.weekday) list_site = [] #session timespan in second for each user list_time = list((tmp_df.iloc[:, 0].shift(-1)-tmp_df.iloc[:, 0])/np.timedelta64(1,&#39;s&#39;))[:-1] #round list list_time = [round(x) for x in list_time] for site in tmp_df.site: list_site.append(df_site_dict.get(site)[0]) count = 0 #iterating over the beginning of the window depending on its width for start in range(0, (len(list_site) + window_size), window_size): ind_1 = start ind_2 = start + session_length #parameter for the condition sess_uniq = [] #number of unique sites if ind_2 &lt;= (len(list_site)-1): sess = list_site[ind_1 : ind_2] sess_time = list_time[ind_1 : ind_2-1] sess_uniq = list(filter(lambda num: num != 0, sess)) data.append(sess + sess_time + [sum(sess_time)] + [len(set(sess_uniq))] + [start_hour_site[start]] + [day_of_week_site[start]] + [user]) elif(len(list_site[ind_1:]) !=0): sess = list_site[ind_1:] + [0 for _ in range(session_length - len(list_site[ind_1:]))] sess_uniq = list(filter(lambda num: num != 0, sess)) sess_time = list_time[ind_1:] + [0 for _ in range(session_length - len(list_time[ind_1:])-1)] data.append(sess + sess_time + [sum(sess_time)] + [len(set(sess_uniq))] + [start_hour_site[start]] + [day_of_week_site[start]] + [user]) #user = user + 1 return pd.DataFrame(data, columns=feature_names) . Let&#39;s check the function on a toy example. . t_start = time.time() feature_names = [&#39;site&#39; + str(i) for i in range(1,11)] + [&#39;time_diff&#39; + str(j) for j in range(1,10)] + [&#39;session_timespan&#39;, &#39;#unique_sites&#39;, &#39;start_hour&#39;, &#39;day_of_week&#39;, &#39;target&#39;] train_data_toy = prepare_train_set_with_fe(os.path.join(PATH_TO_DATA, &#39;3users/*.csv&#39;), site_freq_path=os.path.join(PATH_TO_DATA, &#39;site_freq_3users.pkl&#39;), feature_names=feature_names, session_length=10) print(&quot;Time elapsed&quot;, time.time() - t_start) . Time elapsed 2.094503402709961 . train_data_toy . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 time_diff1 time_diff2 time_diff3 time_diff4 time_diff5 time_diff6 time_diff7 time_diff8 time_diff9 session_timespan #unique_sites start_hour day_of_week target . 0 3 | 2 | 2 | 7 | 2 | 1 | 8 | 5 | 9 | 10 | 287 | 1184 | 6278 | 186 | 2 | 1 | 2 | 3 | 55 | 7998 | 8 | 9 | 4 | 1 | . 1 3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 3 | 55 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 2 | 12 | 4 | 1 | . 2 3 | 2 | 6 | 6 | 2 | 0 | 0 | 0 | 0 | 0 | 287 | 1184 | 6278 | 186 | 0 | 0 | 0 | 0 | 0 | 7935 | 3 | 9 | 4 | 2 | . 3 4 | 1 | 2 | 1 | 2 | 1 | 1 | 5 | 11 | 4 | 287 | 1184 | 6278 | 186 | 2 | 1 | 2 | 3 | 55 | 7998 | 5 | 9 | 4 | 3 | . 4 4 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 287 | 1184 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1471 | 3 | 12 | 4 | 3 | . I will apply the function prepare_train_set_with_fe to data for 10 users, I will specify session_length=10. . start = time.time() train_data_10users = prepare_train_set_with_fe(os.path.join(PATH_TO_DATA, &#39;10users/*.csv&#39;), site_freq_path=os.path.join(PATH_TO_DATA, &#39;site_freq_10users.pkl&#39;), feature_names=feature_names, session_length=10) end = time.time() print(&quot;Time elapsed&quot;, timedelta(seconds=end-start)) . Time elapsed 0:00:10.294475 . train_data_10users.head() . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 time_diff1 time_diff2 time_diff3 time_diff4 time_diff5 time_diff6 time_diff7 time_diff8 time_diff9 session_timespan #unique_sites start_hour day_of_week target . 0 192 | 574 | 133 | 3 | 133 | 133 | 3 | 133 | 203 | 133 | 10 | 0 | 0 | 1 | 20 | 1 | 0 | 1 | 0 | 33 | 5 | 8 | 4 | 31 | . 1 415 | 193 | 674 | 254 | 133 | 31 | 393 | 3305 | 217 | 55 | 1 | 0 | 163 | 105 | 0 | 1 | 3 | 3 | 8 | 284 | 10 | 8 | 4 | 31 | . 2 55 | 3 | 55 | 55 | 5 | 293 | 415 | 333 | 897 | 55 | 0 | 14 | 1 | 242 | 0 | 0 | 1 | 0 | 0 | 258 | 7 | 8 | 4 | 31 | . 3 473 | 3306 | 473 | 55 | 55 | 55 | 55 | 937 | 199 | 123 | 2 | 1 | 0 | 1 | 25 | 1 | 0 | 0 | 0 | 30 | 6 | 8 | 4 | 31 | . 4 342 | 55 | 5 | 3307 | 258 | 211 | 3308 | 2086 | 675 | 2086 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 6 | 9 | 8 | 4 | 31 | . I will apply the function prepare_train_set_with_fe to the data for 150 users, I will specify session_length=10. . start = time.time() train_data_150users = prepare_train_set_with_fe(os.path.join(PATH_TO_DATA, &#39;150users/*.csv&#39;), site_freq_path=os.path.join(PATH_TO_DATA, &#39;site_freq_150users.pkl&#39;), feature_names=feature_names, session_length=10) end = time.time() print(&quot;Time elapsed&quot;, timedelta(seconds=end-start)) . Time elapsed 0:01:42.615568 . I will save the signs session_timespan, #unique_sites, start_hour and day_of_week to pickle files for 10 and 150 users. . new_feature_names = [&#39;time_diff&#39; + str(j) for j in range(1,10)] + [&#39;session_timespan&#39;, &#39;#unique_sites&#39;, &#39;start_hour&#39;,&#39;day_of_week&#39;] . new_features_10users = train_data_10users[new_feature_names] new_features_150users = train_data_150users[new_feature_names] . with open(os.path.join(PATH_TO_DATA, &#39;new_features_10users.pkl&#39;), &#39;wb&#39;) as new_features_10users_pkl: pickle.dump(new_features_10users, new_features_10users_pkl) with open(os.path.join(PATH_TO_DATA, &#39;new_features_150users.pkl&#39;), &#39;wb&#39;) as new_features_150users_pkl: pickle.dump(new_features_150users, new_features_150users_pkl) . Visual data analysis . For fun, for fun, we will give users names and associate colors with them. . id_name_dict = {128: &#39;Mary-Kate&#39;, 39: &#39;Ashley&#39;, 207: &#39;Lindsey&#39;, 127: &#39;Naomi&#39;, 237: &#39;Avril&#39;, 33: &#39;Bob&#39;, 50: &#39;Bill&#39;, 31: &#39;John&#39;, 100: &#39;Dick&#39;, 241: &#39;Ed&#39;} train_data_10users[&#39;target&#39;] = train_data_10users[&#39;target&#39;].map(id_name_dict) . color_dic = {&#39;Mary-Kate&#39;: &#39;pink&#39;, &#39;Ashley&#39;: &#39;darkviolet&#39;, &#39;Lindsey&#39;:&#39;blueviolet&#39;, &#39;Naomi&#39;: &#39;hotpink&#39;, &#39;Avril&#39;: &#39;orchid&#39;, &#39;Bob&#39;: &#39;firebrick&#39;, &#39;Bill&#39;: &#39;gold&#39;, &#39;John&#39;: &#39;forestgreen&#39;, &#39;Dick&#39;: &#39;slategrey&#39;, &#39;Ed&#39;:&#39;brown&#39;} . 1. I will build a histogram of the distribution of the session length in seconds (session_timespan). I will limit x to the value 200 (otherwise the tail is too heavy). I will make a histogram of the color dark violet. . x = train_data_10users[train_data_10users.session_timespan &lt; 200].session_timespan . fig, ax = plt.subplots(figsize=(15, 5)) sns.distplot(x, bins=100, kde=False, color=&#39;darkviolet&#39;, hist_kws=dict(edgecolor=&quot;k&quot;, linewidth=2)) plt.title(&quot;Session length distribution&quot;) plt.xlabel(&quot;Session length in seconds&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() . 2. I will build a histogram of the distribution of the number of unique sites in the session (#unique_sites). I will make a histogram of the aqua color. . ax =sns.catplot(x=&quot;#unique_sites&quot;, kind=&quot;count&quot;, color=&#39;aqua&#39;, data=train_data_10users, height=5, aspect=2.5) plt.title(&quot;Distribution of the number of unique sites&quot;, fontsize=12) plt.xlabel(&quot;Number&quot;, fontsize=10) plt.ylabel(&quot;Frequency&quot;, fontsize=10) plt.show() . 3. I will build histograms of the distribution of the number of unique sites in the session (#unique_sites) for each of the 10 users individually. I use subplots to place all 10 images on one big one. I will mark each picture with a legend, the user name will be written on the legend. For each user, I will color the histogram with his/her color (color_dic). . list_user = list(color_dic.keys()) fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20)) set_user = set(train_data_10users.target) user = 1 for idx, ax in enumerate(axes): for i, a in enumerate(ax): sns.countplot(train_data_10users[train_data_10users.target == list_user[user-1]][&quot;#unique_sites&quot;],color=color_dic[list_user[user-1]], ax=axes[idx, i]) axes[idx, i].set_xlabel(&#39;Number of unique sites&#39;) axes[idx, i].set_ylabel(&#39;Frequency&#39;) axes[idx, i].legend([list_user[user-1]]) user = user + 1 . 4. I will build a histogram of the distribution of the session start hour (start_hour). I will make a histogram of the darkgreen color. . ax =sns.catplot(x=&quot;start_hour&quot;, kind=&quot;count&quot;, color=&#39;darkgreen&#39;, data=train_data_10users, order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], height=5, aspect=2.5) plt.title(&quot;Allocation of the session start hour&quot;, fontsize=12) plt.xlabel(&quot;Hour&quot;, fontsize=10) plt.ylabel(&quot;Frequency&quot;, fontsize=10) plt.show() . 5. I will build histograms of the distribution of the session start hour (start_hour) for each of the 10 users individually. Use subplots to place all 10 images on one big one. Mark each picture with a legend, the user name should be written on the legend. For each user, color the histogram with his/her color (color_dic). Sign the axes in Russian in each of the 10 histogramsÑŽ . fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20)) set_user = set(train_data_10users.target) user = 1 for idx, ax in enumerate(axes): for i, a in enumerate(ax): sns.countplot(train_data_10users[train_data_10users.target == list_user[user-1]][&quot;start_hour&quot;], color=color_dic[list_user[user-1]], order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], ax=axes[idx, i]) axes[idx, i].set_xlabel(&#39;Session start hour&#39;) axes[idx, i].set_ylabel(&#39;Frequency&#39;) axes[idx, i].legend([list_user[user-1]]) user = user + 1 . 6. I will build a histogram of the distribution of the day of the week on which the session started (day_of_week). I will make a histogram of the sienna color. . ax =sns.catplot(x=&quot;day_of_week&quot;, kind=&quot;count&quot;, color=&#39;sienna&#39;, data=train_data_10users, height=5, aspect=2.5) plt.title(&quot;Distribution of the week in which the session started&quot;, fontsize=12) plt.xlabel(&quot;Day of the week&quot;, fontsize=10) plt.ylabel(&quot;Frequency&quot;, fontsize=10) plt.show() . 7. I will build histograms of the distribution of the day of the week on which the session started (day_of_week) for each of the 10 users individually. I use subplots to place all 10 images on one big one. I will change the labels on the X axis to [&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;] - the set_xticklabels method. I will mark each picture with a legend, the user name will be written on the legend. For each user, I will color the histogram with his/her color (color_dic). . fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20)) user = 1 for idx, ax in enumerate(axes): for i, a in enumerate(ax): sns.countplot(train_data_10users[train_data_10users.target == list_user[user-1]][&quot;day_of_week&quot;], color=color_dic[list_user[user-1]], order = [0, 1, 2, 3, 4, 5, 6], ax=axes[idx, i]) axes[idx, i].set_xlabel(&#39;Day of the week of the session start&#39;) axes[idx, i].set_ylabel(&#39;Frequency&#39;) axes[idx, i].legend([list_user[user-1]]) axes[idx, i].set_xticklabels([&#39;Mon&#39;, &#39;Tue&#39;, &#39;Wed&#39;, &#39;Thu&#39;, &#39;Fri&#39;, &#39;Sat&#39;, &#39;Sun&#39;]) user = user+1 . 8. Conclusions about each user according to the constructed graphs. . &#39;Mary-Kate&#39; - the frequency of the number of unique sites coincides with the general trend, the start time is an hour later than the usual one, and the start day of the session usually fell at the end of the working week with a maximum on Friday, although it is more common to start on Wednesdays | &#39;Ashley&#39; - the frequency of the number of unique sites is slightly lower from the general trend, the start time is in the morning with a maximum at 10 am, and the start day of the session usually fell at the end of the working week with a maximum on Friday, as with the last user, although it is more common to start on Wednesdays | &#39;Lindsey&#39; - the frequency of the number of unique sites coincides with the general trend. It was also typical for one more unique site. The start time usually came from 8 to 10 o&#39;clock. The day of the beginning of the session is a pronounced Wednesday compared to other days. | &#39;Naomi&#39; - the frequency of the number of unique sites is slightly shifted towards an increase compared to the general trend. The start time is usually more often the same was an hour later. The day of the beginning of the session is pronounced Monday with a drop to Wednesday. The rest of the days are at the minimum level. | &#39;Avril&#39; - the frequency of the number of unique sites is characteristic of 2. In the general trend, this amount also stands out slightly. slightly shifted in the direction of increase compared to the general trend. It was also typical for one more. The distribution of the start time usually coincides with the general picture, but in contrast has a more pronounced tail in the evening. The start day is Wednesday or Saturday. | &#39;Bob&#39; is characterized by a slightly reduced number of unique sites. The frequency of the start time coincides with the maximum total start time of 14 hours. The day of the week is more typical Thursday. The remaining days at the beginning are approximately equal. | &#39;Bill&#39; is most characteristic of 2 unique sites, and then there is a downgrade. The start time is most distinguished from 9 to 10 in the morning and from 19 to 21 in the evening. The most typical start days are weekends. | &#39;John&#39; - the frequency of a large number of unique sites coincides with the general trend and is a maximum of 7. The most pronounced hours are 12 and, as with the general trend, 14. The start days of the week are Tuesday and Wednesday with a maximum on Wednesday. | &#39;Dick&#39; - the number of unique sites is one less than in the general trend. The hour of the start is maximum at 13 and 14, with a maximum at 14 as with the total. The days of the week are Tuesday, Wednesday, Thursday and Sunday. The rest are two times smaller and almost equivalent. | &#39;Ed&#39; - the histogram of the number of unique sites coincides with the general histogram. The start time is shifted up by 16. The maximum is on Wednesday, both at general, and on Saturday and Sunday.The rest of the days are not pronounced. | I will upload the frequency dictionary of sites for 10 users saved earlier in the pickle file. . with open(os.path.join(PATH_TO_DATA, &#39;site_freq_10users.pkl&#39;), &#39;rb&#39;) as fid: site_freq_10 = pickle.load(fid) . I will determine the top 10 most visited sites (top 10_sites) and the corresponding number of visits (top 10_frags). . ten_best = list(site_freq_10.keys())[:10] for site in list(ten_best): print(site) . s.youtube.com www.google.fr www.google.com mail.google.com www.facebook.com apis.google.com r3sn-gxo5uxg-jqbe.googlevideo.com r1sn-gxo5uxg-jqbe.googlevideo.com plus.google.com accounts.google.com . feature_names_two = [&#39;site&#39; + str(i) for i in range(1,11)] train_data_10users_array = train_data_10users[feature_names_two].values unique, counts = np.unique(train_data_10users_array, return_counts=True) ten_max_site = {} for u, c in dict(zip(unique, counts)).items(): if u == 0: continue elif u &gt; 10: break else: ten_max_site[ten_best[u-1]] = c . top10_freqs = list(ten_max_site.values()) top10_sites = list(ten_max_site.keys()) . df_top10sites = pd.DataFrame({&#39;Top 10 sites&#39; : top10_sites, &#39;Top freqs&#39; : top10_freqs}) . df_top10sites . Top 10 sites Top freqs . 0 s.youtube.com | 8300 | . 1 www.google.fr | 7813 | . 2 www.google.com | 5441 | . 3 mail.google.com | 4158 | . 4 www.facebook.com | 4141 | . 5 apis.google.com | 3758 | . 6 r3sn-gxo5uxg-jqbe.googlevideo.com | 3244 | . 7 r1sn-gxo5uxg-jqbe.googlevideo.com | 3094 | . 8 plus.google.com | 2630 | . 9 accounts.google.com | 2089 | . 9. I will draw a seaborn barplot showing the frequency of visits to the top 10 sites. I will make the site signatures vertical, otherwise they merge (xticks). . fig, ax = plt.subplots(figsize=(15, 5)) ax = sns.barplot(x=top10_sites, y=top10_freqs) plt.xticks(rotation = 90) plt.show() . Checking the constructed features . This part is rather technical, its meaning is to make sure that we have all created the session_timespan, #unique_sites, start_hour and day_of_week attributes equally. . 10. I will output the median session duration (session_timespan) for sessions of 10 users. . np.median(train_data_10users[&#39;session_timespan&#39;]) . 37.0 . 11. I will output the median day of the week on which the session started for sessions of 10 users. . np.median(train_data_10users[&#39;day_of_week&#39;]) . 2.0 . 12. I will output the median session start hour for sessions of 150 users. . np.median(train_data_150users[&#39;start_hour&#39;]) . 13.0 . 13. I will output the median value of the number of unique sites in the sessions of 150 users. . np.median(train_data_150users[&#39;#unique_sites&#39;]) . 7.0 . Further construction of features . This is a creative task, here you need to figure out how else to take into account the time of visiting web pages and other signs. . Next week we will use a &quot;bag&quot; of sites to classify sessions by belonging to different users, and we will add these new features that we will create now and see if the model improves. Therefore, you can create them as separate matrices and save them separately as well. . In this part of the task, you can build and visually explore a variety of signs (nothing limits your imagination): . year, month and day of the session start | the hour of the session start (taking into account the year, month and day) time of day | average time spent on the site, you can calculate, for example, for the top 30 popular sites | indicators of visits to popular sites (say, also for the top 30 popular sites) | frequency of visits to Facebook | ... | . I will write a function to create new features and apply it to the source data - directories with 10 and 150 files. I will do this only for the dataset obtained with the parameters session_length=10 and window_size=10. I serialize the resulting matrices using pickle. The function can return both new signs only, and old ones with new ones. At the same time, the signature of the function may be different â€“ there is already freedom of my choice. . def feature_engineering(train_data): df_new = train_data.copy() #let&#39;s introduce a binary indicator 0 - weekdays, 1 - weekends df_new[&#39;weekday&#39;] = df_new[&#39;day_of_week&#39;].apply(lambda x: 1 if x &gt; 4 else 0) #were there any top 10 sites in this session site_col_list = [&#39;site&#39; + str(i) for i in range(1,11)] df_new[&#39;popular&#39;] = df_new[df_new[site_col_list]&lt;11].sum(axis=1).apply(lambda x: 1 if x &gt; 0 else 0) #number of popular sites df_new[&#39;count_popular&#39;] = 10 - df_new[site_col_list][df_new[site_col_list]&lt;11].isna().sum(axis=1) #time spent on popular websites time_diff_num = [&#39;time_diff&#39; + str(i) for i in range(1,10)] row = 0 rows = [] cols = [] data = [] tmp_arr = np.nan_to_num(np.array(df_new[site_col_list][df_new[site_col_list]&lt;11])) for arr in tmp_arr: for key, value in enumerate(arr): if value != 0: rows.append(row) cols.append(key) data.append(value) row = row + 1 row_count = -1 sum_time = 0 sum_time_sess = [0]*df_new.shape[0] for r, c in zip(rows, cols): if(c==9): continue elif r==row_count: sum_time_sess[row_count] = sum_time_sess[row_count] + df_new.loc[r][c+10] else: sum_time_sess[r] = df_new.loc[r][c+10] row_count = r df_new[&#39;time_popular&#39;] = sum_time_sess return(df_new) . new_futures = feature_engineering(train_data_10users) . 14. I will build pictures for new signs, examine them, comment on the results. . ax =sns.catplot(x=&quot;start_hour&quot;, kind=&quot;count&quot;, hue=&quot;weekday&quot;, data=new_futures, order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], height=5, aspect=2.5) # title new_title = &#39;Day of the week&#39; ax._legend.set_title(new_title) # replace labels new_labels = [&#39;Weekdays&#39;,&#39;Weekends&#39;] for t, l in zip(ax._legend.texts, new_labels): t.set_text(l) #sns.plt.show() . fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20)) user = 1 for idx, ax in enumerate(axes): for i, a in enumerate(ax): sns.countplot(new_futures[(new_futures.target == list_user[user-1])][&quot;start_hour&quot;], hue = &#39;weekday&#39;, data = new_futures, order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], ax=axes[idx, i]) axes[idx, i].set_xlabel(&#39;Ð§Ð°Ñ Ð½Ð°Ñ‡Ð°Ð»Ð° ÑÐµÑÑÐ¸Ð¸ &#39;) axes[idx, i].set_ylabel(&#39;Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð°&#39;) axes[idx, i].legend([&#39;Ð‘ÑƒÐ´Ð½Ð¸&#39;, &#39;Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ&#39;], title = list_user[user-1]) user = user + 1 . Consideration of the time distribution depending on whether it is weekends or weekdays brought additional characteristics for users. Three users either do not go online at all, or use the computer very little on weekends, surfing the Internet on weekends is significantly reduced for four, the remaining three have almost no changes, only two are characterized by a shift in activity. . fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20)) user = 1 for idx, ax in enumerate(axes): for i, a in enumerate(ax): sns.countplot(new_futures[(new_futures.target == list_user[user-1])][&quot;start_hour&quot;], hue = &#39;popular&#39;, data = new_futures, order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], ax=axes[idx, i]) axes[idx, i].set_xlabel(&#39;Ð§Ð°Ñ Ð½Ð°Ñ‡Ð°Ð»Ð° ÑÐµÑÑÐ¸Ð¸ &#39;) axes[idx, i].set_ylabel(&#39;Ð§Ð°ÑÑ‚Ð¾Ñ‚Ð°&#39;) axes[idx, i].legend([&#39;ÐŸÐ¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ&#39;, &#39;ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ&#39;], title = list_user[user-1]) user = user + 1 . For four users, the frequency of visits to popular sites coincides with the rest, although some have a shift in the estimate for the morning hours. Three users have an increase at lunchtime from 13 to 15 hours. . new_futures.head() . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 time_diff1 time_diff2 time_diff3 time_diff4 time_diff5 time_diff6 time_diff7 time_diff8 time_diff9 session_timespan #unique_sites start_hour day_of_week target weekday popular count_popular time_popular . 0 192 | 574 | 133 | 3 | 133 | 133 | 3 | 133 | 203 | 133 | 10 | 0 | 0 | 1 | 20 | 1 | 0 | 1 | 0 | 33 | 5 | 8 | 4 | John | 0 | 1 | 2 | 1 | . 1 415 | 193 | 674 | 254 | 133 | 31 | 393 | 3305 | 217 | 55 | 1 | 0 | 163 | 105 | 0 | 1 | 3 | 3 | 8 | 284 | 10 | 8 | 4 | John | 0 | 0 | 0 | 0 | . 2 55 | 3 | 55 | 55 | 5 | 293 | 415 | 333 | 897 | 55 | 0 | 14 | 1 | 242 | 0 | 0 | 1 | 0 | 0 | 258 | 7 | 8 | 4 | John | 0 | 1 | 2 | 14 | . 3 473 | 3306 | 473 | 55 | 55 | 55 | 55 | 937 | 199 | 123 | 2 | 1 | 0 | 1 | 25 | 1 | 0 | 0 | 0 | 30 | 6 | 8 | 4 | John | 0 | 0 | 0 | 0 | . 4 342 | 55 | 5 | 3307 | 258 | 211 | 3308 | 2086 | 675 | 2086 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 0 | 6 | 9 | 8 | 4 | John | 0 | 1 | 1 | 1 | . x = new_futures[new_futures.time_popular &lt; 200].time_popular fig, ax = plt.subplots(figsize=(15, 5)) sns.distplot(x, bins=200, kde=False, color=&#39;darkviolet&#39;, hist_kws=dict(edgecolor=&quot;k&quot;, linewidth=2)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f607f9b4950&gt; . fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 20)) user = 1 for idx, ax in enumerate(axes): for i, a in enumerate(ax): x = new_futures[(new_futures.time_popular &lt; 200) &amp; (new_futures.target == list_user[user-1])].time_popular sns.distplot(x, bins=20, kde=False, color=&#39;darkviolet&#39;, hist_kws=dict(edgecolor=&quot;k&quot;, linewidth=2), ax=axes[idx, i]) axes[idx, i].set_xlabel(&#39;Time on popular sites in seconds&#39;) axes[idx, i].set_ylabel(&#39;Frequency&#39;) axes[idx, i].set_title(list_user[user-1]) plt.tight_layout() user = user + 1 . Thanks to the analysis, it is clear that people usually visit the top 10 sites for a short time. You can also count separately for a popular social network. But even now, three fans stand out among the users. . In the end, I will save only those signs in the pickle files that I assume will help identify the user more accurately. This applies to the signs that were created at the beginning (session_timespan, #unique_sites, start_hour, day_of_week), and my own. You can create all these attributes not only for sessions from 10 sites, but also for other combinations of session_length and window_size parameters. . selected_features_10users = feature_engineering(train_data_10users) selected_features_150users = feature_engineering(train_data_150users) . with open(os.path.join(PATH_TO_DATA, &#39;selected_features_10users.pkl&#39;), &#39;wb&#39;) as selected_features_10users_pkl: pickle.dump(selected_features_10users, selected_features_10users_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;selected_features_150users.pkl&#39;), &#39;wb&#39;) as selected_features_150users_pkl: pickle.dump(selected_features_150users, selected_features_150users_pkl, protocol=2) .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/english/2019/08/03/project-identification-of-internet-users-week3.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/english/2019/08/03/project-identification-of-internet-users-week3.html",
            "date": " â€¢ Aug 3, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Graduation project "Identification of Internet users" - Preparation and initial analysis of data. Programming Assignment",
            "content": "Week 2. Preparation and initial data analysis . In the second week, we will continue to prepare data for further analysis and construction of forecast models. Specifically, earlier we determined that a session is a sequence of 10 sites visited by a user, now we will make the session length a parameter, and then when training predictive models we will choose the best session length. We will also get acquainted with the preprocessed data and statistically test the first hypotheses related to our observations. . Plan 2 weeks: . Preparation of several training samples for comparison | Primary data analysis, hypothesis testing | . Preparation of several training samples for comparison . Let&#39;s make the number of sites in the session a parameter in order to further compare classification models trained on different samples â€“ with 5, 7, 10 and 15 sites in the session. Moreover, so far we have taken 10 sites in a row, without crossing. Now let&#39;s apply the idea of a sliding window - sessions will overlap. . Example: for a session length of 10 and a window width of 7, a file of 30 records will generate not 3 sessions, as before (1-10, 11-20, 21-30), but 5 (1-10, 8-17, 15-24, 22-30, 29-30). At the same time, there will be one zero in the penultimate session, and 8 zeros in the last one. . Let&#39;s create several samples for different combinations of session length and window width parameters. All of them are presented in the table below: . . session_length -&gt;window_size 5 7 10 15 . 5 | v | v | v | v | . 7 | | v | v | v | . 10 | | | v | v | . In total, there should be 18 sparse matrices - the 9 combinations of session formation parameters indicated in the table for samples of 10 and 150 users. At the same time, we have already made 2 selections in the last part, they correspond to a combination of parameters: session_length=10, window_size=10, which are marked in the table above with a green check mark (done). . Implementing the function prepar_sparse_train_set_window. . Arguments: . path_to_csv_files â€“ path to the directory with csv files | site_freq_path - path to the pickle file with the frequency dictionary obtained in part 1 of the project | session_length â€“ session length (parameter) | window_size â€“ window width (parameter) | . The function should return 2 objects: . a sparse matrix X_sparse (two-dimensional Scipy.sparse.csr_matrix), in which rows correspond to sessions from session_length sites, and max(site_id) columns correspond to the number of site_id visits in the session. | vector y (Numpy array) of &quot;responses&quot; in the form of user IDs that belong to sessions from X_sparse | . Details: . Modify the function created in part 1 of the prepare_train_set | Some sessions may be repeated â€“ leave it as it is, do not delete duplicates | We measure the execution time of the loop iterations using time from time, tqdm from tqdm or using the log_progress widget (an article about it on Habrahabr) | 150 files from capstone_websites_data/150users/ should be processed in a few seconds (depending on the input parameters). If it takes longerâ€“ it&#39;s not scary, but the function can be accelerated. | . import os import time import pickle import math import pylab import collections import pandas as pd import numpy as np import scipy.sparse as sps import scipy.stats as stats import matplotlib.pyplot as plt from glob import glob from tqdm.auto import tqdm from scipy.sparse import csr_matrix from datetime import timedelta from scipy import stats from statsmodels.stats.proportion import proportion_confint from collections import Counter import warnings warnings.filterwarnings(&#39;ignore&#39;) . /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/capstone_user_identification&#39; . def prepare_sparse_train_set_window(path_to_csv_files, site_freq_path, session_length=10, window_size=10): stock_files = sorted(glob(path_to_csv_files)) ##create a shared dataframe with all users and sites df = pd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) ##let&#39;s read the file with the site name, identification number and frequency with open(site_freq_path, &quot;rb&quot;) as fp: df_site_dict = pickle.load(fp) #create number list site list_all_site = [] user_list = [] for filename in stock_files: tmp_df = pd.read_csv(filename) user = filename[-12:-4] list_site = [] #we will read the session separately for each user and convert the sites into their identifiers for site in tmp_df.site: list_site.append(df_site_dict.get(site)[0]) count = 0 #iterating over the beginning of the window depending on its width for start in range(0, (len(list_site) + window_size), window_size): ind_1 = start ind_2 = start + session_length #parameter for the condition if ind_2 &lt;= (len(list_site)-1): sess = list_site[ind_1 : ind_2] list_all_site.append(sess) user_list.append(user) elif(len(list_site[ind_1:]) !=0): sess = list_site[ind_1:] + [0 for _ in range(session_length - len(list_site[ind_1:]))] list_all_site.append(sess) user_list.append(user) #now the discharged matrix X_toy = pd.DataFrame(list_all_site).values X_sparse_toy = csr_matrix((np.ones(X_toy.size, dtype=int), X_toy.reshape(-1), np.arange(X_toy.shape[0] + 1) * X_toy.shape[1]))[:, 1:] return(X_sparse_toy, np.array(user_list)) . Let&#39;s apply the resulting function with the parameters session_length=5 and window_size=3 to the toy example to make sure that everything works as it should. . start = time.time() X_toy_s5_w3, y_s5_w3 = prepare_sparse_train_set_window(os.path.join(PATH_TO_DATA, &#39;3users/*.csv&#39;), os.path.join(PATH_TO_DATA, &#39;site_freq_3users.pkl&#39;), session_length=5, window_size=3) end = time.time() print(timedelta(seconds=end-start)) . 0:00:00.022730 . X_toy_s5_w3.todense() . matrix([[0, 3, 1, 0, 0, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0], [0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0], [3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0], [3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1], [1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) . y_s5_w3 . array([&#39;user0001&#39;, &#39;user0001&#39;, &#39;user0001&#39;, &#39;user0001&#39;, &#39;user0001&#39;, &#39;user0002&#39;, &#39;user0002&#39;, &#39;user0003&#39;, &#39;user0003&#39;, &#39;user0003&#39;, &#39;user0003&#39;, &#39;user0003&#39;], dtype=&#39;&lt;U8&#39;) . Everything coincided with the presented example . . Let&#39;s run the created function 16 times using cycles by the number of num_users users (10 or 150), the values of the parameter session_length (15, 10, 7 or 5) and the values of the parameter window_size (10, 7 or 5). Serialize all 16 sparse matrices (training samples) and vectors (target class labels â€“ user ID) into the files X_sparse_{num_users}users_s{session_length}_w{window_size}.pkl and y_{num_users}users_s{session_length}_w{window_size}.pkl. . To make sure that we will continue to work with identical objects, we will write to the list data_lengths the number of rows in all the sparse matrices obtained (16 values). If some will match, it&#39;s fine (you can figure out why). . import itertools start = time.time() data_lengths = [] user_tmp=[] for num_users in [10, 150]: for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]): if (window_size &lt;= session_length) and ((window_size, session_length) != (10, 10)): if num_users == 10: path = os.path.join(PATH_TO_DATA, &#39;10users/*.csv&#39;) unpickled_df = os.path.join(PATH_TO_DATA, &#39;site_freq_10users.pkl&#39;) else: path = os.path.join(PATH_TO_DATA, &#39;150users/*.csv&#39;) unpickled_df = os.path.join(PATH_TO_DATA, &#39;site_freq_150users.pkl&#39;) print(&quot;NUM USER = &quot;, num_users, &quot; Window Size = &quot;, window_size, &quot; Session Length = &quot;, session_length) end = time.time() print(timedelta(seconds=end-start)) X_sparse, y = prepare_sparse_train_set_window(path, unpickled_df, session_length, window_size) data_lengths.append(X_sparse.shape[0]) user_tmp.append(len(y)) file_name = os.path.join(PATH_TO_DATA, &#39;sparse/X_sparse_%dusers_s%d_w%d.pkl&#39; % (num_users, session_length, window_size)) with open(file_name, &#39;wb&#39;) as fp: pickle.dump(X_sparse, fp) file_name = os.path.join(PATH_TO_DATA,&#39;sparse/y_%dusers_s%d_w%d.pkl&#39; % (num_users, session_length, window_size)) with open(file_name,&#39;wb&#39;) as fp: pickle.dump(y, fp) data_lengths.append(X_sparse.shape[0]) end = time.time() print(timedelta(seconds=end-start)) . NUM USER = 10 Window Size = 10 Session Length = 15 0:00:00.003979 NUM USER = 10 Window Size = 7 Session Length = 15 0:00:00.457232 NUM USER = 10 Window Size = 7 Session Length = 10 0:00:01.009698 NUM USER = 10 Window Size = 7 Session Length = 7 0:00:01.452218 NUM USER = 10 Window Size = 5 Session Length = 15 0:00:01.917664 NUM USER = 10 Window Size = 5 Session Length = 10 0:00:02.484663 NUM USER = 10 Window Size = 5 Session Length = 7 0:00:02.953907 NUM USER = 10 Window Size = 5 Session Length = 5 0:00:03.404516 NUM USER = 150 Window Size = 10 Session Length = 15 0:00:03.863693 NUM USER = 150 Window Size = 7 Session Length = 15 0:00:08.548853 NUM USER = 150 Window Size = 7 Session Length = 10 0:00:13.495581 NUM USER = 150 Window Size = 7 Session Length = 7 0:00:18.213572 NUM USER = 150 Window Size = 5 Session Length = 15 0:00:22.648985 NUM USER = 150 Window Size = 5 Session Length = 10 0:00:28.012223 NUM USER = 150 Window Size = 5 Session Length = 7 0:00:33.058571 NUM USER = 150 Window Size = 5 Session Length = 5 0:00:37.866631 0:00:42.397943 . It is important to note that after I disabled tqdm, the processing time dropped from 1.5 hours to 51 seconds. . Write it to a file answer2_1.txt all numbers from the list data_length s separated by a space. The resulting file will be the answer to 1 question of the test. . def write_answer_to_file(answer, file_address): with open(file_address, &#39;w&#39;) as out_f: out_f.write(str(answer)) . write_answer_to_file(&#39; &#39;.join([str(elem) for elem in data_lengths]), &#39;answer2_1.txt&#39;) . It is important to note that a space-separated response was not accepted on Stepik. And the gaps had to be removed. . Primary data analysis, hypothesis testing . Let&#39;s read the train_data_10 users.csv file prepared for 1 week in the DataFrame. Next, we will work with him. . train_df = pd.read_csv(os.path.join(PATH_TO_DATA, &#39;train_data_10users.csv&#39;), index_col=&#39;session_id&#39;) . train_df.head() . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 user_id . session_id . 0 192 | 574 | 133 | 3 | 133 | 133 | 3 | 133 | 203 | 133 | user0031 | . 1 415 | 193 | 674 | 254 | 133 | 31 | 393 | 3305 | 217 | 55 | user0031 | . 2 55 | 3 | 55 | 55 | 5 | 293 | 415 | 333 | 897 | 55 | user0031 | . 3 473 | 3306 | 473 | 55 | 55 | 55 | 55 | 937 | 199 | 123 | user0031 | . 4 342 | 55 | 5 | 3307 | 258 | 211 | 3308 | 2086 | 675 | 2086 | user0031 | . train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14061 entries, 0 to 14060 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 site1 14061 non-null int64 1 site2 14061 non-null int64 2 site3 14061 non-null int64 3 site4 14061 non-null int64 4 site5 14061 non-null int64 5 site6 14061 non-null int64 6 site7 14061 non-null int64 7 site8 14061 non-null int64 8 site9 14061 non-null int64 9 site10 14061 non-null int64 10 user_id 14061 non-null object dtypes: int64(10), object(1) memory usage: 1.3+ MB . Distribution of the target class: . train_df[&#39;user_id&#39;].value_counts() . user0128 2796 user0039 2204 user0207 1868 user0127 1712 user0237 1643 user0033 1022 user0050 802 user0031 760 user0100 720 user0241 534 Name: user_id, dtype: int64 . Let&#39;s calculate the distribution of the number of unique sites in each session out of 10 sites visited in a row. . num_unique_sites = [np.unique(train_df.values[i, :-1]).shape[0] for i in range(train_df.shape[0])] . pd.Series(num_unique_sites).value_counts() . 7 2308 6 2197 8 2046 5 1735 9 1394 2 1246 4 1163 3 894 10 651 1 427 dtype: int64 . pd.Series(num_unique_sites).hist(); . Let&#39;s check with the help of the QQ-raft and the Shapiro-Wilk criterion that this value is distributed normally. The answer to the second question in the test will be a file with the word &quot;YES&quot; or &quot;NO&quot;, depending on whether the number of unique sites in the session is normally distributed. . stats.probplot(num_unique_sites, dist=&quot;norm&quot;, plot=pylab) pylab.show() . stat, p = stats.shapiro(num_unique_sites) print(&#39;Statistics=%.3f, p=%.3f&#39; % (stat, p)) # interpret alpha = 0.05 if p &gt; alpha: print(&#39;Sample looks Gaussian (fail to reject H0)&#39;) else: print(&#39;Sample does not look Gaussian (reject H0)&#39;) . Statistics=0.955, p=0.000 Sample does not look Gaussian (reject H0) . Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼ . Let&#39;s test the hypothesis that a user will visit a site at least once that he has previously visited in a session of 10 sites. Let&#39;s check using the binomial criterion for the share that the proportion of cases when the user has re-visited a site (that is, the number of unique sites in a session &lt; 10) is large: more than 95% (note that the alternative to the fact that the share is 95% is one-sided). The answer to the 3rd question in the test will be the resulting p-value. . has_two_similar = (np.array(num_unique_sites) &lt; 10).astype(&#39;int&#39;) . len(num_unique_sites) . 14061 . stats.binom_test(has_two_similar.sum(), has_two_similar.shape[0], 0.95, alternative=&#39;greater&#39;) . 0.02207653769072678 . p-value value 0.022 . Let&#39;s construct a Wilson confidence interval for this fraction of 95%. Round the border of the interval to 3 decimal places. . wilson_interval = proportion_confint(has_two_similar.sum(), has_two_similar.shape[0], method=&#39;wilson&#39;) wilson_interval . (0.9501028841411286, 0.9570527377232229) . Intervals of 0.950 and 0.957 . What is the 95% confidence interval for the average frequency of site appearance in the sample? It is necessary to build a 95% confidence interval for the average frequency of site appearance in the sample (in all, not only for those sites that have been visited at least 1000 times) based on bootstrap. We use as many bootstrap subsamples as there were sites in the original sample of 10 users. We will take subsamples from the calculated list of site visit frequencies â€“ there is no need to count these frequencies again. It should be taken into account that the frequency of zero appearance (a site with an index of 0 appeared where sessions were shorter than 10 sites) should not be included. Round the boundaries of the interval to 3 decimal places. . Bagging (from Bootstrap aggregation) is one of the first and simplest types of ensembles. It was invented by Leo Breiman in 1994. Bagging is based on the statistical bootstrap method, which allows you to evaluate many statistics of complex distributions. . The bootstrap method is as follows. Let there be a sample X of size N. We will uniformly take from the sample N objects with a return. This means that we will N choose an arbitrary object of the sample (we believe that each object &quot;gets&quot; with the same probability 1/N), and each time we choose from all the original N objects. You can imagine a bag from which the balls are taken out:the ball selected at some step is returned back to the bag, and the next choice is again made equally likely from the same number of balls. Note that due to the return, there will be repeats among them. Denote the new selection by X1. Repeating the procedure M times, we will generate M subsamples X1...XM. Now we have a sufficiently large number of samples and can evaluate various statistics of the initial distribution.link . def get_bootstrap_samples(data, n_samples, random_seed=56): # function for generating subsamples using bootstrap np.random.seed(random_seed) indices = np.random.randint(0, len(data), (n_samples, len(data))) samples = data[indices] return samples . def stat_intervals(stat, alpha): # function for interval estimation boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)]) return boundaries . stock_files = sorted(glob(os.path.join(PATH_TO_DATA, &#39;10users/*.csv&#39;))) df = pd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) . df.head() . timestamp site . 0 2013-11-15 08:12:07 | fpdownload2.macromedia.com | . 1 2013-11-15 08:12:17 | laposte.net | . 2 2013-11-15 08:12:17 | www.laposte.net | . 3 2013-11-15 08:12:17 | www.google.com | . 4 2013-11-15 08:12:18 | www.laposte.net | . sorted_site = dict(collections.OrderedDict(sorted(Counter(df.site).items(), key=lambda kv: kv[1], reverse = True))) . sorted_site_1000 = {} for key, value in sorted_site.items(): if value &gt;= 1000: sorted_site_1000[key] = value plt.hist(list(sorted_site_1000.values())) plt.show() . site_freq = list(sorted_site.values()) . site_mean_scores = list(map(np.mean, get_bootstrap_samples(np.array(site_freq), len(site_freq)))) . print (&quot;95% confidence interval for the ILEC median repair time:&quot;, stat_intervals(site_mean_scores, 0.05)) . 95% confidence interval for the ILEC median repair time: [22.53311622 36.08414411] . As a result, we got that with 95% probability the average frequency of sites lies between 22,515 and 35,763 .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/english/2019/07/20/project-identification-of-internet-users-week2.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/english/2019/07/20/project-identification-of-internet-users-week2.html",
            "date": " â€¢ Jul 20, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Graduation project "Identification of Internet users" - Preparing data for analysis and model building. Programming Assignment",
            "content": "1 week. Data preparation for analysis and model building. Programming Assignment . The first part of the project is devoted to the preparation of data for further descriptive analysis and the construction of predictive models. It will be necessary to write code for preprocessing the data (the websites initially visited are indicated for each user in a separate file) and forming a single training sample. Also in this part we will get acquainted with the allowed data format (Scipy.sparse matrices), which is well suited for this task. . Preparation of a training sample | Working with sparse data format | . Data preparation for analysis and model building . The first part of the project is devoted to the preparation of data for further descriptive analysis and the construction of predictive models. It will be necessary to write code for preprocessing the data (the websites initially visited are indicated for each user in a separate file) and forming a single training sample. Also in this part we will get acquainted with the sparse data format (Scipy.sparse matrices), which is well suited for this task. . Preparation of the training sample . . import os import math import collections import time import pickle import pandas as pd import numpy as np from tqdm.auto import tqdm from glob import glob from collections import Counter from scipy.sparse import csr_matrix . PATH_TO_DATA = &#39;/content/drive/MyDrive/DATA/Stepik/capstone_user_identification&#39; . According to the task, it is necessary to implement the function prepare_train_set, which takes as input the path to the directory with csv files path_to_csv_files and the parameter session_length â€“ the length of the session, and returns 2 objects: . DataFrame in which rows correspond to unique sessions from session_length sites, session_length columns correspond to the indexes of these session_length sites and the last column is the user ID | a frequency dictionary of sites of the form {&#39;site_string&#39;: [site_id, site_freq]}, for example, for a recent toy example it would be {&#39;vk.com &#39;: (1, 2), &#39;google.com &#39;: (2, 2), &#39;yandex.ru &#39;: (3, 3), &#39;facebook.com &#39;: (4, 1)} | . Details: . It is necessary to use glob (or analogues) to crawl files in the directory. For certainty, sort the list of files lexicographically. It is convenient to use tqdm to track the number of completed iterations of the loop | Create a frequency dictionary of unique sites (like {&#39;site_string&#39;: (site_id, site_freq)}) and fill it in as you read the files. Start with 1 | It is recommended to give smaller indexes to more frequently encountered sites (the principle of the smallest description) | Do not do entity recognition, count google.com , http://www.google.com and www.google.com different sites | Most likely, the number of records in the file is not a multiple of the number of session_length. Then the last session will be shorter. Fill in the remainder with zeros. That is, if there are 24 entries in the file and sessions of length 10, then the 3rd session will consist of 4 sites, and we will match the vector [site1_id, site2_id, site3_id, site4_id, 0, 0, 0, 0, 0, 0, user_id] | As a result, some sessions may be repeated â€“ leave as is, do not delete duplicates. If all sites are the same in two sessions, but the sessions belong to different users, then leave it as it is, this is a natural uncertainty in the data. | It is necessary not to leave the site 0 in the frequency dictionary | . def prepare_train_set(path_to_csv_files, session_length=10): stock_files = sorted(glob(path_to_csv_files)) #create a shared dataframe with all users and sites df = pd.concat((pd.read_csv(file) for file in stock_files), ignore_index=True) #create a dictionary with the frequency of sites and sort it sorted_site = dict(collections.OrderedDict(sorted(Counter(df.site).items(), key=lambda kv: kv[1], reverse = True))) #define the site_id and add it to the tuple to add to the dictionary sorted_site_list = list(sorted_site.keys()) df_site_dict_2 = {} for i, site in enumerate(sorted_site, 1): df_site_dict_2[site] = (i, sorted_site.get(site)) #creating a list of sites from site_id list_all_site = [] user = 1 for filename in tqdm((stock_files), desc=&#39;Loop2&#39;): tmp_df = pd.read_csv(filename) list_site = [] #I go through the sites in each file and transform them into site_id for site in tqdm(tmp_df.site, desc = &#39;Loop3&#39;): list_site.append(df_site_dict_2.get(site)[0]) #adding zeros to a session of 10 sites multiple_len = (math.ceil(len(list_site)/session_length))*session_length tmp = [0] * (multiple_len - len(list_site)) list_site.extend(tmp) count = 0 #combining all the lists into one main one while (count &lt; (len(list_site)/session_length)): ind_1 = count * session_length count = count + 1 ind_2 = count * session_length sess = list_site[ind_1 : ind_2] sess.append(user) list_all_site.append(sess) user = user + 1 #creating a dataframe from the main one name_site = [] for i in range(session_length): name_site.append(&#39;site&#39;+str(i+1)) name_site.append(&#39;user_id&#39;) dt_tmp = pd.DataFrame(list_all_site, columns=name_site) return(dt_tmp, df_site_dict_2) . I will conduct a test on three users and check the task . path = os.path.join(PATH_TO_DATA, &#39;3users/*.csv&#39;) . path . &#39;/content/drive/MyDrive/DATA/Stepik/capstone_user_identification/3users/*.csv&#39; . start = time.time() train_data_toy, site_freq_3users = prepare_train_set(path) end = time.time() print(end - start) . 2.2746620178222656 . site_freq_3users . {&#39;accounts.google.com&#39;: (8, 1), &#39;apis.google.com&#39;: (9, 1), &#39;football.kulichki.ru&#39;: (6, 2), &#39;geo.mozilla.org&#39;: (7, 1), &#39;google.com&#39;: (1, 9), &#39;mail.google.com&#39;: (5, 2), &#39;meduza.io&#39;: (4, 3), &#39;oracle.com&#39;: (2, 8), &#39;plus.google.com&#39;: (10, 1), &#39;vk.com&#39;: (3, 3), &#39;yandex.ru&#39;: (11, 1)} . train_data_toy . site1 site2 site3 site4 site5 site6 site7 site8 site9 site10 user_id . 0 3 | 2 | 2 | 7 | 2 | 1 | 8 | 5 | 9 | 10 | 1 | . 1 3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 2 3 | 2 | 6 | 6 | 2 | 0 | 0 | 0 | 0 | 0 | 2 | . 3 4 | 1 | 2 | 1 | 2 | 1 | 1 | 5 | 11 | 4 | 3 | . 4 4 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Everything works, I switch to 10 users . **Part 1. How many unique sessions out of 10 sites in Vyborg with 10 users? . path = os.path.join(PATH_TO_DATA, &#39;10users/*.csv&#39;) . start = time.time() train_data_toy10, site_freq_10users = prepare_train_set(path) end = time.time() print(end - start) . 1.2744076251983643 . len(train_data_toy10) . 14061 . As a result, I received 14061 unique sessions from 10 sites from 10 users. . Part 2. How many unique sites are there in a sample of 10 users? . len(site_freq_10users) . 4913 . In total, I received 4913 unique sites . Part 3. How many unique sessions out of 10 sites in the sample with 150 users? . path = os.path.join(PATH_TO_DATA, &#39;150users/*.csv&#39;) . start = time.time() train_data_toy150, site_freq_150users = prepare_train_set(path) end = time.time() print(end - start) . 16.766276597976685 . len(train_data_toy150) . 137019 . It doesn&#39;t seem to be the most efficient implementation of the function. She worked for me in 1.5 minutes, and for the teacher in 1.7 seconds. I got the result - 137019 . Part 4. How many unique sites are there in the sample of 150 users? . len(site_freq_150users) . 27797 . There are 27797 unique sites in the sample of 150 users. . Part 5. What are the top 10 most popular sites among the 150 users visited . list(site_freq_150users.keys())[:10] . [&#39;www.google.fr&#39;, &#39;www.google.com&#39;, &#39;www.facebook.com&#39;, &#39;apis.google.com&#39;, &#39;s.youtube.com&#39;, &#39;clients1.google.com&#39;, &#39;mail.google.com&#39;, &#39;plus.google.com&#39;, &#39;safebrowsing-cache.google.com&#39;, &#39;www.youtube.com&#39;] . Working with sparse data format . If you think about it like that, then the obtained signs site1, ..., site 10 do not make sense as signs in the classification problem. But if you use the idea of a bag of words from text analysisâ€“ this is another matter. Let&#39;s create new matrices in which the rows will correspond to sessions from 10 sites, and the columns will correspond to site indexes. At the intersection of the row $i$ and the column $j$ will be the number $n_{ij}$ â€“ how many times the site $j$ met in the session number $i$. We will do this using sparse Scipy â€“ [csr_matrix] matrices(https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html ). First you need to test it on a toy example, then apply it for 10 and 150 users. . Please note that in short sessions, less than 10 sites, we have zeros left, so the first sign (how many times 0 was caught) is different in meaning from the rest (how many times a site with the index $i$ was caught). Therefore, the first column of the sparse matrix will need to be deleted. . X_toy, y_toy = train_data_toy.iloc[:, :-1].values, train_data_toy.iloc[:, -1].values . X_toy . array([[ 3, 2, 2, 7, 2, 1, 8, 5, 9, 10], [ 3, 1, 1, 1, 0, 0, 0, 0, 0, 0], [ 3, 2, 6, 6, 2, 0, 0, 0, 0, 0], [ 4, 1, 2, 1, 2, 1, 1, 5, 11, 4], [ 4, 1, 2, 0, 0, 0, 0, 0, 0, 0]]) . There are two types of matrices - dense and sparse. . A sparse matrix is a matrix with predominantly zero elements. Otherwise, if most of the matrix elements are nonzero, the matrix is considered dense. . There is no unity among experts in determining exactly what number of non-zero elements makes the matrix sparse. Different authors offer different options. . X_sparse_toy = csr_matrix((np.ones(X_toy.size, dtype=int), X_toy.reshape(-1), np.arange(X_toy.shape[0] + 1) * X_toy.shape[1]))[:, 1:] . The dimension of the sparse matrix should be equal to 11, since in the toy example 3 users visited 11 unique sites. . X_sparse_toy.todense() . matrix([[1, 3, 1, 0, 1, 0, 1, 1, 1, 1, 0], [3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0], [4, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1], [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]]) . train_data_toy10 . X_10users, y_10users = train_data_toy10.iloc[:, :-1].values, train_data_toy10.iloc[:, -1].values X_150users, y_150users = train_data_toy150.iloc[:, :-1].values, train_data_toy150.iloc[:, -1].values . X_sparse_10users = csr_matrix((np.ones(X_10users.size, dtype=int), X_10users.reshape(-1), np.arange(X_10users.shape[0] + 1) * X_10users.shape[1]))[:, 1:] . X_sparse_150users = csr_matrix((np.ones(X_150users.size, dtype=int), X_150users.reshape(-1), np.arange(X_150users.shape[0] + 1) * X_150users.shape[1]))[:, 1:] . Save these sparse matrices using pickle (serialization in Python), we will also save the vectors y_10users, y_150users - target values (user id) in samples of 10 and 150 users. The fact that the names of these matrices begin with X and y hints that we will test the first classification models on these data. Finally, we will also save the frequency dictionaries of sites for 3, 10 and 150 users. . with open(os.path.join(PATH_TO_DATA, &#39;X_sparse_10users.pkl&#39;), &#39;wb&#39;) as X10_pkl: pickle.dump(X_sparse_10users, X10_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;y_10users.pkl&#39;), &#39;wb&#39;) as y10_pkl: pickle.dump(y_10users, y10_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;X_sparse_150users.pkl&#39;), &#39;wb&#39;) as X150_pkl: pickle.dump(X_sparse_150users, X150_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;y_150users.pkl&#39;), &#39;wb&#39;) as y150_pkl: pickle.dump(y_150users, y150_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;site_freq_3users.pkl&#39;), &#39;wb&#39;) as site_freq_3users_pkl: pickle.dump(site_freq_3users, site_freq_3users_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;site_freq_10users.pkl&#39;), &#39;wb&#39;) as site_freq_10users_pkl: pickle.dump(site_freq_10users, site_freq_10users_pkl, protocol=2) with open(os.path.join(PATH_TO_DATA, &#39;site_freq_150users.pkl&#39;), &#39;wb&#39;) as site_freq_150users_pkl: pickle.dump(site_freq_150users, site_freq_150users_pkl, protocol=2) . Just to be safe, let&#39;s check that the number of columns in the sparse matrices &#39;X_sparse_10users` and &#39;X_sparse_150users&#39; is equal to the previously calculated numbers of unique sites for 10 and 150 users, respectively. . assert X_sparse_10users.shape[1] == len(site_freq_10users) . assert X_sparse_150users.shape[1] == len(site_freq_150users) .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/english/2019/07/10/project-identification-of-internet-users-week1.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/english/2019/07/10/project-identification-of-internet-users-week1.html",
            "date": " â€¢ Jul 10, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Graduation project "Identification of Internet users" Task",
            "content": "Identification of Internet users . In this project, we will solve the problem of identifying a user by his behavior on the Internet. This is a complex and interesting task at the intersection of data analysis and behavioral psychology. As an example, Yandex solves the problem of identifying a mailbox cracker by his behavior. In a nutshell, the hacker will behave differently from the owner of the mailbox: he may not delete messages immediately after reading, as the owner did, he will check the messages differently and even move the mouse in his own way. Then such an attacker can be identified and &quot;thrown out&quot; of the mailbox by inviting the owner to enter by SMS code. This pilot project is described in an article on Habrahabr. Similar things are done, for example, in Google Analytics and are described in scientific articles, you can find a lot by the phrases &quot;Traversal Pattern Mining&quot; and &quot;Sequential Pattern Mining&quot;. . We will solve a similar problem: by a sequence of several websites visited in a row by the same person, we will identify this person. The idea is this: Internet users follow links in different ways, and this can help identify them (someone first to the mail, then to read about football, then news, contact, then finally to work, someone to work immediately, if possible). . We will use data from the article &quot;A Tool for Classification of Sequential Data&quot;. And although we cannot recommend this article (the described methods are far from state-of-the-art, it is better to refer to the book &quot;Frequent Pattern Mining&quot; and the latest articles with ICDM), but the data there are collected neatly and are of interest. . There is data from the Blaise Pascal University proxy servers, their appearance is very simple: user ID, timestamp, visited website. . You can download the source data from the link in the article (there is also a description), for this task there is enough data not for all 3000 users, but for 10 and 150. Link to the archive capstone_user_identification.zip (~7 Mb, expanded ~60 Mb). . In the course of the project, you will have 4 tasks of the Programming Assignment type, dedicated to data preprocessing, primary analysis, visual data analysis, comparison of classification models and setting up the selected model and studying its retraining. You will also have 3 mutually evaluated tasks (Peer Review) - on data visualization (including with newly created features), on evaluating the results of participation in competition Kaggle Inclass and throughout the project as a whole. . During the project, we will work with the Vowpal Wabbit library. If there are problems with its installation, you can use the Docker image, for example, the one described in Wiki of the open course repository OpenDataScience on machine learning. . The project plan is as follows: . 1 week. Preparing data for analysis and model building. Programming Assignment . The first part of the project is devoted to the preparation of data for further descriptive analysis and the construction of predictive models. It will be necessary to write code for preprocessing the data (the websites initially visited are indicated for each user in a separate file) and forming a single training sample. Also in this part we will get acquainted with the sparse data format (Scipy.sparse matrices), which is well suited for this task. . Preparing a training sample | Working with the sparse data format | . 2 week. Preparation and initial analysis of data. Programming Assignment . In the second week, we will continue to prepare data for further analysis and construction of forecast models. Specifically, earlier we determined that a session is a sequence of 10 sites visited by a user, now we will make the session length a parameter, and then when training predictive models we will choose the best session length. We will also get acquainted with the preprocessed data and statistically test the first hypotheses related to our observations. . Preparation of several training samples for comparison | Primary data analysis, hypothesis testing | . 3 week. Visual data analysis and feature construction. Peer-Review . In week 3, we will be engaged in visual data analysis and feature construction. First, we will build and analyze several signs together, then you will be able to come up with and describe various signs yourself. The task has the form of a Peer-Review, so creativity is actively welcome here. If you use IPython widgets, the Plotly library, animations and other interactive tools, it will only be better for everyone. . Visual data analysis | Building features | . 4 weeks. Comparison of classification algorithms. Programming Assignment . Here we will finally approach the training of classification models, compare several algorithms on cross-validation, and figure out which session length parameters (session_length and window_size) are better to use. Also, for the selected algorithm, we will construct validation curves (how the classification quality depends on one of the hyperparameters of the algorithm) and learning curves (how the classification quality depends on the sample size). . Comparison of several algorithms in sessions from 10 sites | Selection of parameters - session length and window width | User-specific identification and learning curves | . Week 5. Kaggle Inclass User Identification competition. Peer-Review . Here we will recall the concept of stochastic gradient descent and try the Scikit-learn SGDClassifier classifier, which works much faster on large samples than the algorithms we tested in week 4. We will also get acquainted with the data of the Kaggle user identification competition and make the first parcels in it. At the end of this week, those who beat the benchmarks specified in the competition will receive additional points. . Week 6. Vowpal Wabbit. Tutorial + Programming Assignment . This week we will get acquainted with the popular Vowpal Wabbit library and try it on web session data. We will get acquainted with the Scikit-learn data on the news, first in binary classification mode, then in multiclass mode. Then we will classify movie reviews from the IMDB website. Finally, let&#39;s apply Vowpal Wabbit to web session data. There is a lot of material, but Vowpal Wabbit is worth it! . Article about Vowpal Wabbit | Applying Vowpal Wabbit to site visit data | . Week 7. Design of the final project. Peer-Review . At the very end, mutual verification of the final versions of the project awaits you. It will be possible to roam around here, because there is freedom of creativity at every stage of the project: you can use all the source data for 3000 users, you can create your own interesting signs, build beautiful pictures, use your models or ensembles of models and draw conclusions. Therefore, the advice is as follows: as the tasks are completed, copy the code and description in parallel to the .ipynb file of the project or describe the results along the way in a text editor. .",
            "url": "https://zmey56.github.io/blog//graduation%20project/machine%20learning/stepik/yandex/english/2019/07/01/project-identification-of-internet-users-task.html",
            "relUrl": "/graduation%20project/machine%20learning/stepik/yandex/english/2019/07/01/project-identification-of-internet-users-task.html",
            "date": " â€¢ Jul 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, Iâ€™m Alexander Gladkikh, and I made my own website on GitHub dedicated to my Hobbies: Machine learning, Deep Learning, and algorithmic trading. . Participated in research at the scientific Institute in the following areas: psychology of behavior, physiology of nutrition, exposure to extreme temperatures and high pressure. I have a patent for an invention, and I am the author of more than 30 scientific papers. Participated in decommissioning of the radioactive waste storage facility, liquidation of the Soviet legacy, preparation of materials for environmental impact assessment and environmental expertise. Throughout my time, I have been studying the psychology of safe behavior and, thanks to my passion, additionally supervised the areas of digitalization in the field of occupational safety. . Iâ€™ve always had a passion for machine learning, so Iâ€™ve been slowly switching to learn more about machine learning, etc. At first I was self-taught, and then I took courses on Yandex. After that, I also worked on the formation of investment portfolios using machine learning. As a result, over the past two years, I have managed to show profitability for my clients above 23 and 17% annually. I am currently undergoing training in artificial intelligence. . I take part in kaggle competitions, have knowledge of R and Python (Pandas, numpy, Scipy, Scikit-learn, XGBoost), Java. . My degrees . Corporate Energy University, 2020 . Digital production technologies in the power industry . YANDEX, MIPT, 2019 . Machine learning and data analysis . City Business School, 2019 . MINI-MBA Professional . Postgraduate Kola Science Centre, 2008 . ecology . Petrozavodsk State University (PetrSU), 2002 . ecology . Licenses &amp; certifications . Market infrastructure, investment methods and technologies . Coursera, Higher School of Economics, 2020 . Python for Finance: Investment Fundamentals &amp; Data Analytics . Udemy, 2020 . Python for Financial Analysis and Algorithmic Trading . Udemy, 2019 . SQL for Data Science . Coursera, 2019 . Dive into Python . Coursera, 2018 .",
          "url": "https://zmey56.github.io/blog//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://zmey56.github.io/blog//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}