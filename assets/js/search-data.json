{
  
    
        "post0": {
            "title": "Russian - Анализ ETF с использованием Python",
            "content": "Как использовать библиотеки Python, такие как Pandas, Matplotlib и Seaborn, для получения информации из ежедневных данных о ценах и объемах c фондового рынка. . С проникновением аналитики во многие сферы нашей жизни она не могла обойти стороной финансы. В этой статье рассмотрим ее применение для анализа ETF с целью их анализа, в том числе и с применением визуализиции. . 1. &#1054; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Для анализа будем использовать данные ETF с валютным хейджом: FXCN, FXRL, FXIT, FXUS и FXRU. Временной ряд рассмотрим за три года с 2018 по 2020 года. Само исследование проведем в Google Colaboratory. . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import pandas as pd import numpy as np import matplotlib.pyplot as plt from google.colab import files import warnings warnings.filterwarnings(&quot;ignore&quot;) . Сначало необходимо загрузить данные, которые представлены в формате CSV. . uploaded = files.upload() for fn in uploaded.keys(): print(&#39;User uploaded file «{name}» with length {length} bytes&#39;.format(name=fn, length=len(uploaded[fn]))) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving FXGD.csv to FXGD (1).csv Saving FXRU.csv to FXRU (1).csv Saving FXUS.csv to FXUS (1).csv Saving FXIT.csv to FXIT (1).csv Saving FXRL.csv to FXRL (1).csv Saving FXCN.csv to FXCN (1).csv User uploaded file «FXGD.csv» with length 53856 bytes User uploaded file «FXRU.csv» with length 32600 bytes User uploaded file «FXUS.csv» with length 56015 bytes User uploaded file «FXIT.csv» with length 56137 bytes User uploaded file «FXRL.csv» with length 55985 bytes User uploaded file «FXCN.csv» with length 56038 bytes . После этого прочтем данные с диска. Дальше необходимо создать два двадатафрейма - один с ценами закрытия, а другой с объемами торговли: . fxgd =pd.read_csv(&#39;/content/FXGD.csv&#39;) fxrl =pd.read_csv(&#39;/content/FXRL.csv&#39;) fxit =pd.read_csv(&#39;/content/FXIT.csv&#39;) fxus =pd.read_csv(&#39;/content/FXUS.csv&#39;) fxru =pd.read_csv(&#39;/content/FXRU.csv&#39;, sep=&#39;;&#39;) fxcn =pd.read_csv(&#39;/content/FXCN.csv&#39;) . def changeDF(df): df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) name =[x for x in globals() if globals()[x] is df][0] df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;HIGH&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) df = df.set_index([&#39;date&#39;]) df.columns = [name+&#39;_cl&#39;, name + &#39;_vol&#39;] return df . # df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) # name =[x for x in globals() if globals()[x] is df][0] # df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;CLOSE&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) # df = df.set_index([&#39;date&#39;]) # df.columns = [name] # return df . fxgd_change = changeDF(fxgd) fxrl_change = changeDF(fxrl) fxit_change = changeDF(fxit) fxus_change = changeDF(fxus) fxru_change = changeDF(fxru) fxcn_change = changeDF(fxcn) . etf = pd.concat([fxgd_change, fxrl_change, fxit_change, fxus_change, fxru_change, fxcn_change], axis=1) . etf.head() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . date . 2018-01-03 529.0 | 4340 | 1950.5 | 443 | 3612.0 | 581 | 2738.0 | 1049 | 641.0 | 139.0 | 2635.0 | 2098 | . 2018-01-04 527.0 | 1489 | 1992.0 | 659 | 3641.0 | 647 | 2745.0 | 586 | 639.0 | 128.0 | 2655.0 | 1331 | . 2018-01-05 526.0 | 1911 | 2004.5 | 846 | 3646.0 | 876 | 2744.0 | 322 | 637.0 | 306.0 | 2640.0 | 1664 | . 2018-01-09 525.5 | 5044 | 2024.0 | 2570 | 3673.0 | 1833 | 2766.0 | 653 | 638.0 | 448.0 | 2670.0 | 2304 | . 2018-01-10 527.5 | 9808 | 2030.0 | 765 | 3660.0 | 2485 | 2758.0 | 407 | 637.0 | 369.0 | 2665.0 | 1910 | . C FXRU пришлось немного поработать в EXCEL, так как скачанные данные прибивили лишний ноль к значению. По этому при загрузке пришлось указывать явный разделитель. . Дальше проверим наш датасет на предмет наличия значений NULL . print(etf.isnull().sum()) . fxgd_cl 0 fxgd_vol 0 fxrl_cl 0 fxrl_vol 0 fxit_cl 0 fxit_vol 0 fxus_cl 0 fxus_vol 0 fxru_cl 4 fxru_vol 4 fxcn_cl 0 fxcn_vol 0 dtype: int64 . Выбросим их, чтоб не мешали в дальнейшем расчете: . etf.dropna(inplace=True, axis=0) . Дальше имеет смысл посмотреть тип значений: . etf.dtypes . fxgd_cl float64 fxgd_vol int64 fxrl_cl float64 fxrl_vol int64 fxit_cl float64 fxit_vol int64 fxus_cl float64 fxus_vol int64 fxru_cl float64 fxru_vol float64 fxcn_cl float64 fxcn_vol int64 dtype: object . И посмотрим размер датасета: . etf.shape . (752, 12) . Так же дальше интересно посмотреть как вели себя ETF в последние полгода. Это можно сделать при помощи функции describe: . etf[-120:].describe() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . count 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | . mean 973.690000 | 148832.116667 | 3121.208333 | 8971.225000 | 8860.925000 | 14784.708333 | 4758.458333 | 12685.116667 | 950.133333 | 32841.958333 | 3878.233333 | 17515.733333 | . std 36.967338 | 94656.673543 | 169.290817 | 6816.481138 | 572.529639 | 8391.294562 | 267.491739 | 7730.347512 | 28.046367 | 14373.882822 | 219.697987 | 14471.016798 | . min 878.000000 | 34678.000000 | 2848.500000 | 2907.000000 | 7513.000000 | 4769.000000 | 4140.000000 | 4392.000000 | 880.800000 | 11069.000000 | 3422.000000 | 4448.000000 | . 25% 946.100000 | 87124.250000 | 2998.000000 | 5388.500000 | 8466.500000 | 9388.750000 | 4578.000000 | 8902.000000 | 935.900000 | 21355.000000 | 3721.000000 | 8495.250000 | . 50% 985.900000 | 127780.500000 | 3083.000000 | 7584.500000 | 9051.500000 | 12681.500000 | 4807.500000 | 11277.000000 | 951.450000 | 29360.000000 | 3898.000000 | 12329.500000 | . 75% 1001.650000 | 175438.250000 | 3219.500000 | 10754.250000 | 9306.250000 | 17640.000000 | 4982.000000 | 13963.750000 | 971.050000 | 42583.750000 | 4064.500000 | 22652.250000 | . max 1033.600000 | 666819.000000 | 3488.500000 | 67809.000000 | 9776.000000 | 63506.000000 | 5157.000000 | 73672.000000 | 1012.800000 | 91275.000000 | 4312.000000 | 101084.000000 | . pct_chg_etf[:50].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | . mean 0.018909 | 0.128734 | 0.141815 | 0.037679 | 0.010791 | 0.088726 | . std 0.634123 | 0.840398 | 1.222263 | 1.014099 | 0.544054 | 1.462912 | . min -1.291513 | -1.680871 | -4.318305 | -3.804348 | -1.550388 | -4.403670 | . 25% -0.381599 | -0.359217 | -0.432242 | -0.326851 | -0.312745 | -0.562852 | . 50% -0.094162 | 0.266967 | 0.175959 | 0.073884 | 0.000000 | 0.190041 | . 75% 0.379604 | 0.689444 | 0.847795 | 0.543863 | 0.319361 | 1.135292 | . max 1.826923 | 2.127660 | 3.065569 | 2.749529 | 1.107595 | 2.909091 | . В результате видно в каких пределах в последние полгода ETF провели большую часть аремени с вероятностью 75%. . После построим графики движения цены во времени. . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf.index, etf[&#39;fxgd_cl&#39;], &#39;tab:blue&#39; ) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf.index, etf[&#39;fxrl_cl&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf.index, etf[&#39;fxit_cl&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf.index, etf[&#39;fxus_cl&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf.index, etf[&#39;fxru_cl&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf.index, etf[&#39;fxcn_cl&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . Ежедневное процентное изменение цены etf вычисляется на основе процентного изменения между ценами закрытия 2 последовательных дней. Предположим, что цена закрытия вчера составляла 500 рублей, а сегодня она закрылась по 550 рублей. Таким образом, процентное изменение составляет 10%. т. е. ((550-500) / 500)*100. Здесь нет никакой тайны! . Далее, мы введем новый столбец, обозначающий дневную доходность в цене etf. Вычислить можно с помощью встроенной функции pct_change() в python. Так же немного переставлю колонки, чтоб визуально лучше воспринималось. . etf.columns . Index([&#39;fxgd_cl&#39;, &#39;fxgd_vol&#39;, &#39;fxrl_cl&#39;, &#39;fxrl_vol&#39;, &#39;fxit_cl&#39;, &#39;fxit_vol&#39;, &#39;fxus_cl&#39;, &#39;fxus_vol&#39;, &#39;fxru_cl&#39;, &#39;fxru_vol&#39;, &#39;fxcn_cl&#39;, &#39;fxcn_vol&#39;], dtype=&#39;object&#39;) . etf_cl = etf[[&#39;fxgd_cl&#39;, &#39;fxrl_cl&#39;, &#39;fxit_cl&#39;, &#39;fxus_cl&#39;, &#39;fxru_cl&#39;, &#39;fxcn_cl&#39;]] etf_cl_pct = etf_cl.pct_change()*100 etf_cl_pct.columns = [&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;] etf_vol = etf[[&#39;fxgd_vol&#39;, &#39;fxrl_vol&#39;, &#39;fxit_vol&#39;, &#39;fxus_vol&#39;, &#39;fxru_vol&#39;, &#39;fxcn_vol&#39;]] etf_new = pd.concat([etf_cl, etf_vol, etf_cl_pct], axis = 1) . etf_new.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-03 529.0 | 1950.5 | 3612.0 | 2738.0 | 641.0 | 2635.0 | 4340 | 443 | 581 | 1049 | 139.0 | 2098 | NaN | NaN | NaN | NaN | NaN | NaN | . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . etf_new = etf_new.dropna() . Представим изменение ежедневной доходности в виде графика во времени: . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf_new.index, etf_new[&#39;fxgd_cl_pct&#39;], &#39;tab:blue&#39;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf_new.index, etf_new[&#39;fxrl_cl_pct&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf_new.index, etf_new[&#39;fxit_cl_pct&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf_new.index, etf_new[&#39;fxus_cl_pct&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf_new.index, etf_new[&#39;fxru_cl_pct&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf_new.index, etf_new[&#39;fxcn_cl_pct&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . В течение большей части времени доходность составляет от -2% до 2% со скачками без пересечения отметки в 6% с обеих сторон. Наиболее шумной выглядит ETF FXCN. . Так же можно проверить новостные статьи за те дни, когда наблюдался резкий рост/падение цен на etf и понять чем было обусловлено. . Построим гистограмму распределения ежедневных доходов: . import seaborn as sns sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(15,15)) sns.histplot(data=etf_new[&#39;fxgd_cl_pct&#39;], kde=True, color=&quot;orange&quot;, ax=axs[0, 0]) axs[0,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxrl_cl_pct&#39;], kde=True, color=&quot;olive&quot;, ax=axs[0, 1]) axs[0,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxit_cl_pct&#39;], kde=True, color=&quot;gold&quot;, ax=axs[1, 0]) axs[1,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxus_cl_pct&#39;], kde=True, color=&quot;grey&quot;, ax=axs[1, 1]) axs[1,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxru_cl_pct&#39;], kde=True, color=&quot;teal&quot;, ax=axs[2, 0]) axs[2,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxcn_cl_pct&#39;], kde=True, color=&quot;brown&quot;, ax=axs[2, 1]) axs[2,1].set_xlim(-10,10) plt.show() . etf_new[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | . mean 0.084329 | 0.084240 | 0.140564 | 0.089850 | 0.056374 | 0.065255 | . std 1.081425 | 1.163047 | 1.398492 | 1.141144 | 0.771131 | 1.414819 | . min -5.709816 | -8.065290 | -6.874365 | -8.567335 | -5.198422 | -5.273973 | . 25% -0.456676 | -0.444714 | -0.574001 | -0.444633 | -0.346166 | -0.752409 | . 50% 0.030111 | 0.126835 | 0.206940 | 0.137979 | 0.026178 | 0.128783 | . 75% 0.622500 | 0.718721 | 0.887283 | 0.645403 | 0.445645 | 0.899653 | . max 5.619982 | 7.784431 | 8.297990 | 6.079599 | 4.604008 | 6.554307 | . Гистограммы ежедневных доходностей центрированы вокруг среднего значения, которое для всех etf было больше нуля и говорит о положительном тренде. Видно, что доходность для всех ETF большую часть времени лежала в пределах от -2,5 до 2,5%. Наибольшую доходность показали - FXIT, а наименьшую - FXRU. . &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1090;&#1088;&#1077;&#1085;&#1076;&#1072; . Затем мы добавляем новый столбец &quot;Тренд&quot;, значения которого основаны на ежедневном процентном изменении, которое мы рассчитали выше. Тенденция определяется отношением снизу. Скопируем датасет в новый, с которым и продолжим работу. . def trend(x): if x &gt; -0.5 and x &lt;= 0.5: return &#39;Практически или без изменений&#39; elif x &gt; 0.5 and x &lt;= 1.5: return &#39;Небольшой позитив&#39; elif x &gt; -1.5 and x &lt;= -0.5: return &#39;Небольшой негатив&#39; elif x &gt; 1.5 and x &lt;= 2.5: return &#39;Позитив&#39; elif x &gt; -2.5 and x &lt;= -1.5: return &#39;Негатив&#39; elif x &gt; 2.5 and x &lt;= 5: return &#39;Значительный позитив&#39; elif x &gt; -5 and x &lt;= -2.5: return &#39;Значительный негатив&#39; elif x &gt; 5: return &#39;Максимальный позитив&#39; elif x &lt;= -5: return &#39;Максимальный негатив&#39; . etf_trend = etf_new.copy() . etf_trend.columns[12:] . Index([&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;], dtype=&#39;object&#39;) . for stock in etf_trend.columns[12:]: etf_trend[&quot;Trend_&quot; + str(stock)] = np.zeros(etf_trend[stock].count()) etf_trend[&quot;Trend_&quot;+ str(stock)] = etf_trend[stock].apply(lambda x:trend(x)) . etf_trend.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct Trend_fxgd_cl_pct Trend_fxrl_cl_pct Trend_fxit_cl_pct Trend_fxus_cl_pct Trend_fxru_cl_pct Trend_fxcn_cl_pct . date . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | Практически или без изменений | Позитив | Небольшой позитив | Практически или без изменений | Практически или без изменений | Небольшой позитив | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | Практически или без изменений | Небольшой позитив | Небольшой позитив | Небольшой позитив | Практически или без изменений | Небольшой позитив | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | . 2018-01-11 526.0 | 2042.0 | 3673.0 | 2755.0 | 635.0 | 2650.0 | 5548 | 1220 | 1282 | 968 | 326.0 | 1722 | -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts() . Практически или без изменений 351 Небольшой позитив 166 Небольшой негатив 141 Позитив 44 Негатив 25 Значительный позитив 13 Значительный негатив 7 Максимальный негатив 2 Максимальный позитив 2 Name: Trend_fxgd_cl_pct, dtype: int64 . Дальше можно взглянуть как вели себя акции акцииETF в последние 3 года. Для этого их изменения можно визуализировать при помощи круговых диаграмм, где каждый сектор представляет процент дней, в течение которых происходил каждый тренд. Для построения будем использовать функцию groupby() со столбцом тренда. . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(20,17)) axs[0, 0].pie(etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].pie(etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].pie(etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].pie(etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].pie(etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].pie(etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 1].set_title(&#39;FXCN&#39;) plt.show() . За рассматриваемый период с 2018 года по 2020 года большую часть времени ETF практически не изменялись, или изменялись незначительно при заданных параметрах. Так же важно отметить, что при небольших изменениях они как правило были позитивными. При более больших - это соотношение сохранялось кроме FXRU. . 6. &#1045;&#1078;&#1077;&#1076;&#1085;&#1077;&#1074;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1080; &#1086;&#1073;&#1098;&#1077;&#1084;&#1099; . Следующим шагом продолжим работу с объемами: . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) axs[0].stem(etf_trend.index[-253:], etf_trend[&#39;fxgd_cl_pct&#39;][-253:]) axs[0].plot((etf_trend[&#39;fxgd_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[0].set_title(&#39;FXGD&#39;) axs[1].stem(etf_trend.index[-253:], etf_trend[&#39;fxrl_cl_pct&#39;][-253:]) axs[1].plot((etf_trend[&#39;fxrl_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[1].set_title(&#39;FXRL&#39;) axs[2].stem(etf_trend.index[-253:], etf_trend[&#39;fxit_cl_pct&#39;][-253:]) axs[2].plot((etf_trend[&#39;fxit_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[2].set_title(&#39;FXIT&#39;) axs[3].stem(etf_trend.index[-253:], etf_trend[&#39;fxus_cl_pct&#39;][-253:]) axs[3].plot((etf_trend[&#39;fxus_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[3].set_title(&#39;FXUS&#39;) axs[4].stem(etf_trend.index[-253:], etf_trend[&#39;fxru_cl_pct&#39;][-253:]) axs[4].plot((etf_trend[&#39;fxru_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[4].set_title(&#39;FXRU&#39;) axs[5].stem(etf_trend.index[-253:], etf_trend[&#39;fxcn_cl_pct&#39;][-253:]) axs[5].plot((etf_trend[&#39;fxcn_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[5].set_title(&#39;FXCN&#39;) . Text(0.5, 1.0, &#39;FXCN&#39;) . Сопоставляя ежедневный объем торговли(зеленым цветом) с ежедневной доходностью(синим цветом), было отмечено, что часто для ETF характерно, что когда объем торгов высок, наблюдается сравнительно высокий рост или падение цены. Объем торгов ETF в сочетании с ростом или падениемы на данный инструмент является показателем доверия трейдеров и инвесторов к конкретному ETF. . &#1050;&#1086;&#1088;&#1088;&#1077;&#1083;&#1103;&#1094;&#1080;&#1086;&#1085;&#1085;&#1099;&#1081; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079; ETF . Основное правило диверсификации - не клади все яйца в одну корзинку. По этому если мы решили собирать портфель из ETF, то они не должны быть сильно взаимосвязаны друг с другом. Математическим языком - коэффициент корреляции Пирсона между любой парой должен быть близок к 0. Смысл - они не должны падать синхронно, чтоб инвестиции не превратились в 0. . Проанализировать корреляцию между различными ETF можно с помощью парной диаграммы Seaborn. Для удобства оставим только процентные изменения за день в отдельном новом датафрейме. . pct_chg_etf = etf_new[etf_new.columns[12:]] . sns.set(style = &#39;ticks&#39;, font_scale = 1.25) sns.pairplot(pct_chg_etf) plt.show() . pct_chg_etf.corr() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . fxgd_cl_pct 1.000000 | -0.236301 | 0.107085 | 0.119773 | 0.590029 | 0.139992 | . fxrl_cl_pct -0.236301 | 1.000000 | 0.335061 | 0.300352 | -0.384120 | 0.232063 | . fxit_cl_pct 0.107085 | 0.335061 | 1.000000 | 0.895261 | 0.138016 | 0.641551 | . fxus_cl_pct 0.119773 | 0.300352 | 0.895261 | 1.000000 | 0.202682 | 0.610576 | . fxru_cl_pct 0.590029 | -0.384120 | 0.138016 | 0.202682 | 1.000000 | 0.197225 | . fxcn_cl_pct 0.139992 | 0.232063 | 0.641551 | 0.610576 | 0.197225 | 1.000000 | . На графике визуально можно увидеть наличие корреляции между различными ETF. Обратите внимание, что корреляционный анализ выполняется для ежедневного процентного изменения(дневной доходности) цены ETF, а не для их цены. . Из полученных графиков ясно видно, что следующие FXIT и FXUS не следует класть в одну корзину, так как между нми наблюдается сильная зависимость. Остальные могут быть включены в портфель, поскольку ни одна из двух оставшихся ETF не демонстрирует какой-либо существенной корреляции. . Но у визуального анализа есть существенный недостаток - он не предоставляет подробной информации о количественной оценки взаимосвязи, таких как значение R Пирсона и p нулевой гипотезы. В связи с чем при визуальном анализе остается под вопросом FXCN - есть ли у данного ETF сильная взаимосвязь с FXUS или нет. . Один из способов решения данного вопроса - построение графиков seaborn.jointplot с подробной информацией по значению R Пирсона (коэффициент корреляции Пирсона) для каждой пары ETF. Значение R Пирсона колеблется от -1 до 1. Отрицательное значение указывает на отрицательную линейную связь, в то время как положительное значение указывает на положительную связь. Значение R Пирсона ближе к 1 (или -1) указывает на сильную корреляцию, в то время как значение ближе к 0 указывает на слабую корреляцию. . Так же чем интересны данные графики - построение гистограмм распределения по краям, а так же значение p-value. . Но если рассматривать все пары, то нам потребуется большое количество графиков. По этому остановимся только на тех, которые вызывают сомнения: . pct_chg_etf.head() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-04 -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . 2018-01-11 -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | . from scipy.stats import stats a_1 = pct_chg_etf.fxit_cl_pct b_1 = pct_chg_etf.fxus_cl_pct b_2 = pct_chg_etf.fxcn_cl_pct g_1 = sns.jointplot(&#39;fxit_cl_pct&#39;, &#39;fxcn_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_1, p_1 = stats.pearsonr(a_1, b_1) g_1.ax_joint.annotate(f&#39;$ rho = {r_1:.3f}, p = {p_1:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_1.ax_joint.scatter(a_1, b_1) g_1.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxus&#39;, size=15) g_2 = sns.jointplot(&#39;fxus_cl_pct&#39;, &#39;fxit_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_2, p_2 = stats.pearsonr(a_1, b_2) g_2.ax_joint.annotate(f&#39;$ rho = {r_2:.3f}, p = {p_2:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_2.ax_joint.scatter(a_1, b_2) g_2.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxcn&#39;, size=15) plt.tight_layout() plt.show() . Первый гррафик подтвердил наличие сильной взаимосвязи между FXIT и FXUS, что говорит о нежелательности их брать в один портфель. В свою очередь корреляция между FXCN и FXIT оказалась ниже 0,7, что говорит о возможности совместного нахождения в одной корзине. . 9. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1074;&#1086;&#1083;&#1072;&#1090;&#1080;&#1083;&#1100;&#1085;&#1086;&#1089;&#1090;&#1080; . Волатильность-один из важнейших показателей на финансовых рынках. Говорят, что ценная бумага обладает высокой волатильностью, если ее стоимость может резко измениться за короткий промежуток времени. С другой стороны, более низкая волатильность означает, что стоимость имеет тенденцию быть относительно стабильной в течение определенного периода времени. Эти изменения обусловлены несколькими факторами, включая спрос и предложение, настроения, жадность, страх и т.д. Математически волатильность измеряется с помощью статистической меры, называемой &quot;стандартным отклонением&quot;, которая измеряет отклонение актива от его средней стоимости. . Произведем рассчет 5-дневной скользящей средней дневной доходности и стандартного отклонения. После этого построим график. Все это можно выполнить при помощи функций Pandas rolling() и std(). . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) for i, etf in enumerate(pct_chg_etf.columns): axs[i].plot(pct_chg_etf[etf].rolling(5).std()*np.sqrt(5)) axs[i].plot(pct_chg_etf[etf].rolling(7).mean()) axs[i].set_title(etf[:4], size=20) . volatility = pct_chg_etf[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;,&#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].rolling(5).std()*np.sqrt(5) . volatility[:150].plot(linewidth=4, figsize = (35, 15)) plt.legend(loc=2, prop={&#39;size&#39;: 16}) . &lt;matplotlib.legend.Legend at 0x7f6bad8d31d0&gt; . Как результат вы можете заметить, что наиболее сильная низкая волатильность характерна для ETF на российские акции - FXRU. Многие трейдеры и инвесторы ищут инвестиции с более высокой волатильностью, чтобы получать более высокую прибыль. Если финансовый инструмент не движется, он не только обладает низкой волатильностью, но и имеет низкий потенциал прибыли. С другой стороны, ценные бумаги с очень высоким уровнем волатильности могут иметь огромный потенциал прибыли, но риск также высок. .",
            "url": "https://zmey56.github.io/blog//russian/python/algotrading/portfolio/optimization/2021/09/18/data-analysis-visualization-finance.html",
            "relUrl": "/russian/python/algotrading/portfolio/optimization/2021/09/18/data-analysis-visualization-finance.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Russian - Оптимизации портфеля с помощью Python и PyPortfolioOpt",
            "content": "&#1054;&#1087;&#1090;&#1080;&#1084;&#1080;&#1079;&#1072;&#1094;&#1080;&#1080; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Вычисление оптимизированных весов активов в портфеле используя Портфельную теорию Марковица при помощи Python . &#1055;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1100;&#1085;&#1072;&#1103; &#1090;&#1077;&#1086;&#1088;&#1080;&#1103; &#1052;&#1072;&#1088;&#1082;&#1086;&#1074;&#1080;&#1094;&#1072; . Портфельная теория Марковица(далее ПТМ) (Modern portfolio theory) — разработанная Гарри Марковицем методика формирования инвестиционного портфеля, направленная на оптимальный выбор активов, исходя из требуемого соотношения доходность/риск. Сформулированные им в 1950-х годах идеи составляют основу современной портфельной теории. . Основные положения портфельной теории были сформулированы Гарри Марковицем при подготовке им докторской диссертации в 1950—1951 годах. . Рождением же портфельной теории Марковица считается опубликованная в «Финансовом журнале» в 1952 году статья «Выбор портфеля». В ней он впервые предложил математическую модель формирования оптимального портфеля и привёл методы построения портфелей при определённых условиях. Основная заслуга Марковица состояла в предложении вероятностной формализации понятий «доходность» и «риск», что позволило перевести задачу выбора оптимального портфеля на формальный математический язык. Надо отметить, что в годы создания теории Марковиц работал в RAND Corp., вместе с одним из основателей линейной и нелинейной оптимизации — Джорджем Данцигом и сам участвовал в решении указанных задач. Поэтому собственная теория, после необходимой формализации, хорошо ложилась в указанное русло. . Марковиц постоянно занимается усовершенствованием своей теории и в 1959 году выпускает первую посвящённую ей монографию «Выбор портфеля: эффективная диверсификация инвестиций». . В 1990 году, когда Марковицу вручают Нобелевскую премию, выходит книга «Средне-дисперсионный анализ при выборе портфеля и рынка капитала» ссылка. . &#1054;&#1089;&#1085;&#1086;&#1074;&#1072; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . Ожидаемая доходность портфеля(Portfolio Expected Return) | Ожидаемая доходность портфеля будет зависеть от ожидаемой доходности каждого из активов, входящих в него. Такой подход позволяет снизить риск за счет диверсификации и одновременно максимизировать доход инвестора, поскольку убытки по одним инвестициям будут компенсированы доходом по другим. . Ожидаемая доходность портфеля представляет собой суммарную ожидаемую доходность входящих в него ценных бумаг, взвешенную с учетом их доли в портфеле. . $$ E(R_{p}) = sum_{i=1}^nw_{i}E(R_{i}) $$ 2. &#1044;&#1080;&#1089;&#1087;&#1077;&#1088;&#1089;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; (Portfolio Variance ) . Дисперсия портфеля - это процесс, который определяет степень риска или волатильности, связанной с инвестиционным портфелем. Основная формула для расчета этой дисперсии фокусируется на взаимосвязи между так называемой дисперсией доходности и ковариацией, связанной с каждой из ценных бумаг, найденных в портфеле, а также с процентом или частью портфеля, который представляет каждая ценная бумага. . $$ sigma_{p}^{2} = sum_{i}^{} omega_{i}^{2} sigma_{i}^{2}+ sum_{i}^{} sum_{j neq i}^{} omega_{i}^{} omega_{j}^{} sigma_{i}^{} sigma_{j}^{} rho_{ij} $$ 3. &#1050;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; &#1064;&#1072;&#1088;&#1087;&#1072; (Sharpe Ratio) . Коэффициент Шарпа измеряет доходность инвестиций по отношению к безрисковой ставке (казначейской ставке) и степени риска. В целом, более высокое значение коэффициента Шарпа указывает на лучшие и более прибыльные инвестиции. Таким образом, если сравнивать два портфеля с одинаковыми рисками, то при прочих равных условиях было бы лучше инвестировать в портфель с более высоким коэффициентом Шарпа. . $$ frac{R_{p} - R_{f}}{ sigma_{p}} $$$$R_{p} - доходность портфеля R_{f} - безрисковая ставка sigma_{p} - стандартное отклонение доходности портфеля$$ 4. &#1069;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1072;&#1103; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094;&#1072; (The Efficient Frontier ) . Определение и рисунок из Википедии: . Граница эффективности (англ. Efficient frontier) в портфельной теории Марковица — инвестиционный портфель, оптимизированный в отношении риска и доходности. Формально границей эффективности является набор портфелей, удовлетворяющих такому условию, что не существует другого портфеля с более высокой ожидаемой доходностью, но с таким же стандартным отклонением доходности. Понятие границы эффективности было впервые сформулировано Гарри Марковицем в 1952 году в модели Марковица. . Портфель может быть охарактеризован как «эффективный», если он имеет максимально возможный ожидаемый уровень доходности для своего уровня риска (который представлен стандартным отклонением доходности портфеля). Так, на график соотношения риска и доходности может быть нанесена любая возможная комбинация рискованных активов, и совокупность всех таких возможных портфелей определяет регион в этом пространстве. При отсутствии в портфеле безрискового актива граница эффективности определяется верхней (восходящей) частью гиперболы, ограничивающей область допустимых решений для всех соотношений активов в портфеле. . В случае же, если в портфель может быть включён безрисковый актив, граница эффективности вырождается в отрезок прямой линии, исходящий от значения доходности безрискового актива на оси ординат (ожидаемая доходность портфеля) и проходящий по касательной к границе области допустимых решений. Все портфели на отрезке между собственно безрисковым активом и точкой касания состоят из комбинации безрискового актива и рисковых активов, в то время как все портфели на линии выше и справа от точки касания образуются короткой позицией в безрисковом активе и инвестированием в рисковые активы. . . . &#1054;&#1087;&#1090;&#1080;&#1084;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; &#1085;&#1072; Python . 1. &#1048;&#1084;&#1087;&#1086;&#1088;&#1090; &#1085;&#1077;&#1086;&#1073;&#1093;&#1086;&#1076;&#1080;&#1084;&#1099;&#1093; &#1073;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1082; . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import matplotlib.pyplot as plt import numpy as np import pandas as pd import pandas_datareader as web from matplotlib.ticker import FuncFormatter . Непосредственно для анализа и оптимизации портфеля существует библиотека PyPortfolioOpt. Так как она не входит в стандартный набор, то ее необходимо установить. . !pip install PyPortfolioOpt #Installing the Portfolio Optimzation Library . Collecting PyPortfolioOpt Downloading https://files.pythonhosted.org/packages/46/55/7d39d78d554ee33a7317e345caf01339da11406c28f18bc48794fe967935/PyPortfolioOpt-1.4.1-py3-none-any.whl (56kB) |████████████████████████████████| 61kB 3.2MB/s Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.1.5) Requirement already satisfied: scipy&lt;2.0,&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.4.1) Collecting cvxpy&lt;2.0.0,&gt;=1.1.10 Downloading https://files.pythonhosted.org/packages/83/47/fd1e818b8da30ef18695a0fbf9b66611ab18506f0a44fc69480a75f4db1b/cvxpy-1.1.12.tar.gz (1.3MB) |████████████████████████████████| 1.3MB 7.9MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: numpy&lt;2.0,&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2018.9) Requirement already satisfied: ecos&gt;=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.0.7.post1) Requirement already satisfied: scs&gt;=1.1.6 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.1.3) Requirement already satisfied: osqp&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.6.2.post0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.19-&gt;PyPortfolioOpt) (1.15.0) Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp&gt;=0.4.1-&gt;cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.1.5.post0) Building wheels for collected packages: cvxpy Building wheel for cvxpy (PEP 517) ... done Created wheel for cvxpy: filename=cvxpy-1.1.12-cp37-cp37m-linux_x86_64.whl size=2731641 sha256=2c888a76787438c69d6a1dce26762a30ead689ee8b9da895efc81ad29620fbdf Stored in directory: /root/.cache/pip/wheels/9b/62/55/1da181c05c710c5d99bd560edebec3bd6a61cb69acef9dc00e Successfully built cvxpy Installing collected packages: cvxpy, PyPortfolioOpt Found existing installation: cvxpy 1.0.31 Uninstalling cvxpy-1.0.31: Successfully uninstalled cvxpy-1.0.31 Successfully installed PyPortfolioOpt-1.4.1 cvxpy-1.1.12 . Импортируем функции для дальнейшей работы: . from pypfopt.efficient_frontier import EfficientFrontier from pypfopt import risk_models from pypfopt import expected_returns from pypfopt.cla import CLA import pypfopt.plotting as pplt from matplotlib.ticker import FuncFormatter from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices . 2. &#1055;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1087;&#1086; &#1072;&#1082;&#1094;&#1080;&#1103;&#1084; &#1080;&#1079; &#1080;&#1085;&#1090;&#1077;&#1088;&#1085;&#1077;&#1090;&#1072; . Сначало установим опять пакет, который не входит в стандартный набор. Он позволяет получить данные по акциям с сайтя yahoo. . Тикеры, которые будут использоваться для анализа - одна из компаний входящих в лидеры в своем секторе. . !pip install yfinance --upgrade --no-cache-dir . Collecting yfinance Downloading https://files.pythonhosted.org/packages/a7/ee/315752b9ef281ba83c62aa7ec2e2074f85223da6e7e74efb4d3e11c0f510/yfinance-0.1.59.tar.gz Requirement already satisfied, skipping upgrade: pandas&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0) Requirement already satisfied, skipping upgrade: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB) |████████████████████████████████| 6.3MB 6.4MB/s Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... done Created wheel for yfinance: filename=yfinance-0.1.59-py2.py3-none-any.whl size=23442 sha256=519c6bb89355fc0fab0d0a1c7f12df703543e4aadbde996b33dcf9592621bb6e Stored in directory: /tmp/pip-ephem-wheel-cache-weglluyo/wheels/f8/2a/0f/4b5a86e1d52e451757eb6bc17fd899629f0925c777741b6d04 Successfully built yfinance Installing collected packages: lxml, yfinance Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.3 yfinance-0.1.59 . import yfinance as yf tickers = [&#39;LKOH.ME&#39;,&#39;GMKN.ME&#39;, &#39;DSKY.ME&#39;, &#39;NKNC.ME&#39;, &#39;MTSS.ME&#39;, &#39;IRAO.ME&#39;, &#39;SBER.ME&#39;, &#39;AFLT.ME&#39;] df_stocks= yf.download(tickers, start=&#39;2018-01-01&#39;, end=&#39;2020-12-31&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . df_stocks.head() . AFLT.ME DSKY.ME GMKN.ME IRAO.ME LKOH.ME MTSS.ME NKNC.ME SBER.ME . Date . 2018-01-03 127.199066 | 70.177948 | 8249.352539 | 3.025520 | 2844.152100 | 197.995163 | 33.301483 | 145.441605 | . 2018-01-04 134.899857 | 71.621933 | 8455.913086 | 3.181567 | 2910.237305 | 202.738434 | 33.173889 | 149.769119 | . 2018-01-05 133.450317 | 71.621933 | 8441.316406 | 3.160166 | 2967.178467 | 202.199417 | 33.237682 | 149.643677 | . 2018-01-09 136.349426 | 71.116539 | 8521.605469 | 3.103989 | 3016.222900 | 203.421158 | 33.556660 | 150.772598 | . 2018-01-10 135.262268 | 71.658035 | 8507.006836 | 3.087939 | 3026.613525 | 204.427307 | 33.811848 | 149.116821 | . Дальше необходимо проверить есть ли среди полученных значений NaN. В случае их наличия они будут мешать дальнейшему исследованию. Для того, чтобы это решить, необходимо рассмотреть или иную акцию, или заменить их для примера средней ценой между днем до и после значения NaN. . nullin_df = pd.DataFrame(df_stocks,columns=tickers) print(nullin_df.isnull().sum()) . LKOH.ME 0 GMKN.ME 0 DSKY.ME 0 NKNC.ME 0 MTSS.ME 0 IRAO.ME 0 SBER.ME 0 AFLT.ME 0 dtype: int64 . 3. &#1056;&#1072;&#1089;&#1095;&#1077;&#1090;&#1099; . Перейдем к расчетам по оптимизации портфеля и начнем с определения ожидаемой доходности и дисперсии портфеля. Далее сохраним значения весов портфеля с максимальным коэффициентом Шарпа и минимальной диспрсией. . # mu = expected_returns.mean_historical_return(df_stocks) # #Sample Variance of Portfolio # Sigma = risk_models.sample_cov(df_stocks) # #Max Sharpe Ratio - Tangent to the EF # ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks # sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks # sharpe_pwt=ef.clean_weights() # print(sharpe_pwt) #Годовая доходность mu = expected_returns.mean_historical_return(df_stocks) #Дисперсия портфеля Sigma = risk_models.sample_cov(df_stocks) . ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks sharpe_pwt=ef.clean_weights() print(sharpe_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.0), (&#39;DSKY.ME&#39;, 0.22606), (&#39;GMKN.ME&#39;, 0.48796), (&#39;IRAO.ME&#39;, 0.0), (&#39;LKOH.ME&#39;, 0.0), (&#39;MTSS.ME&#39;, 0.02953), (&#39;NKNC.ME&#39;, 0.25645), (&#39;SBER.ME&#39;, 0.0)]) . Необходимо обратить внимание, что если изменить weight_bounds=(0,1) на weight_bounds=(-1,1), то в портфеле будут учитываться и короткие позиции по акциям. . Дальше посмотрим общие характеристики по портфелю. . ef.portfolio_performance(verbose=True) . Expected annual return: 37.1% Annual volatility: 20.7% Sharpe Ratio: 1.70 . (0.37123023494063007, 0.20717177784552962, 1.695357536597058) . Теперь посмотрим портфель с минимальной волатильностью: . ef1 = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks minvol=ef1.min_volatility() minvol_pwt=ef1.clean_weights() print(minvol_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.02876), (&#39;DSKY.ME&#39;, 0.24503), (&#39;GMKN.ME&#39;, 0.10403), (&#39;IRAO.ME&#39;, 0.0938), (&#39;LKOH.ME&#39;, 0.01168), (&#39;MTSS.ME&#39;, 0.41967), (&#39;NKNC.ME&#39;, 0.09704), (&#39;SBER.ME&#39;, 0.0)]) . ef1.portfolio_performance(verbose=True, risk_free_rate = 0.27) . Expected annual return: 24.0% Annual volatility: 16.9% Sharpe Ratio: -0.18 . (0.239915644698749, 0.16885732511472468, -0.17816434839774456) . 4. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1075;&#1088;&#1072;&#1092;&#1080;&#1082;&#1072; &#1101;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1099;&#1093; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094; . Заключительным шагом является построение эффективной границы для визуального представления и расчет распределения активов. Тут встречается одна сложность, решить которую пока что мне не удалось - пакет создан для анализа в долларах и в результате в выводе присутствует их обозначение. Но с другой стороны наличия значка &quot;$&quot; не должно сильно мешать. . Анализ произведем для суммы в 100 000 рублей. . cl_obj = CLA(mu, Sigma) ax = pplt.plot_efficient_frontier(cl_obj, showfig = False) ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: &#39;{:.0%}&#39;.format(x))) ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: &#39;{:.0%}&#39;.format(y))) . Первым этапом посчитаем портфель с минимальной волатильностью: . latest_prices = get_latest_prices(df_stocks) # Allocate Portfolio Value in $ as required to show number of shares/stocks to buy, also bounds for shorting will affect allocation #Min Volatility Portfolio Allocation $10000 allocation_minv, rem_minv = DiscreteAllocation(minvol_pwt, latest_prices, total_portfolio_value=100000).lp_portfolio() print(allocation_minv) print(&quot;Leftover Fund value in$ after building minimum volatility portfolio is ${:.2f}&quot;.format(rem_minv)) print(&quot;Осталось денежных средств после построения портфеля с минимальной волатильностью составляет {:.2f} рублей&quot;.format(rem_minv)) print() . {&#39;AFLT.ME&#39;: 41, &#39;DSKY.ME&#39;: 181, &#39;IRAO.ME&#39;: 1765, &#39;LKOH.ME&#39;: 1, &#39;MTSS.ME&#39;: 127, &#39;NKNC.ME&#39;: 107} Leftover Fund value in$ after building minimum volatility portfolio is $6152.03 Осталось денежных средств после построения портфеля с минимальной волатильностью составляет 6152.03 рублей . Вторым шагом портфель с максимальным коэффициентом Шарпа: . latest_prices1 = get_latest_prices(df_stocks) allocation_shp, rem_shp = DiscreteAllocation(sharpe_pwt, latest_prices1, total_portfolio_value=100000).lp_portfolio() print(allocation_shp) print(&quot;Leftover Fund value in$ after building Max Sharpe ratio portfolio is ${:.2f}&quot;.format(rem_shp)) print(&quot;Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа {:.2f} рублей&quot;.format(rem_shp)) #allocation using integer programming via PyPortfolioOpt User Guide #Alex Putkov code used for guidance and reference in applying integer programming . {&#39;DSKY.ME&#39;: 167, &#39;GMKN.ME&#39;: 2, &#39;MTSS.ME&#39;: 9, &#39;NKNC.ME&#39;: 283} Leftover Fund value in$ after building Max Sharpe ratio portfolio is $1319.05 Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа 1319.05 рублей . В результтате нам предлагается купить для оптимального портфеля 167 акций Детского мира, 2 акции Норильского никеля, 9 акций МТС и 283 акцию Нижнекамскнефтехим. В результате у нас еще останется 1319 рублей. .",
            "url": "https://zmey56.github.io/blog//russian/python/algotrading/portfolio/optimization/2021/05/16/automating-portfolio-optimization.html",
            "relUrl": "/russian/python/algotrading/portfolio/optimization/2021/05/16/automating-portfolio-optimization.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Russian - Использование API Fmp Cloud для отбора акций по дивидендам на Nasdaq с помощью Python",
            "content": "&#1048;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077; API Fmp Cloud &#1076;&#1083;&#1103; &#1086;&#1090;&#1073;&#1086;&#1088;&#1072; &#1072;&#1082;&#1094;&#1080;&#1081; &#1087;&#1086; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1076;&#1072;&#1084; &#1085;&#1072; Nasdaq &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Акции с высокой дивидентной доходностью часто являются отличной инвестиционной стратегией для инвесторов, стремящихся получать приток денежных средств каждый год. В данной статье буден создан скрипт на Python для отбора их на бирже NASDAQ. . &#1063;&#1090;&#1086; &#1090;&#1072;&#1082;&#1086;&#1077; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1090;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100;? . Возьму определение из Википедии. Дивиде́ндная дохо́дность (англ. dividend yield) — это отношение величины годового дивиденда на акцию к цене акции. Данная величина выражается чаще всего в процентах. . Пример . При цене акции ОАО «Лукойл» 1124,37 рублей и дивиденде 28 рублей на акцию дивидендная доходность будет равна: . . Так же необходимо обратить внимание, что многие растущие компании, такие как для примера Amazon и Yandex, не выплачивают дивиденды, поскольку они реинвестируют всю прибыль в развитие бизнеса. Поэтому дивидендная доходность для этих фирм будет равна нулю. . &#1056;&#1072;&#1089;&#1095;&#1077;&#1090; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1076;&#1085;&#1086;&#1081; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1080; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Расчет дивидендной доходности является простой задачей, которую можно выполнить с помощью финансового API под названием fmpcloud и Python. Этот API предлагает несколько бесплатных запросов в день после регистрации. . Первым делом нужно извлечь список тикеров для всех акций, торгующихся на Nasdaq, по которым собираемся рассчитать дивидендную доходность. | import requests demo = &#39;39b9d9eeb3ba3fe57e039284db7ed2c0&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] for ticker in tickers: symbols.append(ticker[&#39;symbol&#39;]) print(symbols) #[&#39;SMMCW&#39;, &#39;VOD&#39;, &#39;TRMD&#39;, &#39;TRMB&#39;, &#39;NBL&#39;, &#39;EMMA&#39;,...] . TypeError Traceback (most recent call last) &lt;ipython-input-25-1a9ccd9b857e&gt; in &lt;module&gt;() 8 symbols = [] 9 for ticker in tickers: &gt; 10 symbols.append(ticker[&#39;symbol&#39;]) 11 print(symbols) 12 #[&#39;SMMCW&#39;, &#39;VOD&#39;, &#39;TRMD&#39;, &#39;TRMB&#39;, &#39;NBL&#39;, &#39;EMMA&#39;,...] TypeError: string indices must be integers . companydata = requests.get(f&#39;https://fmpcloud.io/api/v3/profile/SMTA?apikey={demo}&#39;) companydata = companydata.json() . companydata . {&#39;Error Message&#39;: &#39;Limit Reach . Please upgrade your plan or visit our documentation for more details at https://fmpcloud.io/plans &#39;} . https://fmpcloud.io/api/v3/profile/AAPL?apikey=39b9d9eeb3ba3fe57e039284db7ed2c0 . File &#34;&lt;ipython-input-26-eff50d69cb6e&gt;&#34;, line 1 https://fmpcloud.io/api/v3/profile/AAPL?apikey=39b9d9eeb3ba3fe57e039284db7ed2c0 ^ SyntaxError: invalid syntax . len(symbols) . 5261 . &quot;TBK&quot; in symbols . False . После необходимо пройтись по полученому списку акций и получить финансовую информацию по компании. Так же необходимо понимать, что получаем только последние данные, а не за все время существование компании. | DivYield = {} for company in symbols: try: companydata = requests.get(f&#39;https://fmpcloud.io/api/v3/profile/{company}?apikey={demo}&#39;) companydata = companydata.json() latest_Annual_Dividend = companydata[0][&#39;lastDiv&#39;] price = companydata[0][&#39;price&#39;] market_Capitalization = companydata[0][&#39;mktCap&#39;] name = companydata[0][&#39;companyName&#39;] exchange = companydata[0][&#39;exchange&#39;] dividend_Yield= latest_Annual_Dividend/price DivYield[company] = {} DivYield[company][&#39;Dividend_Yield&#39;] = dividend_Yield DivYield[company][&#39;latest_Price&#39;] = price DivYield[company][&#39;latest_Dividend&#39;] = latest_Annual_Dividend DivYield[company][&#39;market_Capit_in_M&#39;] = market_Capitalization/1000000 DivYield[company][&#39;company_Name&#39;] = name DivYield[company][&#39;exchange&#39;] = exchange except: pass print(DivYield) . {&#39;ATEN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 9.25, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 712.34336, &#39;company_Name&#39;: &#39;A10 Networks Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ABM&#39;: {&#39;Dividend_Yield&#39;: 0.01836213668499607, &#39;latest_Price&#39;: 50.92, &#39;latest_Dividend&#39;: 0.935, &#39;market_Capit_in_M&#39;: 3416.20736, &#39;company_Name&#39;: &#39;ABM Industries Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;EMD&#39;: {&#39;Dividend_Yield&#39;: 0.0870722433460076, &#39;latest_Price&#39;: 13.15, &#39;latest_Dividend&#39;: 1.145, &#39;market_Capit_in_M&#39;: 798.80986, &#39;company_Name&#39;: &#39;Western Asset Emerging Markets Debt Fund Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NM-PG&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 12.2, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 77.285168, &#39;company_Name&#39;: &#39;Navios Maritime Holdings Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ARCH&#39;: {&#39;Dividend_Yield&#39;: 0.01232134056185313, &#39;latest_Price&#39;: 40.58, &#39;latest_Dividend&#39;: 0.5, &#39;market_Capit_in_M&#39;: 621.55981, &#39;company_Name&#39;: &#39;Arch Resources Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CPL&#39;: {&#39;Dividend_Yield&#39;: 0.013637500000000002, &#39;latest_Price&#39;: 17.36, &#39;latest_Dividend&#39;: 0.236747, &#39;market_Capit_in_M&#39;: 10258.0419, &#39;company_Name&#39;: &#39;CPFL Energia S.A. American Depositary Shares&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MFGP&#39;: {&#39;Dividend_Yield&#39;: 0.09148698884758365, &#39;latest_Price&#39;: 8.07, &#39;latest_Dividend&#39;: 0.7383000000000001, &#39;market_Capit_in_M&#39;: 2676.69786, &#39;company_Name&#39;: &#39;Micro Focus Intl PLC ADS each representing One Ord Sh&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FMO&#39;: {&#39;Dividend_Yield&#39;: 0.09384775808133473, &#39;latest_Price&#39;: 9.59, &#39;latest_Dividend&#39;: 0.9, &#39;market_Capit_in_M&#39;: 67.97536, &#39;company_Name&#39;: &#39;Fiduciary/Claymore Energy Infrastructure Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BEN&#39;: {&#39;Dividend_Yield&#39;: 0.04672578444747613, &#39;latest_Price&#39;: 29.32, &#39;latest_Dividend&#39;: 1.37, &#39;market_Capit_in_M&#39;: 14818.4453, &#39;company_Name&#39;: &#39;Franklin Resources Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;A&#39;: {&#39;Dividend_Yield&#39;: 0.00744126373185791, &#39;latest_Price&#39;: 124.71, &#39;latest_Dividend&#39;: 0.9279999999999999, &#39;market_Capit_in_M&#39;: 37998.887, &#39;company_Name&#39;: &#39;Agilent Technologies Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BCX&#39;: {&#39;Dividend_Yield&#39;: 0.05553047404063205, &#39;latest_Price&#39;: 8.86, &#39;latest_Dividend&#39;: 0.49199999999999994, &#39;market_Capit_in_M&#39;: 782.47974, &#39;company_Name&#39;: &#39;BlackRock Resources and Commodities Strategy Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MOON&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 37.89, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Direxion Moonshot Innovators ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;AMTR&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 34.6748, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;ETRACS Alerian Midstream Energy Total Return Index ETN&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;ACI&#39;: {&#39;Dividend_Yield&#39;: 0.010070493454179255, &#39;latest_Price&#39;: 19.86, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 9245.4851, &#39;company_Name&#39;: &#39;Albertsons Companies, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ET&#39;: {&#39;Dividend_Yield&#39;: 0.11878517519516558, &#39;latest_Price&#39;: 7.7114, &#39;latest_Dividend&#39;: 0.9159999999999999, &#39;market_Capit_in_M&#39;: 20847.6938, &#39;company_Name&#39;: &#39;Energy Transfer LP&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SID&#39;: {&#39;Dividend_Yield&#39;: 0.00031323414252153485, &#39;latest_Price&#39;: 6.385, &#39;latest_Dividend&#39;: 0.002, &#39;market_Capit_in_M&#39;: 8908.6075, &#39;company_Name&#39;: &#39;Companhia Siderúrgica Nacional&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JNPR&#39;: {&#39;Dividend_Yield&#39;: 0.03154574132492114, &#39;latest_Price&#39;: 25.36, &#39;latest_Dividend&#39;: 0.8, &#39;market_Capit_in_M&#39;: 8322.4422, &#39;company_Name&#39;: &#39;Juniper Networks Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 88.55, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 3264.90061, &#39;company_Name&#39;: &#39;Fabrinet&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ICL&#39;: {&#39;Dividend_Yield&#39;: 0.03030508819884212, &#39;latest_Price&#39;: 5.8901, &#39;latest_Dividend&#39;: 0.1785, &#39;market_Capit_in_M&#39;: 7426.0029, &#39;company_Name&#39;: &#39;Icl Group Ltd&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;IR&#39;: {&#39;Dividend_Yield&#39;: 0.01056934888822415, &#39;latest_Price&#39;: 50.145, &#39;latest_Dividend&#39;: 0.53, &#39;market_Capit_in_M&#39;: 20998.9714, &#39;company_Name&#39;: &#39;Ingersoll Rand Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;HT&#39;: {&#39;Dividend_Yield&#39;: 0.025442980463425715, &#39;latest_Price&#39;: 11.005, &#39;latest_Dividend&#39;: 0.28, &#39;market_Capit_in_M&#39;: 427.472736, &#39;company_Name&#39;: &#39;Hersha Hospitality Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;DFIN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 27.59, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 918.56218, &#39;company_Name&#39;: &#39;Donnelley Financial Solutions Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BLK&#39;: {&#39;Dividend_Yield&#39;: 0.019933113918675017, &#39;latest_Price&#39;: 753.52, &#39;latest_Dividend&#39;: 15.02, &#39;market_Capit_in_M&#39;: 115012.772, &#39;company_Name&#39;: &#39;BlackRock Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FSD&#39;: {&#39;Dividend_Yield&#39;: 0.08770822397491015, &#39;latest_Price&#39;: 15.0499, &#39;latest_Dividend&#39;: 1.3200000000000003, &#39;market_Capit_in_M&#39;: 503.721664, &#39;company_Name&#39;: &#39;First Trust High Income Long/Short Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;HCXZ&#39;: {&#39;Dividend_Yield&#39;: 0.05181674565560822, &#39;latest_Price&#39;: 25.32, &#39;latest_Dividend&#39;: 1.312, &#39;market_Capit_in_M&#39;: 1318.60224, &#39;company_Name&#39;: &#39;Hercules Capital, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;HCXY&#39;: {&#39;Dividend_Yield&#39;: 0.05792592592592593, &#39;latest_Price&#39;: 27.0, &#39;latest_Dividend&#39;: 1.564, &#39;market_Capit_in_M&#39;: 1425.59731, &#39;company_Name&#39;: &#39;Hercules Capital, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ACTV&#39;: {&#39;Dividend_Yield&#39;: 0.0009788758589635664, &#39;latest_Price&#39;: 35.7553, &#39;latest_Dividend&#39;: 0.035, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;LeaderShares Activist Leaders ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;MDLX&#39;: {&#39;Dividend_Yield&#39;: 0.7320394960844399, &#39;latest_Price&#39;: 2.3496, &#39;latest_Dividend&#39;: 1.72, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Medley LLC 6.875% Senior Notes due 2026&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BMA&#39;: {&#39;Dividend_Yield&#39;: 0.16821241585639493, &#39;latest_Price&#39;: 13.37, &#39;latest_Dividend&#39;: 2.249, &#39;market_Capit_in_M&#39;: 1366.80166, &#39;company_Name&#39;: &#39;Banco Macro S.A. ADR (representing Ten Class B)&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;EROS&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 2.56, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 453.83168, &#39;company_Name&#39;: &#39;Eros International PLC&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;USA&#39;: {&#39;Dividend_Yield&#39;: 0.08569545154911008, &#39;latest_Price&#39;: 7.585, &#39;latest_Dividend&#39;: 0.65, &#39;market_Capit_in_M&#39;: 1646.64282, &#39;company_Name&#39;: &#39;Liberty All-Star Equity Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KT&#39;: {&#39;Dividend_Yield&#39;: 0.03662496909765142, &#39;latest_Price&#39;: 12.135, &#39;latest_Dividend&#39;: 0.444444, &#39;market_Capit_in_M&#39;: 5782.3642, &#39;company_Name&#39;: &#39;KT Corporation&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MDLQ&#39;: {&#39;Dividend_Yield&#39;: 0.7199203187250998, &#39;latest_Price&#39;: 2.51, &#39;latest_Dividend&#39;: 1.8070000000000002, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Medley LLC 7.25% Notes due 2024&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;STZ.B&#39;: {&#39;Dividend_Yield&#39;: 0.0794392523364486, &#39;latest_Price&#39;: 8.56, &#39;latest_Dividend&#39;: 0.68, &#39;market_Capit_in_M&#39;: 1148.91482, &#39;company_Name&#39;: &#39;Constellation Brands, Inc&#39;, &#39;exchange&#39;: &#39;YHD&#39;}, &#39;MCV&#39;: {&#39;Dividend_Yield&#39;: 0.046052104208416836, &#39;latest_Price&#39;: 24.95, &#39;latest_Dividend&#39;: 1.149, &#39;market_Capit_in_M&#39;: 1359.13139, &#39;company_Name&#39;: &#39;Medley Capital Corporation 6.125% Senior Notes due 2023&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KBH&#39;: {&#39;Dividend_Yield&#39;: 0.01052400789300592, &#39;latest_Price&#39;: 45.61, &#39;latest_Dividend&#39;: 0.48, &#39;market_Capit_in_M&#39;: 4508.5809, &#39;company_Name&#39;: &#39;KB Home&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AAN&#39;: {&#39;Dividend_Yield&#39;: 0.008978451715881885, &#39;latest_Price&#39;: 25.06, &#39;latest_Dividend&#39;: 0.22500000000000003, &#39;market_Capit_in_M&#39;: 857.17478, &#39;company_Name&#39;: &#34;Aaron&#39;s Inc&#34;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;IQV&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 190.87, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 36602.569, &#39;company_Name&#39;: &#39;IQVIA Holdings Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KEG&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 104.14, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 35159.642, &#39;company_Name&#39;: &#39;Key Energy Services Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KEM&#39;: {&#39;Dividend_Yield&#39;: 0.00735023888276369, &#39;latest_Price&#39;: 27.21, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 1592.01357, &#39;company_Name&#39;: &#39;KEMET Corporation&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;WYND&#39;: {&#39;Dividend_Yield&#39;: 0.027053140096618356, &#39;latest_Price&#39;: 51.75, &#39;latest_Dividend&#39;: 1.4, &#39;market_Capit_in_M&#39;: 4443.3219, &#39;company_Name&#39;: &#39;Wyndham Destinations Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CLPR&#39;: {&#39;Dividend_Yield&#39;: 0.04804045512010114, &#39;latest_Price&#39;: 7.91, &#39;latest_Dividend&#39;: 0.38, &#39;market_Capit_in_M&#39;: 127.059912, &#39;company_Name&#39;: &#39;Clipper Realty Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MZA&#39;: {&#39;Dividend_Yield&#39;: 0.04249311294765841, &#39;latest_Price&#39;: 14.52, &#39;latest_Dividend&#39;: 0.6170000000000001, &#39;market_Capit_in_M&#39;: 67.338536, &#39;company_Name&#39;: &#39;Blackrock Muniyield Arizona Fund Inc&#39;, &#39;exchange&#39;: &#39;NYSE American&#39;}, &#39;ACA&#39;: {&#39;Dividend_Yield&#39;: 0.0031883434164693882, &#39;latest_Price&#39;: 62.7285, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 3021.93306, &#39;company_Name&#39;: &#39;Arcosa Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TIF&#39;: {&#39;Dividend_Yield&#39;: 0.015664386028087864, &#39;latest_Price&#39;: 111.08, &#39;latest_Dividend&#39;: 1.7399999999999998, &#39;market_Capit_in_M&#39;: 116379.902, &#39;company_Name&#39;: &#39;Tiffany &amp; Co&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RRTS&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 5.475, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 207.461984, &#39;company_Name&#39;: &#39;Roadrunner Transportation Systems Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GNL&#39;: {&#39;Dividend_Yield&#39;: 0.08712129462243809, &#39;latest_Price&#39;: 18.3652, &#39;latest_Dividend&#39;: 1.6, &#39;market_Capit_in_M&#39;: 1663.41875, &#39;company_Name&#39;: &#39;Global Net Lease Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BF.B&#39;: {&#39;Dividend_Yield&#39;: 0.03376635967038294, &#39;latest_Price&#39;: 10.315, &#39;latest_Dividend&#39;: 0.3483, &#39;market_Capit_in_M&#39;: 94.31324, &#39;company_Name&#39;: &#39;Brown-Forman Corp&#39;, &#39;exchange&#39;: &#39;YHD&#39;}, &#39;ESBA&#39;: {&#39;Dividend_Yield&#39;: 0.009400179051029543, &#39;latest_Price&#39;: 11.17, &#39;latest_Dividend&#39;: 0.105, &#39;market_Capit_in_M&#39;: 3149.66093, &#39;company_Name&#39;: &#39;Empire State Realty OP LP&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;NEWM&#39;: {&#39;Dividend_Yield&#39;: 0.06493506493506493, &#39;latest_Price&#39;: 29.26, &#39;latest_Dividend&#39;: 1.9, &#39;market_Capit_in_M&#39;: 7005.6346, &#39;company_Name&#39;: &#39;New Media Investment Group Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GDI&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 32.79, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 6728.9016, &#39;company_Name&#39;: &#39;Gardner Denver Holdings Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;EAB&#39;: {&#39;Dividend_Yield&#39;: 0.02450278440731901, &#39;latest_Price&#39;: 25.14, &#39;latest_Dividend&#39;: 0.616, &#39;market_Capit_in_M&#39;: 1181.08224, &#39;company_Name&#39;: &#39;Entergy Arkansas Inc. First Mortgage Bonds 4.90% Series Due December 1 2052&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;EAE&#39;: {&#39;Dividend_Yield&#39;: 0.023756466374850776, &#39;latest_Price&#39;: 25.13, &#39;latest_Dividend&#39;: 0.597, &#39;market_Capit_in_M&#39;: 1180.61235, &#39;company_Name&#39;: &#39;Entergy Arkansas Inc. First Mortgage Bonds 4.75% Series due June 1 2063&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;VCV&#39;: {&#39;Dividend_Yield&#39;: 0.04409199048374306, &#39;latest_Price&#39;: 12.61, &#39;latest_Dividend&#39;: 0.5559999999999999, &#39;market_Capit_in_M&#39;: 603.58144, &#39;company_Name&#39;: &#39;Invesco California Value Municipal Income Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CXO&#39;: {&#39;Dividend_Yield&#39;: 0.009146341463414637, &#39;latest_Price&#39;: 65.6, &#39;latest_Dividend&#39;: 0.6000000000000001, &#39;market_Capit_in_M&#39;: 12876.9526, &#39;company_Name&#39;: &#39;Concho Resources Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MOSC&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 9.9, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 426.937472, &#39;company_Name&#39;: &#39;Mosaic Acquisition Corp. Class A&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PRI&#39;: {&#39;Dividend_Yield&#39;: 0.011253369272237196, &#39;latest_Price&#39;: 148.4, &#39;latest_Dividend&#39;: 1.67, &#39;market_Capit_in_M&#39;: 5865.8063, &#39;company_Name&#39;: &#39;Primerica Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CBX&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 2.7, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;CBX&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AWK&#39;: {&#39;Dividend_Yield&#39;: 0.015019115237575098, &#39;latest_Price&#39;: 146.48, &#39;latest_Dividend&#39;: 2.2, &#39;market_Capit_in_M&#39;: 26577.1848, &#39;company_Name&#39;: &#39;American Water Works Company Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GIG&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 11.15, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 129.746968, &#39;company_Name&#39;: &#39;GigCapital Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;IRET&#39;: {&#39;Dividend_Yield&#39;: 0.03976143141153081, &#39;latest_Price&#39;: 70.42, &#39;latest_Dividend&#39;: 2.8, &#39;market_Capit_in_M&#39;: 913.76288, &#39;company_Name&#39;: &#39;Investors Real Estate Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TNP&#39;: {&#39;Dividend_Yield&#39;: 0.021052631578947368, &#39;latest_Price&#39;: 9.5, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 178.419504, &#39;company_Name&#39;: &#39;Tsakos Energy Navigation Ltd&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;IF&#39;: {&#39;Dividend_Yield&#39;: 0.009287343532684284, &#39;latest_Price&#39;: 7.19, &#39;latest_Dividend&#39;: 0.066776, &#39;market_Capit_in_M&#39;: 381.07, &#39;company_Name&#39;: &#39;Aberdeen Indonesia Fund Inc. (Common Stock&#39;, &#39;exchange&#39;: &#39;NYSE American&#39;}, &#39;CRK&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 5.64, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1310.79808, &#39;company_Name&#39;: &#39;Comstock Resources Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ICAN&#39;: {&#39;Dividend_Yield&#39;: 0.015962213796735125, &#39;latest_Price&#39;: 28.485, &#39;latest_Dividend&#39;: 0.45468366, &#39;market_Capit_in_M&#39;: 4.27275, &#39;company_Name&#39;: &#39;SerenityShares Impact&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;GNC&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 6.27, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 6985.0936, &#39;company_Name&#39;: &#39;GNC Holdings Inc. Class A&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ISG&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 24.85, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 96534.79, &#39;company_Name&#39;: &#39;ING Group N.V. Perpetual Dent Secs 6.125%&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FTK&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 1.8, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 130.586944, &#39;company_Name&#39;: &#39;Flotek Industries Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AJXA&#39;: {&#39;Dividend_Yield&#39;: 0.07020534676481985, &#39;latest_Price&#39;: 25.81, &#39;latest_Dividend&#39;: 1.812, &#39;market_Capit_in_M&#39;: 459.0, &#39;company_Name&#39;: &#39;Great Ajax Corp. 7.25% Convertible Senior Notes due 2024&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GSH&#39;: {&#39;Dividend_Yield&#39;: 0.04602829162132753, &#39;latest_Price&#39;: 9.19, &#39;latest_Dividend&#39;: 0.423, &#39;market_Capit_in_M&#39;: 2217.68474, &#39;company_Name&#39;: &#39;Guangshen Railway Company Limited&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;DPZ&#39;: {&#39;Dividend_Yield&#39;: 0.008940618483638396, &#39;latest_Price&#39;: 366.865, &#39;latest_Dividend&#39;: 3.2800000000000002, &#39;market_Capit_in_M&#39;: 14234.5083, &#39;company_Name&#39;: &#34;Domino&#39;s Pizza Inc&#34;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 20.6, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1909.48403, &#39;company_Name&#39;: &#39;Knowles Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KW&#39;: {&#39;Dividend_Yield&#39;: 0.0537109375, &#39;latest_Price&#39;: 20.48, &#39;latest_Dividend&#39;: 1.1, &#39;market_Capit_in_M&#39;: 2889.40032, &#39;company_Name&#39;: &#39;Kennedy-Wilson Holdings Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CURO&#39;: {&#39;Dividend_Yield&#39;: 0.014656895403064625, &#39;latest_Price&#39;: 15.01, &#39;latest_Dividend&#39;: 0.22, &#39;market_Capit_in_M&#39;: 623.41331, &#39;company_Name&#39;: &#39;CURO Group Holdings Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JMEI&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 19.93, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 227.594624, &#39;company_Name&#39;: &#39;Jumei International Holding Limited American Depositary Shares each representing one Class A&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ERA&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 5.16, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 111.071064, &#39;company_Name&#39;: &#39;Era Group Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AA&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 32.93, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 6135.5505, &#39;company_Name&#39;: &#39;Alcoa Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GWR&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 111.88, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 6381.6013, &#39;company_Name&#39;: &#39;Genesee &amp; Wyoming Inc. Class A&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;WMS&#39;: {&#39;Dividend_Yield&#39;: 0.0036409608091024018, &#39;latest_Price&#39;: 98.875, &#39;latest_Dividend&#39;: 0.36, &#39;market_Capit_in_M&#39;: 7008.0031, &#39;company_Name&#39;: &#39;Advanced Drainage Systems Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TLDH&#39;: {&#39;Dividend_Yield&#39;: 0.07878613133424638, &#39;latest_Price&#39;: 27.746, &#39;latest_Dividend&#39;: 2.186, &#39;market_Capit_in_M&#39;: 8.363848, &#39;company_Name&#39;: &#39;FlexShares Currency Hedged Morningstar DM ex-US Factor Tilt Index Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;SAND&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 6.655, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1301.51834, &#39;company_Name&#39;: &#39;Sandstorm Gold Ltd&#39;, &#39;exchange&#39;: &#39;NYSE American&#39;}, &#39;BK&#39;: {&#39;Dividend_Yield&#39;: 0.025941422594142262, &#39;latest_Price&#39;: 47.8, &#39;latest_Dividend&#39;: 1.24, &#39;market_Capit_in_M&#39;: 41918.255, &#39;company_Name&#39;: &#39;Bank of New York Mellon Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;DWIN&#39;: {&#39;Dividend_Yield&#39;: 0.001132852729145211, &#39;latest_Price&#39;: 9.71, &#39;latest_Dividend&#39;: 0.011, &#39;market_Capit_in_M&#39;: 250.409248, &#39;company_Name&#39;: &#39;PowerShares DWA Tactical Multi-Asset Income Portfolio&#39;, &#39;exchange&#39;: &#39;Nasdaq Global Market&#39;}, &#39;T&#39;: {&#39;Dividend_Yield&#39;: 0.06797385620915032, &#39;latest_Price&#39;: 30.6, &#39;latest_Dividend&#39;: 2.08, &#39;market_Capit_in_M&#39;: 218436.878, &#39;company_Name&#39;: &#39;AT&amp;T Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;V&#39;: {&#39;Dividend_Yield&#39;: 0.005838591204444863, &#39;latest_Price&#39;: 212.38, &#39;latest_Dividend&#39;: 1.24, &#39;market_Capit_in_M&#39;: 468266.025, &#39;company_Name&#39;: &#39;Visa Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;DNB&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 24.045, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 10325.8614, &#39;company_Name&#39;: &#39;Dun &amp; Bradstreet Holdings Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ATHM&#39;: {&#39;Dividend_Yield&#39;: 0.01699129714048902, &#39;latest_Price&#39;: 96.52, &#39;latest_Dividend&#39;: 1.6400000000000001, &#39;market_Capit_in_M&#39;: 12063.0692, &#39;company_Name&#39;: &#39;Autohome Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MIXT&#39;: {&#39;Dividend_Yield&#39;: 0.018317612161661105, &#39;latest_Price&#39;: 13.485, &#39;latest_Dividend&#39;: 0.24701299999999998, &#39;market_Capit_in_M&#39;: 297.012512, &#39;company_Name&#39;: &#39;MiX Telematics Limited American Depositary Shares each representing 25&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ARDC&#39;: {&#39;Dividend_Yield&#39;: 0.07846381723802025, &#39;latest_Price&#39;: 14.9878, &#39;latest_Dividend&#39;: 1.176, &#39;market_Capit_in_M&#39;: 343.443936, &#39;company_Name&#39;: &#39;Ares Dynamic Credit Allocation Fund Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ED&#39;: {&#39;Dividend_Yield&#39;: 0.04135237068965518, &#39;latest_Price&#39;: 74.24, &#39;latest_Dividend&#39;: 3.0700000000000003, &#39;market_Capit_in_M&#39;: 25421.1871, &#39;company_Name&#39;: &#39;Consolidated Edison Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CHSP&#39;: {&#39;Dividend_Yield&#39;: 0.0562390158172232, &#39;latest_Price&#39;: 28.45, &#39;latest_Dividend&#39;: 1.6, &#39;market_Capit_in_M&#39;: 1718.203353, &#39;company_Name&#39;: &#39;Chesapeake Lodging Trust of Beneficial Interest&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JAG&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 8.23, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1756.45466, &#39;company_Name&#39;: &#39;Jagged Peak Energy Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SLB&#39;: {&#39;Dividend_Yield&#39;: 0.018188432157148056, &#39;latest_Price&#39;: 27.49, &#39;latest_Dividend&#39;: 0.5, &#39;market_Capit_in_M&#39;: 38438.441, &#39;company_Name&#39;: &#39;Schlumberger NV&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;DOV&#39;: {&#39;Dividend_Yield&#39;: 0.014314706095528013, &#39;latest_Price&#39;: 137.97, &#39;latest_Dividend&#39;: 1.9749999999999999, &#39;market_Capit_in_M&#39;: 19847.9503, &#39;company_Name&#39;: &#39;Dover Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;INSI&#39;: {&#39;Dividend_Yield&#39;: 0.0629936616284739, &#39;latest_Price&#39;: 20.51, &#39;latest_Dividend&#39;: 1.2919999999999998, &#39;market_Capit_in_M&#39;: 216.98144, &#39;company_Name&#39;: &#39;Insight Select Income Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CRS&#39;: {&#39;Dividend_Yield&#39;: 0.019138755980861247, &#39;latest_Price&#39;: 41.8, &#39;latest_Dividend&#39;: 0.8, &#39;market_Capit_in_M&#39;: 2007.48672, &#39;company_Name&#39;: &#39;Carpenter Technology Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;INST&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 49.0, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1874.89677, &#39;company_Name&#39;: &#39;Instructure Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JEC&#39;: {&#39;Dividend_Yield&#39;: 0.007921714818266543, &#39;latest_Price&#39;: 85.84, &#39;latest_Dividend&#39;: 0.68, &#39;market_Capit_in_M&#39;: 11438.0933, &#39;company_Name&#39;: &#39;Jacobs Engineering Group Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MCRN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 16.8, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1184.3159, &#39;company_Name&#39;: &#39;Milacron Holdings Corp.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FELP&#39;: {&#39;Dividend_Yield&#39;: 1.5170670037926675, &#39;latest_Price&#39;: 0.1582, &#39;latest_Dividend&#39;: 0.24, &#39;market_Capit_in_M&#39;: 23.08043, &#39;company_Name&#39;: &#39;Foresight Energy LP representing Limited Partner Interests&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GLL&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 38.99, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 21.649623, &#39;company_Name&#39;: &#39;ProShares UltraShort Gold&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;HCP&#39;: {&#39;Dividend_Yield&#39;: 0.046879949318973706, &#39;latest_Price&#39;: 31.57, &#39;latest_Dividend&#39;: 1.48, &#39;market_Capit_in_M&#39;: 15089.9231, &#39;company_Name&#39;: &#39;HCP Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;HCR&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 0.1523, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 15.211129, &#39;company_Name&#39;: &#39;Hi-Crush Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JHS&#39;: {&#39;Dividend_Yield&#39;: 0.05845951283739302, &#39;latest_Price&#39;: 15.19, &#39;latest_Dividend&#39;: 0.888, &#39;market_Capit_in_M&#39;: 176.911856, &#39;company_Name&#39;: &#39;John Hancock Income Securities Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;IRM&#39;: {&#39;Dividend_Yield&#39;: 0.06629183400267737, &#39;latest_Price&#39;: 37.35, &#39;latest_Dividend&#39;: 2.476, &#39;market_Capit_in_M&#39;: 10772.524, &#39;company_Name&#39;: &#39;Iron Mountain Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CDAY&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 81.39, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 12112.0532, &#39;company_Name&#39;: &#39;Ceridian HCM Holding Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NUW&#39;: {&#39;Dividend_Yield&#39;: 0.028641370869033043, &#39;latest_Price&#39;: 16.34, &#39;latest_Dividend&#39;: 0.4679999999999999, &#39;market_Capit_in_M&#39;: 253.299408, &#39;company_Name&#39;: &#39;Nuveen Amt-Free Municipal Value Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ZTS&#39;: {&#39;Dividend_Yield&#39;: 0.005401626842907982, &#39;latest_Price&#39;: 157.36, &#39;latest_Dividend&#39;: 0.8500000000000001, &#39;market_Capit_in_M&#39;: 74796.04, &#39;company_Name&#39;: &#39;Zoetis Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;LOR&#39;: {&#39;Dividend_Yield&#39;: 0.0754132231404959, &#39;latest_Price&#39;: 9.68, &#39;latest_Dividend&#39;: 0.7300000000000002, &#39;market_Capit_in_M&#39;: 66.600144, &#39;company_Name&#39;: &#39;Lazard World Dividend &amp; Income Fund Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;KWR&#39;: {&#39;Dividend_Yield&#39;: 0.0065504933865211, &#39;latest_Price&#39;: 238.15, &#39;latest_Dividend&#39;: 1.56, &#39;market_Capit_in_M&#39;: 4251.85843, &#39;company_Name&#39;: &#39;Quaker Chemical Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BIT&#39;: {&#39;Dividend_Yield&#39;: 0.08336227856894758, &#39;latest_Price&#39;: 17.8498, &#39;latest_Dividend&#39;: 1.4880000000000004, &#39;market_Capit_in_M&#39;: 670.03866, &#39;company_Name&#39;: &#39;Blackrock Multi-Sector Income Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SNX&#39;: {&#39;Dividend_Yield&#39;: 0.001751574227336819, &#39;latest_Price&#39;: 114.183, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 5846.1148, &#39;company_Name&#39;: &#39;SYNNEX Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SRI&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 32.3, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 872.27117, &#39;company_Name&#39;: &#39;Stoneridge Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SLDA&#39;: {&#39;Dividend_Yield&#39;: 0.06710122699386503, &#39;latest_Price&#39;: 26.08, &#39;latest_Dividend&#39;: 1.75, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Sutherland Asset Management Corporation 7.00% Convertible Senior Notes due 2023&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TGI&#39;: {&#39;Dividend_Yield&#39;: 0.002185792349726776, &#39;latest_Price&#39;: 18.3, &#39;latest_Dividend&#39;: 0.04, &#39;market_Capit_in_M&#39;: 1006.10285, &#39;company_Name&#39;: &#39;Triumph Group Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GHLD&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 14.565, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 873.89997, &#39;company_Name&#39;: &#39;Guild Holdings Company&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;HIG-PG&#39;: {&#39;Dividend_Yield&#39;: 0.0539568345323741, &#39;latest_Price&#39;: 27.8, &#39;latest_Dividend&#39;: 1.5, &#39;market_Capit_in_M&#39;: 20651.2026, &#39;company_Name&#39;: &#39;The Hartford Financial Services Group, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GPI&#39;: {&#39;Dividend_Yield&#39;: 0.0038625930030077565, &#39;latest_Price&#39;: 157.925, &#39;latest_Dividend&#39;: 0.61, &#39;market_Capit_in_M&#39;: 2857.76358, &#39;company_Name&#39;: &#39;Group 1 Automotive Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ELJ&#39;: {&#39;Dividend_Yield&#39;: 0.025969529085872578, &#39;latest_Price&#39;: 25.27, &#39;latest_Dividend&#39;: 0.65625, &#39;market_Capit_in_M&#39;: 178.36, &#39;company_Name&#39;: &#39;Entergy Louisiana LLC&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;WLH&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 24.37, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 945.03936, &#39;company_Name&#39;: &#39;Lyon William Homes (Class A)&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;HOS&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 0.2998, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 11.390392, &#39;company_Name&#39;: &#39;Hornbeck Offshore Services&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;I&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 0.3826, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 54.362104, &#39;company_Name&#39;: &#39;Intelsat S.A.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;GSX&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 34.475, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 8227.3203, &#39;company_Name&#39;: &#39;GSX Techedu Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ZAYO&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 34.99, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 8321.2877, &#39;company_Name&#39;: &#39;Zayo Group Holdings Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PACD&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 0.229, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 17.221578, &#39;company_Name&#39;: &#39;Pacific Drilling SA&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;VPC&#39;: {&#39;Dividend_Yield&#39;: 0.08784246575342466, &#39;latest_Price&#39;: 23.36, &#39;latest_Dividend&#39;: 2.052, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Virtus Private Credit ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;LXU&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 4.85, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 145.682848, &#39;company_Name&#39;: &#39;LSB Industries Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AGRO&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 8.0, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 938.376, &#39;company_Name&#39;: &#39;Adecoagro SA&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BAP&#39;: {&#39;Dividend_Yield&#39;: 0.28144338869949953, &#39;latest_Price&#39;: 137.87, &#39;latest_Dividend&#39;: 38.8026, &#39;market_Capit_in_M&#39;: 10996.7186, &#39;company_Name&#39;: &#39;Credicorp Ltd&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AMC&#39;: {&#39;Dividend_Yield&#39;: 0.0029850746268656712, &#39;latest_Price&#39;: 10.05, &#39;latest_Dividend&#39;: 0.03, &#39;market_Capit_in_M&#39;: 5044.6275, &#39;company_Name&#39;: &#39;AMC Entertainment Holdings Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FLOW&#39;: {&#39;Dividend_Yield&#39;: 0.0013736263736263737, &#39;latest_Price&#39;: 65.52, &#39;latest_Dividend&#39;: 0.09, &#39;market_Capit_in_M&#39;: 2761.62867, &#39;company_Name&#39;: &#39;SPX FLOW Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PHM&#39;: {&#39;Dividend_Yield&#39;: 0.010044427274483292, &#39;latest_Price&#39;: 51.77, &#39;latest_Dividend&#39;: 0.52, &#39;market_Capit_in_M&#39;: 13691.9224, &#39;company_Name&#39;: &#39;Pultegroup Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SRL&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 8.6299, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 108.346672, &#39;company_Name&#39;: &#39;Scully Royalty Ltd&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;OKE&#39;: {&#39;Dividend_Yield&#39;: 0.0745911447945752, &#39;latest_Price&#39;: 50.14, &#39;latest_Dividend&#39;: 3.74, &#39;market_Capit_in_M&#39;: 22311.4977, &#39;company_Name&#39;: &#39;ONEOK Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RCL&#39;: {&#39;Dividend_Yield&#39;: 0.009199198018634273, &#39;latest_Price&#39;: 84.79, &#39;latest_Dividend&#39;: 0.78, &#39;market_Capit_in_M&#39;: 21584.0584, &#39;company_Name&#39;: &#39;Royal Caribbean Cruises Ltd&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TEX&#39;: {&#39;Dividend_Yield&#39;: 0.0025895554596460937, &#39;latest_Price&#39;: 46.34, &#39;latest_Dividend&#39;: 0.12, &#39;market_Capit_in_M&#39;: 3231.13549, &#39;company_Name&#39;: &#39;Terex Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TSU&#39;: {&#39;Dividend_Yield&#39;: 0.019540390879478828, &#39;latest_Price&#39;: 12.28, &#39;latest_Dividend&#39;: 0.239956, &#39;market_Capit_in_M&#39;: 5816.0532, &#39;company_Name&#39;: &#39;TIM Participacoes S.A. American Depositary Shares (Each representing 5)&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RWGE&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 10.3575, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 388.40624, &#39;company_Name&#39;: &#39;Regalwood Global Energy Ltd. Class A&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RPT&#39;: {&#39;Dividend_Yield&#39;: 0.00641025641025641, &#39;latest_Price&#39;: 11.7, &#39;latest_Dividend&#39;: 0.075, &#39;market_Capit_in_M&#39;: 948.78221, &#39;company_Name&#39;: &#39;RPT Realty&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NID&#39;: {&#39;Dividend_Yield&#39;: 0.03669595782073813, &#39;latest_Price&#39;: 14.225, &#39;latest_Dividend&#39;: 0.5219999999999999, &#39;market_Capit_in_M&#39;: 666.81683, &#39;company_Name&#39;: &#39;Nuveen Intermediate Duration Municipal Term Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PMF&#39;: {&#39;Dividend_Yield&#39;: 0.04996568291008924, &#39;latest_Price&#39;: 14.57, &#39;latest_Dividend&#39;: 0.7280000000000002, &#39;market_Capit_in_M&#39;: 375.627712, &#39;company_Name&#39;: &#39;PIMCO Municipal Income Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JMLP&#39;: {&#39;Dividend_Yield&#39;: 0.040540540540540536, &#39;latest_Price&#39;: 1.11, &#39;latest_Dividend&#39;: 0.045, &#39;market_Capit_in_M&#39;: 14.985222, &#39;company_Name&#39;: &#39;Nuveen All Cap Energy MLP Opportunities Fund of Beneficial Interest&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;POL&#39;: {&#39;Dividend_Yield&#39;: 0.024321133412042506, &#39;latest_Price&#39;: 25.41, &#39;latest_Dividend&#39;: 0.6180000000000001, &#39;market_Capit_in_M&#39;: 2322.40538, &#39;company_Name&#39;: &#39;PolyOne Corporation&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RTN&#39;: {&#39;Dividend_Yield&#39;: 0.008058310533515732, &#39;latest_Price&#39;: 116.96, &#39;latest_Dividend&#39;: 0.9425, &#39;market_Capit_in_M&#39;: 32566.4584, &#39;company_Name&#39;: &#39;Raytheon Company&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RTW&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 0.2534, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 16.547147, &#39;company_Name&#39;: &#39;RTW Retailwinds, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SMTA&#39;: {&#39;Dividend_Yield&#39;: 11.711829077644607, &#39;latest_Price&#39;: 0.7676, &#39;latest_Dividend&#39;: 8.99, &#39;market_Capit_in_M&#39;: 33.143432, &#39;company_Name&#39;: &#39;Spirit MTA REIT&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;THGA&#39;: {&#39;Dividend_Yield&#39;: 0.03130914826498423, &#39;latest_Price&#39;: 25.36, &#39;latest_Dividend&#39;: 0.794, &#39;market_Capit_in_M&#39;: 1075.264, &#39;company_Name&#39;: &#39;The Hanover Insurance Group Inc. 6.35% Subordinated Debentures due 2053&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NNC&#39;: {&#39;Dividend_Yield&#39;: 0.0021961932650073207, &#39;latest_Price&#39;: 13.66, &#39;latest_Dividend&#39;: 0.03, &#39;market_Capit_in_M&#39;: 224.276704, &#39;company_Name&#39;: &#39;Nuveen North Carolina Quality Municipal Income Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NX&#39;: {&#39;Dividend_Yield&#39;: 0.012070916635231988, &#39;latest_Price&#39;: 26.51, &#39;latest_Dividend&#39;: 0.32, &#39;market_Capit_in_M&#39;: 891.80173, &#39;company_Name&#39;: &#39;Quanex Building Products Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;VSTO&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 30.415, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1773.90323, &#39;company_Name&#39;: &#39;Vista Outdoor Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;LTM&#39;: {&#39;Dividend_Yield&#39;: 0.02928348909657321, &#39;latest_Price&#39;: 3.21, &#39;latest_Dividend&#39;: 0.094, &#39;market_Capit_in_M&#39;: 1831.80902, &#39;company_Name&#39;: &#39;LATAM Airlines Group S.A.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NPTN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 11.025, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 559.87155, &#39;company_Name&#39;: &#39;NeoPhotonics Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PRH&#39;: {&#39;Dividend_Yield&#39;: 0.02829736211031175, &#39;latest_Price&#39;: 25.02, &#39;latest_Dividend&#39;: 0.708, &#39;market_Capit_in_M&#39;: 10684.6413, &#39;company_Name&#39;: &#39;Prudential Financial Inc. 5.70% Junior Subordinated Notes due 2053&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JHD&#39;: {&#39;Dividend_Yield&#39;: 0.04141414141414141, &#39;latest_Price&#39;: 9.9, &#39;latest_Dividend&#39;: 0.41, &#39;market_Capit_in_M&#39;: 267.934576, &#39;company_Name&#39;: &#39;Nuveen High Income December 2019 Target Term Fund of Beneficial Interest&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JHY&#39;: {&#39;Dividend_Yield&#39;: 0.007457983193277312, &#39;latest_Price&#39;: 9.52, &#39;latest_Dividend&#39;: 0.07100000000000001, &#39;market_Capit_in_M&#39;: 150.40744, &#39;company_Name&#39;: &#39;Nuveen High Income 2020 Target Term Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ROAN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 1.52, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 234.58768, &#39;company_Name&#39;: &#39;Roan Resources, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PVT&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 9.9, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 284.624992, &#39;company_Name&#39;: &#39;Pivotal Acquisition Corp.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NTC&#39;: {&#39;Dividend_Yield&#39;: 0.15946843853820603, &#39;latest_Price&#39;: 12.04, &#39;latest_Dividend&#39;: 1.9200000000000004, &#39;market_Capit_in_M&#39;: 172.521152, &#39;company_Name&#39;: &#39;Nuveen Connecticut Quality Municipal Income Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;NTX&#39;: {&#39;Dividend_Yield&#39;: 0.0030396174863387975, &#39;latest_Price&#39;: 14.64, &#39;latest_Dividend&#39;: 0.0445, &#39;market_Capit_in_M&#39;: 145.794048, &#39;company_Name&#39;: &#39;Nuveen Texas Quality Municipal Income Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JMF&#39;: {&#39;Dividend_Yield&#39;: 0.040559440559440565, &#39;latest_Price&#39;: 1.43, &#39;latest_Dividend&#39;: 0.058, &#39;market_Capit_in_M&#39;: 58.32498, &#39;company_Name&#39;: &#39;Nuveen Energy MLP Total Return Fund of Beneficial Interest&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SPAQ&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 8.96, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 618.17274, &#39;company_Name&#39;: &#39;Spartan Energy Acquisition Corp.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BKI&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 72.58, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 11380.5445, &#39;company_Name&#39;: &#39;Black Knight Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;WBC&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 136.46, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 7010.7146, &#39;company_Name&#39;: &#39;Wabco Holdings Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TMV&#39;: {&#39;Dividend_Yield&#39;: 0.002470966147763776, &#39;latest_Price&#39;: 80.94, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 216.004324, &#39;company_Name&#39;: &#39;Direxion Daily 20-Year Treasury Bear 3X&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;SRTY&#39;: {&#39;Dividend_Yield&#39;: 0.0147577053109851, &#39;latest_Price&#39;: 11.1806, &#39;latest_Dividend&#39;: 0.165, &#39;market_Capit_in_M&#39;: 55.061162, &#39;company_Name&#39;: &#39;ProShares UltraPro Short Russell2000&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;OEC&#39;: {&#39;Dividend_Yield&#39;: 0.010162601626016262, &#39;latest_Price&#39;: 19.68, &#39;latest_Dividend&#39;: 0.2, &#39;market_Capit_in_M&#39;: 1190.87424, &#39;company_Name&#39;: &#39;Orion Engineered Carbons SA&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MSL&#39;: {&#39;Dividend_Yield&#39;: 0.003496503496503497, &#39;latest_Price&#39;: 11.44, &#39;latest_Dividend&#39;: 0.04, &#39;market_Capit_in_M&#39;: 191.415216, &#39;company_Name&#39;: &#39;MidSouth Bancorp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TU&#39;: {&#39;Dividend_Yield&#39;: 0.046466165413533836, &#39;latest_Price&#39;: 19.95, &#39;latest_Dividend&#39;: 0.9269999999999999, &#39;market_Capit_in_M&#39;: 25875.1488, &#39;company_Name&#39;: &#39;Telus Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;TWND&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 9.95, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 415.681152, &#39;company_Name&#39;: &#39;Tailwind Acquisition Corp.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SCHI&#39;: {&#39;Dividend_Yield&#39;: 0.022664326116637407, &#39;latest_Price&#39;: 51.27, &#39;latest_Dividend&#39;: 1.162, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Schwab 5-10 Year Corporate Bond ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;SJI&#39;: {&#39;Dividend_Yield&#39;: 0.05307299755935212, &#39;latest_Price&#39;: 22.535, &#39;latest_Dividend&#39;: 1.196, &#39;market_Capit_in_M&#39;: 2276.80128, &#39;company_Name&#39;: &#39;South Jersey Industries Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ZJPN&#39;: {&#39;Dividend_Yield&#39;: 0.01643795438303498, &#39;latest_Price&#39;: 91.3739, &#39;latest_Dividend&#39;: 1.502, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;SPDR Solactive Japan ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;YXI&#39;: {&#39;Dividend_Yield&#39;: 0.0007751937984496124, &#39;latest_Price&#39;: 14.19, &#39;latest_Dividend&#39;: 0.011, &#39;market_Capit_in_M&#39;: 6.285545, &#39;company_Name&#39;: &#39;ProShares Short FTSE China 50&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;UPV&#39;: {&#39;Dividend_Yield&#39;: 0.0011399831214439335, &#39;latest_Price&#39;: 58.7728, &#39;latest_Dividend&#39;: 0.067, &#39;market_Capit_in_M&#39;: 7.34022, &#39;company_Name&#39;: &#39;ProShares Ultra FTSE Europe&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;FT&#39;: {&#39;Dividend_Yield&#39;: 0.051405622489959855, &#39;latest_Price&#39;: 7.47, &#39;latest_Dividend&#39;: 0.3840000000000001, &#39;market_Capit_in_M&#39;: 187.73528, &#39;company_Name&#39;: &#39;Franklin Universal Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;FRC&#39;: {&#39;Dividend_Yield&#39;: 0.004746844831576012, &#39;latest_Price&#39;: 168.533, &#39;latest_Dividend&#39;: 0.8, &#39;market_Capit_in_M&#39;: 29363.755, &#39;company_Name&#39;: &#39;First Republic Bank&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BOOT&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 62.45, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1811.34349, &#39;company_Name&#39;: &#39;Boot Barn Holdings Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;UTRN&#39;: {&#39;Dividend_Yield&#39;: 0.0846598285954366, &#39;latest_Price&#39;: 31.6561, &#39;latest_Dividend&#39;: 2.68, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Vesper U.S. Large Cap Short-Term Reversal Strategy ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;RUSL&#39;: {&#39;Dividend_Yield&#39;: 0.015394679459223723, &#39;latest_Price&#39;: 22.93, &#39;latest_Dividend&#39;: 0.353, &#39;market_Capit_in_M&#39;: 138.227922, &#39;company_Name&#39;: &#39;Direxion Daily Russia Bull 3x Shares&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;URA&#39;: {&#39;Dividend_Yield&#39;: 0.013928475396611994, &#39;latest_Price&#39;: 18.595, &#39;latest_Dividend&#39;: 0.259, &#39;market_Capit_in_M&#39;: 246.09468, &#39;company_Name&#39;: &#39;Global X Uranium&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;RL&#39;: {&#39;Dividend_Yield&#39;: 0.0057747188181970785, &#39;latest_Price&#39;: 119.14, &#39;latest_Dividend&#39;: 0.688, &#39;market_Capit_in_M&#39;: 8711.3257, &#39;company_Name&#39;: &#39;Ralph Lauren Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AFG&#39;: {&#39;Dividend_Yield&#39;: 0.0336991272790115, &#39;latest_Price&#39;: 115.73, &#39;latest_Dividend&#39;: 3.9000000000000004, &#39;market_Capit_in_M&#39;: 9998.9903, &#39;company_Name&#39;: &#39;American Financial Group Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;JMPB&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 25.48, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;JMP Group Inc 8.00% Senior Notes due 2023&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;CLI&#39;: {&#39;Dividend_Yield&#39;: 0.024405125076266018, &#39;latest_Price&#39;: 16.39, &#39;latest_Dividend&#39;: 0.4, &#39;market_Capit_in_M&#39;: 1487.04819, &#39;company_Name&#39;: &#39;Mack-Cali Realty Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ADC&#39;: {&#39;Dividend_Yield&#39;: 0.03619513641755635, &#39;latest_Price&#39;: 67.44, &#39;latest_Dividend&#39;: 2.4410000000000003, &#39;market_Capit_in_M&#39;: 4287.36333, &#39;company_Name&#39;: &#39;Agree Realty Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;WDR&#39;: {&#39;Dividend_Yield&#39;: 0.04983057604145904, &#39;latest_Price&#39;: 25.085, &#39;latest_Dividend&#39;: 1.25, &#39;market_Capit_in_M&#39;: 1559.74003, &#39;company_Name&#39;: &#39;Waddell &amp; Reed Financial Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;AMX&#39;: {&#39;Dividend_Yield&#39;: 0.025787545787545784, &#39;latest_Price&#39;: 13.65, &#39;latest_Dividend&#39;: 0.352, &#39;market_Capit_in_M&#39;: 45449.855, &#39;company_Name&#39;: &#39;America Movil S.A.B. de C.V.n Depository Receipt Series L&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SSG&#39;: {&#39;Dividend_Yield&#39;: 0.0044362292051756, &#39;latest_Price&#39;: 10.82, &#39;latest_Dividend&#39;: 0.048, &#39;market_Capit_in_M&#39;: 4.852045, &#39;company_Name&#39;: &#39;ProShares UltraShort Semiconductors&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;TOTL&#39;: {&#39;Dividend_Yield&#39;: 0.02862476664592408, &#39;latest_Price&#39;: 48.21, &#39;latest_Dividend&#39;: 1.38, &#39;market_Capit_in_M&#39;: 3139.2, &#39;company_Name&#39;: &#39;SPDR DoubleLine Total Return Tactical&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;SPVM&#39;: {&#39;Dividend_Yield&#39;: 0.013369098712446351, &#39;latest_Price&#39;: 46.6, &#39;latest_Dividend&#39;: 0.623, &#39;market_Capit_in_M&#39;: 2.772458, &#39;company_Name&#39;: &#39;PowerShares S&amp;P 500 Value With Momentum Portfolio&#39;, &#39;exchange&#39;: &#39;BATS Exchange&#39;}, &#39;HVT-A&#39;: {&#39;Dividend_Yield&#39;: 0.07108108108108109, &#39;latest_Price&#39;: 37.0, &#39;latest_Dividend&#39;: 2.6300000000000003, &#39;market_Capit_in_M&#39;: 651.89562, &#39;company_Name&#39;: &#39;Haverty Furniture Companies, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SPUN&#39;: {&#39;Dividend_Yield&#39;: 0.011370558375634518, &#39;latest_Price&#39;: 19.7, &#39;latest_Dividend&#39;: 0.224, &#39;market_Capit_in_M&#39;: 3.94, &#39;company_Name&#39;: &#39;VanEck Vectors Spin-Off&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;AADR&#39;: {&#39;Dividend_Yield&#39;: 0.0010815173527037934, &#39;latest_Price&#39;: 61.95, &#39;latest_Dividend&#39;: 0.067, &#39;market_Capit_in_M&#39;: 109.0125, &#39;company_Name&#39;: &#39;AdvisorShares Dorsey Wright ADR&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;ACSG&#39;: {&#39;Dividend_Yield&#39;: 0.01693972179289026, &#39;latest_Price&#39;: 32.35, &#39;latest_Dividend&#39;: 0.548, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Xtrackers MSCI ACWI ex USA ESG Leaders Equity ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;USCI&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 35.09, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 443.40576, &#39;company_Name&#39;: &#39;United States Commodity Index Fund ETV&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;WBIG&#39;: {&#39;Dividend_Yield&#39;: 0.010803983174385029, &#39;latest_Price&#39;: 25.1759, &#39;latest_Dividend&#39;: 0.272, &#39;market_Capit_in_M&#39;: 103.949951, &#39;company_Name&#39;: &#39;WBI Tactical LCY Shares&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;REVS&#39;: {&#39;Dividend_Yield&#39;: 0.2452205882352941, &#39;latest_Price&#39;: 19.04, &#39;latest_Dividend&#39;: 4.669, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Columbia Research Enhanced Value ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;LOUP&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 53.15, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Innovator Loup Frontier Tech ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;DLPH&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 17.02, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1469.67181, &#39;company_Name&#39;: &#39;Delphi Technologies PLC&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SDT&#39;: {&#39;Dividend_Yield&#39;: 0.705351264929935, &#39;latest_Price&#39;: 0.170128, &#39;latest_Dividend&#39;: 0.12, &#39;market_Capit_in_M&#39;: 4.763584, &#39;company_Name&#39;: &#39;SandRidge Mississippian Trust I of Beneficial Interest&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ITEQ&#39;: {&#39;Dividend_Yield&#39;: 0.006914977212006915, &#39;latest_Price&#39;: 63.63, &#39;latest_Dividend&#39;: 0.44, &#39;market_Capit_in_M&#39;: 59.52, &#39;company_Name&#39;: &#39;BlueStar Israel Technology&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;PAMC&#39;: {&#39;Dividend_Yield&#39;: 0.0024798154555940024, &#39;latest_Price&#39;: 34.68, &#39;latest_Dividend&#39;: 0.08600000000000001, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Pacer Lunt MidCap Multi-Factor Alternator ETF&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;COTV&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 44.75, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 4250.775, &#39;company_Name&#39;: &#39;Cotiviti Holdings Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PPMC&#39;: {&#39;Dividend_Yield&#39;: 0.004118746633716693, &#39;latest_Price&#39;: 22.0941, &#39;latest_Dividend&#39;: 0.091, &#39;market_Capit_in_M&#39;: 2.637346, &#39;company_Name&#39;: &#39;Portfolio S&amp;P Mid Cap&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;MOTO&#39;: {&#39;Dividend_Yield&#39;: 0.025801169590643273, &#39;latest_Price&#39;: 42.75, &#39;latest_Dividend&#39;: 1.103, &#39;market_Capit_in_M&#39;: 22426.837, &#39;company_Name&#39;: &#39;SmartETFs Smart Transportation &amp; Technology ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;BID&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 26.12, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 1101.9584, &#39;company_Name&#39;: &#34;Sotheby&#39;s&#34;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;MLPZ&#39;: {&#39;Dividend_Yield&#39;: 0.2626108998732573, &#39;latest_Price&#39;: 3.945, &#39;latest_Dividend&#39;: 1.036, &#39;market_Capit_in_M&#39;: 29.673096, &#39;company_Name&#39;: &#39;UBS AG ETRACS ETRACS 2xMonthly Leveraged S&amp;P MLP Index ETN Series B due February 12 2046&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;FLHK&#39;: {&#39;Dividend_Yield&#39;: 0.02842689352167901, &#39;latest_Price&#39;: 28.5645, &#39;latest_Dividend&#39;: 0.812, &#39;market_Capit_in_M&#39;: 20.6514, &#39;company_Name&#39;: &#39;Franklin FTSE Hong Kong&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;QSY&#39;: {&#39;Dividend_Yield&#39;: 0.015562240113244275, &#39;latest_Price&#39;: 113.03, &#39;latest_Dividend&#39;: 1.7590000000000003, &#39;market_Capit_in_M&#39;: 39.88224, &#39;company_Name&#39;: &#39;WisdomTree U.S. Quality Shareholder Yield Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;MDYV&#39;: {&#39;Dividend_Yield&#39;: 0.01438815990234971, &#39;latest_Price&#39;: 65.54, &#39;latest_Dividend&#39;: 0.9430000000000001, &#39;market_Capit_in_M&#39;: 1500.24, &#39;company_Name&#39;: &#39;SPDR S&amp;P 400 Mid Cap Value ETF (based on S&amp;P MidCap 400 Value Index--symbol: MGD&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;CCOR&#39;: {&#39;Dividend_Yield&#39;: 0.013473605860846253, &#39;latest_Price&#39;: 29.0197, &#39;latest_Dividend&#39;: 0.391, &#39;market_Capit_in_M&#39;: 85.930847, &#39;company_Name&#39;: &#39;Cambria Core Equity&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;PHYL&#39;: {&#39;Dividend_Yield&#39;: 0.0727873789431606, &#39;latest_Price&#39;: 40.7763, &#39;latest_Dividend&#39;: 2.968, &#39;market_Capit_in_M&#39;: 314.828608, &#39;company_Name&#39;: &#39;PGIM Active High Yield Bond ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;IVR-PA&#39;: {&#39;Dividend_Yield&#39;: 0.07876422764227642, &#39;latest_Price&#39;: 24.6, &#39;latest_Dividend&#39;: 1.9376, &#39;market_Capit_in_M&#39;: 1996.51405, &#39;company_Name&#39;: &#39;Invesco Mortgage Capital Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PAF&#39;: {&#39;Dividend_Yield&#39;: 0.03974153892423801, &#39;latest_Price&#39;: 53.1995, &#39;latest_Dividend&#39;: 2.11423, &#39;market_Capit_in_M&#39;: 31456.0369, &#39;company_Name&#39;: &#39;PowerShares FTSE RAFI Asia Pacific ex-Japan Portfolio&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;RXN&#39;: {&#39;Dividend_Yield&#39;: 0.007013815090329437, &#39;latest_Price&#39;: 47.05, &#39;latest_Dividend&#39;: 0.33, &#39;market_Capit_in_M&#39;: 5627.8385, &#39;company_Name&#39;: &#39;Rexnord Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;OEW&#39;: {&#39;Dividend_Yield&#39;: 0.01921727297687905, &#39;latest_Price&#39;: 33.1431, &#39;latest_Dividend&#39;: 0.63692, &#39;market_Capit_in_M&#39;: 3.31431, &#39;company_Name&#39;: &#39;Guggenheim ETF Trust&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;OASI&#39;: {&#39;Dividend_Yield&#39;: 0.07174335295702028, &#39;latest_Price&#39;: 25.989, &#39;latest_Dividend&#39;: 1.864538, &#39;market_Capit_in_M&#39;: 9.09615, &#39;company_Name&#39;: &#34;O&#39;Shares FTSE Asia Pacific Quality Dividend&#34;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;GDXX&#39;: {&#39;Dividend_Yield&#39;: 0.0020659527326440177, &#39;latest_Price&#39;: 54.16, &#39;latest_Dividend&#39;: 0.111892, &#39;market_Capit_in_M&#39;: 11.923059, &#39;company_Name&#39;: &#39;ProShares Ultra Gold Miners&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;EMEM&#39;: {&#39;Dividend_Yield&#39;: 0.04736217133163698, &#39;latest_Price&#39;: 23.58, &#39;latest_Dividend&#39;: 1.1168, &#39;market_Capit_in_M&#39;: 10.190521, &#39;company_Name&#39;: &#39;Virtus Glovista Emerging Markets&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;PPEM&#39;: {&#39;Dividend_Yield&#39;: 0.028714725027405102, &#39;latest_Price&#39;: 20.6166, &#39;latest_Dividend&#39;: 0.592, &#39;market_Capit_in_M&#39;: 533.376832, &#39;company_Name&#39;: &#39;Portfolio Emerging Markets&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;SDEM&#39;: {&#39;Dividend_Yield&#39;: 0.057001239157373, &#39;latest_Price&#39;: 12.105, &#39;latest_Dividend&#39;: 0.6900000000000002, &#39;market_Capit_in_M&#39;: 20.084307, &#39;company_Name&#39;: &#39;Global X MSCI SuperDividend Emerging Markets&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;ESNG&#39;: {&#39;Dividend_Yield&#39;: 0.00961881589618816, &#39;latest_Price&#39;: 61.65, &#39;latest_Dividend&#39;: 0.593, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Direxion Shares ETF Trust - Direxion MSCI USA ESG Universal Movers Versus Laggards ETF&#39;, &#39;exchange&#39;: &#39;NYSE American&#39;}, &#39;SYG&#39;: {&#39;Dividend_Yield&#39;: 0.004251251477875884, &#39;latest_Price&#39;: 119.259, &#39;latest_Dividend&#39;: 0.507, &#39;market_Capit_in_M&#39;: 37.166445, &#39;company_Name&#39;: &#39;SPDR MFS Systematic Growth Equity&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;GOEX&#39;: {&#39;Dividend_Yield&#39;: 0.032421015576872304, &#39;latest_Price&#39;: 28.4075, &#39;latest_Dividend&#39;: 0.921, &#39;market_Capit_in_M&#39;: 33.374527, &#39;company_Name&#39;: &#39;Global X Gold Explorers&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;RTEC&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 23.67, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 762.174, &#39;company_Name&#39;: &#39;Rudolph Technologies Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;PSB-PZ&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 26.44, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;PS Business Parks, Inc.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SHNY&#39;: {&#39;Dividend_Yield&#39;: 0.036303999999999996, &#39;latest_Price&#39;: 5.0, &#39;latest_Dividend&#39;: 0.18152, &#39;market_Capit_in_M&#39;: 4.000005, &#39;company_Name&#39;: &#39;Direxion Funds&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;PACE-UN&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 10.8469, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;TPG Pace Tech Opportunities Corp.&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;BXMX&#39;: {&#39;Dividend_Yield&#39;: 0.06398809523809523, &#39;latest_Price&#39;: 13.44, &#39;latest_Dividend&#39;: 0.86, &#39;market_Capit_in_M&#39;: 1398.92928, &#39;company_Name&#39;: &#39;Nuveen S&amp;P 500 BuyWrite Income Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;SZK&#39;: {&#39;Dividend_Yield&#39;: 0.001883305672281272, &#39;latest_Price&#39;: 8.4957, &#39;latest_Dividend&#39;: 0.016, &#39;market_Capit_in_M&#39;: 1.542786, &#39;company_Name&#39;: &#39;ProShares UltraShort Consumer Goods&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;EGPT&#39;: {&#39;Dividend_Yield&#39;: 0.024732884843688168, &#39;latest_Price&#39;: 25.27, &#39;latest_Dividend&#39;: 0.625, &#39;market_Capit_in_M&#39;: 57.739904, &#39;company_Name&#39;: &#39;VanEck Vectors Egypt Index&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;EELV&#39;: {&#39;Dividend_Yield&#39;: 0.01601713062098501, &#39;latest_Price&#39;: 23.35, &#39;latest_Dividend&#39;: 0.374, &#39;market_Capit_in_M&#39;: 342.912, &#39;company_Name&#39;: &#39;PowerShares S&amp;P Emerging Markets Low Volatility Portfolio&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;SDGA&#39;: {&#39;Dividend_Yield&#39;: 0.011997253361931288, &#39;latest_Price&#39;: 25.9226, &#39;latest_Dividend&#39;: 0.311, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;Impact Shares Sustainable Development Goals Global Equity ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;EBND&#39;: {&#39;Dividend_Yield&#39;: 0.038769587928032506, &#39;latest_Price&#39;: 25.845, &#39;latest_Dividend&#39;: 1.002, &#39;market_Capit_in_M&#39;: 739.568, &#39;company_Name&#39;: &#39;SPDR Bloomberg Barclays Emerging Markets Local Bond&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;MOM&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 21.5366, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 4.689, &#39;company_Name&#39;: &#39;AGFiQ U.S. Market Neutral Momentum Fund&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;ELS&#39;: {&#39;Dividend_Yield&#39;: 0.021534653465346533, &#39;latest_Price&#39;: 64.64, &#39;latest_Dividend&#39;: 1.392, &#39;market_Capit_in_M&#39;: 11783.8715, &#39;company_Name&#39;: &#39;Equity LifeStyle Properties Inc&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;RISE&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 19.945, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 3588.29338, &#39;company_Name&#39;: &#39;Sit Rising Rate&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;ZIG&#39;: {&#39;Dividend_Yield&#39;: 0.001623762376237624, &#39;latest_Price&#39;: 25.25, &#39;latest_Dividend&#39;: 0.041, &#39;market_Capit_in_M&#39;: 0.0, &#39;company_Name&#39;: &#39;The Acquirers Fund ETF&#39;, &#39;exchange&#39;: &#39;NYSEArca&#39;}, &#39;AVYA&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 27.87, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 2338.3767, &#39;company_Name&#39;: &#39;Avaya Holdings Corp&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;COMB&#39;: {&#39;Dividend_Yield&#39;: 0.0006462035541195476, &#39;latest_Price&#39;: 24.76, &#39;latest_Dividend&#39;: 0.016, &#39;market_Capit_in_M&#39;: 60.687, &#39;company_Name&#39;: &#39;GraniteShares Bloomberg Commodity Broad Strategy No K-1&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange Arca&#39;}, &#39;IPOD&#39;: {&#39;Dividend_Yield&#39;: 0.0, &#39;latest_Price&#39;: 10.93, &#39;latest_Dividend&#39;: 0.0, &#39;market_Capit_in_M&#39;: 628.47501, &#39;company_Name&#39;: &#39;Social Capital Hedosophia Holdi&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}, &#39;ES&#39;: {&#39;Dividend_Yield&#39;: 0.026813110181311016, &#39;latest_Price&#39;: 86.04, &#39;latest_Dividend&#39;: 2.307, &#39;market_Capit_in_M&#39;: 29539.3382, &#39;company_Name&#39;: &#39;Eversource Energy&#39;, &#39;exchange&#39;: &#39;New York Stock Exchange&#39;}} . Сбор данных может занять значительное по продолжительности время. После их можно представить в виде отсортированного DataFrame, где сверху будут акций с высокой дивидендной доходностью. . import pandas as pd DivYield_dataframe = pd.DataFrame.from_dict(DivYield, orient=&#39;index&#39;) DivYield_dataframe = DivYield_dataframe.sort_values([&#39;Dividend_Yield&#39;], ascending=[False]) DivYield_dataframe.head(15) . Dividend_Yield latest_Price latest_Dividend market_Capit_in_M company_Name exchange . SMTA 11.711829 | 0.767600 | 8.9900 | 33.143432 | Spirit MTA REIT | New York Stock Exchange | . FELP 1.517067 | 0.158200 | 0.2400 | 23.080430 | Foresight Energy LP representing Limited Partn... | New York Stock Exchange | . MDLX 0.732039 | 2.349600 | 1.7200 | 0.000000 | Medley LLC 6.875% Senior Notes due 2026 | New York Stock Exchange | . MDLQ 0.719920 | 2.510000 | 1.8070 | 0.000000 | Medley LLC 7.25% Notes due 2024 | New York Stock Exchange | . SDT 0.705351 | 0.170128 | 0.1200 | 4.763584 | SandRidge Mississippian Trust I of Beneficial ... | New York Stock Exchange | . BAP 0.281443 | 137.870000 | 38.8026 | 10996.718600 | Credicorp Ltd | New York Stock Exchange | . MLPZ 0.262611 | 3.945000 | 1.0360 | 29.673096 | UBS AG ETRACS ETRACS 2xMonthly Leveraged S&amp;P M... | New York Stock Exchange Arca | . REVS 0.245221 | 19.040000 | 4.6690 | 0.000000 | Columbia Research Enhanced Value ETF | NYSEArca | . BMA 0.168212 | 13.370000 | 2.2490 | 1366.801660 | Banco Macro S.A. ADR (representing Ten Class B) | New York Stock Exchange | . NTC 0.159468 | 12.040000 | 1.9200 | 172.521152 | Nuveen Connecticut Quality Municipal Income Fund | New York Stock Exchange | . ET 0.118785 | 7.711400 | 0.9160 | 20847.693800 | Energy Transfer LP | New York Stock Exchange | . FMO 0.093848 | 9.590000 | 0.9000 | 67.975360 | Fiduciary/Claymore Energy Infrastructure Fund | New York Stock Exchange | . MFGP 0.091487 | 8.070000 | 0.7383 | 2676.697860 | Micro Focus Intl PLC ADS each representing One... | New York Stock Exchange | . VPC 0.087842 | 23.360000 | 2.0520 | 0.000000 | Virtus Private Credit ETF | NYSEArca | . FSD 0.087708 | 15.049900 | 1.3200 | 503.721664 | First Trust High Income Long/Short Fund | New York Stock Exchange | . &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1087;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1088;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1072; &#1080; &#1079;&#1072;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1077; . Предварительно проведем расчет средней дивидендной доходности по акциям которые платят дивиденды: . meanDivNasdaq = DivYield_dataframe[DivYield_dataframe[&#39;Dividend_Yield&#39;]&gt;0][&#39;Dividend_Yield&#39;].mean() print(&quot;Средняя дивидендная доходность по рынку Nasdaq равна &quot;, &quot;{:.2%}&quot;.format(meanDivNasdaq)) . Средняя дивидендная доходность по рынку Nasdaq равна 12.22% . Самой высокой дивидендной доходностью в полученных результатах у акций компании Triumph Bancorp Inc — 21,57%. Правда по ним никогда не платили дивиденды. Так что в системе похоже сидит баг. Так же по другим рынкам заметил, что в список могут включаться акции по которым перестали платить дивиденды давно. А так, как подписка Free ограничена по количеству запросов, то подстроить ее не удалось. Так же в том случае, если при проверке выясняется, что дивиденды платили недавно, то все равно необходимо быть осторожным при выборе компаний по данному показателю, так как он может являться результатом падения цены акций и как следствия ростом дивидендной доходности. Так же выплата высоких дивидендов может не сохраниться в будущем, тем более если у компании возникнут финансовые проблемы. . Основной смысл в следующем - анализ дивидендной доходности не должен быть единственным критерием. Я для одного из своих портфелей так же смотрю: EPS, EBITDA, FCF, срок выплаты дивидендов, капитализация компании, чистая рентабельность (отношение выручки к прибыли) и коэффициент Net Debt/EBITDA. . Но как говориться - все вышеприведенное не является инвестиционной рекомендацией и выбор остается за каждым самостоятельно. .",
            "url": "https://zmey56.github.io/blog//russian/python/algotrading/dividend/2021/04/10/high-divident-stocks.html",
            "relUrl": "/russian/python/algotrading/dividend/2021/04/10/high-divident-stocks.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Russian - Solution Lesson 9 on Fast.ai",
            "content": "!pip install fastai --upgrade . Requirement already up-to-date: fastai in /opt/conda/envs/fastai/lib/python3.8/site-packages (2.2.5) Requirement already satisfied, skipping upgrade: spacy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (2.3.2) Requirement already satisfied, skipping upgrade: torchvision&lt;0.9,&gt;=0.8 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (0.8.1) Requirement already satisfied, skipping upgrade: pandas in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (1.1.0) Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (2.24.0) Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (5.3.1) Requirement already satisfied, skipping upgrade: pillow&gt;6.0.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (7.2.0) Requirement already satisfied, skipping upgrade: pip in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (20.2.2) Requirement already satisfied, skipping upgrade: scipy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (1.5.2) Requirement already satisfied, skipping upgrade: torch&lt;1.8,&gt;=1.7.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (1.7.0) Requirement already satisfied, skipping upgrade: scikit-learn in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (0.23.2) Requirement already satisfied, skipping upgrade: packaging in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (20.4) Requirement already satisfied, skipping upgrade: fastprogress&gt;=0.2.4 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (1.0.0) Requirement already satisfied, skipping upgrade: fastcore&lt;1.4,&gt;=1.3.8 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (1.3.13) Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/fastai/lib/python3.8/site-packages (from fastai) (3.3.1) Requirement already satisfied, skipping upgrade: thinc==7.4.1 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (7.4.1) Requirement already satisfied, skipping upgrade: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (3.0.2) Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (49.6.0.post20200814) Requirement already satisfied, skipping upgrade: srsly&lt;1.1.0,&gt;=1.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (1.0.2) Requirement already satisfied, skipping upgrade: catalogue&lt;1.1.0,&gt;=0.0.7 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (1.0.0) Requirement already satisfied, skipping upgrade: blis&lt;0.5.0,&gt;=0.4.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (0.4.1) Requirement already satisfied, skipping upgrade: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (1.0.0) Requirement already satisfied, skipping upgrade: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (2.0.3) Requirement already satisfied, skipping upgrade: wasabi&lt;1.1.0,&gt;=0.4.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (0.7.1) Requirement already satisfied, skipping upgrade: tqdm&lt;5.0.0,&gt;=4.38.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (4.48.2) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (1.19.1) Requirement already satisfied, skipping upgrade: plac&lt;1.2.0,&gt;=0.9.6 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from spacy-&gt;fastai) (0.9.6) Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from pandas-&gt;fastai) (2020.1) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from pandas-&gt;fastai) (2.8.1) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;fastai) (2020.6.20) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;fastai) (1.25.10) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;fastai) (2.10) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;fastai) (3.0.4) Requirement already satisfied, skipping upgrade: dataclasses in /opt/conda/envs/fastai/lib/python3.8/site-packages (from torch&lt;1.8,&gt;=1.7.0-&gt;fastai) (0.6) Requirement already satisfied, skipping upgrade: typing-extensions in /opt/conda/envs/fastai/lib/python3.8/site-packages (from torch&lt;1.8,&gt;=1.7.0-&gt;fastai) (3.7.4.3) Requirement already satisfied, skipping upgrade: future in /opt/conda/envs/fastai/lib/python3.8/site-packages (from torch&lt;1.8,&gt;=1.7.0-&gt;fastai) (0.18.2) Requirement already satisfied, skipping upgrade: joblib&gt;=0.11 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from scikit-learn-&gt;fastai) (0.16.0) Requirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from scikit-learn-&gt;fastai) (2.1.0) Requirement already satisfied, skipping upgrade: pyparsing&gt;=2.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;fastai) (2.4.7) Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;fastai) (1.15.0) Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from matplotlib-&gt;fastai) (1.2.0) Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from matplotlib-&gt;fastai) (0.10.0) . Сначало пришлось выполнить настройки. Во первых получить файл kaggle.json, который необходимо скачать из свой учетной записи на kaggle. Он находится в подразделе Account. Немного прокрутить вниз до подраздела API и нажать на кнопку Create new API token. После этого его закачал при помощи своего Jupyter notebook, так же в Jupyter запустил терминал находясь в той же папке где и файл и выполнил команду: mv kaggle.json ~/.kaggle/kaggle.json . &#1043;&#1083;&#1091;&#1073;&#1086;&#1082;&#1086;&#1077; &#1087;&#1086;&#1075;&#1088;&#1091;&#1078;&#1077;&#1085;&#1080;&#1077; &#1074; &#1090;&#1072;&#1073;&#1083;&#1080;&#1095;&#1085;&#1086;&#1077; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080;&#1088;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077; . &#1050;&#1072;&#1090;&#1077;&#1075;&#1086;&#1088;&#1080;&#1072;&#1083;&#1100;&#1085;&#1099;&#1077; &#1074;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1103; . Непрерывные и категориальные переменные: Непрерывные переменные-это числовые данные (например &quot;возраст&quot;), которые могут складываться и умножаться. Категориальные переменные - это дискретные значения (например &quot;идентификатор фильма&quot;), для которых сложение и умножение не имеют никакого смысла значения (даже если они представлены в виде чисел). . &#1047;&#1072; &#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1072;&#1084;&#1080; &#1043;&#1083;&#1091;&#1073;&#1086;&#1082;&#1086;&#1075;&#1086; &#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1103; . Последние исследования показали, что подавляющее большинство наборов данных лучше всего моделируется всего двумя методами: . Ансамблями деревьев решений (например, случайные леса и механизмы градиентного бустинга), которые в основном используются для структурированных данных (таких, которые вы можете найти в таблице базы данных большинства компаний) | Многослойными нейронными сетями, обучаемыми с помощью SGD (т. е. неглубокого и/или глубокого обучения), в основном для неструктурированных данных (таких как аудио, изображения и естественного языка) | . The Dataset . Набор данных, который мы используем в этой главе, взят из соревнования на Kaggle &quot;Blue Book for Bulldozers&quot;, которое имеет следующее описание: &quot;Цель конкурса-предсказать цену продажи конкретной единицы тяжелой техники на аукционе на основе ее применения, типа оборудования и конфигурации. Эти данные получены из размещенных результатов аукциона и включают информацию об использовании и конфигурации оборудования.&quot; . Kaggle Competitions . Необходимо открыть загруженный файл kaggle.json и скопировать его содержимое в следующую ячейку . creds = &#39;{&quot;username&quot;:&quot;zmey56&quot;,&quot;key&quot;:&quot;dc7d155a89b02fbe2ef2078f6e098e71&quot;}&#39; . cred_path = Path(&#39;~/.kaggle/kaggle.json&#39;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write(creds) cred_path.chmod(0o600) . Теперь вы можете скачать наборы данных из Kaggle! Выберите путь для загрузки набора: . path = URLs.path(&#39;bluebook&#39;) path . Path(&#39;/storage/archive/bluebook&#39;) . И используйте Kaggle API, чтобы загрузить набор данных по этому пути и извлечь его: . if not path.exists(): path.mkdir() api.competition_download_cli(&#39;bluebook-for-bulldozers&#39;, path=path) file_extract(path/&#39;bluebook-for-bulldozers.zip&#39;) path.ls(file_type=&#39;text&#39;) . (#7) [Path(&#39;median_benchmark.csv&#39;),Path(&#39;ValidSolution.csv&#39;),Path(&#39;Machine_Appendix.csv&#39;),Path(&#39;random_forest_benchmark_test.csv&#39;),Path(&#39;TrainAndValid.csv&#39;),Path(&#39;Test.csv&#39;),Path(&#39;Valid.csv&#39;)] . Теперь, когда мы загрузили наш набор данных, давайте взглянем на него! . &#1055;&#1086;&#1089;&#1084;&#1086;&#1090;&#1088;&#1080;&#1090;&#1077; &#1085;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1077; . Kaggle предоставляет информацию о некоторых полях набора данных. Ключевыми значениями в train.csv являются: . SalesID:: Уникальный идентификатор продажи. | MachineID:: Уникальный идентификатор машины. Машина может быть продана несколько раз. | saleprice:: За что машина продается на аукционе (предоставляется только в train.csv). | saledate:: Дата продажи. | . Давайте загрузим наши данные и посмотрим на столбцы . df = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) . df.columns . Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;) . Cледующим шагом обрабатываются порядковые столбцы. Например - уровни ProductSize: . df[&#39;ProductSize&#39;].unique() . array([nan, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;], dtype=object) . Мы можем рассказать Pandas о подходящем порядке этих уровней примерно так . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; . df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) . В этом случае Kaggle говорит нам, какую метрику использовать: лог среднеквадратичной ошибки (RMSLE) между фактической и прогнозируемой аукционными ценами. Нам нужно сделать только небольшое количество обработки, чтобы использовать это: мы берем лог цен, так что rmse его даст нам то, что нам в конечном счете нужно: . dep_var = &#39;SalePrice&#39; . df[dep_var] = np.log(df[dep_var]) . &#1044;&#1077;&#1088;&#1077;&#1074;&#1100;&#1103; &#1088;&#1077;&#1096;&#1077;&#1085;&#1080;&#1103; . &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Заменим каждый столбец данных набором столбцов метаданных даты, таких как праздник, день недели и месяц. Эти столбцы содержат категориальные данные, которые, как мы подозреваем, будут полезны. fastai поставляется с функцией, которая сделает это за нас—нам просто нужно передать имя столбца, содержащего данные: . df = add_datepart(df, &#39;saledate&#39;) . Давайте то же самое сделаем для тестового набора . df_test = pd.read_csv(path/&#39;Test.csv&#39;, low_memory=False) df_test = add_datepart(df_test, &#39;saledate&#39;) . Мы видим, что теперь в нашем фрейме данных появилось много новых столбцов . &#39; &#39;.join(o for o in df.columns if o.startswith(&#39;sale&#39;)) . &#39;saleWeek saleYear saleMonth saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed&#39; . &#1048;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077; TabularPandas &#1080; TabularProc . Categorify-это TabularProc, который заменяет столбец числовым категориальным столбцом. FillMissing-это TabularProc, который заменяет отсутствующие значения медианой столбца и создает новый логический столбец, который имеет значение True для любой строки, где значение отсутствовало. Эти два преобразования необходимы почти для каждого табличного набора данных, который вы будете использовать, так что это хорошая стартовая точка для обработки ваших данных: . procs = [Categorify, FillMissing] . Так как это временной ряд, то разделение на обучающие и проверочные случайным образом не получиться.В некоторых случаях это можно сделать просто случайным образом, выбрав подмножество точек данных. В связи с этим мы используем np.where, полезную функцию, которая возвращает (в качестве первого элемента кортежа) индексы всех истинных значений: . cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . TabularPandas нужно сказать, какие столбцы являются непрерывными, а какие категориальными. Мы можем справиться с этим автоматически с помощью вспомогательной функции cont_cat_split. Но это меня привело к ошибке: . TypeError: Cannot interpret &#39;UInt32Dtype()&#39; as a data type . Поиск по форумам не дал много ответов, но одно из решений удалось найти: . df = df.astype({&#39;saleWeek&#39; : &#39;uint32&#39;}) . def cont_cat_split(df, max_card=20, dep_var=None): cont_names, cat_names = [], [] for label in df: if label in L(dep_var): continue # mod to detect ProductSize type properly if (df[label].dtype.name == &#39;category&#39;): cat_names.append(label) continue if (np.issubdtype(df[label].dtype, np.integer) and df[label].unique().shape[0] &gt; max_card or np.issubdtype(df[label].dtype, np.floating)): cont_names.append(label) else: cat_names.append(label) return cont_names, cat_names . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . TabularPandas ведет себя очень похоже на fastai Datasets объект, в том числе предоставление train и valid признаков . len(to.train),len(to.valid) . (404710, 7988) . Мы видим, что данные по-прежнему отображаются в виде строк для категорий (здесь мы показываем только несколько строк, потому что полная таблица слишком велика, чтобы поместиться на странице): . to.show(3) . UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na SalesID MachineID saleWeek ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleDay saleDayofweek saleDayofyear SalePrice . 0 Low | 521D | 521 | D | #na# | #na# | #na# | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | 1163635200 | False | False | 1139246 | 999089 | 46 | 3157 | 121 | 3.0 | 2004 | 68.0 | 2006 | 11 | 16 | 3 | 320 | 11.097410 | . 1 Low | 950FII | 950 | F | II | #na# | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | 23.5 | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | 1080259200 | False | False | 1139248 | 117657 | 13 | 77 | 121 | 3.0 | 1996 | 4640.0 | 2004 | 3 | 26 | 4 | 86 | 10.950807 | . 2 High | 226 | 226 | #na# | #na# | #na# | #na# | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | New York | SSL | Skid Steer Loaders | #na# | OROPS | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Auxiliary | #na# | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | None or Unspecified | Standard | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | False | False | False | False | False | False | 1077753600 | False | False | 1139249 | 434808 | 9 | 7009 | 121 | 3.0 | 2001 | 2838.0 | 2004 | 2 | 26 | 3 | 57 | 9.210340 | . to1 = TabularPandas(df, procs, [&#39;state&#39;, &#39;ProductGroup&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;], [], y_names=dep_var, splits=splits) to1.show(3) . state ProductGroup Drive_System Enclosure SalePrice . 0 Alabama | WL | #na# | EROPS w AC | 11.097410 | . 1 North Carolina | WL | #na# | EROPS w AC | 10.950807 | . 2 New York | SSL | #na# | OROPS | 9.210340 | . Однако все базовые элементы являются числовыми: . to.items.head(3) . SalesID SalePrice MachineID saleWeek ... saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na . 0 1139246 | 11.097410 | 999089 | 46 | ... | 1 | 2647 | 1 | 1 | . 1 1139248 | 10.950807 | 117657 | 13 | ... | 1 | 2148 | 1 | 1 | . 2 1139249 | 9.210340 | 434808 | 9 | ... | 1 | 2131 | 1 | 1 | . 3 rows × 67 columns . to1.items[[&#39;state&#39;, &#39;ProductGroup&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;]].head(3) . state ProductGroup Drive_System Enclosure . 0 1 | 6 | 0 | 3 | . 1 33 | 6 | 0 | 3 | . 2 32 | 3 | 0 | 6 | . Преобразование категориальных столбцов в числа осуществляется путем простой замены каждого числом уникального значения. Мы можем увидеть их, посмотрев на атрибут classes: . to.classes[&#39;ProductSize&#39;] . [&#39;#na#&#39;, &#39;Large&#39;, &#39;Large / Medium&#39;, &#39;Medium&#39;, &#39;Small&#39;, &#39;Mini&#39;, &#39;Compact&#39;] . Поскольку обработка данных занимает около минуты, чтобы добраться до этой точки, мы должны сохранить ее—таким образом, в будущем мы сможем продолжить нашу работу отсюда, не повторяя предыдущие шаги. fastai предоставляет метод сохранения, который использует систему pickle Python для сохранения практически любого объекта Python: . save_pickle(path/&#39;to.pkl&#39;,to) . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1076;&#1077;&#1088;&#1077;&#1074;&#1100;&#1077;&#1074; &#1088;&#1077;&#1096;&#1077;&#1085;&#1080;&#1081; . Для начала определим наши независимые и зависимые переменные: . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Теперь, когда все наши данные числовые и нет пропущенных значений, мы можем создать дерево решений: . m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(xs, y); . Чтобы все было просто, мы сказали sklearn просто создать четыре листовых узла. Чтобы увидеть, чему он научился, мы можем показать дерево . draw_tree(m, xs, leaves_parallel=True, precision=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 Coupler_System ≤ 0.5 mse = 0.48 samples = 404710 value = 10.1 1 YearMade ≤ 1991.5 mse = 0.42 samples = 360847 value = 10.21 0&#45;&gt;1 True 2 mse = 0.12 samples = 43863 value = 9.21 0&#45;&gt;2 False 3 mse = 0.37 samples = 155724 value = 9.97 1&#45;&gt;3 4 ProductSize ≤ 4.5 mse = 0.37 samples = 205123 value = 10.4 1&#45;&gt;4 5 mse = 0.31 samples = 182403 value = 10.5 4&#45;&gt;5 6 mse = 0.17 samples = 22720 value = 9.62 4&#45;&gt;6 Мы можем показать разложение с помощью мощной библиотеки dtreeviz Теренса Парра: . samp_idx = np.random.permutation(len(y))[:500] dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 2021-02-04T16:51:38.742136 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ leaf5 2021-02-04T16:51:39.271160 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node4-&gt;leaf5 leaf6 2021-02-04T16:51:39.356836 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node4-&gt;leaf6 node1 2021-02-04T16:51:38.846082 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node1-&gt;node4 leaf3 2021-02-04T16:51:39.062398 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node1-&gt;leaf3 leaf2 2021-02-04T16:51:39.438554 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node0 2021-02-04T16:51:38.955728 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node0-&gt;node1 &lt; node0-&gt;leaf2 &#8805; С целью удобства визуализации давайте заменим отсутствующие значения, которые у нас обозначены через 1000 на 1950 год: . xs.loc[xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 valid_xs.loc[valid_xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 . Проверим насколько устойчивы деревья решений к проблемам с данными! . m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y) dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 2021-02-04T16:51:41.675735 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ leaf5 2021-02-04T16:51:42.122806 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node4-&gt;leaf5 leaf6 2021-02-04T16:51:42.206816 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node4-&gt;leaf6 node1 2021-02-04T16:51:41.787721 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node1-&gt;node4 leaf3 2021-02-04T16:51:42.035076 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node1-&gt;leaf3 leaf2 2021-02-04T16:51:42.279968 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node0 2021-02-04T16:51:41.919666 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ node0-&gt;node1 &lt; node0-&gt;leaf2 &#8805; Теперь пусть алгоритм дерева решений построит дерево побольше. Здесь мы не передаем никаких критериев остановки, таких как max_leaf_nodes: . m = DecisionTreeRegressor() m.fit(xs, y); . Мы создадим небольшую функцию для проверки среднеквадратичной ошибки нашей модели (m_rmse), поскольку именно так оценивался конкурс: . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . m_rmse(m, xs, y) . 0.0 . Проверим набор валидации, чтобы убедиться, что мы не переобучены: . m_rmse(m, valid_xs, valid_y) . 0.331671 . Упс—похоже, мы сильно переобучились. Вот почему: . m.get_n_leaves(), len(xs) . (324552, 404710) . Давайте изменим правило остановки и скажем sklearn, чтобы каждый конечный узел содержал по крайней мере 25 записей: . m = DecisionTreeRegressor(min_samples_leaf=25) m.fit(to.train.xs, to.train.y) m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.248595, 0.323399) . Давайте еще раз проверим количество листьев: . m.get_n_leaves() . 12397 . Random Forests . Чтобы увидеть, как это работает на практике, давайте начнем создавать свой собственный случайный лес! . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1089;&#1083;&#1091;&#1095;&#1072;&#1081;&#1085;&#1086;&#1075;&#1086; &#1083;&#1077;&#1089;&#1072; . В следующей функции n_estimators задает количество деревьев, которое нам необходимо, max_samples - сколько строк использовать из выборки для обучения каждого дерева, max_features - число признаков, по которым ищется разбиение. Вы можете указать конкретное число или процент признаков, либо выбрать из доступных значений: &quot;auto&quot; (все признаки), &quot;sqrt&quot;, &quot;log2&quot;. По дефолту стоит &quot;auto&quot;. Мы также можем указать, когда следует прекратить разбиение дерева используя параметр min_samples_leaf. Наконец, мы передаем n_jobs=-1, чтобы сказать sklearn использовать все наши процессоры для параллельного построения деревьев: . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y); . Наша валидация RMSE теперь значительно улучшена по сравнению с нашим последним результатом, полученным DecisionTreeRegressor, который построил только одно дерево для всех доступных данных: . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.170808, 0.232406) . Чтобы увидеть влияние n_estimators, давайте получим прогнозы от каждого отдельного дерева в нашем лесу (они находятся в атрибуте estimators): . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . Как вы можете видеть, preds.mean(0) дает те же результаты, что и наш случайный лес: . r_mse(preds.mean(0), valid_y) . 0.232406 . Давайте посмотрим, что происходит с RMSE, когда мы добавляем все больше и больше деревьев. Как вы можете видеть, улучшение становиться более пологим после около 30 деревьев: . plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]); . Out-of-Bag Error . Если простым языком, то поскольку каждое дерево было обучено с различным случайно выбранным подмножеством строк, ошибка out-of-bag немного похожа на представление о том, что каждое дерево, следовательно, также имеет свой собственный набор проверки. Этот набор проверки-это просто строки, которые не были выбраны для обучения этого дерева. . Прогнозы OOB доступны в атрибуте oobprediction. Обратите внимание, что мы сравниваем их с обучающими метками, так как это вычисляется на деревьях с использованием обучающего набора. . r_mse(m.oob_prediction_, y) . 0.21052 . &#1048;&#1085;&#1090;&#1077;&#1088;&#1087;&#1088;&#1080;&#1090;&#1072;&#1094;&#1080;&#1103; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . &#1044;&#1086;&#1089;&#1090;&#1086;&#1074;&#1077;&#1088;&#1085;&#1086;&#1089;&#1090;&#1100; &#1087;&#1088;&#1086;&#1075;&#1085;&#1086;&#1079;&#1072; &#1085;&#1072; &#1086;&#1089;&#1085;&#1086;&#1074;&#1072;&#1085;&#1080;&#1080; &#1076;&#1080;&#1089;&#1087;&#1077;&#1088;&#1089;&#1080;&#1080; . В предыдущем разделе, посвященном созданию случайного леса, мы видели, как получить прогнозы по набору проверки, используя списки Python для каждого дерева в лесу: . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . preds.shape . (40, 7988) . Теперь у нас есть прогноз для каждого дерева и каждого аукциона (40 деревьев и 7 988 аукционов) в наборе проверки. . Используя это, мы можем получить стандартное отклонение прогнозов по всем деревьям для каждого аукциона: . preds_std = preds.std(0) . Посмотрим на стандартные отклонения для прогнозов для первых пяти аукционов—то есть первых пяти строк проверочного набора: . preds_std[:5] . array([0.20852237, 0.11403559, 0.09531696, 0.23696253, 0.11246767]) . &#1042;&#1072;&#1078;&#1085;&#1086;&#1089;&#1090;&#1100; &#1087;&#1088;&#1080;&#1079;&#1085;&#1072;&#1082;&#1086;&#1074; . Важность можем получить их непосредственно из случайного леса sklearn, заглянув в атрибут featureimportances. Вот простая функция, которую мы можем использовать, чтобы поместить их в DataFrame и отсортировать: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . Важность признаков для нашей модели показывают, что первые несколько наиболее важных столбцов имеют гораздо более высокие оценки важности, чем остальные, причем (что неудивительно) YearMade и ProductSize находятся в верхней части списка: . fi = rf_feat_importance(m, xs) fi[:10] . cols imp . 59 YearMade | 0.171312 | . 30 Coupler_System | 0.119788 | . 6 ProductSize | 0.114872 | . 7 fiProductClassDesc | 0.068519 | . 56 ModelID | 0.054451 | . 3 fiSecondaryDesc | 0.053365 | . 50 saleElapsed | 0.050239 | . 12 Enclosure | 0.038051 | . 31 Grouser_Tracks | 0.037815 | . 32 Hydraulics_Flow | 0.032734 | . График важности признаков показывает это более наглядно: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . &#1059;&#1076;&#1072;&#1083;&#1077;&#1085;&#1080;&#1077; &#1055;&#1077;&#1088;&#1077;&#1084;&#1077;&#1085;&#1085;&#1099;&#1093; &#1089; &#1053;&#1080;&#1079;&#1082;&#1086;&#1081; &#1042;&#1072;&#1078;&#1085;&#1086;&#1089;&#1090;&#1100;&#1102; . Вполне вероятно, что мы могли бы провести достаточно неплохой расчет и без столбцов с низкой важностью. Давайте попробуем оставить только те, важность которых превышает 0,005: . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . 21 . Теперь переообучим нашу модель: . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . m = rf(xs_imp, y) . И вот результат: . m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y) . (0.181104, 0.230574) . Наша точность примерно такая же, но у нас гораздо меньше признаков для изучения: . len(xs.columns), len(xs_imp.columns) . (66, 21) . Это также облегчает интерпретацию нашего графика важности признаков. Давайте посмотрим на это еще раз: . plot_fi(rf_feat_importance(m, xs_imp)); . &#1059;&#1076;&#1072;&#1083;&#1077;&#1085;&#1080;&#1077; &#1076;&#1091;&#1073;&#1083;&#1080;&#1088;&#1091;&#1102;&#1097;&#1080;&#1093; &#1087;&#1088;&#1080;&#1079;&#1085;&#1072;&#1082;&#1086;&#1074; . Определение сходства: Наиболее похожие пары определяются путем вычисления ранговой корреляции, что означает, что все значения заменяются их рангом, а затем вычисляется корреляция. . cluster_columns(xs_imp) . Давайте попробуем удалить некоторые из этих тесно связанных признаков, чтобы увидеть, можно ли упростить модель, не влияя на точность. Во-первых, мы создаем функцию, которая быстро обучает случайный лес и возвращает оценку OOB, используя более низкий max_samples и более высокий min_samples_leaf. Оценка OOB - это число, возвращаемое sklearn, которое колеблется между 1,0 для идеальной модели и 0,0 для случайной модели. (В статистике это называется R^2, хотя детали не важны для этого объяснения.) Нам не нужно, чтобы оно был очень точным—мы просто будем использовать его для сравнения различных моделей, основанных на удалении некоторых, возможно дублирующих столбцов: . def get_oob(df): m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True) m.fit(df, y) return m.oob_score_ . Вот наш Baseline . get_oob(xs_imp) . 0.8764410377988594 . Теперь мы попытаемся удалить каждую из наших потенциально дублирующих переменных, по одной за раз: . {c:get_oob(xs_imp.drop(c, axis=1)) for c in ( &#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;ProductGroupDesc&#39;,&#39;ProductGroup&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;Hydraulics_Flow&#39;,&#39;Grouser_Tracks&#39;, &#39;Coupler_System&#39;)} . {&#39;saleYear&#39;: 0.8761576456556869, &#39;saleElapsed&#39;: 0.8720694164876017, &#39;ProductGroupDesc&#39;: 0.8773236585083313, &#39;ProductGroup&#39;: 0.8767961703098911, &#39;fiModelDesc&#39;: 0.8763622619440036, &#39;fiBaseModel&#39;: 0.8756228700540764, &#39;Hydraulics_Flow&#39;: 0.8778530782106663, &#39;Grouser_Tracks&#39;: 0.8772923083390904, &#39;Coupler_System&#39;: 0.8778145616354095} . Теперь давайте попробуем отбросить несколько переменных. Мы отбросим по одному от каждой из плотно взаимосвязанных в парах. Посмотрим, что это даст: . to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] get_oob(xs_imp.drop(to_drop, axis=1)) . 0.8751218470772454 . Хорошо выглядит и не намного хуже, чем модель со всеми значениями. Давайте создадим DataFrame без этих столбцов и сохраним его: . xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) . Старый код не сработал, пришлось опять искать ответ . #(path/&#39;valid_xs_final.pkl&#39;).save(valid_xs_final) save_pickle(path/&#39;xs_final.pkl&#39;, xs_final) save_pickle(path/&#39;valid_xs_final.pkl&#39;, valid_xs_final) . Мы можем его загрузить . xs_final = load_pickle(path/&#39;xs_final.pkl&#39;) valid_xs_final = load_pickle(path/&#39;valid_xs_final.pkl&#39;) . Теперь мы можем снова проверить наш RMSE, чтобы убедиться, что точность существенно не изменилась. . m = rf(xs_final, y) m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) . (0.183382, 0.23278) . &#1063;&#1072;&#1089;&#1090;&#1080;&#1095;&#1085;&#1072;&#1103; &#1047;&#1072;&#1074;&#1080;&#1089;&#1080;&#1084;&#1086;&#1089;&#1090;&#1100; . Как мы уже видели, два наиболее важных предиктора-это ProductSize и YearMade. Мы хотели бы понять взаимосвязь между этими предикторами и ценой продажи. Рекомендуется сначала проверить количественно значение для каждой категории (при помощи метода Pandas value_counts): . p = valid_xs_final[&#39;ProductSize&#39;].value_counts(sort=False).plot.barh() c = to.classes[&#39;ProductSize&#39;] plt.yticks(range(len(c)), c); . Самая большая группа - #na#, это метка, которую fastai применяет к пропущенным значениям. . Давайте сделаем то же самое для YearMade. Поскольку это числовая значение, нам нужно нарисовать гистограмму, которая сгруппирует значения по годам: . ax = valid_xs_final[&#39;YearMade&#39;].hist() . Мы заменим каждое значение в столбце YearMade на значение из 1950, а затем рассчитаем прогнозируемую цену продажи для каждого аукциона и возьмем среднее значение по всем аукционам. Затем мы делаем то же самое для 1951, 1952 и так далее до нашего последнего 2011 года. Это изолирует эффект для YearMade. . С помощью этих средних значений мы можем построить график с годами по оси x и каждым из полученных прогнозов-по оси y. Давайте посмотрим: . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_final, [&#39;YearMade&#39;,&#39;ProductSize&#39;], grid_resolution=20, ax=ax); . &#1059;&#1090;&#1077;&#1095;&#1082;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Тривиальным примером утечки может служить модель, которая использует саму цель в качестве входных данных:&quot;в дождливые дни идет дождь&quot;. . &#1048;&#1085;&#1090;&#1077;&#1088;&#1087;&#1088;&#1077;&#1090;&#1072;&#1090;&#1086;&#1088; &#1076;&#1077;&#1088;&#1077;&#1074;&#1072; . Давайте выберем первые несколько строк нашего набора проверки: . row = valid_xs_final.iloc[:5] . Затем мы можем передать их treeinterpreter: . prediction,bias,contributions = treeinterpreter.predict(m, row.values) . prediction-это просто предсказание, которое делает случайный лес. bias-это предсказание, основанное на принятии среднего значения зависимой переменной (то есть модели, которая является корнем каждого дерева). contributions—это самый интересный бит-он говорит нам об общем изменении предсказания из-за каждой из независимых переменных. Следовательно, сумма contributions плюс bias должна равняться prediction для каждой строки. Давайте посмотрим только на первый ряд: . prediction[0], bias[0], contributions[0].sum() . (array([10.01255406]), 10.104746057831763, -0.092191998676771) . Самый ясный способ отображения вкладов-это график водопада (waterfall plot). Он показывает, как положительные и отрицательные вклады всех независимых переменных суммируются, чтобы создать окончательный прогноз, который является правым столбцом с надписью &quot;net&quot;: . waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, rotation_value=45,formatting=&#39;{:,.3f}&#39;); . &#1069;&#1082;&#1089;&#1090;&#1088;&#1072;&#1087;&#1086;&#1083;&#1103;&#1094;&#1080;&#1103; &#1080; &#1085;&#1077;&#1081;&#1088;&#1086;&#1085;&#1085;&#1099;&#1077; &#1089;&#1077;&#1090;&#1080; . &#1055;&#1088;&#1086;&#1073;&#1083;&#1077;&#1084;&#1072; &#1101;&#1082;&#1089;&#1090;&#1088;&#1086;&#1087;&#1086;&#1083;&#1103;&#1094;&#1080;&#1080; . Давайте рассмотрим простую задачу построения прогнозов из 40 точек данных, показывающих слегка зашумленную линейную зависимость: . x_lin = torch.linspace(0,20, steps=40) y_lin = x_lin + torch.randn_like(x_lin) plt.scatter(x_lin, y_lin); . Хотя у нас есть только одна независимая переменная, sklearn ожидает матрицу независимых переменных, а не один вектор. Поэтому мы должны превратить наш вектор в матрицу с одним столбцом. Другими словами, мы должны изменить форму с [40] на [40,1]. Один из способов сделать это-с помощью метода unsqueeze, который добавляет новую единичную ось к тензору в требуемом измерении: . xs_lin = x_lin.unsqueeze(1) x_lin.shape,xs_lin.shape . (torch.Size([40]), torch.Size([40, 1])) . Более гибкий подход состоит в том, чтобы сделать срез массива или тензора со специальным значением None, которое вводит тогда дополнительную единичную ось: . x_lin[:,None].shape . torch.Size([40, 1]) . Теперь мы можем создать случайный лес для этих данных. Мы будем использовать только первые 30 строк для обучения модели: . m_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30]) . Затем мы протестируем модель на полном наборе данных. Синие точки-это обучающие данные, а красные-предсказания: . plt.scatter(x_lin, y_lin, 20) plt.scatter(x_lin, m_lin.predict(xs_lin), color=&#39;red&#39;, alpha=0.5); . Случайные леса не могут экстраполироваться для данных, которые находятся за пределами тех, на которых они проходили обучение. . &#1055;&#1086;&#1080;&#1089;&#1082; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1079;&#1072; &#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1072;&#1084;&#1080; . Иногда трудно понять, распределен ли ваш тестовый набор таким же образом, как и ваши обучающие данные, или, если он отличается, какие столбцы отвечают за это. На самом деле есть простой способ понять это-использовать случайный лес! . Чтобы увидеть это в действии, давайте объединим наши обучающие и проверочные наборы вместе, создадим зависимую переменную, представляющую, из какого набора данных берется каждая строка, построим случайный лес, используя эти данные, и получим важность: . df_dom = pd.concat([xs_final, valid_xs_final]) is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final)) m = rf(df_dom, is_valid) rf_feat_importance(m, df_dom)[:6] . cols imp . 6 saleElapsed | 0.904446 | . 11 SalesID | 0.070529 | . 14 MachineID | 0.020080 | . 0 YearMade | 0.001697 | . 15 Tire_Size | 0.000548 | . 13 Hydraulics | 0.000527 | . Давайте получим базовую линию RMSE исходной модели случайного леса, а затем посмотрим, каков эффект удаления каждого из этих столбцов по очереди: . m = rf(xs_final, y) print(&#39;orig&#39;, m_rmse(m, valid_xs_final, valid_y)) for c in (&#39;SalesID&#39;,&#39;saleElapsed&#39;,&#39;MachineID&#39;): m = rf(xs_final.drop(c,axis=1), y) print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y)) . orig 0.232436 SalesID 0.23066 saleElapsed 0.235298 MachineID 0.231625 . Похоже, мы можем удалить SalesID и MachineID без потери какой-либо точности. Давайте проверим: . time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) m = rf(xs_final_time, y) m_rmse(m, valid_xs_time, valid_y) . 0.228595 . Одна вещь, которая может помочь в нашем случае, - это просто избегать использования старых данных. Часто старые данные показывают отношения, которых больше нет. Давайте попробуем просто использовать данные за последние несколько лет: . xs[&#39;saleYear&#39;].hist(); . Вот результат обучения на этом подмножестве: . filt = xs[&#39;saleYear&#39;]&gt;2004 xs_filt = xs_final_time[filt] y_filt = y[filt] . m = rf(xs_filt, y_filt) m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y) . (0.177588, 0.229683) . &#1048;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077; &#1085;&#1077;&#1081;&#1088;&#1086;&#1085;&#1099;&#1093; &#1089;&#1077;&#1090;&#1077;&#1081; . Мы можем использовать тот же подход для построения нейросетевой модели. Давайте сначала повторим шаги, которые мы предприняли для настройки объекта TabularPandas: . df_nn = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) df_nn[dep_var] = np.log(df_nn[dep_var]) df_nn = add_datepart(df_nn, &#39;saledate&#39;) . Мы можем обрезать те же столбцы, что мы уже сделали в случайном лесу: . df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] . cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var) . Есть одна переменная, которую мы абсолютно не хотим рассматривать как категориальную: переменная saleElapsed. Категориальная переменная по определению не может экстраполироваться за пределы диапазона значений, которые она видела, но мы хотим иметь возможность предсказывать аукционные цены продажи в будущем. Поэтому нам нужно сделать ее непрерывной переменной: . cont_nn.append(&#39;saleElapsed&#39;) cat_nn.remove(&#39;saleElapsed&#39;) . Кроме того, чтобы использовать ее как непрерывную переменную, мы должны убедиться, что она имеет числовой тип: . df_nn[&#39;saleElapsed&#39;] = df_nn[&#39;saleElapsed&#39;].astype(int) . Давайте посмотрим на мощность каждой из категориальных переменных: . df_nn_final[cat_nn].nunique() . YearMade 73 Coupler_System 2 ProductSize 6 fiProductClassDesc 74 ModelID 5281 fiSecondaryDesc 177 Enclosure 6 Hydraulics_Flow 3 fiModelDesc 5059 fiModelDescriptor 140 ProductGroup 6 Hydraulics 12 Tire_Size 17 Drive_System 4 dtype: int64 . Давайте посмотрим, какое влияние удаление одного из этих столбцов модели оказывает на случайный лес: . xs_filt2 = xs_filt.drop(&#39;fiModelDescriptor&#39;, axis=1) valid_xs_time2 = valid_xs_time.drop(&#39;fiModelDescriptor&#39;, axis=1) m2 = rf(xs_filt2, y_filt) m_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y) . (0.176528, 0.229976) . Mинимальное влияние, поэтому мы удалим его: . cat_nn.remove(&#39;fiModelDescriptor&#39;) . Для нейроной сети мы добавляем процесс нормализации: . df_nn_final = df_nn_final.astype({&quot;saleElapsed&quot;: float}) . df_nn_final.dtypes . YearMade int64 Coupler_System object ProductSize category fiProductClassDesc object ModelID int64 fiSecondaryDesc object saleElapsed float64 Enclosure object Hydraulics_Flow object fiModelDesc object fiModelDescriptor object ProductGroup object Hydraulics object Tire_Size object Drive_System object SalePrice float64 dtype: object . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . Табличные модели и данные обычно не требуют большого объема оперативной памяти GPU, поэтому мы можем использовать большие размеры пакетов . dls = to_nn.dataloaders(1024) . Давайте найдем min и max нашей зависимой переменной: . y = to_nn.train.y y.min(),y.max() . (8.465899, 11.863583) . По умолчанию для табличных данных fastai создает нейронную сеть с двумя скрытыми слоями, с 200 и 100 активациями соответственно. Это довольно хорошо работает для небольших наборов данных, но здесь у нас есть довольно большой набор данных, поэтому мы увеличиваем размеры слоев до 500 и 250: . from fastai.tabular.all import * . learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) . learn.lr_find() . SuggestedLRs(lr_min=0.002754228748381138, lr_steep=0.0002754228771664202) . Мы будем тренироваться с fit_one_cycle в течение нескольких эпох и посмотрим, как это выглядит: . learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss time . 0 | 0.069288 | 0.061883 | 00:07 | . 1 | 0.055936 | 0.058179 | 00:07 | . 2 | 0.049000 | 0.057057 | 00:07 | . 3 | 0.043699 | 0.051876 | 00:07 | . 4 | 0.040626 | 0.050812 | 00:07 | . Мы можем использовать нашу функцию r_mse для сравнения результата с результатом случайного леса, полученным ранее: . preds,targs = learn.get_preds() r_mse(preds,targs) . 0.225416 . Это немного лучше, чем случайный лес (хотя обучение заняло больше времени, и оно более требовательно к настройке гиперпараметров). . Прежде чем мы двинемся дальше, давайте сохраним нашу модель на случай, если мы захотим вернуться к ней позже: . learn.save(&#39;nn&#39;) . Path(&#39;models/nn.pth&#39;) . &#1040;&#1085;&#1089;&#1072;&#1084;&#1073;&#1083;&#1100; . Как мы видели ранее, случайный лес сам по себе является ансамблем. Мы можем включить случайный лес в другой ансамбль—ансамбль случайного леса и нейронной сети! . Одна незначительная проблема заключается в том, что наша модель PyTorch и наша модель sklearn создают данные разных типов: PyTorch дает нам тензор ранга-2 (то есть матрицу столбцов), тогда как NumPy дает нам массив ранга-1 (вектор). squeeze удаляет любые единичные оси из тензора, а to_np преобразует его в массив NumPy: . rf_preds = m.predict(valid_xs_time) ens_preds = (to_np(preds.squeeze()) + rf_preds) /2 . Это дает нам лучший результат, чем любая модель, построенная сама по себе: . r_mse(ens_preds,valid_y) . 0.222111 .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2021/02/11/09-tabular.html",
            "relUrl": "/russian/fast.ai/solution/2021/02/11/09-tabular.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Russian - Fastbook Chapter 9 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы к девятой части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, как и к осталььным частям прошу писать в коментариях - поправлю. . 1. Что такое непрерывная переменная? . Это понятие относится к числовым переменным, которые имеют непрерывный диапазон значений (например, возраст) . 2. Что такое категориальная переменная? . Это понятия относится к переменным, которые принимать дискретные значения в соответствие с различными категориями. . 3. Укажите 2 слова, которые используются для возможных значений категориальной переменной. . Уровни или категории . 4. Что такое “основной слой(dense layer)”? . Эквивалентно тому, что мы называем линейными слоями. . 5. Как элементы эмбедингов уменьшают использование памяти и ускоряют работу нейронных сетей? . Особенно для больших наборов данных представление данных в виде one-hot encoded векторов может быть очень неэффективным (а также разбросанным). С другой стороны, использование элементов эмбедингов позволяет данным иметь гораздо более эффективное (плотное) представление данных в памяти. Это также приведет к ускорению работы модели. . 6. Для каких наборов данных особенно полезны элементы эмбендинга? . Это особенно полезно для наборов данных с объектами, имеющими большое количество элементов (объекты имеют множество возможных категорий). Другие методы часто переобучаются для таких данных. . 7. Каковы два основных семейства алгоритмов машинного обучения? . Ансамбль деревьев решений лучше всего подходит для структурированных (табличных) данных | Многослойные нейронные сети лучше всего подходят для неструктурированных данных (аудио, зрение, текст и т. д.) | . 8. Почему некоторые категориальные столбцы нуждаются в особом порядке в своих классах? Как это делается pandas? . Категории по своей сути могут иметь некоторый порядок, и при использовании set_categories с аргументом ordered=True и переходящий в упорядоченном списке эта информация представлена в DataFrame pandas. . 9. Подведите итог тому, что делает алгоритм дерева решений. . Основная идея алгоритма дерева решений состоит в том, чтобы определить, как группировать данные на основе “вопросов”, которые мы задаем о данных. То есть мы продолжаем разбивать данные на основе уровней или значений признаков и генерируем прогнозы на основе среднего целевого значения точек данных в этой группе. Вот алгоритм: . Циклически просматривайте по очереди каждый столбец набора данных | Для каждого столбца выполните цикл через каждый возможный уровень | Попробуйте разделить данные на две группы в зависимости от того, больше или меньше они определенного значения (или если это категориальная переменная, то в зависимости от того, равны они или не равны этой категориальной переменной) | Найдите среднюю цену продажи для каждой из этих двух групп и посмотрите, насколько она близка к фактической цене продажи каждого из предметов оборудования в этой группе. То есть рассматривайте это как очень простую “модель”, где наши прогнозы-это просто средняя цена продажи группы товаров | Пройдя по всем столбцам и возможным уровням для каждого из них, выберите точку разделения, которая дала лучшие прогнозы | Теперь у нас есть две разные группы для наших данных, основанные на этом выбранном разделении. Рассматривайте каждый из них как отдельные наборы данных и найдите лучшее разделение для каждого из них, вернувшись к шагу один для каждой группы | Продолжайте этот процесс рекурсивно и до тех пор, пока не достигнете некоторого критерия остановки для каждой группы — например, прекратите дальнейшее разделение группы, когда в ней всего 20 элементов. | . 10. Почему дата отличается от обычной категориальной или непрерывной переменной и как вы можете предварительно обработать ее, чтобы она могла использоваться в модели? . Некоторые даты (например, некоторые из них являются праздниками, выходными и т. д.) не могут быть описаны как просто порядковые переменные. Вместо этого мы можем сгенерировать множество различных категориальных признаков о свойствах данной даты (например, это будний день? это конец месяца? и т. д.) . 11. Должны ли вы выбрать случайный проверочный набор в конкурсе бульдозеров? Если нет, то какой набор валидации вы должны выбрать? . Нет, проверочный набор должен быть максимально похож на тестовый набор. В этом случае тестовый набор содержит данные из более поздних данных, поэтому мы должны разделить данные по датам и включить более поздние даты в набор проверки. . 12. Что такое метод pickle и для чего он полезен? . Позволяет сохранить практически любой объект Python в виде файла. . 13. Как вычисляются значения в дереве решений? . Проходя по дереву, основанному на ответе на вопросы о данных, мы достигаем узлов, которые сообщают нам среднее значение данных в этой группе, mse и количество значений в этой группе. . 14.Как мы справляемся с выбросами, прежде чем построить дерево решений? . Иногда трудно даже понять, распределен ли ваш тестовый набор таким же образом, как и ваши обучающие данные, или, если он отличается, то какие столбцы отражают эту разницу. На самом деле есть очень простой способ понять это, а именно использовать случайный лес! . Но в этом случае мы не используем случайный лес для предсказания нашей фактической зависимой переменной. Вместо этого мы пытаемся предсказать, находится ли строка в наборе проверки или в обучающем наборе. . 15. Как мы обрабатываем категориальные переменные в дереве решений? . Мы преобразуем категориальные переменные в целые числа, где целые числа соответствуют определенным уровням категориальной переменной. В связи с чем нет ничего сложного, чтобы заставить их работать в деревьях решений (в отличие от нейронных сетей, где мы используем слои встраивания). . 16. Что такое пакетирование (bagging)? . Обучение несколько моделей на случайных подмножествах данных и использование их ансамбля для прогнозирования. . 17. В чем разница между max_samples и max_features при создании случайного леса? . При работе со случайными лесами, мы обучаем несколько деревьев решений на случайных подмножествах данных. max_samples определяет сколько мы будем использовать число экземпляров, или строк из табличного DataSet для каждого дерева решений. max_features определяет, сколько признаков, или столбцов из табличного DataSet мы используем для каждого дерева решений. . 18. Если вы увеличите n_estimators до очень высокого значения, может ли это привести к переобучению? Почему или почему нет? . Более высокие значения n_estimators означают, что используется больше деревьев решений. Однако, поскольку деревья независимы друг от друга, использование более высоких n_estimators не приводит к переобучению. . 19. Что такое out of bag error? . Это встроенная версия расчёта ошибки теста. Это очень удобно, потому что вам не нужно откладывать набор данных в самом начале. Проверочный набор не требуется. . 20. Составьте список причин, по которым ошибка проверочного набора модели может быть хуже ошибки OOB. Как вы можете проверить свою гипотезу? . Основная причина может заключаться в том, что модель плохо обобщается. Это связано возможно с тем, что данные проверки имеют несколько иное распределение, чем данные, на которых обучалась модель. . 21. Как вы можете ответить на каждый из этих вопросов случайным лесом? Как они работают?: . Насколько мы уверены в своих прогнозах, используя определенный ряд данных? . Посмотрите на стандартное отклонение между оценками . Для прогнозирования с помощью определенного ряда данных, каковы были наиболее важные факторы, и как они повлияли на это предсказание? . Использование пакета treeinterpreter для проверки того, как прогноз изменяется по мере прохождения дерева, суммируя вклады от каждого разделения/функции. Используйте график waterfall для визуализации. . Какие столбцы являются самыми сильными предсказателями? . Посмотрите на важность признака . Как меняются предсказания, когда мы меняем эти столбцы? . Посмотрите на графики частичной зависимости. . 22. Какова цель удаления неважных переменных? . Иногда лучше иметь более интерпретируемую модель с меньшим количеством признаков, поэтому удаление неважных переменных помогает в этом отношении. . 23. Что такое хороший тип графика для отображения результатов интерпретатора дерева? . Waterfall график. . 24. В чем заключается проблема экстраполяции ? . Модель трудно экстраполировать на данные, которые находятся за пределами области обучающих данных. Это особенно важно для случайных лесов. С другой стороны, нейронные сети имеют лежащие в основе линейные слои, поэтому они потенциально могут лучше обобщать. . 25. Как вы можете определить, распределен ли ваш тестовый или проверочный набор по-другому, чем ваш тренировочный? . Мы можем сделать это, обучив модель классифицировать, являются ли данные обучающими или проверочными. Если данные имеют различные распределения (out-of-domain данные), то модель может правильно классифицировать между двумя наборами данных. . 26. Почему мы делаем saleElapsed непрерывной переменной, даже если она имеет менее 9000 различных значений? . Это переменная, которая меняется со временем, и поскольку мы хотим, чтобы наша модель экстраполировалась на будущие результаты, мы делаем ее непрерывной переменной. . 27. Что такое бустинг? . Мы обучаем модель, которая использует не весь набор данных, и обучаем последующие модели, которые предсказывают ошибку исходной модели. Затем мы добавляем предсказания всех моделей, чтобы получить окончательный прогноз. . 28. Как мы могли бы использовать embeddings со случайным лесом? Будем ли мы ожидать, что это поможет? . Embeddings содержат более богатые представления категориальных признаков и, безусловно, могут улучшить производительность моделей, таких как случайные леса. Вместо того чтобы передавать необработанные категориальные столбцы, Embeddings могут быть переданы в модель случайного леса. . 29. Почему мы можем не всегда использовать нейронную сеть для табличного моделирования? . Мы можем не использовать их, потому что их труднее и долше обучать, и они менее понятны. Вместо этого первым выбором должны быть случайные леса, а уже попробовать использовать нейронные сети, чтобы улучшить эти результаты или добавить их в ансамбль. .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2021/02/08/fastai-chapter9-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2021/02/08/fastai-chapter9-solution.html",
            "date": " • Feb 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Russian - Solution Lesson 8 on Fast.ai",
            "content": "&#1057;&#1086;&#1074;&#1084;&#1077;&#1089;&#1090;&#1085;&#1072;&#1103; &#1092;&#1080;&#1083;&#1100;&#1090;&#1088;&#1072;&#1094;&#1080;&#1103; &#1087;&#1088;&#1080; Deep Dive . Одна очень распространенная проблема, которую нужно решить, - это когда у вас есть несколько пользователей и несколько продуктов, и вы хотите рекомендовать, какие продукты, скорее всего, будут интересны для этих пользователей на основе схожих интересов. Существует общее решение этой проблемы, называемое коллаборативной фильтрацией, которое работает следующим образом: посмотрите, какие продукты использовал или любил текущий пользователь, найдите других пользователей, которые использовали или любили похожие продукты, а затем порекомендуйте другие продукты, которые эти пользователи использовали или любили. . &#1055;&#1077;&#1088;&#1074;&#1099;&#1081; &#1074;&#1079;&#1075;&#1083;&#1103;&#1076; &#1085;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1077; . MovieLens - этот набор данных содержит десятки миллионов рейтингов фильмов (сочетание идентификатора фильма, идентификатора пользователя и числового рейтинга) . from fastai.collab import * from fastai.tabular.all import * path = untar_data(URLs.ML_100k) . Согласно README, основная таблица находится в файле u.data. Колонки в нем разделены табуляциейю Столбцы - пользователь, фильм, рейтинг и время. Поскольку эти имена не закодированы, нам нужно указать их при чтении файла при помощи Pandas. . ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . Посмотрим эти данные в привычной для пользователя виде . . . Необходимо заполнить пустые места. . Если предположить, что факторы варьируются от -1 до +1, причем положительные числа указывают на более сильную тягу, а отрицательные-на более слабую, а категории-это научная фантастика, боевик и старые фильмы, то мы могли бы представить фильм &quot;Последний Скайуокер&quot; как: . last_skywalker = np.array([0.98,0.9,-0.9]) . Если предположить научно-фантастические фильмы как 0,98, а не очень старый как -0,9, то пользователь, который любит современные научно-фантастические боевики: . user1 = np.array([0.9,0.8,-0.6]) . теперь можно вычислить соответствие между этими комбинациями: . (user1*last_skywalker).sum() . 2.1420000000000003 . С другой стороны, мы могли бы представить фильм &quot;Касабланка&quot; как: . casablanca = np.array([-0.99,-0.3,0.8]) . соответствие между этими комбинациями: . (user1*casablanca).sum() . -1.611 . &#1048;&#1089;&#1089;&#1083;&#1077;&#1076;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077; &#1089;&#1082;&#1088;&#1099;&#1090;&#1099;&#1093; &#1092;&#1072;&#1082;&#1090;&#1086;&#1088;&#1086;&#1074; . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; DataLoaders . Когда данные выводят на печть, то мы предпочитаем видеть названия фильмов, а не их идентификаторы. Таблица u.item содержит замену идентификаторов заголовками: . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . Мы можем объединить это с нашей таблицей рейтингов, чтобы соответсвующие рейтинги пользователей с названием фильмов: . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . Затем мы можем построить объект Dataloader из этой таблицы. По умолчанию в нем первый столбец занимают пользователи, второй столбец элементы (здесь наши фильмы) и третий столбец - рейтинги. Нам нужно изменить значение item_name, чтобы использовать заголовки фильмов вместо их идентификаторов: . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . Чтобы изобразить совместную фильтрацию в PyTorch, мы не можем просто использовать напрямую кросс-табличное отоброжение, особенно если мы хотим, чтобы оно вписывалось в нашу структуру глубокого обучения. Мы можем представить наши таблицы скрытых факторов фильмов и пользователей в виде простых матриц: . dls.classes . {&#39;user&#39;: [&#39;#na#&#39;, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943], &#39;title&#39;: [&#39;#na#&#39;, &#34;&#39;Til There Was You (1997)&#34;, &#39;1-900 (1994)&#39;, &#39;101 Dalmatians (1996)&#39;, &#39;12 Angry Men (1957)&#39;, &#39;187 (1997)&#39;, &#39;2 Days in the Valley (1996)&#39;, &#39;20,000 Leagues Under the Sea (1954)&#39;, &#39;2001: A Space Odyssey (1968)&#39;, &#39;3 Ninjas: High Noon At Mega Mountain (1998)&#39;, &#39;39 Steps, The (1935)&#39;, &#39;8 1/2 (1963)&#39;, &#39;8 Heads in a Duffel Bag (1997)&#39;, &#39;8 Seconds (1994)&#39;, &#39;A Chef in Love (1996)&#39;, &#39;Above the Rim (1994)&#39;, &#39;Absolute Power (1997)&#39;, &#39;Abyss, The (1989)&#39;, &#39;Ace Ventura: Pet Detective (1994)&#39;, &#39;Ace Ventura: When Nature Calls (1995)&#39;, &#39;Across the Sea of Time (1995)&#39;, &#39;Addams Family Values (1993)&#39;, &#39;Addicted to Love (1997)&#39;, &#39;Addiction, The (1995)&#39;, &#39;Adventures of Pinocchio, The (1996)&#39;, &#39;Adventures of Priscilla, Queen of the Desert, The (1994)&#39;, &#39;Adventures of Robin Hood, The (1938)&#39;, &#39;Affair to Remember, An (1957)&#39;, &#39;African Queen, The (1951)&#39;, &#39;Afterglow (1997)&#39;, &#39;Age of Innocence, The (1993)&#39;, &#39;Aiqing wansui (1994)&#39;, &#39;Air Bud (1997)&#39;, &#39;Air Force One (1997)&#39;, &#39;Air Up There, The (1994)&#39;, &#39;Airheads (1994)&#39;, &#39;Akira (1988)&#39;, &#39;Aladdin (1992)&#39;, &#39;Aladdin and the King of Thieves (1996)&#39;, &#39;Alaska (1996)&#39;, &#39;Albino Alligator (1996)&#39;, &#39;Alice in Wonderland (1951)&#39;, &#39;Alien (1979)&#39;, &#39;Alien 3 (1992)&#39;, &#39;Alien: Resurrection (1997)&#39;, &#39;Aliens (1986)&#39;, &#39;All About Eve (1950)&#39;, &#39;All Dogs Go to Heaven 2 (1996)&#39;, &#39;All Over Me (1997)&#39;, &#39;All Things Fair (1996)&#39;, &#39;Alphaville (1965)&#39;, &#39;Amadeus (1984)&#39;, &#39;Amateur (1994)&#39;, &#39;Amazing Panda Adventure, The (1995)&#39;, &#39;American Buffalo (1996)&#39;, &#39;American Dream (1990)&#39;, &#39;American President, The (1995)&#39;, &#39;American Strays (1996)&#39;, &#39;American Werewolf in London, An (1981)&#39;, &#39;American in Paris, An (1951)&#39;, &#39;Amistad (1997)&#39;, &#34;Amityville 1992: It&#39;s About Time (1992)&#34;, &#39;Amityville 3-D (1983)&#39;, &#39;Amityville Curse, The (1990)&#39;, &#39;Amityville Horror, The (1979)&#39;, &#39;Amityville II: The Possession (1982)&#39;, &#39;Amityville: A New Generation (1993)&#39;, &#39;Amityville: Dollhouse (1996)&#39;, &#39;Amos &amp; Andrew (1993)&#39;, &#39;An Unforgettable Summer (1994)&#39;, &#39;Anaconda (1997)&#39;, &#39;Anastasia (1997)&#39;, &#39;Andre (1994)&#39;, &#39;Angel Baby (1995)&#39;, &#39;Angel and the Badman (1947)&#39;, &#39;Angel on My Shoulder (1946)&#39;, &#39;Angela (1995)&#39;, &#39;Angels and Insects (1995)&#39;, &#39;Angels in the Outfield (1994)&#39;, &#39;Angus (1995)&#39;, &#39;Anna (1996)&#39;, &#39;Anna Karenina (1997)&#39;, &#39;Anne Frank Remembered (1995)&#39;, &#39;Annie Hall (1977)&#39;, &#39;Another Stakeout (1993)&#39;, &#34;Antonia&#39;s Line (1995)&#34;, &#39;Aparajito (1956)&#39;, &#39;Apartment, The (1960)&#39;, &#39;Apocalypse Now (1979)&#39;, &#39;Apollo 13 (1995)&#39;, &#39;Apostle, The (1997)&#39;, &#39;Apple Dumpling Gang, The (1975)&#39;, &#34;April Fool&#39;s Day (1986)&#34;, &#39;Apt Pupil (1998)&#39;, &#39;Aristocats, The (1970)&#39;, &#39;Army of Darkness (1993)&#39;, &#39;Around the World in 80 Days (1956)&#39;, &#39;Arrival, The (1996)&#39;, &#39;Arsenic and Old Lace (1944)&#39;, &#39;As Good As It Gets (1997)&#39;, &#39;Assassins (1995)&#39;, &#39;Assignment, The (1997)&#39;, &#39;Associate, The (1996)&#39;, &#39;Audrey Rose (1977)&#39;, &#39;August (1996)&#39;, &#39;Austin Powers: International Man of Mystery (1997)&#39;, &#39;Awfully Big Adventure, An (1995)&#39;, &#39;Ayn Rand: A Sense of Life (1997)&#39;, &#39;B*A*P*S (1997)&#39;, &#39;B. Monkey (1998)&#39;, &#39;Babe (1995)&#39;, &#39;Baby-Sitters Club, The (1995)&#39;, &#39;Babyfever (1994)&#39;, &#39;Babysitter, The (1995)&#39;, &#39;Back to the Future (1985)&#39;, &#39;Backbeat (1993)&#39;, &#39;Bad Boys (1995)&#39;, &#39;Bad Company (1995)&#39;, &#39;Bad Girls (1994)&#39;, &#39;Bad Moon (1996)&#39;, &#39;Bad Taste (1987)&#39;, &#39;Ballad of Narayama, The (Narayama Bushiko) (1958)&#39;, &#39;Balto (1995)&#39;, &#39;Bananas (1971)&#39;, &#39;Band Wagon, The (1953)&#39;, &#39;Barb Wire (1996)&#39;, &#39;Barbarella (1968)&#39;, &#39;Barcelona (1994)&#39;, &#39;Basic Instinct (1992)&#39;, &#39;Basketball Diaries, The (1995)&#39;, &#39;Basquiat (1996)&#39;, &#39;Bastard Out of Carolina (1996)&#39;, &#39;Batman &amp; Robin (1997)&#39;, &#39;Batman (1989)&#39;, &#39;Batman Forever (1995)&#39;, &#39;Batman Returns (1992)&#39;, &#39;Baton Rouge (1988)&#39;, &#39;Bean (1997)&#39;, &#39;Beans of Egypt, Maine, The (1994)&#39;, &#39;Beat the Devil (1954)&#39;, &#39;Beautician and the Beast, The (1997)&#39;, &#39;Beautiful Girls (1996)&#39;, &#39;Beautiful Thing (1996)&#39;, &#39;Beauty and the Beast (1991)&#39;, &#39;Beavis and Butt-head Do America (1996)&#39;, &#39;Bed of Roses (1996)&#39;, &#39;Bedknobs and Broomsticks (1971)&#39;, &#39;Before Sunrise (1995)&#39;, &#39;Before and After (1996)&#39;, &#39;Before the Rain (Pred dozhdot) (1994)&#39;, &#39;Being Human (1993)&#39;, &#39;Being There (1979)&#39;, &#39;Believers, The (1987)&#39;, &#39;Belle de jour (1967)&#39;, &#39;Ben-Hur (1959)&#39;, &#39;Benny &amp; Joon (1993)&#39;, &#39;Bent (1997)&#39;, &#39;Best Men (1997)&#39;, &#39;Best of the Best 3: No Turning Back (1995)&#39;, &#39;Better Off Dead... (1985)&#39;, &#39;Beverly Hillbillies, The (1993)&#39;, &#39;Beverly Hills Cop III (1994)&#39;, &#39;Beverly Hills Ninja (1997)&#39;, &#39;Bewegte Mann, Der (1994)&#39;, &#39;Beyond Bedlam (1993)&#39;, &#39;Beyond Rangoon (1995)&#39;, &#39;Bhaji on the Beach (1993)&#39;, &#39;Big Bang Theory, The (1994)&#39;, &#39;Big Blue, The (Grand bleu, Le) (1988)&#39;, &#39;Big Bully (1996)&#39;, &#39;Big Green, The (1995)&#39;, &#39;Big Lebowski, The (1998)&#39;, &#39;Big Night (1996)&#39;, &#39;Big One, The (1997)&#39;, &#39;Big Sleep, The (1946)&#39;, &#39;Big Squeeze, The (1996)&#39;, &#39;Billy Madison (1995)&#39;, &#39;Bio-Dome (1996)&#39;, &#39;Bird of Prey (1996)&#39;, &#39;Birdcage, The (1996)&#39;, &#39;Birds, The (1963)&#39;, &#39;Bitter Moon (1992)&#39;, &#39;Bitter Sugar (Azucar Amargo) (1996)&#39;, &#39;Black Beauty (1994)&#39;, &#39;Black Sheep (1996)&#39;, &#39;Blade Runner (1982)&#39;, &#39;Blink (1994)&#39;, &#39;Bliss (1997)&#39;, &#39;Blob, The (1958)&#39;, &#39;Blood &amp; Wine (1997)&#39;, &#39;Blood Beach (1981)&#39;, &#34;Blood For Dracula (Andy Warhol&#39;s Dracula) (1974)&#34;, &#39;Bloodsport 2 (1995)&#39;, &#39;Bloody Child, The (1996)&#39;, &#39;Blown Away (1994)&#39;, &#39;Blue Angel, The (Blaue Engel, Der) (1930)&#39;, &#39;Blue Chips (1994)&#39;, &#39;Blue Sky (1994)&#39;, &#39;Blue in the Face (1995)&#39;, &#39;Blues Brothers 2000 (1998)&#39;, &#39;Blues Brothers, The (1980)&#39;, &#39;Bob Roberts (1992)&#39;, &#39;Body Parts (1991)&#39;, &#39;Body Snatcher, The (1945)&#39;, &#39;Body Snatchers (1993)&#39;, &#39;Bogus (1996)&#39;, &#39;Bonheur, Le (1965)&#39;, &#39;Bonnie and Clyde (1967)&#39;, &#39;Boogie Nights (1997)&#39;, &#39;Boomerang (1992)&#39;, &#39;Boot, Das (1981)&#39;, &#39;Booty Call (1997)&#39;, &#39;Bottle Rocket (1996)&#39;, &#39;Bound (1996)&#39;, &#39;Boxing Helena (1993)&#39;, &#34;Boy&#39;s Life 2 (1997)&#34;, &#39;Boys (1996)&#39;, &#39;Boys Life (1995)&#39;, &#39;Boys in Venice (1996)&#39;, &#39;Boys of St. Vincent, The (1993)&#39;, &#39;Boys on the Side (1995)&#39;, &#39;Boys, Les (1997)&#39;, &#39;Brady Bunch Movie, The (1995)&#39;, &#39;Braindead (1992)&#39;, &#34;Bram Stoker&#39;s Dracula (1992)&#34;, &#39;Brassed Off (1996)&#39;, &#39;Braveheart (1995)&#39;, &#39;Brazil (1985)&#39;, &#39;Bread and Chocolate (Pane e cioccolata) (1973)&#39;, &#39;Breakdown (1997)&#39;, &#34;Breakfast at Tiffany&#39;s (1961)&#34;, &#39;Breaking the Waves (1996)&#39;, &#39;Bride of Frankenstein (1935)&#39;, &#39;Bridge on the River Kwai, The (1957)&#39;, &#39;Bridges of Madison County, The (1995)&#39;, &#39;Bringing Up Baby (1938)&#39;, &#39;Broken Arrow (1996)&#39;, &#39;Broken English (1996)&#39;, &#39;Bronx Tale, A (1993)&#39;, &#39;Brother Minister: The Assassination of Malcolm X (1994)&#39;, &#34;Brother&#39;s Kiss, A (1997)&#34;, &#39;Brothers McMullen, The (1995)&#39;, &#39;Brothers in Trouble (1995)&#39;, &#39;Browning Version, The (1994)&#39;, &#39;Buddy (1997)&#39;, &#39;Bulletproof (1996)&#39;, &#39;Bullets Over Broadway (1994)&#39;, &#39;Burnt By the Sun (1994)&#39;, &#39;Burnt Offerings (1976)&#39;, &#39;Bushwhacked (1995)&#39;, &#39;Butch Cassidy and the Sundance Kid (1969)&#39;, &#39;Butcher Boy, The (1998)&#39;, &#39;Butterfly Kiss (1995)&#39;, &#39;Bye Bye, Love (1995)&#39;, &#34;C&#39;est arrivé près de chez vous (1992)&#34;, &#39;Cabin Boy (1994)&#39;, &#39;Cable Guy, The (1996)&#39;, &#39;Calendar Girl (1993)&#39;, &#39;Canadian Bacon (1994)&#39;, &#39;Candidate, The (1972)&#39;, &#39;Candyman (1992)&#39;, &#39;Candyman: Farewell to the Flesh (1995)&#39;, &#39;Cape Fear (1962)&#39;, &#39;Cape Fear (1991)&#39;, &#39;Captives (1994)&#39;, &#39;Career Girls (1997)&#39;, &#39;Careful (1992)&#39;, &#34;Carlito&#39;s Way (1993)&#34;, &#39;Carmen Miranda: Bananas Is My Business (1994)&#39;, &#39;Caro Diario (Dear Diary) (1994)&#39;, &#39;Carpool (1996)&#39;, &#39;Carrie (1976)&#39;, &#39;Carried Away (1996)&#39;, &#39;Carrington (1995)&#39;, &#39;Casablanca (1942)&#39;, &#39;Casino (1995)&#39;, &#39;Casper (1995)&#39;, &#39;Castle Freak (1995)&#39;, &#39;Cat People (1982)&#39;, &#39;Cat on a Hot Tin Roof (1958)&#39;, &#34;Cats Don&#39;t Dance (1997)&#34;, &#39;Catwalk (1995)&#39;, &#39;Caught (1996)&#39;, &#39;Celestial Clockwork (1994)&#39;, &#39;Celluloid Closet, The (1995)&#39;, &#39;Celtic Pride (1996)&#39;, &#39;Cement Garden, The (1993)&#39;, &#39;Cemetery Man (Dellamorte Dellamore) (1994)&#39;, &#39;Century (1993)&#39;, &#39;Chain Reaction (1996)&#39;, &#39;Chairman of the Board (1998)&#39;, &#39;Chamber, The (1996)&#39;, &#39;Charade (1963)&#39;, &#39;Chasers (1994)&#39;, &#39;Chasing Amy (1997)&#39;, &#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Children of the Revolution (1996)&#39;, &#39;Chinatown (1974)&#39;, &#39;Christmas Carol, A (1938)&#39;, &#39;Chungking Express (1994)&#39;, &#39;Ciao, Professore! (1993)&#39;, &#39;Cinderella (1950)&#39;, &#39;Cinema Paradiso (1988)&#39;, &#39;Circle of Friends (1995)&#39;, &#39;Citizen Kane (1941)&#39;, &#39;Citizen Ruth (1996)&#39;, &#39;City Hall (1996)&#39;, &#34;City Slickers II: The Legend of Curly&#39;s Gold (1994)&#34;, &#39;City of Angels (1998)&#39;, &#39;City of Industry (1997)&#39;, &#39;City of Lost Children, The (1995)&#39;, &#39;Clean Slate (1994)&#39;, &#39;Clean Slate (Coup de Torchon) (1981)&#39;, &#39;Clear and Present Danger (1994)&#39;, &#39;Clerks (1994)&#39;, &#39;Client, The (1994)&#39;, &#39;Cliffhanger (1993)&#39;, &#39;Clockers (1995)&#39;, &#39;Clockwork Orange, A (1971)&#39;, &#39;Close Shave, A (1995)&#39;, &#39;Clueless (1995)&#39;, &#39;Cobb (1994)&#39;, &#39;Cold Comfort Farm (1995)&#39;, &#39;Coldblooded (1995)&#39;, &#39;Collectionneuse, La (1967)&#39;, &#39;Colonel Chabert, Le (1994)&#39;, &#39;Color of Night (1994)&#39;, &#39;Commandments (1997)&#39;, &#39;Con Air (1997)&#39;, &#39;Conan the Barbarian (1981)&#39;, &#39;Condition Red (1995)&#39;, &#39;Coneheads (1993)&#39;, &#39;Congo (1995)&#39;, &#39;Conspiracy Theory (1997)&#39;, &#39;Contact (1997)&#39;, &#39;Contempt (Mépris, Le) (1963)&#39;, &#39;Convent, The (Convento, O) (1995)&#39;, &#39;Cook the Thief His Wife &amp; Her Lover, The (1989)&#39;, &#39;Cool Hand Luke (1967)&#39;, &#39;Cool Runnings (1993)&#39;, &#39;Cop Land (1997)&#39;, &#39;Cops and Robbersons (1994)&#39;, &#39;Copycat (1995)&#39;, &#39;Corrina, Corrina (1994)&#39;, &#39;Cosi (1996)&#39;, &#39;Country Life (1994)&#39;, &#39;Courage Under Fire (1996)&#39;, &#39;Cowboy Way, The (1994)&#39;, &#39;Craft, The (1996)&#39;, &#39;Crash (1996)&#39;, &#39;Crimson Tide (1995)&#39;, &#39;Critical Care (1997)&#39;, &#39;Cronos (1992)&#39;, &#39;Crooklyn (1994)&#39;, &#39;Crossfire (1947)&#39;, &#39;Crossing Guard, The (1995)&#39;, &#39;Crow, The (1994)&#39;, &#39;Crow: City of Angels, The (1996)&#39;, &#39;Crows and Sparrows (1949)&#39;, &#39;Crucible, The (1996)&#39;, &#39;Crude Oasis, The (1995)&#39;, &#39;Crumb (1994)&#39;, &#39;Cry, the Beloved Country (1995)&#39;, &#39;Crying Game, The (1992)&#39;, &#39;Curdled (1996)&#39;, &#39;Cure, The (1995)&#39;, &#39;Cutthroat Island (1995)&#39;, &#39;Cyclo (1995)&#39;, &#39;Cyrano de Bergerac (1990)&#39;, &#39;Cérémonie, La (1995)&#39;, &#39;D3: The Mighty Ducks (1996)&#39;, &#39;Dadetown (1995)&#39;, &#39;Daens (1992)&#39;, &#39;Damsel in Distress, A (1937)&#39;, &#39;Dances with Wolves (1990)&#39;, &#39;Dangerous Beauty (1998)&#39;, &#39;Dangerous Ground (1997)&#39;, &#39;Dangerous Minds (1995)&#39;, &#34;Daniel Defoe&#39;s Robinson Crusoe (1996)&#34;, &#34;Dante&#39;s Peak (1997)&#34;, &#39;Dark City (1998)&#39;, &#39;Dave (1993)&#39;, &#39;Davy Crockett, King of the Wild Frontier (1955)&#39;, &#39;Day the Earth Stood Still, The (1951)&#39;, &#39;Day the Sun Turned Cold, The (Tianguo niezi) (1994)&#39;, &#39;Daylight (1996)&#39;, &#39;Days of Thunder (1990)&#39;, &#39;Daytrippers, The (1996)&#39;, &#39;Dazed and Confused (1993)&#39;, &#39;Dead Man (1995)&#39;, &#39;Dead Man Walking (1995)&#39;, &#39;Dead Poets Society (1989)&#39;, &#39;Dead Presidents (1995)&#39;, &#39;Dear God (1996)&#39;, &#39;Death and the Maiden (1994)&#39;, &#39;Death in Brunswick (1991)&#39;, &#39;Death in the Garden (Mort en ce jardin, La) (1956)&#39;, &#39;Deceiver (1997)&#39;, &#39;Deconstructing Harry (1997)&#39;, &#39;Deep Rising (1998)&#39;, &#39;Deer Hunter, The (1978)&#39;, &#39;Delicatessen (1991)&#39;, &#39;Delta of Venus (1994)&#39;, &#39;Demolition Man (1993)&#39;, &#39;Denise Calls Up (1995)&#39;, &#39;Desert Winds (1995)&#39;, &#39;Designated Mourner, The (1997)&#39;, &#39;Desperado (1995)&#39;, &#39;Desperate Measures (1998)&#39;, &#39;Destiny Turns on the Radio (1995)&#39;, &#39;Devil in a Blue Dress (1995)&#39;, &#34;Devil&#39;s Advocate, The (1997)&#34;, &#34;Devil&#39;s Own, The (1997)&#34;, &#39;Diabolique (1996)&#39;, &#39;Dial M for Murder (1954)&#39;, &#39;Die Hard (1988)&#39;, &#39;Die Hard 2 (1990)&#39;, &#39;Die Hard: With a Vengeance (1995)&#39;, &#39;Die xue shuang xiong (Killer, The) (1989)&#39;, &#39;Dingo (1992)&#39;, &#39;Dirty Dancing (1987)&#39;, &#39;Disclosure (1994)&#39;, &#39;Diva (1981)&#39;, &#39;Dolores Claiborne (1994)&#39;, &#39;Don Juan DeMarco (1995)&#39;, &#34;Don&#39;t Be a Menace to South Central While Drinking Your Juice in the Hood (1996)&#34;, &#39;Donnie Brasco (1997)&#39;, &#39;Doom Generation, The (1995)&#39;, &#39;Doors, The (1991)&#39;, &#39;Double Happiness (1994)&#39;, &#39;Double Team (1997)&#39;, &#39;Double vie de Véronique, La (Double Life of Veronique, The) (1991)&#39;, &#39;Down Periscope (1996)&#39;, &#39;Down by Law (1986)&#39;, &#39;Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)&#39;, &#39;Dracula: Dead and Loving It (1995)&#39;, &#39;Dragonheart (1996)&#39;, &#39;Dream Man (1995)&#39;, &#39;Dream With the Fishes (1997)&#39;, &#39;Drop Dead Fred (1991)&#39;, &#39;Drop Zone (1994)&#39;, &#39;Drunks (1995)&#39;, &#39;Duck Soup (1933)&#39;, &#39;Dumb &amp; Dumber (1994)&#39;, &#39;Dumbo (1941)&#39;, &#39;Dunston Checks In (1996)&#39;, &#39;Duoluo tianshi (1995)&#39;, &#39;E.T. the Extra-Terrestrial (1982)&#39;, &#39;East of Eden (1955)&#39;, &#39;Eat Drink Man Woman (1994)&#39;, &#39;Ed (1996)&#39;, &#39;Ed Wood (1994)&#39;, &#34;Ed&#39;s Next Move (1996)&#34;, &#39;Eddie (1996)&#39;, &#39;Edge, The (1997)&#39;, &#39;Eighth Day, The (1996)&#39;, &#39;Emma (1996)&#39;, &#39;Empire Strikes Back, The (1980)&#39;, &#39;Enchanted April (1991)&#39;, &#39;Endless Summer 2, The (1994)&#39;, &#34;Enfer, L&#39; (1994)&#34;, &#39;English Patient, The (1996)&#39;, &#39;Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)&#39;, &#39;Entertaining Angels: The Dorothy Day Story (1996)&#39;, &#39;Eraser (1996)&#39;, &#39;Escape from L.A. (1996)&#39;, &#39;Escape from New York (1981)&#39;, &#39;Escape to Witch Mountain (1975)&#39;, &#39;Etz Hadomim Tafus (Under the Domin Tree) (1994)&#39;, &#34;Eve&#39;s Bayou (1997)&#34;, &#39;Even Cowgirls Get the Blues (1993)&#39;, &#39;Evening Star, The (1996)&#39;, &#39;Event Horizon (1997)&#39;, &#39;Everest (1998)&#39;, &#39;Every Other Weekend (1990)&#39;, &#39;Everyone Says I Love You (1996)&#39;, &#39;Evil Dead II (1987)&#39;, &#39;Evita (1996)&#39;, &#39;Excess Baggage (1997)&#39;, &#39;Executive Decision (1996)&#39;, &#39;Exit to Eden (1994)&#39;, &#39;Exotica (1994)&#39;, &#39;Extreme Measures (1996)&#39;, &#39;Eye for an Eye (1996)&#39;, &#34;Eye of Vichy, The (Oeil de Vichy, L&#39;) (1993)&#34;, &#39;Face/Off (1997)&#39;, &#39;Faces (1968)&#39;, &#39;Fair Game (1995)&#39;, &#39;FairyTale: A True Story (1997)&#39;, &#39;Faithful (1996)&#39;, &#39;Fall (1997)&#39;, &#39;Fallen (1998)&#39;, &#39;Falling in Love Again (1980)&#39;, &#39;Family Thing, A (1996)&#39;, &#39;Fan, The (1996)&#39;, &#39;Fantasia (1940)&#39;, &#39;Far From Home: The Adventures of Yellow Dog (1995)&#39;, &#39;Farewell My Concubine (1993)&#39;, &#39;Farewell to Arms, A (1932)&#39;, &#39;Fargo (1996)&#39;, &#39;Farinelli: il castrato (1994)&#39;, &#39;Farmer &amp; Chase (1995)&#39;, &#39;Fast, Cheap &amp; Out of Control (1997)&#39;, &#39;Faster Pussycat! Kill! Kill! (1965)&#39;, &#39;Fatal Instinct (1993)&#39;, &#39;Father of the Bride (1950)&#39;, &#39;Father of the Bride Part II (1995)&#39;, &#34;Fathers&#39; Day (1997)&#34;, &#39;Faust (1994)&#39;, &#39;Fausto (1993)&#39;, &#39;Favor, The (1994)&#39;, &#39;Fear (1996)&#39;, &#39;Fear of a Black Hat (1993)&#39;, &#39;Fear, The (1995)&#39;, &#39;Fearless (1993)&#39;, &#39;Feast of July (1995)&#39;, &#39;Feeling Minnesota (1996)&#39;, &#39;Female Perversions (1996)&#39;, &#39;Field of Dreams (1989)&#39;, &#39;Fierce Creatures (1997)&#39;, &#39;Fifth Element, The (1997)&#39;, &#39;Fille seule, La (A Single Girl) (1995)&#39;, &#39;Fire Down Below (1997)&#39;, &#39;Fire on the Mountain (1996)&#39;, &#39;Firestorm (1998)&#39;, &#39;Firm, The (1993)&#39;, &#39;First Kid (1996)&#39;, &#39;First Knight (1995)&#39;, &#39;First Wives Club, The (1996)&#39;, &#39;Fish Called Wanda, A (1988)&#39;, &#39;Fled (1996)&#39;, &#39;Flesh and Bone (1993)&#39;, &#39;Flintstones, The (1994)&#39;, &#39;Flipper (1996)&#39;, &#39;Flirt (1995)&#39;, &#39;Flirting With Disaster (1996)&#39;, &#39;Flower of My Secret, The (Flor de mi secreto, La) (1995)&#39;, &#39;Flubber (1997)&#39;, &#39;Fluke (1995)&#39;, &#39;Fly Away Home (1996)&#39;, &#39;Fog, The (1980)&#39;, &#39;Fools Rush In (1997)&#39;, &#39;For Ever Mozart (1996)&#39;, &#39;For Love or Money (1993)&#39;, &#39;For Richer or Poorer (1997)&#39;, &#39;For Whom the Bell Tolls (1943)&#39;, &#39;For the Moment (1994)&#39;, &#39;Forbidden Christ, The (Cristo proibito, Il) (1950)&#39;, &#39;Forbidden Planet (1956)&#39;, &#39;Foreign Correspondent (1940)&#39;, &#39;Foreign Student (1994)&#39;, &#39;Forget Paris (1995)&#39;, &#39;Forrest Gump (1994)&#39;, &#39;Four Days in September (1997)&#39;, &#39;Four Rooms (1995)&#39;, &#39;Four Weddings and a Funeral (1994)&#39;, &#39;Fox and the Hound, The (1981)&#39;, &#39;Foxfire (1996)&#39;, &#39;Frankie Starlight (1995)&#39;, &#39;Free Willy (1993)&#39;, &#39;Free Willy 2: The Adventure Home (1995)&#39;, &#39;Free Willy 3: The Rescue (1997)&#39;, &#39;Freeway (1996)&#39;, &#39;French Kiss (1995)&#39;, &#39;French Twist (Gazon maudit) (1995)&#39;, &#39;Fresh (1994)&#39;, &#39;Friday (1995)&#39;, &#39;Fried Green Tomatoes (1991)&#39;, &#39;Frighteners, The (1996)&#39;, &#39;Frisk (1995)&#39;, &#39;From Dusk Till Dawn (1996)&#39;, &#39;Fugitive, The (1993)&#39;, &#39;Full Metal Jacket (1987)&#39;, &#39;Full Monty, The (1997)&#39;, &#39;Full Speed (1996)&#39;, &#39;Funeral, The (1996)&#39;, &#39;Funny Face (1957)&#39;, &#39;Further Gesture, A (1996)&#39;, &#39;G.I. Jane (1997)&#39;, &#39;Gabbeh (1996)&#39;, &#39;Game, The (1997)&#39;, &#39;Gandhi (1982)&#39;, &#39;Gang Related (1997)&#39;, &#39;Garden of Finzi-Contini, The (Giardino dei Finzi-Contini, Il) (1970)&#39;, &#39;Gaslight (1944)&#39;, &#39;Gate of Heavenly Peace, The (1995)&#39;, &#39;Gattaca (1997)&#39;, &#39;Gay Divorcee, The (1934)&#39;, &#39;George of the Jungle (1997)&#39;, &#39;Georgia (1995)&#39;, &#39;Germinal (1993)&#39;, &#39;Geronimo: An American Legend (1993)&#39;, &#39;Get Shorty (1995)&#39;, &#39;Get on the Bus (1996)&#39;, &#39;Getaway, The (1994)&#39;, &#39;Getting Away With Murder (1996)&#39;, &#39;Getting Even with Dad (1994)&#39;, &#39;Ghost (1990)&#39;, &#39;Ghost and Mrs. Muir, The (1947)&#39;, &#39;Ghost and the Darkness, The (1996)&#39;, &#39;Ghost in the Shell (Kokaku kidotai) (1995)&#39;, &#39;Ghosts of Mississippi (1996)&#39;, &#39;Giant (1956)&#39;, &#39;Gigi (1958)&#39;, &#34;Gilligan&#39;s Island: The Movie (1998)&#34;, &#39;Girl 6 (1996)&#39;, &#39;Girl in the Cadillac (1995)&#39;, &#39;Girls Town (1996)&#39;, &#39;Glass Shield, The (1994)&#39;, &#39;Glengarry Glen Ross (1992)&#39;, &#39;Glimmer Man, The (1996)&#39;, &#39;Glory (1989)&#39;, &#39;Go Fish (1994)&#39;, &#39;Godfather, The (1972)&#39;, &#39;Godfather: Part II, The (1974)&#39;, &#39;Gold Diggers: The Secret of Bear Mountain (1995)&#39;, &#39;Golden Earrings (1947)&#39;, &#39;GoldenEye (1995)&#39;, &#34;Gone Fishin&#39; (1997)&#34;, &#39;Gone with the Wind (1939)&#39;, &#39;Good Man in Africa, A (1994)&#39;, &#39;Good Morning (1971)&#39;, &#39;Good Will Hunting (1997)&#39;, &#39;Good, The Bad and The Ugly, The (1966)&#39;, &#39;GoodFellas (1990)&#39;, &#39;Goofy Movie, A (1995)&#39;, &#39;Gordy (1995)&#39;, &#39;Grace of My Heart (1996)&#39;, &#39;Graduate, The (1967)&#39;, &#39;Grand Day Out, A (1992)&#39;, &#39;Grass Harp, The (1995)&#39;, &#39;Grateful Dead (1995)&#39;, &#39;Grease (1978)&#39;, &#39;Grease 2 (1982)&#39;, &#39;Great Day in Harlem, A (1994)&#39;, &#39;Great Dictator, The (1940)&#39;, &#39;Great Escape, The (1963)&#39;, &#39;Great Expectations (1998)&#39;, &#39;Great Race, The (1965)&#39;, &#39;Great White Hype, The (1996)&#39;, &#34;Gridlock&#39;d (1997)&#34;, &#39;Grifters, The (1990)&#39;, &#39;Grosse Fatigue (1994)&#39;, &#39;Grosse Pointe Blank (1997)&#39;, &#39;Groundhog Day (1993)&#39;, &#39;Grumpier Old Men (1995)&#39;, &#39;Guantanamera (1994)&#39;, &#39;Guilty as Sin (1993)&#39;, &#39;Gumby: The Movie (1995)&#39;, &#39;Hackers (1995)&#39;, &#39;Half Baked (1998)&#39;, &#39;Halloween: The Curse of Michael Myers (1995)&#39;, &#39;Hamlet (1996)&#39;, &#39;Hana-bi (1997)&#39;, &#39;Happy Gilmore (1996)&#39;, &#39;Hard Eight (1996)&#39;, &#39;Hard Rain (1998)&#39;, &#39;Hard Target (1993)&#39;, &#39;Harlem (1993)&#39;, &#39;Harold and Maude (1971)&#39;, &#39;Harriet the Spy (1996)&#39;, &#39;Hate (Haine, La) (1995)&#39;, &#39;Haunted World of Edward D. Wood Jr., The (1995)&#39;, &#39;He Walked by Night (1948)&#39;, &#39;Head Above Water (1996)&#39;, &#39;Hear My Song (1991)&#39;, &#39;Hearts and Minds (1996)&#39;, &#39;Heat (1995)&#39;, &#39;Heathers (1989)&#39;, &#39;Heaven &amp; Earth (1993)&#39;, &#34;Heaven&#39;s Prisoners (1996)&#34;, &#39;Heavenly Creatures (1994)&#39;, &#39;Heavy (1995)&#39;, &#39;Heavy Metal (1981)&#39;, &#39;Heavyweights (1994)&#39;, &#39;Hedd Wyn (1992)&#39;, &#39;Heidi Fleiss: Hollywood Madam (1995) &#39;, &#39;Hellraiser: Bloodline (1996)&#39;, &#39;Henry V (1989)&#39;, &#39;Herbie Rides Again (1974)&#39;, &#39;Hercules (1997)&#39;, &#39;Here Comes Cookie (1935)&#39;, &#39;Hideaway (1995)&#39;, &#39;High Noon (1952)&#39;, &#39;High School High (1996)&#39;, &#39;Higher Learning (1995)&#39;, &#39;Highlander (1986)&#39;, &#39;Highlander III: The Sorcerer (1994)&#39;, &#39;His Girl Friday (1940)&#39;, &#39;Hollow Reed (1996)&#39;, &#39;Homage (1995)&#39;, &#39;Home Alone (1990)&#39;, &#39;Home Alone 3 (1997)&#39;, &#39;Home for the Holidays (1995)&#39;, &#39;Homeward Bound II: Lost in San Francisco (1996)&#39;, &#39;Homeward Bound: The Incredible Journey (1993)&#39;, &#39;Hoodlum (1997)&#39;, &#39;Hoop Dreams (1994)&#39;, &#39;Horse Whisperer, The (1998)&#39;, &#39;Horseman on the Roof, The (Hussard sur le toit, Le) (1995)&#39;, &#39;Hostile Intentions (1994)&#39;, &#39;Hot Shots! Part Deux (1993)&#39;, &#39;Hotel de Love (1996)&#39;, &#39;Hour of the Pig, The (1993)&#39;, &#39;House Arrest (1996)&#39;, &#39;House Party 3 (1994)&#39;, &#39;House of Yes, The (1997)&#39;, &#39;House of the Spirits, The (1993)&#39;, &#39;Houseguest (1994)&#39;, &#39;How to Be a Player (1997)&#39;, &#39;How to Make an American Quilt (1995)&#39;, &#39;Howling, The (1981)&#39;, &#39;Hudsucker Proxy, The (1994)&#39;, &#39;Hugo Pool (1997)&#39;, &#39;Hunchback of Notre Dame, The (1996)&#39;, &#39;Hungarian Fairy Tale, A (1987)&#39;, &#39;Hunt for Red October, The (1990)&#39;, &#39;Hunted, The (1995)&#39;, &#39;Hurricane Streets (1998)&#39;, &#39;Hush (1998)&#39;, &#34;I Can&#39;t Sleep (J&#39;ai pas sommeil) (1994)&#34;, &#34;I Don&#39;t Want to Talk About It (De eso no se habla) (1993)&#34;, &#39;I Know What You Did Last Summer (1997)&#39;, &#39;I Like It Like That (1994)&#39;, &#39;I Love Trouble (1994)&#39;, &#39;I Shot Andy Warhol (1996)&#39;, &#34;I&#39;ll Do Anything (1994)&#34;, &#34;I&#39;m Not Rappaport (1996)&#34;, &#39;I, Worst of All (Yo, la peor de todas) (1990)&#39;, &#39;I.Q. (1994)&#39;, &#39;Ice Storm, The (1997)&#39;, &#39;If Lucy Fell (1996)&#39;, &#39;Ill Gotten Gains (1997)&#39;, &#39;Immortal Beloved (1994)&#39;, &#39;In &amp; Out (1997)&#39;, &#39;In Love and War (1996)&#39;, &#39;In the Army Now (1994)&#39;, &#39;In the Bleak Midwinter (1995)&#39;, &#39;In the Company of Men (1997)&#39;, &#39;In the Line of Duty 2 (1987)&#39;, &#39;In the Line of Fire (1993)&#39;, &#39;In the Mouth of Madness (1995)&#39;, &#39;In the Name of the Father (1993)&#39;, &#39;In the Realm of the Senses (Ai no corrida) (1976)&#39;, &#39;Incognito (1997)&#39;, &#39;Independence Day (ID4) (1996)&#39;, &#39;Indian Summer (1996)&#39;, &#39;Indian in the Cupboard, The (1995)&#39;, &#39;Indiana Jones and the Last Crusade (1989)&#39;, &#39;Infinity (1996)&#39;, &#39;Inkwell, The (1994)&#39;, &#39;Innocent Sleep, The (1995)&#39;, &#39;Innocents, The (1961)&#39;, &#39;Inspector General, The (1949)&#39;, &#39;Interview with the Vampire (1994)&#39;, &#39;Intimate Relations (1996)&#39;, &#39;Inventing the Abbotts (1997)&#39;, &#39;Invitation, The (Zaproszenie) (1986)&#39;, &#39;Island of Dr. Moreau, The (1996)&#39;, &#39;It Could Happen to You (1994)&#39;, &#39;It Happened One Night (1934)&#39;, &#39;It Takes Two (1995)&#39;, &#34;It&#39;s My Party (1995)&#34;, &#34;It&#39;s a Wonderful Life (1946)&#34;, &#39;JLG/JLG - autoportrait de décembre (1994)&#39;, &#39;Jack (1996)&#39;, &#39;Jack and Sarah (1995)&#39;, &#39;Jackal, The (1997)&#39;, &#39;Jackie Brown (1997)&#39;, &#34;Jackie Chan&#39;s First Strike (1996)&#34;, &#39;Jade (1995)&#39;, &#39;James and the Giant Peach (1996)&#39;, &#39;Jane Eyre (1996)&#39;, &#34;Jason&#39;s Lyric (1994)&#34;, &#39;Jaws (1975)&#39;, &#39;Jaws 2 (1978)&#39;, &#39;Jaws 3-D (1983)&#39;, &#39;Jean de Florette (1986)&#39;, &#39;Jefferson in Paris (1995)&#39;, &#39;Jeffrey (1995)&#39;, &#39;Jerky Boys, The (1994)&#39;, &#39;Jerry Maguire (1996)&#39;, &#39;Jimmy Hollywood (1994)&#39;, &#39;Jingle All the Way (1996)&#39;, &#34;Joe&#39;s Apartment (1996)&#34;, &#39;Johnny 100 Pesos (1993)&#39;, &#39;Johnny Mnemonic (1995)&#39;, &#39;Johns (1996)&#39;, &#39;Journey of August King, The (1995)&#39;, &#39;Joy Luck Club, The (1993)&#39;, &#39;Jude (1996)&#39;, &#39;Judge Dredd (1995)&#39;, &#39;Judgment Night (1993)&#39;, &#39;Jumanji (1995)&#39;, &#39;Jungle Book, The (1994)&#39;, &#39;Jungle2Jungle (1997)&#39;, &#39;Junior (1994)&#39;, &#34;Jupiter&#39;s Wife (1994)&#34;, &#39;Jurassic Park (1993)&#39;, &#39;Juror, The (1996)&#39;, &#39;Jury Duty (1995)&#39;, &#39;Just Cause (1995)&#39;, &#39;Kalifornia (1993)&#39;, &#39;Kama Sutra: A Tale of Love (1996)&#39;, &#39;Kansas City (1996)&#39;, &#39;Kaspar Hauser (1993)&#39;, &#39;Kazaam (1996)&#39;, &#39;Keys to Tulsa (1997)&#39;, &#39;Kicked in the Head (1997)&#39;, &#39;Kicking and Screaming (1995)&#39;, &#34;Kid in King Arthur&#39;s Court, A (1995)&#34;, &#39;Kids (1995)&#39;, &#39;Kids in the Hall: Brain Candy (1996)&#39;, &#39;Kika (1993)&#39;, &#39;Killer (Bulletproof Heart) (1994)&#39;, &#39;Killer: A Journal of Murder (1995)&#39;, &#39;Killing Fields, The (1984)&#39;, &#39;Killing Zoe (1994)&#39;, &#39;Kim (1950)&#39;, &#39;King of New York (1990)&#39;, &#39;King of the Hill (1993)&#39;, &#39;Kingpin (1996)&#39;, &#39;Kiss Me, Guido (1997)&#39;, &#39;Kiss of Death (1995)&#39;, &#39;Kiss the Girls (1997)&#39;, &#39;Kissed (1996)&#39;, &#39;Kolya (1996)&#39;, &#39;Koyaanisqatsi (1983)&#39;, &#39;Kull the Conqueror (1997)&#39;, &#39;Kundun (1997)&#39;, &#39;L.A. Confidential (1997)&#39;, &#39;Lady of Burlesque (1943)&#39;, &#39;Ladybird Ladybird (1994)&#39;, &#39;Lamerica (1994)&#39;, &#39;Land Before Time III: The Time of the Great Giving (1995) (V)&#39;, &#39;Land and Freedom (Tierra y libertad) (1995)&#39;, &#39;Larger Than Life (1996)&#39;, &#39;Lashou shentan (1992)&#39;, &#39;Lassie (1994)&#39;, &#39;Last Action Hero (1993)&#39;, &#39;Last Dance (1996)&#39;, &#39;Last Klezmer: Leopold Kozlowski, His Life and Music, The (1995)&#39;, &#39;Last Man Standing (1996)&#39;, &#39;Last Summer in the Hamptons (1995)&#39;, &#39;Last Supper, The (1995)&#39;, &#39;Last Time I Committed Suicide, The (1997)&#39;, &#39;Last Time I Saw Paris, The (1954)&#39;, &#39;Last of the Mohicans, The (1992)&#39;, &#39;Late Bloomers (1996)&#39;, &#39;Laura (1944)&#39;, &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, &#39;Lawnmower Man, The (1992)&#39;, &#39;Lawrence of Arabia (1962)&#39;, &#39;Lay of the Land, The (1997)&#39;, &#39;Leading Man, The (1996)&#39;, &#39;Leave It to Beaver (1997)&#39;, &#39;Leaving Las Vegas (1995)&#39;, &#39;Legal Deceit (1997)&#39;, &#39;Legends of the Fall (1994)&#39;, &#39;Leopard Son, The (1996)&#39;, &#39;Letter From Death Row, A (1998)&#39;, &#39;Liar Liar (1997)&#39;, &#39;Liebelei (1933)&#39;, &#39;Life Less Ordinary, A (1997)&#39;, &#39;Life with Mikey (1993)&#39;, &#39;Lightning Jack (1994)&#39;, &#39;Like Water For Chocolate (Como agua para chocolate) (1992)&#39;, &#39;Line King: Al Hirschfeld, The (1996)&#39;, &#39;Lion King, The (1994)&#39;, &#39;Little Big League (1994)&#39;, &#39;Little Buddha (1993)&#39;, &#39;Little City (1998)&#39;, &#39;Little Lord Fauntleroy (1936)&#39;, &#39;Little Odessa (1994)&#39;, &#39;Little Princess, A (1995)&#39;, &#39;Little Princess, The (1939)&#39;, &#39;Little Rascals, The (1994)&#39;, &#39;Little Women (1994)&#39;, &#39;Live Nude Girls (1995)&#39;, &#39;Living in Oblivion (1995)&#39;, &#39;Loaded (1994)&#39;, &#39;Local Hero (1983)&#39;, &#39;Loch Ness (1995)&#39;, &#39;Locusts, The (1997)&#39;, &#39;Lone Star (1996)&#39;, &#39;Long Kiss Goodnight, The (1996)&#39;, &#39;Looking for Richard (1996)&#39;, &#39;Lord of Illusions (1995)&#39;, &#39;Losing Chase (1996)&#39;, &#39;Losing Isaiah (1995)&#39;, &#39;Lost Highway (1997)&#39;, &#39;Lost Horizon (1937)&#39;, &#39;Lost World: Jurassic Park, The (1997)&#39;, &#39;Lost in Space (1998)&#39;, &#39;Lotto Land (1995)&#39;, &#39;Love &amp; Human Remains (1993)&#39;, &#39;Love Affair (1994)&#39;, &#39;Love Bug, The (1969)&#39;, &#39;Love Is All There Is (1996)&#39;, &#39;Love Jones (1997)&#39;, &#39;Love Serenade (1996)&#39;, &#39;Love and Death on Long Island (1997)&#39;, &#39;Love and Other Catastrophes (1996)&#39;, &#39;Love and a .45 (1994)&#39;, &#39;Love in the Afternoon (1957)&#39;, &#39;Love! Valour! Compassion! (1997)&#39;, &#34;Lover&#39;s Knot (1996)&#34;, &#39;Low Down Dirty Shame, A (1994)&#39;, &#39;Low Life, The (1994)&#39;, &#39;M (1931)&#39;, &#39;M*A*S*H (1970)&#39;, &#39;M. Butterfly (1993)&#39;, &#39;MURDER and murder (1996)&#39;, &#39;Ma vie en rose (My Life in Pink) (1997)&#39;, &#39;Machine, The (1994)&#39;, &#39;Mad City (1997)&#39;, &#39;Mad Dog Time (1996)&#39;, &#39;Mad Love (1995)&#39;, &#39;Madame Butterfly (1995)&#39;, &#39;Made in America (1993)&#39;, &#39;Madness of King George, The (1994)&#39;, &#39;Madonna: Truth or Dare (1991)&#39;, &#39;Magic Hour, The (1998)&#39;, &#39;Magnificent Seven, The (1954)&#39;, &#39;Major Payne (1994)&#39;, &#39;Malice (1993)&#39;, &#39;Mallrats (1995)&#39;, &#39;Maltese Falcon, The (1941)&#39;, &#39;Mamma Roma (1962)&#39;, &#39;Man Who Knew Too Little, The (1997)&#39;, &#39;Man Who Would Be King, The (1975)&#39;, &#39;Man Without a Face, The (1993)&#39;, &#39;Man from Down Under, The (1943)&#39;, &#39;Man in the Iron Mask, The (1998)&#39;, &#39;Man of No Importance, A (1994)&#39;, &#39;Man of the House (1995)&#39;, &#39;Man of the Year (1995)&#39;, &#39;Manchurian Candidate, The (1962)&#39;, &#39;Manhattan (1979)&#39;, &#39;Manhattan Murder Mystery (1993)&#39;, &#39;Manny &amp; Lo (1996)&#39;, &#39;Manon of the Spring (Manon des sources) (1986)&#39;, &#34;Margaret&#39;s Museum (1995)&#34;, &#39;Mark of Zorro, The (1940)&#39;, &#39;Marked for Death (1990)&#39;, &#39;Marlene Dietrich: Shadow and Light (1996) &#39;, &#39;Mars Attacks! (1996)&#39;, &#34;Marvin&#39;s Room (1996)&#34;, &#39;Mary Poppins (1964)&#39;, &#39;Mary Reilly (1996)&#39;, &#34;Mary Shelley&#39;s Frankenstein (1994)&#34;, &#39;Mask, The (1994)&#39;, &#34;Mat&#39; i syn (1997)&#34;, &#39;MatchMaker, The (1997)&#39;, &#39;Matilda (1996)&#39;, &#39;Maverick (1994)&#39;, &#39;Maximum Risk (1996)&#39;, &#39;Maya Lin: A Strong Clear Vision (1994)&#39;, &#39;Maybe, Maybe Not (Bewegte Mann, Der) (1994)&#39;, &#34;McHale&#39;s Navy (1997)&#34;, &#39;Mediterraneo (1991)&#39;, &#39;Meet John Doe (1941)&#39;, &#39;Meet Me in St. Louis (1944)&#39;, &#39;Meet Wally Sparks (1997)&#39;, &#39;Men With Guns (1997)&#39;, &#39;Men in Black (1997)&#39;, &#39;Men of Means (1998)&#39;, &#39;Menace II Society (1993)&#39;, &#39;Mercury Rising (1998)&#39;, &#39;Metisse (Café au Lait) (1993)&#39;, &#39;Metro (1997)&#39;, &#39;Miami Rhapsody (1995)&#39;, &#39;Michael (1996)&#39;, &#39;Michael Collins (1996)&#39;, &#34;Microcosmos: Le peuple de l&#39;herbe (1996)&#34;, &#39;Midnight Dancers (Sibak) (1994)&#39;, &#39;Midnight in the Garden of Good and Evil (1997)&#39;, &#39;Mighty Aphrodite (1995)&#39;, &#39;Mighty Morphin Power Rangers: The Movie (1995)&#39;, &#39;Mighty, The (1998)&#39;, &#39;Milk Money (1994)&#39;, &#39;Mille bolle blu (1993)&#39;, &#34;Miller&#39;s Crossing (1990)&#34;, &#39;Mimic (1997)&#39;, &#39;Mina Tannenbaum (1994)&#39;, &#39;Miracle on 34th Street (1994)&#39;, &#39;Mirage (1995)&#39;, &#39;Mirror Has Two Faces, The (1996)&#39;, &#39;Mission: Impossible (1996)&#39;, &#39;Misérables, Les (1995)&#39;, &#39;Mixed Nuts (1994)&#39;, &#39;Modern Affair, A (1995)&#39;, &#39;Moll Flanders (1996)&#39;, &#39;Mondo (1996)&#39;, &#39;Money Talks (1997)&#39;, &#39;Money Train (1995)&#39;, &#39;Month by the Lake, A (1995)&#39;, &#39;Monty Python and the Holy Grail (1974)&#39;, &#34;Monty Python&#39;s Life of Brian (1979)&#34;, &#39;Moonlight and Valentino (1995)&#39;, &#39;Mortal Kombat (1995)&#39;, &#39;Mortal Kombat: Annihilation (1997)&#39;, &#39;Mostro, Il (1994)&#39;, &#39;Mother (1996)&#39;, &#39;Mother Night (1996)&#39;, &#39;Mouse Hunt (1997)&#39;, &#34;Mr. Holland&#39;s Opus (1995)&#34;, &#39;Mr. Jones (1993)&#39;, &#39;Mr. Magoo (1997)&#39;, &#39;Mr. Smith Goes to Washington (1939)&#39;, &#39;Mr. Wonderful (1993)&#39;, &#39;Mr. Wrong (1996)&#39;, &#39;Mrs. Brown (Her Majesty, Mrs. Brown) (1997)&#39;, &#39;Mrs. Dalloway (1997)&#39;, &#39;Mrs. Doubtfire (1993)&#39;, &#39;Mrs. Parker and the Vicious Circle (1994)&#39;, &#39;Mrs. Winterbourne (1996)&#39;, &#39;Much Ado About Nothing (1993)&#39;, &#39;Mulholland Falls (1996)&#39;, &#39;Multiplicity (1996)&#39;, &#39;Muppet Treasure Island (1996)&#39;, &#39;Murder at 1600 (1997)&#39;, &#39;Murder in the First (1995)&#39;, &#39;Murder, My Sweet (1944)&#39;, &#34;Muriel&#39;s Wedding (1994)&#34;, &#39;Mute Witness (1994)&#39;, &#34;My Best Friend&#39;s Wedding (1997)&#34;, &#39;My Crazy Life (Mi vida loca) (1993)&#39;, &#39;My Fair Lady (1964)&#39;, &#39;My Family (1995)&#39;, &#39;My Favorite Season (1993)&#39;, &#39;My Favorite Year (1982)&#39;, &#39;My Fellow Americans (1996)&#39;, &#39;My Left Foot (1989)&#39;, &#34;My Life and Times With Antonin Artaud (En compagnie d&#39;Antonin Artaud) (1993)&#34;, &#39;My Life as a Dog (Mitt liv som hund) (1985)&#39;, &#39;My Man Godfrey (1936)&#39;, &#39;My Own Private Idaho (1991)&#39;, &#39;Mystery Science Theater 3000: The Movie (1996)&#39;, &#39;Nadja (1994)&#39;, &#39;Naked (1993)&#39;, &#39;Naked Gun 33 1/3: The Final Insult (1994)&#39;, &#39;Naked in New York (1994)&#39;, &#34;National Lampoon&#39;s Senior Trip (1995)&#34;, &#39;Natural Born Killers (1994)&#39;, &#39;Nell (1994)&#39;, &#39;Nelly &amp; Monsieur Arnaud (1995)&#39;, &#39;Nemesis 2: Nebula (1995)&#39;, &#39;Neon Bible, The (1995)&#39;, &#39;Net, The (1995)&#39;, &#39;NeverEnding Story III, The (1994)&#39;, &#39;New Age, The (1994)&#39;, &#39;New Jersey Drive (1995)&#39;, &#39;New York Cop (1996)&#39;, &#39;Newton Boys, The (1998)&#39;, &#39;Next Karate Kid, The (1994)&#39;, &#39;Next Step, The (1995)&#39;, &#39;Niagara, Niagara (1997)&#39;, &#39;Nick of Time (1995)&#39;, &#39;Nico Icon (1995)&#39;, &#39;Night Falls on Manhattan (1997)&#39;, &#39;Night Flier (1997)&#39;, &#39;Night of the Living Dead (1968)&#39;, &#39;Night on Earth (1991)&#39;, &#39;Nightmare Before Christmas, The (1993)&#39;, &#39;Nightmare on Elm Street, A (1984)&#39;, &#39;Nightwatch (1997)&#39;, &#39;Nikita (La Femme Nikita) (1990)&#39;, &#39;Nil By Mouth (1997)&#39;, &#39;Nina Takes a Lover (1994)&#39;, &#39;Nine Months (1995)&#39;, &#39;Ninotchka (1939)&#39;, &#39;Nixon (1995)&#39;, &#39;No Escape (1994)&#39;, &#39;Nobody Loves Me (Keiner liebt mich) (1994)&#39;, &#34;Nobody&#39;s Fool (1994)&#34;, &#39;Normal Life (1996)&#39;, &#39;North (1994)&#39;, &#39;North by Northwest (1959)&#39;, &#39;Nosferatu (Nosferatu, eine Symphonie des Grauens) (1922)&#39;, &#39;Nosferatu a Venezia (1986)&#39;, &#39;Nothing Personal (1995)&#39;, &#39;Nothing to Lose (1994)&#39;, &#39;Notorious (1946)&#39;, &#39;Now and Then (1995)&#39;, &#39;Nowhere (1997)&#39;, &#39;Nutty Professor, The (1996)&#39;, &#39;Nénette et Boni (1996)&#39;, &#39;Object of My Affection, The (1998)&#39;, &#39;Of Human Bondage (1934)&#39;, &#39;Of Love and Shadows (1994)&#39;, &#39;Office Killer (1997)&#39;, &#39;Old Lady Who Walked in the Sea, The (Vieille qui marchait dans la mer, La) (1991)&#39;, &#39;Old Man and the Sea, The (1958)&#39;, &#39;Old Yeller (1957)&#39;, &#39;Oliver &amp; Company (1988)&#39;, &#39;Omen, The (1976)&#39;, &#39;On Golden Pond (1981)&#39;, &#39;Once Upon a Time in America (1984)&#39;, &#39;Once Upon a Time in the West (1969)&#39;, &#39;Once Upon a Time... When We Were Colored (1995)&#39;, &#39;Once Were Warriors (1994)&#39;, &#39;One Fine Day (1996)&#39;, &#34;One Flew Over the Cuckoo&#39;s Nest (1975)&#34;, &#39;One Night Stand (1997)&#39;, &#39;Only You (1994)&#39;, &#39;Open Season (1996)&#39;, &#39;Operation Dumbo Drop (1995)&#39;, &#39;Original Gangstas (1996)&#39;, &#39;Orlando (1993)&#39;, &#39;Oscar &amp; Lucinda (1997)&#39;, &#39;Othello (1995)&#39;, &#39;Other Voices, Other Rooms (1997)&#39;, &#39;Out to Sea (1997)&#39;, &#39;Outbreak (1995)&#39;, &#39;Outlaw, The (1943)&#39;, &#39;Pagemaster, The (1994)&#39;, &#39;Pallbearer, The (1996)&#39;, &#39;Palmetto (1998)&#39;, &#39;Palookaville (1996)&#39;, &#39;Panther (1995)&#39;, &#39;Paper, The (1994)&#39;, &#39;Paradise Lost: The Child Murders at Robin Hood Hills (1996)&#39;, &#39;Paradise Road (1997)&#39;, &#39;Parent Trap, The (1961)&#39;, &#39;Paris Is Burning (1990)&#39;, &#39;Paris Was a Woman (1995)&#39;, &#39;Paris, France (1993)&#39;, &#39;Paris, Texas (1984)&#39;, &#39;Party Girl (1995)&#39;, &#39;Passion Fish (1992)&#39;, &#39;Pather Panchali (1955)&#39;, &#39;Paths of Glory (1957)&#39;, &#39;Patton (1970)&#39;, &#39;Peacemaker, The (1997)&#39;, &#39;Penny Serenade (1941)&#39;, &#39;People vs. Larry Flynt, The (1996)&#39;, &#39;Perez Family, The (1995)&#39;, &#39;Perfect Candidate, A (1996)&#39;, &#39;Perfect World, A (1993)&#39;, &#39;Persuasion (1995)&#39;, &#39;Pest, The (1997)&#39;, &#34;Pete&#39;s Dragon (1977)&#34;, &#39;Phantom, The (1996)&#39;, &#39;Phantoms (1998)&#39;, &#34;Pharaoh&#39;s Army (1995)&#34;, &#39;Phat Beach (1996)&#39;, &#39;Phenomenon (1996)&#39;, &#39;Philadelphia (1993)&#39;, &#39;Philadelphia Story, The (1940)&#39;, &#39;Piano, The (1993)&#39;, &#39;Picnic (1955)&#39;, &#39;Picture Bride (1995)&#39;, &#39;Picture Perfect (1997)&#39;, &#39;Pie in the Sky (1995)&#39;, &#39;Pillow Book, The (1995)&#39;, &#39;Pink Floyd - The Wall (1982)&#39;, &#39;Pinocchio (1940)&#39;, &#39;Platoon (1986)&#39;, &#39;Playing God (1997)&#39;, &#39;Pocahontas (1995)&#39;, &#39;Poetic Justice (1993)&#39;, &#39;Poison Ivy II (1995)&#39;, &#39;Police Story 4: Project S (Chao ji ji hua) (1993)&#39;, &#39;Pollyanna (1960)&#39;, &#39;Pompatus of Love, The (1996)&#39;, &#39;Ponette (1996)&#39;, &#39;Portrait of a Lady, The (1996)&#39;, &#39;Postino, Il (1994)&#39;, &#39;Postman, The (1997)&#39;, &#39;Powder (1995)&#39;, &#39;Power 98 (1995)&#39;, &#34;Preacher&#39;s Wife, The (1996)&#34;, &#39;Prefontaine (1997)&#39;, &#39;Pretty Woman (1990)&#39;, &#39;Price Above Rubies, A (1998)&#39;, &#39;Priest (1994)&#39;, &#39;Primal Fear (1996)&#39;, &#39;Primary Colors (1998)&#39;, &#39;Princess Bride, The (1987)&#39;, &#39;Princess Caraboo (1994)&#39;, &#39;Prisoner of the Mountains (Kavkazsky Plennik) (1996)&#39;, &#39;Private Benjamin (1980)&#39;, &#39;Private Parts (1997)&#39;, &#39;Professional, The (1994)&#39;, &#39;Program, The (1993)&#39;, &#39;Promesse, La (1996)&#39;, &#39;Promise, The (Versprechen, Das) (1994)&#39;, &#39;Prophecy II, The (1998)&#39;, &#39;Prophecy, The (1995)&#39;, &#39;Psycho (1960)&#39;, &#39;Pulp Fiction (1994)&#39;, &#39;Pump Up the Volume (1990)&#39;, &#39;Purple Noon (1960)&#39;, &#39;Pushing Hands (1992)&#39;, &#34;Pyromaniac&#39;s Love Story, A (1995)&#34;, &#39;Quartier Mozart (1992)&#39;, &#39;Queen Margot (Reine Margot, La) (1994)&#39;, &#39;Quest, The (1996)&#39;, &#39;Quick and the Dead, The (1995)&#39;, &#39;Quiet Man, The (1952)&#39;, &#39;Quiet Room, The (1996)&#39;, &#39;Quiz Show (1994)&#39;, &#39;Race the Sun (1996)&#39;, &#39;Radioland Murders (1994)&#39;, &#39;Raging Bull (1980)&#39;, &#39;Raiders of the Lost Ark (1981)&#39;, &#39;Rainmaker, The (1997)&#39;, &#39;Raise the Red Lantern (1991)&#39;, &#39;Raising Arizona (1987)&#39;, &#39;Ran (1985)&#39;, &#39;Ransom (1996)&#39;, &#39;Raw Deal (1948)&#39;, &#39;Ready to Wear (Pret-A-Porter) (1994)&#39;, &#39;Real Genius (1985)&#39;, &#39;Reality Bites (1994)&#39;, &#39;Rear Window (1954)&#39;, &#39;Rebecca (1940)&#39;, &#39;Rebel Without a Cause (1955)&#39;, &#39;Reckless (1995)&#39;, &#39;Red Corner (1997)&#39;, &#39;Red Firecracker, Green Firecracker (1994)&#39;, &#39;Red Rock West (1992)&#39;, &#39;Ref, The (1994)&#39;, &#39;Relative Fear (1994)&#39;, &#39;Relic, The (1997)&#39;, &#39;Reluctant Debutante, The (1958)&#39;, &#39;Remains of the Day, The (1993)&#39;, &#39;Renaissance Man (1994)&#39;, &#39;Rendezvous in Paris (Rendez-vous de Paris, Les) (1995)&#39;, &#39;Rent-a-Kid (1995)&#39;, &#39;Replacement Killers, The (1998)&#39;, &#39;Reservoir Dogs (1992)&#39;, &#39;Restoration (1995)&#39;, &#39;Return of Martin Guerre, The (Retour de Martin Guerre, Le) (1982)&#39;, &#39;Return of the Jedi (1983)&#39;, &#39;Return of the Pink Panther, The (1974)&#39;, &#39;Rhyme &amp; Reason (1997)&#39;, &#34;Rich Man&#39;s Wife, The (1996)&#34;, &#39;Richard III (1995)&#39;, &#39;Richie Rich (1994)&#39;, &#39;Ridicule (1996)&#39;, &#39;Right Stuff, The (1983)&#39;, &#39;Ripe (1996)&#39;, &#39;Rising Sun (1993)&#39;, &#39;River Wild, The (1994)&#39;, &#39;Road to Wellville, The (1994)&#39;, &#39;Rob Roy (1995)&#39;, &#34;Robert A. Heinlein&#39;s The Puppet Masters (1994)&#34;, &#39;Robin Hood: Men in Tights (1993)&#39;, &#39;Robin Hood: Prince of Thieves (1991)&#39;, &#39;Robocop 3 (1993)&#39;, &#39;Rock, The (1996)&#39;, &#39;Rocket Man (1997)&#39;, &#39;Roman Holiday (1953)&#39;, &#39;Romeo Is Bleeding (1993)&#39;, &#39;Romper Stomper (1992)&#39;, &#34;Romy and Michele&#39;s High School Reunion (1997)&#34;, &#39;Room with a View, A (1986)&#39;, &#39;Roommates (1995)&#39;, &#34;Roseanna&#39;s Grave (For Roseanna) (1997)&#34;, &#39;Rosencrantz and Guildenstern Are Dead (1990)&#39;, &#39;Rosewood (1997)&#39;, &#39;Rough Magic (1995)&#39;, &#39;Ruby in Paradise (1993)&#39;, &#39;Rudy (1993)&#39;, &#39;Ruling Class, The (1972)&#39;, &#39;Rumble in the Bronx (1995)&#39;, &#39;Run of the Country, The (1995)&#39;, &#39;S.F.W. (1994)&#39;, &#39;Sabrina (1954)&#39;, &#39;Sabrina (1995)&#39;, &#39;Safe (1995)&#39;, &#39;Safe Passage (1994)&#39;, &#39;Saint of Fort Washington, The (1993)&#39;, &#39;Saint, The (1997)&#39;, &#39;Salut cousin! (1996)&#39;, &#39;Santa Clause, The (1994)&#39;, &#39;Santa with Muscles (1996)&#39;, &#39;Savage Nights (Nuits fauves, Les) (1992)&#39;, &#39;Scarlet Letter, The (1926)&#39;, &#39;Scarlet Letter, The (1995)&#39;, &#34;Schindler&#39;s List (1993)&#34;, &#39;Schizopolis (1996)&#39;, &#39;Scout, The (1994)&#39;, &#39;Scream (1996)&#39;, &#39;Scream 2 (1997)&#39;, &#39;Scream of Stone (Schrei aus Stein) (1991)&#39;, &#39;Screamers (1995)&#39;, &#39;Search for One-eye Jimmy, The (1996)&#39;, &#39;Searching for Bobby Fischer (1993)&#39;, &#39;Second Jungle Book: Mowgli &amp; Baloo, The (1997)&#39;, &#39;Secret Adventures of Tom Thumb, The (1993)&#39;, &#39;Secret Agent, The (1996)&#39;, &#39;Secret Garden, The (1993)&#39;, &#39;Secret of Roan Inish, The (1994)&#39;, &#39;Secrets &amp; Lies (1996)&#39;, &#39;Selena (1997)&#39;, &#39;Sense and Sensibility (1995)&#39;, &#39;Senseless (1998)&#39;, &#39;Serial Mom (1994)&#39;, &#39;Set It Off (1996)&#39;, &#39;Seven (Se7en) (1995)&#39;, &#39;Seven Years in Tibet (1997)&#39;, &#39;Seventh Seal, The (Sjunde inseglet, Det) (1957)&#39;, &#39;Sex, Lies, and Videotape (1989)&#39;, &#39;Sexual Life of the Belgians, The (1994)&#39;, &#39;Sgt. Bilko (1996)&#39;, &#39;Shadow Conspiracy (1997)&#39;, &#39;Shadow of Angels (Schatten der Engel) (1976)&#39;, &#39;Shadow, The (1994)&#39;, &#39;Shadowlands (1993)&#39;, &#39;Shadows (Cienie) (1988)&#39;, &#39;Shaggy Dog, The (1959)&#39;, &#39;Shall We Dance? (1937)&#39;, &#39;Shall We Dance? (1996)&#39;, &#39;Shallow Grave (1994)&#39;, &#39;Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#34;She&#39;s So Lovely (1997)&#34;, &#34;She&#39;s the One (1996)&#34;, &#39;Shiloh (1997)&#39;, &#39;Shine (1996)&#39;, &#39;Shining, The (1980)&#39;, &#39;Shooter, The (1995)&#39;, &#39;Shooting Fish (1997)&#39;, &#39;Shopping (1994)&#39;, &#39;Short Cuts (1993)&#39;, &#39;Show, The (1995)&#39;, &#39;Showgirls (1995)&#39;, &#39;Silence of the Lambs, The (1991)&#39;, &#39;Silence of the Palace, The (Saimt el Qusur) (1994)&#39;, &#39;Simple Twist of Fate, A (1994)&#39;, &#39;Simple Wish, A (1997)&#39;, &#34;Singin&#39; in the Rain (1952)&#34;, &#39;Sirens (1994)&#39;, &#39;Six Degrees of Separation (1993)&#39;, &#39;Sixth Man, The (1997)&#39;, &#39;Sleeper (1973)&#39;, &#39;Sleepers (1996)&#39;, &#39;Sleepless in Seattle (1993)&#39;, &#39;Sleepover (1995)&#39;, &#39;Sliding Doors (1998)&#39;, &#39;Sling Blade (1996)&#39;, &#39;Slingshot, The (1993)&#39;, &#39;Sliver (1993)&#39;, &#39;Small Faces (1995)&#39;, &#39;Smile Like Yours, A (1997)&#39;, &#34;Smilla&#39;s Sense of Snow (1997)&#34;, &#39;Smoke (1995)&#39;, &#39;Sneakers (1992)&#39;, &#39;Snow White and the Seven Dwarfs (1937)&#39;, &#39;So Dear to My Heart (1949)&#39;, &#39;So I Married an Axe Murderer (1993)&#39;, &#39;Solo (1996)&#39;, &#39;Some Folks Call It a Sling Blade (1993)&#39;, &#39;Some Kind of Wonderful (1987)&#39;, &#39;Some Like It Hot (1959)&#39;, &#34;Some Mother&#39;s Son (1996)&#34;, &#39;Somebody to Love (1994)&#39;, &#34;Someone Else&#39;s America (1995)&#34;, &#39;Something to Talk About (1995)&#39;, &#39;Somewhere in Time (1980)&#39;, &#39;Son in Law (1993)&#39;, &#34;Sophie&#39;s Choice (1982)&#34;, &#39;Soul Food (1997)&#39;, &#39;Sound of Music, The (1965)&#39;, &#39;Space Jam (1996)&#39;, &#39;Spanish Prisoner, The (1997)&#39;, &#39;Spanking the Monkey (1994)&#39;, &#39;Spawn (1997)&#39;, &#39;Specialist, The (1994)&#39;, &#39;Species (1995)&#39;, &#39;Speechless (1994)&#39;, &#39;Speed (1994)&#39;, &#39;Speed 2: Cruise Control (1997)&#39;, &#39;Spellbound (1945)&#39;, &#39;Sphere (1998)&#39;, &#39;Spice World (1997)&#39;, &#39;Spirits of the Dead (Tre passi nel delirio) (1968)&#39;, &#39;Spitfire Grill, The (1996)&#39;, &#39;Sprung (1997)&#39;, &#39;Spy Hard (1996)&#39;, &#39;Squeeze (1996)&#39;, &#39;Stag (1997)&#39;, &#39;Stalingrad (1993)&#39;, &#39;Stalker (1979)&#39;, &#39;Stand by Me (1986)&#39;, &#39;Star Kid (1997)&#39;, &#34;Star Maker, The (Uomo delle stelle, L&#39;) (1995)&#34;, &#39;Star Maps (1997)&#39;, &#39;Star Trek III: The Search for Spock (1984)&#39;, &#39;Star Trek IV: The Voyage Home (1986)&#39;, &#39;Star Trek V: The Final Frontier (1989)&#39;, &#39;Star Trek VI: The Undiscovered Country (1991)&#39;, &#39;Star Trek: First Contact (1996)&#39;, &#39;Star Trek: Generations (1994)&#39;, &#39;Star Trek: The Motion Picture (1979)&#39;, &#39;Star Trek: The Wrath of Khan (1982)&#39;, &#39;Star Wars (1977)&#39;, &#39;Stargate (1994)&#39;, &#39;Stars Fell on Henrietta, The (1995)&#39;, &#39;Starship Troopers (1997)&#39;, &#39;Steal Big, Steal Little (1995)&#39;, &#39;Stealing Beauty (1996)&#39;, &#39;Steel (1997)&#39;, &#39;Stefano Quantestorie (1993)&#39;, &#34;Stephen King&#39;s The Langoliers (1995)&#34;, &#39;Sting, The (1973)&#39;, &#39;Stonewall (1995)&#39;, &#39;Story of Xinghua, The (1993)&#39;, &#39;Strange Days (1995)&#39;, &#39;Stranger in the House (1997)&#39;, &#39;Stranger, The (1994)&#39;, &#39;Strawberry and Chocolate (Fresa y chocolate) (1993)&#39;, &#39;Street Fighter (1994)&#39;, &#39;Streetcar Named Desire, A (1951)&#39;, &#39;Strictly Ballroom (1992)&#39;, &#39;Striking Distance (1993)&#39;, &#39;Stripes (1981)&#39;, &#39;Striptease (1996)&#39;, &#39;Stuart Saves His Family (1995)&#39;, &#39;Stupids, The (1996)&#39;, &#39;SubUrbia (1997)&#39;, &#39;Substance of Fire, The (1996)&#39;, &#39;Substitute, The (1996)&#39;, &#39;Sudden Death (1995)&#39;, &#39;Sudden Manhattan (1996)&#39;, &#39;Sum of Us, The (1994)&#39;, &#39;Sunchaser, The (1996)&#39;, &#39;Sunset Blvd. (1950)&#39;, &#39;Sunset Park (1996)&#39;, &#39;Super Mario Bros. (1993)&#39;, &#39;Supercop (1992)&#39;, &#39;Surviving Picasso (1996)&#39;, &#39;Surviving the Game (1994)&#39;, &#39;Suture (1993)&#39;, &#39;Swan Princess, The (1994)&#39;, &#39;Sweet Hereafter, The (1997)&#39;, &#39;Sweet Nothing (1995)&#39;, &#39;Swept from the Sea (1997)&#39;, &#39;Swimming with Sharks (1995)&#39;, &#39;Swingers (1996)&#39;, &#39;Swiss Family Robinson (1960)&#39;, &#39;Switchback (1997)&#39;, &#39;Switchblade Sisters (1975)&#39;, &#39;Sword in the Stone, The (1963)&#39;, &#39;Symphonie pastorale, La (1946)&#39;, &#39;T-Men (1947)&#39;, &#39;Tainted (1998)&#39;, &#39;Tales From the Crypt Presents: Demon Knight (1995)&#39;, &#39;Tales from the Crypt Presents: Bordello of Blood (1996)&#39;, &#39;Tales from the Hood (1995)&#39;, &#39;Talking About Sex (1994)&#39;, &#39;Tango Lesson, The (1997)&#39;, &#39;Tank Girl (1995)&#39;, &#39;Target (1995)&#39;, &#39;Taxi Driver (1976)&#39;, &#39;Telling Lies in America (1997)&#39;, &#39;Temptress Moon (Feng Yue) (1996)&#39;, &#39;Terminal Velocity (1994)&#39;, &#39;Terminator 2: Judgment Day (1991)&#39;, &#39;Terminator, The (1984)&#39;, &#39;Terror in a Texas Town (1958)&#39;, &#39;Tetsuo II: Body Hammer (1992)&#39;, &#39;That Darn Cat! (1965)&#39;, &#39;That Darn Cat! (1997)&#39;, &#39;That Old Feeling (1997)&#39;, &#39;That Thing You Do! (1996)&#39;, &#39;The Courtyard (1995)&#39;, &#39;The Deadly Cure (1996)&#39;, &#39;The Innocent (1994)&#39;, &#39;Theodore Rex (1995)&#39;, &#39;They Made Me a Criminal (1939)&#39;, &#39;Thieves (Voleurs, Les) (1996)&#39;, &#39;Thin Blue Line, The (1988)&#39;, &#39;Thin Line Between Love and Hate, A (1996)&#39;, &#39;Thin Man, The (1934)&#39;, &#34;Things to Do in Denver when You&#39;re Dead (1995)&#34;, &#39;Thinner (1996)&#39;, &#39;Third Man, The (1949)&#39;, &#39;Thirty-Two Short Films About Glenn Gould (1993)&#39;, &#39;This Is Spinal Tap (1984)&#39;, &#39;Thousand Acres, A (1997)&#39;, &#39;Three Caballeros, The (1945)&#39;, &#39;Three Colors: Blue (1993)&#39;, &#39;Three Colors: Red (1994)&#39;, &#39;Three Colors: White (1994)&#39;, &#39;Three Lives and Only One Death (1996)&#39;, &#39;Three Musketeers, The (1993)&#39;, &#39;Three Wishes (1995)&#39;, &#39;Threesome (1994)&#39;, &#39;Tie Me Up! Tie Me Down! (1990)&#39;, &#39;Tie That Binds, The (1995)&#39;, &#39;Tigrero: A Film That Was Never Made (1994)&#39;, &#39;Time Tracers (1995)&#39;, &#39;Time to Kill, A (1996)&#39;, &#39;Timecop (1994)&#39;, &#39;Tin Cup (1996)&#39;, &#39;Tin Drum, The (Blechtrommel, Die) (1979)&#39;, &#39;Tin Men (1987)&#39;, &#39;Titanic (1997)&#39;, &#39;To Be or Not to Be (1942)&#39;, &#39;To Catch a Thief (1955)&#39;, &#39;To Cross the Rubicon (1991)&#39;, &#39;To Die For (1995)&#39;, &#39;To Gillian on Her 37th Birthday (1996)&#39;, &#39;To Have, or Not (1995)&#39;, &#39;To Kill a Mockingbird (1962)&#39;, &#39;To Live (Huozhe) (1994)&#39;, &#39;To Wong Foo, Thanks for Everything! Julie Newmar (1995)&#39;, &#39;Tokyo Fist (1995)&#39;, &#39;Tom &amp; Viv (1994)&#39;, &#39;Tom and Huck (1995)&#39;, &#39;Tombstone (1993)&#39;, &#39;Tommy Boy (1995)&#39;, &#39;Tomorrow Never Dies (1997)&#39;, &#39;Top Gun (1986)&#39;, &#39;Top Hat (1935)&#39;, &#39;Total Eclipse (1995)&#39;, &#39;Touch (1997)&#39;, &#39;Touch of Evil (1958)&#39;, &#39;Tough and Deadly (1995)&#39;, &#39;Touki Bouki (Journey of the Hyena) (1973)&#39;, &#39;Toy Story (1995)&#39;, &#39;Trainspotting (1996)&#39;, &#39;Transformers: The Movie, The (1986)&#39;, &#39;Traveller (1997)&#39;, &#39;Treasure of the Sierra Madre, The (1948)&#39;, &#39;Trees Lounge (1996)&#39;, &#39;Trial and Error (1997)&#39;, &#39;Trial by Jury (1994)&#39;, &#39;Trigger Effect, The (1996)&#39;, &#39;True Crime (1995)&#39;, &#39;True Lies (1994)&#39;, &#39;True Romance (1993)&#39;, &#39;Truman Show, The (1998)&#39;, &#39;Trust (1990)&#39;, &#39;Truth About Cats &amp; Dogs, The (1996)&#39;, &#39;Truth or Consequences, N.M. (1997)&#39;, &#39;Turbo: A Power Rangers Movie (1997)&#39;, &#39;Turbulence (1997)&#39;, &#39;Turning, The (1992)&#39;, &#39;Twelfth Night (1996)&#39;, &#39;Twelve Monkeys (1995)&#39;, &#39;Twilight (1998)&#39;, &#39;Twin Town (1997)&#39;, &#39;Twisted (1996)&#39;, &#39;Twister (1996)&#39;, &#39;Two Bits (1995)&#39;, &#39;Two Deaths (1995)&#39;, &#39;Two Friends (1986) &#39;, &#39;Two Much (1996)&#39;, &#39;Two if by Sea (1996)&#39;, &#39;Two or Three Things I Know About Her (1966)&#39;, &#39;U Turn (1997)&#39;, &#39;U.S. Marshalls (1998)&#39;, &#34;Ulee&#39;s Gold (1997)&#34;, &#39;Umbrellas of Cherbourg, The (Parapluies de Cherbourg, Les) (1964)&#39;, &#39;Unbearable Lightness of Being, The (1988)&#39;, &#39;Under Siege (1992)&#39;, &#39;Under Siege 2: Dark Territory (1995)&#39;, &#39;Underground (1995)&#39;, &#39;Underneath, The (1995)&#39;, &#39;Underworld (1997)&#39;, &#39;Unforgettable (1996)&#39;, &#39;Unforgiven (1992)&#39;, &#39;Unhook the Stars (1996)&#39;, &#39;Unstrung Heroes (1995)&#39;, &#39;Until the End of the World (Bis ans Ende der Welt) (1991)&#39;, &#39;Unzipped (1995)&#39;, &#39;Up Close and Personal (1996)&#39;, &#39;Up in Smoke (1978)&#39;, &#39;Usual Suspects, The (1995)&#39;, &#39;Vampire in Brooklyn (1995)&#39;, &#39;Van, The (1996)&#39;, &#39;Vanya on 42nd Street (1994)&#39;, &#39;Vegas Vacation (1997)&#39;, &#39;Venice/Venice (1992)&#39;, &#39;Vermin (1998)&#39;, &#39;Vermont Is For Lovers (1992)&#39;, &#39;Vertigo (1958)&#39;, &#39;Very Brady Sequel, A (1996)&#39;, &#39;Very Natural Thing, A (1974)&#39;, &#39;Victor/Victoria (1982)&#39;, &#39;Vie est belle, La (Life is Rosey) (1987)&#39;, &#39;Village of the Damned (1995)&#39;, &#39;Virtuosity (1995)&#39;, &#39;Visitors, The (Visiteurs, Les) (1993)&#39;, &#39;Volcano (1997)&#39;, &#39;Wag the Dog (1997)&#39;, &#39;Waiting for Guffman (1996)&#39;, &#39;Waiting to Exhale (1995)&#39;, &#39;Walk in the Clouds, A (1995)&#39;, &#39;Walk in the Sun, A (1945)&#39;, &#39;Walkabout (1971)&#39;, &#39;Walking Dead, The (1995)&#39;, &#39;Walking and Talking (1996)&#39;, &#39;Wallace &amp; Gromit: The Best of Aardman Animation (1996)&#39;, &#39;War Room, The (1993)&#39;, &#39;War at Home, The (1996)&#39;, &#39;War, The (1994)&#39;, &#39;Warriors of Virtue (1997)&#39;, &#39;Washington Square (1997)&#39;, &#39;Waterworld (1995)&#39;, &#39;Wedding Bell Blues (1996)&#39;, &#39;Wedding Gift, The (1994)&#39;, &#39;Wedding Singer, The (1998)&#39;, &#34;Weekend at Bernie&#39;s (1989)&#34;, &#39;Welcome To Sarajevo (1997)&#39;, &#39;Welcome to the Dollhouse (1995)&#39;, &#34;Wend Kuuni (God&#39;s Gift) (1982)&#34;, &#34;Wes Craven&#39;s New Nightmare (1994)&#34;, &#39;What Happened Was... (1994)&#39;, &#34;What&#39;s Eating Gilbert Grape (1993)&#34;, &#34;What&#39;s Love Got to Do with It (1993)&#34;, &#39;When Harry Met Sally... (1989)&#39;, &#39;When Night Is Falling (1995)&#39;, &#39;When We Were Kings (1996)&#39;, &#39;When a Man Loves a Woman (1994)&#39;, &#39;When the Cats Away (Chacun cherche son chat) (1996)&#39;, &#39;While You Were Sleeping (1995)&#39;, &#39;White Balloon, The (1995)&#39;, &#34;White Man&#39;s Burden (1995)&#34;, &#39;White Squall (1996)&#39;, &#39;Whole Wide World, The (1996)&#39;, &#34;Widows&#39; Peak (1994)&#34;, &#39;Wife, The (1995)&#39;, &#39;Wild America (1997)&#39;, &#39;Wild Bill (1995)&#39;, &#39;Wild Bunch, The (1969)&#39;, &#39;Wild Reeds (1994)&#39;, &#39;Wild Things (1998)&#39;, &#34;William Shakespeare&#39;s Romeo and Juliet (1996)&#34;, &#39;Willy Wonka and the Chocolate Factory (1971)&#39;, &#39;Window to Paris (1994)&#39;, &#39;Wings of Courage (1995)&#39;, &#39;Wings of Desire (1987)&#39;, &#39;Wings of the Dove, The (1997)&#39;, &#39;Winnie the Pooh and the Blustery Day (1968)&#39;, &#39;Winter Guest, The (1997)&#39;, &#39;Wishmaster (1997)&#39;, &#39;With Honors (1994)&#39;, &#39;Withnail and I (1987)&#39;, &#39;Witness (1985)&#39;, &#39;Wizard of Oz, The (1939)&#39;, &#39;Wolf (1994)&#39;, &#39;Woman in Question, The (1950)&#39;, &#39;Women, The (1939)&#39;, &#39;Wonderful, Horrible Life of Leni Riefenstahl, The (1993)&#39;, &#39;Wonderland (1997)&#39;, &#34;Wooden Man&#39;s Bride, The (Wu Kui) (1994)&#34;, &#39;World of Apu, The (Apur Sansar) (1959)&#39;, &#39;Wrong Trousers, The (1993)&#39;, &#39;Wyatt Earp (1994)&#39;, &#39;Yankee Zulu (1994)&#39;, &#39;Year of the Horse (1997)&#39;, &#39;You So Crazy (1994)&#39;, &#39;Young Frankenstein (1974)&#39;, &#39;Young Guns (1988)&#39;, &#39;Young Guns II (1990)&#39;, &#34;Young Poisoner&#39;s Handbook, The (1995)&#34;, &#39;Zeus and Roxanne (1997)&#39;, &#39;unknown&#39;, &#39;Á köldum klaka (Cold Fever) (1994)&#39;]} . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) . Мы можем представить поиск по индексу как матричное произведение. Фокус в том, чтобы заменить наши индексы векторами one-hot-encoded. Вот пример того, что произойдет, если мы умножим вектор на вектор one-hot-encoded, представленный под индексом 3: . one_hot_3 = one_hot(3, n_users).float() . user_factors.t() @ one_hot_3 . tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908]) . Это дает нам тот же вектор, что и под индексом 3 в матрице: . user_factors[3] . tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908]) . &#1050;&#1086;&#1083;&#1083;&#1072;&#1073;&#1086;&#1088;&#1072;&#1090;&#1080;&#1074;&#1085;&#1072;&#1103; &#1092;&#1080;&#1083;&#1100;&#1090;&#1088;&#1072;&#1094;&#1080;&#1103; &#1089; &#1085;&#1091;&#1083;&#1103; . Это пример простого класса: . class Example: def __init__(self, a): self.a = a def say(self,x): return f&#39;Hello {self.a}, {x}.&#39; . Наиболее важной частью этого является специальный метод, называемый init__ (произносится dunder init), который будет вызван при создании объекта. В нем можно провести настройку. . ex = Example(&#39;Sylvain&#39;) ex.say(&#39;nice to meet you&#39;) . &#39;Hello Sylvain, nice to meet you.&#39; . При вызове модуля PyTorch также будет вызван метод в вашем классе под названием forward и в него будут переданы все параметры. Вот класс, определяющий модель точечного произведения: . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . Обратите внимание, что входными данными модели является тензор формы batch_size x 2, где первый столбец (x [:, 0]) содержит идентификаторы пользователей, а второй столбец (x [:, 1]) содержит идентификаторы фильмов. Мы используем вложенные слои для представления наших матриц скрытых факторов пользователей и фильмов: . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . Дальше необходимо создать Learner для оптимизации нашей модели. В прошлом мы использовали специальные функции, такие как cnn_learner, которые настраивали все для нас для конкретного приложения. Поскольку мы здесь делаем все с нуля, мы будем использовать обычный класс Learner: . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) . Теперь мы хотим обучить нашу модель . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.344786 | 1.279100 | 00:14 | . 1 | 1.093332 | 1.109981 | 00:14 | . 2 | 0.958258 | 0.990199 | 00:14 | . 3 | 0.814234 | 0.894916 | 00:14 | . 4 | 0.780714 | 0.882022 | 00:13 | . Первое, что мы можем сделать, чтобы сделать эту модель немного лучше, - это заставить эти прогнозы быть между 0 и 5. Для этого нам просто нужно использовать sigmoid_range . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.986799 | 1.005294 | 00:13 | . 1 | 0.878134 | 0.918898 | 00:13 | . 2 | 0.675850 | 0.875467 | 00:13 | . 3 | 0.483372 | 0.877939 | 00:13 | . 4 | 0.378927 | 0.881887 | 00:13 | . Это потому, что в данный момент у нас есть только веса; у нас нет предвзятости пользователей. Для того, чтобы справиться с этим можно использовать номер для каждого пользователя и для каждого фильма. Итак, прежде всего, давайте скорректируем архитектуру нашей модели: . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . Давайте попробуем потренировать эту модель и посмотрим что происходит: . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.938634 | 0.952516 | 00:15 | . 1 | 0.846664 | 0.865633 | 00:15 | . 2 | 0.608090 | 0.865127 | 00:15 | . 3 | 0.413482 | 0.887318 | 00:15 | . 4 | 0.286971 | 0.894876 | 00:15 | . Как видно к концу обучения модель становиться хуже. Как мы уже видели, это явный признак переобучения. В этом случае не идут по пути приращения данных, а используют другие подходы регуляризации. Например снижение веса. . &#1057;&#1085;&#1080;&#1078;&#1077;&#1085;&#1080;&#1077; &#1074;&#1077;&#1089;&#1086;&#1074; . Как предотвратит переобучение? Идея состоит в том, что чем больше коэффициенты, тем более крутые перепады мы будем иметь в функции потерь. Если мы возьмем базовый пример параболы, y = a (x * 2), то чем больше a, тем уже парабола: . x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . Чтобы использовать снижение весов, просто добавьте wd в fit или fit_one_cycle: . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.932776 | 0.961672 | 00:15 | . 1 | 0.888625 | 0.882614 | 00:15 | . 2 | 0.771066 | 0.832743 | 00:15 | . 3 | 0.599807 | 0.822374 | 00:15 | . 4 | 0.504981 | 0.822528 | 00:15 | . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1074;&#1072;&#1096;&#1077;&#1075;&#1086; &#1089;&#1086;&#1073;&#1089;&#1090;&#1074;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1074;&#1089;&#1090;&#1088;&#1072;&#1080;&#1074;&#1072;&#1102;&#1097;&#1077;&#1075;&#1086;&#1089;&#1103; &#1084;&#1086;&#1076;&#1091;&#1083;&#1103; . До сих пор мы использовали включение, не задумываясь о том, как оно на самом деле работает. Давайте воссоздадим DotProductBias без использования этого класса. Нам понадобится случайно инициализированная весовая матрица для каждого из вложений. Однако мы должны быть осторожны. Вспомните: . class T(Module): def __init__(self): self.a = torch.ones(3) L(T().parameters()) . (#0) [] . Чтобы сообщить модулю, что мы хотим рассматривать тензор как параметр, мы должны обернуть его в класс nn.Parameter. Этот класс на самом деле не добавляет никакой функциональности (кроме автоматического вызова requiresgrad для нас). Он используется только как &quot;маркер&quot;, чтобы показать, что включать в параметры: . class T(Module): def __init__(self): self.a = nn.Parameter(torch.ones(3)) L(T().parameters()) . (#1) [Parameter containing: tensor([1., 1., 1.], requires_grad=True)] . Все модули PyTorch используют nn.Parameter для любых обучаемых параметров, поэтому до сих пор нам не нужно было явно использовать эту оболочку: . class T(Module): def __init__(self): self.a = nn.Linear(1, 3, bias=False) t = T() L(t.parameters()) . (#1) [Parameter containing: tensor([[-0.3292], [-0.8623], [ 0.0592]], requires_grad=True)] . type(t.a.weight) . Мы можем создать тензор в качестве параметра со случайной инициализацией, например: . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) . Давайте снова используем это для создания DotProductBias, но без включения: . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . Затем давайте снова потренируем, чтобы проверить, что мы получаем примерно те же результаты, что и в предыдущем разделе . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.929254 | 0.953444 | 00:13 | . 1 | 0.865246 | 0.878304 | 00:14 | . 2 | 0.720294 | 0.838921 | 00:14 | . 3 | 0.582796 | 0.829129 | 00:14 | . 4 | 0.474043 | 0.829031 | 00:13 | . &#1048;&#1085;&#1090;&#1077;&#1088;&#1087;&#1077;&#1088;&#1077;&#1090;&#1072;&#1094;&#1080;&#1103; &#1074;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1081; &#1080; &#1087;&#1088;&#1077;&#1076;&#1091;&#1073;&#1077;&#1078;&#1076;&#1077;&#1085;&#1080;&#1081; . Проще всего интерпретировать предубеждения. Вот фильмы с самыми низкими значениями вектора предубеждений: . movie_bias = learn.model.movie_bias.squeeze() idxs = movie_bias.argsort()[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, &#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Mortal Kombat: Annihilation (1997)&#39;, &#39;Amityville 3-D (1983)&#39;, &#39;Beautician and the Beast, The (1997)&#39;] . Точно так же здесь представлены фильмы с самым высоким предубеждением: . idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Titanic (1997)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Silence of the Lambs, The (1991)&#39;, &#39;L.A. Confidential (1997)&#39;, &#34;Schindler&#39;s List (1993)&#34;] . Использование метода главных компонент (PCA) . g = ratings.groupby(&#39;title&#39;)[&#39;rating&#39;].count() top_movies = g.sort_values(ascending=False).index.values[:1000] top_idxs = tensor([learn.dls.classes[&#39;title&#39;].o2i[m] for m in top_movies]) movie_w = learn.model.movie_factors[top_idxs].cpu().detach() movie_pca = movie_w.pca(3) fac0,fac1,fac2 = movie_pca.t() idxs = np.random.choice(len(top_movies), 50, replace=False) idxs = list(range(50)) X = fac0[idxs] Y = fac2[idxs] plt.figure(figsize=(12,12)) plt.scatter(X, Y) for i, x, y in zip(top_movies[idxs], X, Y): plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=11) plt.show() . &#1048;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077; fastai.collab . Мы можем создать и обучить модель коллаборативной фильтрации, используя точную структуру, показанную ранее, используя collab_learner из fastai: . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.939463 | 0.954959 | 00:15 | . 1 | 0.841215 | 0.876151 | 00:15 | . 2 | 0.724404 | 0.832099 | 00:16 | . 3 | 0.597228 | 0.816953 | 00:15 | . 4 | 0.481373 | 0.817286 | 00:15 | . Названия слоев можно увидеть, распечатав модель: . learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1665, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1665, 1) ) . Мы можем использовать их, чтобы повторить любой из анализов, которые мы сделали в предыдущем разделе—например: . movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;L.A. Confidential (1997)&#39;, &#39;Titanic (1997)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Silence of the Lambs, The (1991)&#39;, &#39;Rear Window (1954)&#39;] . Embedding Distance . На двумерной карте мы можем вычислить расстояние между двумя координатами, используя формулу Пифагора: $ sqrt{x^{2}+y^{2}}$ (предполагая, что x и y-это расстояния между координатами на каждой оси). Для 50-мерного вложения мы можем сделать точно то же самое, за исключением того, что мы складываем квадраты всех 50 координатных расстояний. . Если бы было два фильма, которые были почти идентичны, то их векторы встраивания также должны были бы быть почти идентичными, потому что пользователи, которым они понравились бы, были бы почти точно такими же. Здесь есть более общая идея: сходство фильмов может быть определено сходством пользователей, которым нравятся эти фильмы. И это непосредственно означает, что расстояние между векторами вложения двух фильмов может определить это сходство. Мы можем использовать это, чтобы найти наиболее похожий фильм на Молчание ягнят: . movie_factors = learn.model.i_weight.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Silence of the Lambs, The (1991)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . &#39;Before the Rain (Pred dozhdot) (1994)&#39; . Deep Learning for Collaborative Filtering . Чтобы превратить нашу архитектуру в модель глубокого обучения, первым шагом необходимо получение результатов поиска включений и объединение активаций вместе. Это дает нам матрицу, которую мы можем затем передовать через линейные слои и нелинейности обычным способом. . Две матрицы вложений могут иметь разные размеры (то есть разное количество скрытых факторов). fastai имеет функцию get_emb_sz, которая возвращает рекомендуемые размеры для включаемых матриц с вашими данными: . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . Давайте реализуем этот класс: . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . И используем для создания модели: . model = CollabNN(*embs) . CollabNN создает наши включаемые слои так же, как и предыдущие классы, за исключением того, что теперь мы используем embs размеры self.layers, что идентично мини-нейронной сети . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.943857 | 0.951898 | 00:17 | . 1 | 0.914170 | 0.900519 | 00:17 | . 2 | 0.850523 | 0.883037 | 00:17 | . 3 | 0.814549 | 0.875490 | 00:17 | . 4 | 0.763638 | 0.878803 | 00:17 | . fastai приводит эту модель в fastai.collab, если вы передаете use_nn=True в своем вызове collab_learner (включая вызов get_emb_sz), и это позволяет вам легко создавать больше слоев. Например, здесь мы создаем два скрытых слоя размером 100 и 50 соответственно: . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 1.009432 | 1.007777 | 00:24 | . 1 | 0.881598 | 0.934248 | 00:25 | . 2 | 0.883872 | 0.896940 | 00:25 | . 3 | 0.819078 | 0.869650 | 00:26 | . 4 | 0.797415 | 0.866991 | 00:26 | . learn. model - это объект типа EmbeddingNN. Давайте взглянем на код fastai для этого класса: . @delegates(TabularModel) class EmbeddingNN(TabularModel): def __init__(self, emb_szs, layers, **kwargs): super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) . &#1042;&#1099;&#1074;&#1086;&#1076;&#1099; . Для нашего первого приложения, не связанного с компьютерным зрением, мы рассмотрели рекомендательные системы и увидели, как градиентный спуск может узнать внутренние факторы или предубеждения о предметах из истории рейтингов. Затем они могут дать нам информацию о данных. . Мы также построили нашу первую модель в PyTorch. Мы сделаем гораздо больше этого в следующем разделе книги, но сначала давайте закончим наше погружение в другие общие приложения глубокого обучения, продолжая с табличными данными. .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2021/01/22/08-collab.html",
            "relUrl": "/russian/fast.ai/solution/2021/01/22/08-collab.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Russian - Fastbook Chapter 8 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы к восьмой части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, как и к осталььным частям прошу писать в коментариях - поправлю. . 1. Какую проблему решает коллаборативная фильтрация? . Она решает задачу прогнозирования интересов пользователей на основе интересов других пользователей и рекомендации на основе этих интересов. . 2. Как она работает? . Ключевая идея коллаборативной фильтрации-скрытые (латентные) факторы. Идея заключается в том, что модель может сказать, какие предметы вам могут понравиться (например, вам нравятся научно-фантастические фильмы/книги), и эти факторы изучаются (с помощью базового градиентного спуска) на основе того, какие предметы нравятся другим пользователям со схожими интересами. . 3. Почему прогностическая модель коллаборативной фильтрации может быть не очень полезной рекомендательной системой? . Если для обучения модели не так много рекомендаций или не достаточно данных о пользователе, чтобы дать полезные рекомендации, то такие системы коллаборативной фильтрации могут оказаться бесполезными. . 4. Как выглядит перекрестное представление данных совместной фильтрации? . В представлении перекрестной таблицы пользователи и элементы представляют собой строки и столбцы (или наоборот) большой матрицы со значениями, заполненными на основе рейтинга элемента выставленного пользователем. . 5. Напишите код для создания перекрестного представления данных MovieLens (возможно, вам придется выполнить некоторый поиск в интернете!) . Выполняется самостоятельно. . 6. Что такое латентный фактор? Почему он “латентный”? . Как описано выше, латентный фактор - это факторы, которые важны для прогнозирования рекомендаций, но явно не указаны модели, а вместо этого исследованы (отсюда и “латентные”). . 7. Что такое скалярное произведение? Вычислить скалярное произведение вручную с использованием чистого питона и списков. . Скалярное произведение - это когда вы умножаете соответствующие элементы двух векторов и складываете их. Если мы представим векторы в виде списков одинакового размера, то вот как мы можем выполнить скалярное произведение: . a = [1, 2, 3, 4] b = [5, 6, 7, 8] dot_product = sum(i[0]*i[1] for i in zip(a,b)) . 8. Что делает pandas.DataFrame.merge? . Он позволяет объединять несколько DataFrame в один. . 9. Что такое матрица эмбедингов? . Это то, что вы получаете через умножение и в случае совместной фильтрации изучаете. . 10. Какова связь между эмбедингом и матрицей one-hot encoded векторов? . Эмбединг - это матрица one-hot encoded векторов, которая в вычислительном отношении более эффективна. . 11. Зачем нам нужен эмбендинг, если мы можем использовать one-hot encoded векторы для того же самого? . Вычислительная эфективность эмбендинга более эфективна. Умножение с помощью one-hot encoded векторов эквивалентно индексации в эмбендинг матрице, что делает эмбендинг слой. Однако градиент вычисляется таким образом, что он эквивалентен умножению с one-hot encoded вектором. . 12. Что содержит эмбендинг перед началом обучения (предполагая, что мы не используем предварительно обученную модель)? . Эмбендинг инициализируется случайным образом. . 13. Создайте класс (по возможности не подглядывая!) и используйте его. . Делать это должен читатель. Пример в главе: . class Example: def __init__(self, a): self.a = a def say(self,x): return f&#39;Hello {self.a}, {x}.&#39; . 14. Что возвращает x[:,0]? . Идентификаторы пользователей . 15. Перепишите класс DotProduct (по возможности не подглядывая!) и обучите с его помощью модель. . Код, приведенный в главе: . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . 16. Какая неплохая функция потерь используется для MovieLens? Почему? . Мы можем использовать среднеквадратичную ошибку (MSE), которая является вполне разумной потерей, поскольку у нас в качестве целей используются количественные значения, и данная ошибка позволяет получить точность модели. . 17. Что произойдет, если мы используем CrossEntropy потери в MovieLens? Как нам нужно будет изменить модель? . Нам нужно было бы обеспечить, чтобы модель выводила 5 прогнозов. Например, с помощью нейросетевой модели нам нужно изменить последний линейный слой, чтобы вывести 5, а не 1, предсказаний. Затем они передаются в CrossEntropy потери. . 18. Что есть использование предвзятости в модели скалярного умножения? . Предвзятость компенсирует тот факт, что некоторые фильмы просто потрясающие или довольно плохие. Это также компенсирует пользователей, которым свойственно давать больше положительных или отрицательных рекомендаций в целом. . 19. Как еще называется понижение веса? . L2 регуляризация. . 20. Напишите уравнение для снижения веса (не подглядывая!) . loss_with_wd = loss + wd * (parameters**2).sum() . 21. Напишите уравнение для градиента снижения веса. Почему он работает? . Добавим к градиентам 2*wd*parameters. Это помогает создавать небольшие, менее неровные/островыпуклые поверхности, которые лучше обобщаются и предотвращают переобучение.,, . 22. Почему уменьшение веса приводит к лучшему обобщению? . Так как оно приведит к более пологим поверхностям, так как более крутые приводят к переобочению. . 23. Что делает argsort в PyTorch? . Получает индексы в том порядке, в котором сортируется исходный тензор в PyTorch. . 24. Дает ли сортировка предубеждений к фильмам тот же результат, что и усреднение в целом рейтингов фильмов? Почему / почему нет? . Нет и есть большое отличие. Предубеждение учитывает жанр, актеров или другие факторы. Например, фильмы с низким значением предубеждения означают, что даже если вам нравятся эти типы фильмов, то вам может все равно не понравиться этот фильм из-за других характеристик. . 25. Как вывести на печать имена и детали слоев в модели? . Просто набрав learn.model . 26. Что такое “проблема начальной загрузки(bootstrapping problem)” в коллаборативной фильтрации? . Что модель / система не может давать никаких рекомендаций или делать какие-либо выводы для пользователей или элементов, о которых она еще не собрала достаточной информации. Это также называется проблемой холодного старта. . 27. Как вы могли бы справиться с проблемой начальной загрузки для новых пользователей? Для новых фильмов? . Вы можете решить эту проблему, придумав среднее подставляемое значение для пользователя или фильма. Или подобрать среднего пользователя/фильм. Кроме того, вы можете задать несколько вопросов, которые помогут инициализировать векторы внедряемых значений для новых пользователей и фильмов. . 28. Как циклы обратной связи могут влиять на системы коллаборативной фильтрации? . Рекомендации могут страдать от предвзятости из-за того, что небольшое количество людей сильно влияет на систему. Например: Очень восторженные поклонники аниме, которые оценивают фильмы гораздо чаще, чем другие, могут заставить систему рекомендовать аниме чаще, чем ожидалось (также и для нелюбителей аниме). . 29. При использовании нейронной сети в коллаборативной фильтрации почему у нас может быть разное количество факторов для фильма и пользователя? . В этом случае мы не берем скалярное произведение, а вместо этого объединяем включаемые матрицы. Поэтому количество факторов может быть разным. . 30. Для чего существует nn.Sequential в модели CollabNN? . Он позволяет нам соединить несколько слоев nn.Module вместе для использования. В этом случае два линейных слоя соединяются вместе, и включения могут быть непосредственно переданы в линейные слои. . 31. Какую модель следует использовать, если мы хотим добавить метаданные о пользователях и элементах или такую информацию, как дата и время, в модель коллабартивной фильтрации? . Мы должны использовать табличную модель, которая обсуждается в следующей главе! .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2021/01/17/fastai-chapter6-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2021/01/17/fastai-chapter6-solution.html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Russian - Solution Lesson 7 on Fast.ai",
            "content": "&#1054;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077; &#1089;&#1086;&#1074;&#1077;&#1088;&#1084;&#1077;&#1085;&#1085;&#1099;&#1093; &#1084;&#1086;&#1076;&#1077;&#1083;&#1077;&#1081; . В этой главе представлены более продвинутые методы обучения модели классификации изображений и получения самых современных результатов. Вы можете пропустить его, если хотите узнать больше о других приложениях глубокого обучения и вернуться к нему позже—знание этого материала не будет предполагаться в последующих главах. . Мы рассмотрим, что такое нормализация, мощный метод увеличения данных, называемый mixup, прогрессивный подход к изменению размера и увеличение времени тестирования. Чтобы показать все это, мы собираемся обучить модель с нуля (не используя трансферное обучение), используя подмножество ImageNet, называемое Imagenette. Он содержит подмножество из 10 очень разных категорий из исходного набора данных ImageNet, что позволяет быстрее обучаться, когда мы хотим поэкспериментировать. . Это будет намного сложнее сделать хорошо, чем с нашими предыдущими наборами данных, потому что мы используем полноразмерные, полноцветные изображения, которые представляют собой фотографии объектов разных размеров, в разных ориентациях, при различном освещении и так далее. Итак, в этой главе мы познакомим вас с некоторыми важными методами получения максимальной отдачи от вашего набора данных, особенно когда вы тренируетесь с нуля или используете трансферное обучение для обучения модели на совершенно ином типе набора данных, чем используемая предварительно обученная модель. . Imagenette . Когда fast.ai создавали, было три основных набора данных, которые люди использовали для построения и тестирования моделей компьютерного зрения: . ImageNet:: 1,3 миллиона изображений различных размеров около 500 пикселей в поперечнике, в 1000 категориях, обучение которым заняло несколько дней | MNIST:: 50 000 28×28-пиксельных оттенков серого рукописных цифр | CIFAR10:: 60 000 32×32-пиксельных цветных изображений в 10 классах | . Проблема заключалась в том, что меньшие наборы данных на самом деле не были обобщены на самом деле на большом наборе данных ImageNet. Подходы, которые хорошо работали в ImageNet, как правило, должны были бы быть разработаны и обучены в ImageNet. Это привело к тому, что многие люди поверили, что только исследователи, имеющие доступ к гигантским вычислительным ресурсам, могут эффективно участвовать в разработке алгоритмов классификации изображений. . Мы думали, что это маловероятно похоже на правду. Мы решили попытаться создать новый набор данных, на котором исследователи могли бы быстро и дешево протестировать свои алгоритмы, но который также дал бы представление о том, как работать с полным набором данных ImageNet. . Примерно через три часа мы создали Imagenette. Мы выбрали 10 классов из полного ImageNet, которые выглядели очень непохожими друг на друга. Как мы и надеялись, нам удалось быстро и дешево создать классификатор, способный распознавать эти классы. Затем мы опробовали несколько алгоритмических настроек, чтобы увидеть, как они повлияли на Imagenette. Мы нашли некоторые из них, которые работали довольно хорошо, и протестировали их также на ImageNet—и мы были очень рады обнаружить, что наши настройки хорошо работали и на ImageNet! . Здесь есть важное сообщение: набор данных, который вы получаете, не обязательно является тем набором данных, который вы хотите. Особенно маловероятно, что это будет тот набор данных, в котором вы хотите заниматься разработкой и прототипированием. Вы должны стремиться к тому, чтобы скорость итерации составляла не более пары минут—то есть, когда вы придумываете новую идею, которую хотите опробовать, вы должны быть в состоянии обучить модель и посмотреть, как она работает в течение нескольких минут. Если эксперимент занимает больше времени, подумайте о том, как вы могли бы сократить набор данных или упростить модель, чтобы повысить скорость экспериментов. Чем больше экспериментов вы сможете провести, тем лучше! . Давайте начнем с этого набора данных: . from fastai.vision.all import * path = untar_data(URLs.IMAGENETTE) . Сначала мы поместим наш набор данных в объект Dataloader, используя трюк изменения размера: . dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = dblock.dataloaders(path, bs=64) . и сделайте тренировочный прогон, который послужит основой: . model = xresnet50() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.604518 | 1.846823 | 0.485810 | 02:06 | . 1 | 1.251086 | 1.524415 | 0.559746 | 02:08 | . 2 | 0.986634 | 0.959455 | 0.693054 | 02:09 | . 3 | 0.733720 | 0.643728 | 0.796117 | 02:08 | . 4 | 0.591311 | 0.593129 | 0.811053 | 02:08 | . Это хороший базовый уровень, поскольку мы не используем предварительно подготовленную модель, но мы можем сделать лучше. При работе с моделями, которые обучаются с нуля или настраиваются на совершенно другой набор данных, чем тот, который используется для предварительного обучения, есть некоторые дополнительные методы, которые действительно важны. В оставшейся части главы мы рассмотрим некоторые ключевые подходы, с которыми вы захотите ознакомиться. Первый - это нормализация ваших данных. . &#1053;&#1086;&#1088;&#1084;&#1072;&#1083;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; . При обучении модели полезно, если ваши входные данные нормализованы—то есть имеют среднее значение 0 и стандартное отклонение 1. Но большинство изображений и библиотек компьютерного зрения используют значения между 0 и 255 для пикселей или между 0 и 1; в любом случае ваши данные не будут иметь среднее значение 0 и стандартное отклонение 1. . Давайте возьмем пакет наших данных и посмотрим на эти значения, усреднив их по всем осям, кроме оси канала, которая является осью 1: . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.4898, 0.4815, 0.4437], device=&#39;cuda:0&#39;), TensorImage([0.2862, 0.2812, 0.3122], device=&#39;cuda:0&#39;)) . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([-0.0622, -0.0261, 0.0051], device=&#39;cuda:0&#39;), TensorImage([1.2568, 1.2598, 1.3187], device=&#39;cuda:0&#39;)) . Давайте проверим, как это повлияло на обучение нашей модели: . model = xresnet50() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.738131 | 2.420559 | 0.356983 | 02:08 | . 1 | 1.275637 | 1.705510 | 0.474981 | 02:09 | . 2 | 0.963854 | 1.504349 | 0.594847 | 02:09 | . 3 | 0.729794 | 0.721336 | 0.784167 | 02:08 | . 4 | 0.617732 | 0.594153 | 0.823002 | 02:09 | . Хотя здесь это лишь немного помогло, нормализация становится особенно важной при использовании предварительно подготовленных моделей. Предварительно обученная модель умеет работать только с данными того типа, который она использовала ранее. Если среднее значение пикселя было 0 в данных, с которыми она была обучена, но ваши данные имеют 0 как минимально возможное значение пикселя, то модель будет видеть что-то очень отличное от того, для чего расчитана! . Это означает, что при распространении модели необходимо также распространять статистику, используемую для нормализации, поскольку любой, кто использует ее для получения результатов или трансферного обучения, должен будет использовать ту же статистику. Точно так же, если вы используете модель, которую обучил кто-то другой, обязательно выясните, какую статистику нормализации они использовали, и применить ее. . Нам не пришлось работать с нормализацией в предыдущих главах, потому что при использовании предварительно обученной модели через cnn_learner библиотека fastai автоматически добавляет соответствующее преобразование Normalize; модель была предварительно обучена с определенной статистикой в Normalize(обычно определяемой из набора данных ImageNet), поэтому библиотека может выполнить это для вас. Обратите внимание, что это относится только к предварительно подготовленным моделям, поэтому нам нужно выполнить это вручную при обучении с нуля. . Все наши тренировки до сих пор проводились с размером 224. Дальше рассмотрим прогрессивное изменение размера. . &#1055;&#1088;&#1086;&#1075;&#1088;&#1077;&#1089;&#1089;&#1080;&#1074;&#1085;&#1086;&#1077; &#1080;&#1079;&#1084;&#1077;&#1085;&#1077;&#1085;&#1080;&#1077; &#1088;&#1072;&#1079;&#1084;&#1077;&#1088;&#1072; . Когда одна из команд студентов с помощью fast.ai выиграла конкурс DAWNBench в 2018 году, одним из самых важных нововведений было что-то очень простое: начать обучение с использованием маленьких изображений и закончить обучение с использованием больших изображений. Тренировка большей части эпох с маленькими по размеру изображениями помогает завершить их быстрее. А завершение обучения с использованием больших изображений значительно повышает конечную точность. Мы называем этот подход прогрессивным изменением размеров. . жаргон:прогрессивное изменение размера: постепенное уыеличение размера изображения по мере тренировки. Как мы уже видели, типы признаков, которые изучаются сверточными нейронными сетями, никоим образом не зависят от размера изображения—ранние слои находят такие вещи, как края и градиенты, а более поздние слои могут находить такие вещи, как носы и закаты солнца. Поэтому, когда мы меняем размер изображения в середине обучения, это не означает, что мы должны перенастроить параметры для нашей модели. . Но очевидно, что есть некоторые различия между маленькими изображениями и большими, поэтому мы не должны ожидать, что наша модель будет продолжать работать точно так же хорошо если не провести каких-либо изменений вообще. Это вам что-то напоминает? Когда мы разработали эту идею, она напомнила нам о трансферном обучении! Мы пытаемся заставить нашу модель научиться делать что-то немного отличное от того, что она научилась делать раньше. Поэтому мы должны иметь возможность использовать метод fine_tune после изменения размера наших изображений. . У прогрессивного изменения размера есть еще одно преимущество: это еще одна форма прироста данных. Поэтому вы должны ожидать лучшего обобщения ваших моделей, которые обучаются с прогрессивным изменением размера. . Для реализации прогрессивного изменения размера наиболее удобно, если вы сначала создадите функцию get_dls, которая принимает размер изображения и размер пакета, как мы это делали в предыдущем разделе, и возвращает ваш DataLoaders: . Теперь вы можете создавать свои DataLoaders с небольшим размером и использовать fit_one_cycle обычным способом, тренируясь на несколько меньших эпохах, чем если бы делали это в противном случае: . dls = get_dls(128, 128) learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.875896 | 1.774017 | 0.500747 | 00:51 | . 1 | 1.293527 | 1.380599 | 0.579910 | 00:52 | . 2 | 0.956708 | 0.844871 | 0.738611 | 00:52 | . 3 | 0.735907 | 0.673869 | 0.802465 | 00:52 | . Затем вы можете заменить DataLoaders внутри Learner и точно настроить их: . learn.dls = get_dls(64, 224) learn.fine_tune(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.846806 | 1.325565 | 0.599328 | 02:09 | . epoch train_loss valid_loss accuracy time . 0 | 0.653103 | 0.797360 | 0.766243 | 02:09 | . 1 | 0.656540 | 0.708841 | 0.784541 | 02:09 | . 2 | 0.582353 | 0.571958 | 0.827857 | 02:09 | . 3 | 0.497656 | 0.468391 | 0.854742 | 02:09 | . 4 | 0.437954 | 0.447989 | 0.861464 | 02:09 | . Как вы можете видеть, мы получаем гораздо лучшую производительность, и первоначальное обучение на небольших изображениях было намного быстрее в каждую эпоху. . Вы можете повторить процесс увеличения размера и обучения большего количества эпох столько раз, сколько захотите, для изображения того, какое хотите — но, конечно, вы не получите никакой пользы, используя размер изображения больше, чем размер ваших изображений оригиналов. . Обратите внимание, что для транферного обучения, прогрессивное изменение размера может фактически навредить результативности. Это, скорее всего, произойдет, если ваша предварительно обученная модель была очень похожа на вашу задачу трансферного обучения и набор данных и была обучена на изображениях одинакового размера, поэтому веса не требовали сильного изменения. В этом случае тренировка на меньших изображениях может повредить предварительно натренированные веса. . С другой стороны, если задача трансферного обучения будет использовать изображения разных размеров, форм или стилей, чем те, которые используются в задаче предварительного обучения, прогрессивное изменение размера, вероятно, поможет. Как всегда, ответ на вопрос &quot;Поможет ли это? &quot;звучит так: &quot;Попробуйте!&quot; . Еще одна вещь,которую мы могли бы попробовать, - это применить увеличение данных к набору проверки. До сих пор мы применяли его только на обучающем наборе; набор проверки всегда получает одни и те же изображения. Но, возможно, мы могли бы попытаться сделать прогнозы для нескольких расширенных версий набора проверки и усреднить их. Далее мы рассмотрим этот подход. . &#1040;&#1091;&#1075;&#1084;&#1077;&#1085;&#1090;&#1072;&#1094;&#1080;&#1103; &#1074;&#1086; &#1074;&#1088;&#1077;&#1084;&#1103; &#1090;&#1077;&#1089;&#1090;&#1072; . Мы использовали случайное кадрирование (обрезку) как способ получить некоторый полезный прирост данных, что приводит к лучшему обобщению и требует меньшего количества обучающих данных. Когда мы используем случайную обрезку, fastai автоматически использует центральную обрезку для набора проверки-то есть он выберет самую большую квадратную область в центре изображения, не выходя за края изображения. . Это часто может быть проблематично. Например, в наборе данных с несколькими метками иногда есть небольшие объекты по краям изображения; они могут быть полностью обрезаны путем обрезки по центру. Даже для таких проблем, как наш пример классификации пород домашних животных, возможно, что некоторые критические признаки, необходимые для идентификации правильной породы, такие как цвет носа, могут быть обрезаны. . Одно из решений этой проблемы состоит в том, чтобы полностью избежать случайной обрезки. Вместо этого мы могли бы просто сжать или растянуть прямоугольные изображения, чтобы они поместились в квадратное пространство. Но тогда мы упускаем очень полезное увеличение данных, а также усложняем распознавание изображений для нашей модели, потому что она должна научиться распознавать сдавленные и сжатые изображения, а не просто правильно пропорциональные изображения. . Другое решение состоит в том, чтобы не просто центрировать кадрирование для проверки, а вместо этого выбрать несколько областей для кадрирования из исходного прямоугольного изображения, пропустить каждую из них через нашу модель и взять максимум или среднее значение прогнозов. Фактически, мы могли бы сделать это не только для разных кадрирований, но и для разных значений всех наших параметров приращением во время тестирования. Это известно как приращение во время тестирования (Test Time Augmentation - TTA). . жаргон:приращение во время тестирования( TTA): во время вывода или проверки создается несколько версий каждого изображения, используя приращение данных, а затем берется среднее или максимальное значение прогнозов для каждой расширенной версии изображения. В зависимости от набора данных приращение во время тестирования может привести к значительному повышению точности. Это не изменит время, необходимое для обучения в общем, но увеличит количество времени, необходимого для проверки или вывода на количество запрошенных изображений, использованных для приращений во время тестирования. По умолчанию, fastai будет пользоваться нерасширенным центрально кадрированным изображением плюс четырьмя случайным образом выбранными дополненительными изображениями. . Вы можете передать любой DataLoader в fastai TTA метод; по умолчанию он будет использовать ваш набор проверки: . preds,targs = learn.tta() accuracy(preds, targs).item() . . 0.8607169389724731 . Как мы видим, использование TTA дает нам хороший прирост результативности без необходимости дополнительной тренировки. Однако это делает вывод медленнее—если вы усредняете пять изображений для TTA, вывод будет в пять раз медленнее. . Мы увидели примеры того, как прирост данных помогает обучать гораздо лучшие модели. Теперь давайте сосредоточимся на новом методе увеличения данных, который называется Mixup. . Mixup . Mixup, представленный в статье 2017 года&quot; mixup: Beyond Empirical Risk Minimization &quot; автора Hongyi Zhang et al., является очень мощным методом прироста данных, который может обеспечить значительно более высокую точность, особенно когда у вас мало данных и нет предварительно подготовленной модели, которая была обучена на данных, подобных вашему набору данных. В статье поясняется: &quot;в то время как прирост данных последовательно приводит к улучшению обобщения, процедура зависит от набора данных и, следовательно, требует использования экспертных знаний.&quot; Например, принято переворачивать изображения как часть увеличения данных, но следует ли переворачивать только горизонтально или также вертикально? Ответ заключается в том, что это зависит от вашего набора данных. Кроме того, если переворачивание (например) не обеспечивает достаточного прироста данных для вас, вы не можете &quot;перевернуть больше.&quot; Полезно иметь методы прироста данных, где вы можете &quot;набрать &quot; или&quot; сбавить &quot; количество изменений, чтобы увидеть, что лучше всего работает для вас. . Mixup работает следующим образом, для каждого изображения: . Выберает другое изображение из набора данных наугад. | Подбирает вес наугад. | Возьмет средневзвешенное значение (используя вес из шага 2) выбранного изображения с вашим изображением; это будет ваша независимая переменная. | Возьмет средневзвешенное значение (с тем же весом) меток этого изображения с метками вашего изображения; это будет ваша зависимая переменная. | Псевдокод этого(где t-вес нашего средневзвешенного значения): . image2,target2 = dataset[randint(0,len(dataset)] t = random_float(0.5,1.0) new_image = t * image1 + (1-t) * image2 new_target = t * target1 + (1-t) * target2 . Чтобы это сработало, наши целевые значения должны быть закодированы one-hot encoded. В статье это описывается с помощью уравнений, приведенных в . . &#1057;&#1090;&#1072;&#1090;&#1100;&#1080; &#1080; &#1084;&#1072;&#1090;&#1077;&#1084;&#1072;&#1090;&#1080;&#1082;&#1072; . С этого момента мы будем рассматривать все больше и больше исследовательских работ. Теперь, когда у вас есть основной жаргон, вы можете быть удивлены, обнаружив, как много из них вы можете понять, немного попрактиковавшись! Одна из проблем, которую вы заметите, заключается в том, что греческие буквы , такие как lambda, появляются в большинстве статей. Есть очень хорошая идея - выучить названия всех греческих букв, так как в противном случае очень трудно читать статьи для себя и запоминать их (или читать код на их основе, так как код часто использует названия греческих букв, написанных по буквам, например lambda). . Большая проблема со статьями заключается в том, что в них используют математику, а не код, чтобы объяснить, что происходит. Если вы не очень хорошо разбираетесь в математике, то поначалу это будет пугать и сбивать с толку. Но помните: то, что показано в математике, - это то, что будет реализовано в коде. Это просто другой способ говорить об одном и том же! Прочитав несколько статей,вы станете понимать все больше и больше символов. Если вы не знаете какой-то символ, попробуйте найти его в списке математических символов Википедии или нарисовать его в Detexify, который (используя машинное обучение!) найдет название вашего нарисованного от руки символа. Затем вы можете поискать это имя в интернете, чтобы узнать, для чего оно предназначено. . church = PILImage.create(get_image_files_sorted(path/&#39;train&#39;/&#39;n03028079&#39;)[0]) gas = PILImage.create(get_image_files_sorted(path/&#39;train&#39;/&#39;n03425413&#39;)[0]) church = church.resize((256,256)) gas = gas.resize((256,256)) tchurch = tensor(church).float() / 255. tgas = tensor(gas).float() / 255. _,axs = plt.subplots(1, 3, figsize=(12,4)) show_image(tchurch, ax=axs[0]); show_image(tgas, ax=axs[1]); show_image((0.3*tchurch + 0.7*tgas), ax=axs[2]); . Третье изображение строится путем сложения 0,3 раза первого умноженного на 0,3 и второго умноженного на 0,7. В этом примере должна ли модель предсказывать &quot;церковь&quot;или&quot; заправочную станцию&quot;? Правильный ответ-30% церкви и 70% бензоколонки, так как именно это мы получим, если возьмем линейную комбинацию OHE(one-hot-encoded) целей . Например, предположим, что у нас есть 10 классов, и &quot;церковь&quot; представлена индексом 2, а &quot;заправочная станция&quot; представлена индексом 7, тогда их OHE значение: . [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] . Итак, наша конечный результат: . [0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0] . Все это делается для нас внутри fastai, добавляя обратный вызов нашему Learner.Callbacks - это то, что происходит внутри fastai для внедрения пользовательского поведения в цикл обучения (например, график скорости обучения или обучение со смешанной точностью). Мы узнаем все о обратных вызовах, в том числе о том, как сделать свой собственный, дальше. . Вот как мы обучаем модель с помощью Mixup: . model = xresnet50() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixUp) learn.fit_one_cycle(5, 3e-3) . Что происходит, когда мы обучаем модель с данными, которые &quot;перемешаны&quot; таким образом? Очевидно, что тренироваться будет сложнее, потому что труднее увидеть, что находится в каждом изображении. И модель должна предсказать две метки на изображение, а не только одну, а также выяснить, насколько каждая из них взвешена. Однако переобучение кажется менее вероятной проблемой, потому что мы не показываем одно и то же изображение в каждую эпоху, а вместо этого показываем случайную комбинацию двух изображений. . Mixup требует гораздо больше эпох для тренировки, чтобы получить лучшую точность, по сравнению с другими подходами к увеличению, которые мы видели. Вы можете попробовать обучить Imagenette с помощью Mixup и без него, используя examples/train_imagenette.py скрипт в репо fastai. На момент написания статьи таблица лидеров в репо Imagenette показывает, что Mixup используется для всех лидирующих результатов для тренировок &gt;80 эпох, а для меньшего количества эпох Mixup не используется. Это также согласуется с нашим опытом использования Mixup. . Одна из причин того, что Mixup настолько увлекателен, заключается в том, что он может быть применен к другим типам данных, кроме фотографий. На самом деле, некоторые люди даже показали хорошие результаты, используя Mixup на активациях внутри своих моделей, а не только на входах—это позволяет использовать Mixup для NLP и других типов данных. . Есть еще одна тонкая проблема, с которой Mixup справляется для нас, а именно то, что на самом деле невозможно с моделями, которые мы видели раньше, чтобы наша потеря когда-либо была абсолютной. Проблема в том, что наши метки равны 1 и 0, но выходы softmax и sigmoid никогда не могут равняться 1 или 0. Это означает, что обучение нашей модели подталкивает наши активации все ближе к этим значениям, так что чем больше эпох мы делаем, тем более экстремальными становятся наши активации. . С Mixup у нас больше нет этой проблемы, потому что наши метки будут точно равны 1 или 0, если мы случайно &quot;смешаемся&quot; с другим изображением того же класса. В остальное время наши метки будут представлять собой линейную комбинацию, такую как 0,7 и 0,3, которые мы получили в примере с церковью и бензоколонкой ранее. . Однако одна из проблем заключается в том, что Mixup &quot;случайно&quot; делает метки больше 0 или меньше 1. То есть мы не говорим явно нашей модели, что хотим изменить цели таким образом. Таким образом, если мы хотим сделать метки ближе или дальше от 0 и 1, мы должны изменить количество в Mixup — что также изменяет количество приращения данных, что может быть не тем, что мы хотим. Однако есть способ справиться с этим более непосредственно, а именно использовать сглаживание меток. . &#1057;&#1075;&#1083;&#1072;&#1078;&#1080;&#1074;&#1072;&#1085;&#1080;&#1077; &#1084;&#1077;&#1090;&#1086;&#1082; . Модель обучена возвращать 0 для всех категорий, кроме одной, для которой она обучена возвращать 1. Это поощряет переобучение и дает вам во время вывода модель, которая не даст значимых вероятностей: она всегда будет говорить 1 для предсказанной категории, даже если она не слишком уверена, просто потому, что она была обучена таким образом. . Это может стать очень вредным, если ваши данные не будут точно размечены. . Вместо этого мы могли бы заменить все наши 1 на число немного меньше 1, а наши 0 на число немного больше 0, а затем обучаться. Это называется сглаживанием меток. Поощряя вашу модель быть менее уверенной, сглаживание меток сделает ваше обучение более надежным, даже если есть неверно помеченные данные. Результатом будет модель, которая лучше делает выводы. .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2021/01/17/07-sizing-and-tta.html",
            "relUrl": "/russian/fast.ai/solution/2021/01/17/07-sizing-and-tta.html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Russian - Solution Lesson 6 on Fast.ai",
            "content": "&#1044;&#1088;&#1091;&#1075;&#1080;&#1077; &#1055;&#1088;&#1086;&#1073;&#1083;&#1077;&#1084;&#1099; &#1057; &#1050;&#1086;&#1084;&#1087;&#1100;&#1102;&#1090;&#1077;&#1088;&#1085;&#1099;&#1084; &#1047;&#1088;&#1077;&#1085;&#1080;&#1077;&#1084; . В предыдущей главе вы познакомились с некоторыми важными практическими приемами обучения моделей на практике. Такие соображения, как выбор скорости обучения и количества эпох, очень важны для получения хороших результатов. . В этой главе мы рассмотрим два других типа проблем компьютерного зрения:многозначную классификацию и регрессию. Первый - это когда вы хотите предсказать более одной метки (а иногда и вовсе ни одной), а второй—когда ваши метки представляют собой одно или несколько чисел (количество, а не категорию). . В процессе будет более глубоко изучаться выходная активация, цели и функции потерь в моделях глубокого обучения. . &#1050;&#1083;&#1072;&#1089;&#1089;&#1080;&#1092;&#1080;&#1082;&#1072;&#1094;&#1080;&#1103; &#1057; &#1053;&#1077;&#1089;&#1082;&#1086;&#1083;&#1100;&#1082;&#1080;&#1084;&#1080; &#1052;&#1077;&#1090;&#1082;&#1072;&#1084;&#1080; . Классификация с несколькими метками относится к проблеме идентификации категорий объектов на изображениях. В классах, которые вы ищете, может быть несколько типов объектов, а может и вовсе не быть объектов. . На практике мы не видели много примеров, когда люди обучали мультиметочные классификаторы для этой цели, но мы очень часто видим, как пользователи и разработчики жалуются на эту проблему. Похоже, что это простое решение совсем не широко понимается и не ценится! Поскольку на практике, вероятно, чаще встречаются некоторые изображения с нулевым совпадением или более чем одним совпадением, мы, вероятно, должны ожидать на практике, что классификаторы с несколькими метками более широко применимы, чем классификаторы с одной меткой. . Сначала давайте посмотрим, как выглядит набор данных с несколькими метками, а затем объясним, как подготовить его для нашей модели. Вы увидите, что архитектура модели не изменилась по сравнению с предыдущей главой; изменилась только функция потерь. Начнем с данных. . &#1044;&#1072;&#1085;&#1085;&#1099;&#1077; . В нашем примере мы будем использовать набор данных PASCAL, который может содержать более одного вида классифицируемых объектов на изображение. . Мы начинаем с загрузки и извлечения набора данных как обычно: . from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) . Этот набор данных отличается от тех, которые мы видели раньше, тем, что они не структурирован по имени файла или папке, а вместо этого предоставляются CSV-файлом (значения, разделенные запятыми), сообщающим нам, какие метки использовать для каждого изображения. Мы можем проверить CSV-файл, прочитав его в DataFrame Pandas: . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . Как вы можете видеть, список категорий на каждом изображении отображается в виде строки, разделенной пробелами. . Sidebar: Pandas &#1080; DataFrames . Нет, на самом деле это не панда! Pandas-это библиотека Python, которая используется для обработки и анализа табличных данных и данных временных рядов. Основным классом является DataFrame, который представляет собой таблицу состоящую из строк и столбцов. Вы можете получить DataFrame из CSV-файла, таблицы базы данных, словарей Python и многих других источников. В Jupiter DataFrame выводится в виде форматированной таблицы, как показано здесь. . Вы можете получить доступ к строкам и столбцам DataFrame с помощью свойства iloc, как если бы это была матрица: . df.iloc[:,0] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . df.iloc[0,:] # Trailing :s are always optional (in numpy, pytorch, pandas, etc.), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . Вы также можете получить столбец по имени, непосредственно индексируя DataFrame: . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . tmp_df = pd.DataFrame({&#39;a&#39;:[1,2], &#39;b&#39;:[3,4]}) tmp_df . a b . 0 1 | 3 | . 1 2 | 4 | . tmp_df[&#39;c&#39;] = tmp_df[&#39;a&#39;]+tmp_df[&#39;b&#39;] tmp_df . a b c . 0 1 | 3 | 4 | . 1 2 | 4 | 6 | . &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; DataBlock . Как мы преобразуем объект DataFrame в объект DataLoaders? Обычно мы предлагаем использовать API блока данных для создания объекта DataLoaders, где это возможно, поскольку он обеспечивает хорошее сочетание гибкости и простоты. Здесь мы покажем вам шаги, которые мы предпринимаем, чтобы использовать API блоков данных для построения объекта DataLoaders на практике, используя этот набор данных в качестве примера. . Как мы уже видели, PyTorch и fastai имеют два основных класса для представления и доступа к обучающему набору или набору проверки: . Dataset:: Коллекция, которая возвращает кортеж вашей независимой и зависимой переменной для одного элемента | DataLoader:: Итератор, который обеспечивает поток мини-пакетов, где каждый мини-пакет является кортежем пакета независимых переменных и пакета зависимых переменных | . Кроме того, fastai предоставляет два класса для объединения ваших обучающих и проверочных наборов: . Datasets:: Объект, содержащий обучающий набор данных и проверочный набор данных | DataLoaders:: Объект, содержащий обучающий загрузчик данных и загрузчик данных проверки | . Поскольку DataLoader строится поверх Dataset и добавляет к нему дополнительные функции (сортировка нескольких элементов в мини-пакет), часто проще всего начать с создания и тестирования Datasets, а затем посмотреть на DataLoaders после того, как они заработают. . Когда мы создаем DataBlock, мы создаем его постепенно, шаг за шагом, и используем блокнот для проверки ваших данных. Это отличный способ убедиться, что вы следите за любыми проблемами. Легко отлаживать, потому что вы знаете, что если возникает проблема, то она находится в строке кода, который вы только что набрали! . Начнем с простейшего случая, который представляет собой блок данных, созданный без параметров: . dblock = DataBlock() . Из этого мы можем создать объект Datasets. Единственное, что нужно, - это источник-в данном случае наш DataFrame: . dsets = dblock.datasets(df) . Он содержит тренировочный и проверочный набор данных, который мы можем индексировать: . len(dsets.train),len(dsets.valid) . (4009, 1002) . x,y = dsets.train[0] x,y . (fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object, fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object) . Как вы можете видеть, возвращается строка DataFrame дважды. Это происходит потому, что по умолчанию блок данных предполагает, что у нас есть две вещи: входные данные и целевые значения. Нам нужно будет получить соответствующие поля из DataFrame, что мы можем сделать, передав функции get_x и get_y: . x[&#39;fname&#39;] . &#39;008663.jpg&#39; . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] . (&#39;005620.jpg&#39;, &#39;aeroplane&#39;) . Как вы можете видеть, вместо того, чтобы определять функцию обычным способом, мы используем ключевое слово Python lambda. Это просто кратчайший путь для определения и последующей ссылки на функцию. Следующий более подробный подход идентичен: . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;002549.jpg&#39;, &#39;tvmonitor&#39;) . Лямбда-функции отлично подходят для быстрой итерации, но они несовместимы с сериализацией, поэтому мы советуем вам использовать более подробный подход, если вы хотите экспортировать своего Learner после обучения (лямбды хороши, если вы просто экспериментируете). . Мы видим, что независимая переменная должна быть преобразована в полный путь, чтобы мы могли открыть ее как изображение, а зависимая переменная должна быть разделена символом пробела( который используется по умолчанию для функции разделения Python), чтобы она стала списком: . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (Path(&#39;/storage/data/pascal_2007/train/002844.jpg&#39;), [&#39;train&#39;]) . Чтобы действительно открыть изображение и выполнить преобразование его в тензоры, нам нужно будет использовать набор преобразований, которые нам предоставляют. Мы можем использовать те же типы блоков, что и раньше, за одним исключением: ImageBlock снова будет работать нормально, потому что у нас есть путь, который указывает на допустимое изображение, но CategoryBlock не будет работать. Проблема в том, что блок возвращает одно целое число, но нам нужно иметь возможность иметь несколько меток для каждого элемента. Чтобы решить эту проблему, мы используем MultiCategoryBlock. Этот тип блока ожидает получить список строк, как и в данном случае, поэтому давайте проверим его: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x375, TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])) . Как вы можете видеть, наш список категорий не закодирован таким же образом, как это было для обычного блока категорий. В том случае у нас было одно целое число, представляющее, какая это категория, основываясь на ее местоположении в нашем vocab. В этом случае, однако, у нас вместо этого есть список нулей с единицей. Например, если есть единица во второй и четвертой позициях, то это означает, что элементы vocab под номером два и четыре присутствуют на этом изображении. Это известно как one-hot encoding. Причина, по которой мы не можем просто использовать список индексов категорий, заключается в том, что каждый список будет иметь разную длину, а PyTorch требует тензоров одинаковой длины. . Давайте проверим, что представляют собой категории для этого примера (мы используем удобную функцию torch.where, которая возращает нам все индексы, где наше условие истинно или ложно): . idxs = torch.where(dsets.train[0][1]==1.)[0] dsets.train.vocab[idxs] . (#1) [&#39;dog&#39;] . С массивами NumPy, тензорами PyTorch и классом L fastai мы можем индексировать непосредственно с помощью списка или вектора, что делает большой код (например, этот пример) гораздо более ясным и кратким. . До сих пор мы игнорировали столбец is_valid, что означает, что блок данных по умолчанию использует случайное разделение. Чтобы явно выбрать элементы нашего набора проверки, нам нужно написать функцию и передать ее splitter (или использовать одну из предопределенных функций или классов fastai). Он будет принимать элементы (здесь весь наш фрейм данных) и должен возвращать два (или более) списка целых чисел: . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . Как мы уже обсуждали, DataLoader собирает элементы из Dataset в мини-пакет. Это кортеж тензоров, где каждый тензор просто куча элементов из местоположений в элементе Dataset. . Теперь, когда мы убедились, что отдельные элементы выглядят нормально, нам нужно сделать еще один шаг, чтобы убедиться, что мы можем создать наши DataLoaders, а именно убедиться, что каждый элемент имеет одинаковый размер. Для этого мы можем использовать RandomResizedCrop: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(df) . И теперь мы можем показать элемент наших данных: . dls.show_batch(nrows=1, ncols=3) . Помните, что если что-то пойдет не так при создании DataLoaders из вашего DataBlock или если вы хотите точно увидеть, что происходит с вашим DataBlock, вы можете использовать метод краткого изложения, который мы представили в предыдущей главе. . Наши данные теперь готовы для обучения модели. Как мы увидим, ничего не изменится, когда мы создадим нашего Learner, но за кулисами fastai библиотека выберет для нас новую функцию потерь: двоичную кросс-энтропию. . &#1041;&#1080;&#1085;&#1072;&#1088;&#1085;&#1072;&#1103; &#1082;&#1088;&#1086;&#1089;&#1089;-&#1101;&#1085;&#1090;&#1088;&#1086;&#1087;&#1080;&#1103; . Теперь создадим нашего Learner. . learn = cnn_learner(dls, resnet18) . Мы также знаем, что модель в Learner, как правило, является объектом класса, наследуемого от nn.Module, и что мы можем вызвать его с помощью скобок, и он будет возвращать активации модели. Вы должны передать ему свою независимую переменную, как мини-пакет. Мы можем попробовать его, взяв мини-пакет из нашего загрузчика данных, а затем передав его модели: . x,y = to_cpu(dls.train.one_batch()) activs = learn.model(x) activs.shape . torch.Size([64, 20]) . Подумайте о том, почему activs имеет такую форму—у нас есть размер пакета 64, и нам нужно рассчитать вероятность каждой из 20 категорий. Вот как выглядит одна из этих активаций: . activs[0] . tensor([ 0.7476, -1.1988, 4.5421, -1.5915, -0.6749, 0.0343, -2.4930, -0.8330, -0.3817, -1.4876, -0.1683, 2.1547, -3.4151, -1.1743, 0.1530, -1.6801, -2.3067, 0.7063, -1.3358, -0.3715], grad_fn=&lt;SelectBackward&gt;) . Данные еще не масштабированы до 0 и 1 . def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return -torch.where(targets==1, inputs, 1-inputs).log().mean() . Обратите внимание, что поскольку у нас есть one-hot-encoded зависимая переменная, мы не можем напрямую использовать nll_loss или softmax (и поэтому мы не можем использовать cross_entropy): . softmax, как мы знаем, требует, чтобы сумма всех предсказаний равнялась 1 и за счет использования exp значительно выделяет одну активацию из других. Однако у нас вполне может быть несколько объектов, которые имеются на изображении, поэтому ограничение максимальной суммы активаций до 1 не является хорошей идеей. Исходя из того же рассуждения, мы также можем хотеть, чтобы сумма была меньше 1, если мы не уверены что какая то категория появиться на изображении. | nll_loss, как мы знаем, возвращает значение только одной активации: единственной активации, соответствующей одной метке для элемента. Это не имеет смысла, когда у нас есть несколько категорий. | . С другой стороны, функция binary_cross_entropy, которая является просто mnist_loss вместе с log, обеспечивает именно то, что нам нужно, благодаря магии элементарных операций PyTorch. Каждая активация будет сравниваться с каждой целью для каждого столбца, поэтому нам не нужно ничего делать, чтобы заставить эту функцию работать для нескольких столбцов. . Python уже предоставляет нам эту функцию. На самом деле, он предоставляет ряд версий, с довольно запутанными названиями! . F.binary_cross_entropy и его модульный эквивалент nn.BCELoss вычисляют кросс-энтропию для одной one-hot-encoded цели, но не включают начальную сигмоиду. Обычно для one-hot-encoded целей вы захотите F.binary_cross_entropy_with_logits (или НН.BCEWithLogitsLoss), которые выполняют как сигмоидную, так и двоичную кросс-энтропию в одной функции, как в предыдущем примере. . Эквивалентом для наборов данных с одной меткой (например, MNIST или Pet dataset), где цель кодируется как одно целое число, является F.nll_loss или nn.NLLLoss для версии без начального softmax и F.cross_entropy или nn.CrossEntropyLoss для версии с начальным softmax. . Поскольку у нас есть цель с one-hot-encoded кодированием, мы будем использовать BCEWithLogitsLoss . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . TensorMultiCategory(1.0342, grad_fn=&lt;AliasBackward&gt;) . На самом деле нам не нужно указывать fastai, что необходимо использовать эту функцию потерь (хотя мы можем, если захотим), так как она установлена автоматически. fastai знает, что DataLoaders имеют многокатегориальные метки, поэтому по умолчанию он будет использовать nn.BCEWithLogitsLoss. . Одно изменение по сравнению с предыдущей главой-это метрика, которую мы используем: поскольку это проблема с несколькими метками, мы не можем использовать функцию точности. Почему это? Ну, точность сравнивала наши результаты с нашими целями следующим образом: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . Предсказанный класс был тем, у кого была самая высокая активация (это то, что делает argmax). Здесь это не работает, потому что у нас может быть более одного предсказания на одном изображении. После применения сигмоиды к нашим активациям (чтобы сопоставить их между 0 и 1), нам нужно решить, какие из них являются 0, а какие 1 за счет порога. Каждое значение выше порога будет рассматриваться как 1, а каждое значение ниже порога будет считаться 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . Если мы передадим accuracy_multi непосредственно в качестве метрики, она будет использовать значение по умолчанию для порога, которое равно 0,5. Возможно мы захотим изменить это значение по умолчанию и создать новую версию accuracy_multi, которая имеет другое значение по умолчанию. Чтобы помочь в этом, в Python есть функция, называемая partial. Это позволяет нам связать функцию с некоторыми аргументами, создавая новую версию этой функции, которая, когда бы она ни вызывалась, всегда включает эти аргументы. Например, вот простая функция, принимающая два аргумента: . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . Мы можем переключиться на французскую версию этой функции с помощью partial: . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . Теперь мы можем тренировать нашу модель. Давайте попробуем установить порог точности для нашей метрики равным 0,2: . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.940148 | 0.697462 | 0.236952 | 00:12 | . 1 | 0.822964 | 0.557433 | 0.286056 | 00:12 | . 2 | 0.602061 | 0.198278 | 0.833446 | 00:12 | . 3 | 0.358782 | 0.123523 | 0.944383 | 00:12 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.130395 | 0.112252 | 0.949342 | 00:14 | . 1 | 0.116924 | 0.105566 | 0.952171 | 00:14 | . 2 | 0.097506 | 0.101672 | 0.951912 | 00:14 | . Выбор порога очень важен. Если вы выберете слишком низкий порог, вы часто не сможете выбрать правильно объекты. Мы можем увидеть это, изменив нашу метрику, а затем вызвав validate, который возвращает ошибку проверки и метрики: . learn.metrics = partial(accuracy_multi, thresh=0.1) learn.validate() . (#2) [0.10167204588651657,0.9306574463844299] . Если вы выберете слишком высокий порог, вы будете получать только те объекты, для которых ваша модель очень уверена: . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.10167204588651657,0.9425497055053711] . Мы можем найти лучший порог, попробовав несколько уровней и увидев, что работает лучше всего. Это намного быстрее, если мы просто возьмем предсказания один раз: . preds,targs = learn.get_preds() . Тогда мы можем вызвать метрику напрямую. Обратите внимание, что по умолчанию get_press применяет для нас результирующую функцию активации (в данном случае сигмоида), поэтому нам нужно будет сказать accuracy_multi, чтобы он ее не применял: . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorMultiCategory(0.9574) . Теперь мы можем использовать этот подход для поиска наилучшего порогового уровня: . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . В этом случае мы используем проверочный набор для выбора гиперпараметра (порога), который является его задачей. Иногда студенты выражают озабоченность тем, что мы, возможно, можем переобучить проверочный набор, поскольку мы пробуем множество значений, чтобы увидеть, какое из них является лучшим. Однако, как вы видите на графике, изменение порога в этом случае приводит к плавной кривой, поэтому мы явно не выбираем какой-то неуместный выброс. Это хороший пример того, где вы должны быть осторожны с различием между теорией (не пытайтесь использовать много значений гиперпараметров, иначе вы можете переобучить) и практикой (если связь гладкая, то это нормально). . На этом заканчивается часть этой главы, посвященная классификации с несколькими метками. Далее мы рассмотрим проблему регрессии. . &#1056;&#1077;&#1075;&#1088;&#1077;&#1089;&#1089;&#1080;&#1103; . Легко представить себе, что модели глубокого обучения делятся на такие области, как компьютерное зрение, НЛП и так далее. И действительно, именно так fastai классифицирует свои приложения-в основном потому, что именно так большинство людей привыкли думать о вещах. . Но на самом деле за этим скрывается более интересная и глубокая перспектива. Модель определяется ее независимыми и зависимыми переменными, а также функцией потерь. Это означает, что на самом деле существует гораздо более широкий спектр моделей, чем просто разделение на основе области. Возможно, у нас есть независимая переменная, которая является изображением, и зависимая, которая является текстом (например, генерируя подпись из изображения); или, возможно, у нас есть независимая переменная, которая является текстом, и зависимая, которая является изображением (например, генерируя изображение из подписи—что на самом деле возможно для глубокого обучения!); или, возможно, у нас есть изображения, тексты и табличные данные в качестве независимых переменных, и мы пытаемся предсказать покупку продукта... возможности действительно безграничны. . Возможность выйти за рамки фиксированных приложений, чтобы создавать свои собственные новые решения для новых проблем, помогает понять API блоков данных (и, возможно, также API среднего уровня, который мы увидим позже в этой книге). В качестве примера рассмотрим проблему регрессии изображений. Это относится к обучению на основе набора данных, где независимая переменная представляет собой изображение, а зависимая переменная-одно или несколько чисел типа float. Часто мы видим, что люди относятся к регрессии изображений как к целому отдельному приложению, но, как вы увидите здесь, мы можем рассматривать его как просто еще один CNN поверх API блока данных. . Мы сразу перейдем к довольно сложному варианту регрессии изображений, потому что мы знаем, что вы к этому готовы! Мы собираемся сделать модель ключевой точки. Ключевой момент относится к определенному месту, представленному на изображении—в этом случае мы будем использовать изображения людей и будем искать центр лица человека на каждом изображении. Это означает, что мы фактически будем предсказывать два значения для каждого изображения: строку и столбец центра лица. . Assemble the Data . Мы будем использовать Biwi Kinect Head Pose dataset для этого раздела. Начнем с загрузки набора данных как обычно . path = untar_data(URLs.BIWI_HEAD_POSE) . Давайте посмотрим, что у нас есть! . path.ls().sorted() . (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...] . Есть 24 каталога, пронумерованные от 01 до 24 (они соответствуют разным сфотографированным людям), и соответствующий файл .obj для каждого (они нам здесь не понадобятся). Давайте заглянем в один из этих каталогов: . (path/&#39;01&#39;).ls().sorted() . (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...] . Внутри подкаталогов у нас есть разные фреймы, каждый из которых поставляется с изображением (_rgb.jpg) и файлом позы (_pose.txt). Мы можем легко получить все файлы изображений рекурсивно с помощью get_image_files, а затем написать функцию, которая преобразует имя файла изображения в связанный с ним файл позы: . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;16/frame_00182_pose.txt&#39;) . Давайте взглянем на наше первое изображение: . im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . Веб-сайт набора данных Biwi используется для объяснения формата связи текстового файла post с каждым изображением, который показывает расположение центра головы. Детали этого не важны для наших целей, поэтому мы просто покажем функцию, которую мы используем для извлечения центральной точки головы: . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . Мы можем передать эту функцию DataBlock как get_y, так как она отвечает за маркировку каждого элемента. Мы изменим размер изображений до половины их входного размера, просто чтобы немного ускорить обучение. . Один важный момент, который следует отметить, заключается в том, что мы не должны просто использовать случайный сплиттер. Причина этого заключается в том, что одни и те же люди появляются на нескольких изображениях в этом наборе данных, но мы хотим убедиться, что наша модель может обобщать людей, которых она еще не видела. Каждая папка в наборе данных содержит изображения для одного человека. Поэтому мы можем создать функцию splitter, которая возвращает true только для одного человека, в результате чего набор проверки содержит только изображения этого человека. . Единственное другое отличие от предыдущих примеров блоков данных состоит в том, что второй блок является точечным блоком. Это необходимо для того, чтобы fastai знал, что метки представляют координаты; таким образом, он знает, что при увеличении данных он должен делать то же самое увеличение для координат, что и для изображений: . get_ctr(img_files[0]) . tensor([324.0023, 251.5637]) . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . Прежде чем делать какое-либо моделирование, мы должны посмотреть на наши данные, чтобы убедиться, что все в порядке: . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . Это выглядит хорошо! Помимо визуального просмотра пакета, неплохо также посмотреть на лежащие в его основе тензоры (особенно если вы студент; это поможет прояснить ваше понимание того, что на самом деле видит ваша модель): . xb,yb = dls.one_batch() xb.shape,yb.shape . ((64, 3, 240, 320), (64, 1, 2)) . Убедитесь, что вы понимаете, почему именно эти формы используются для наших мини-партий. . Вот пример одной строки из зависимой переменной: . yb[0] . TensorPoint([[-0.2811, -0.0472]], device=&#39;cuda:0&#39;) . Как вы можете видеть, нам не нужно было использовать отдельное приложение регрессии изображений; все, что нам нужно было сделать, это пометить данные и сказать fastai, какие типы данных представляют независимые и зависимые переменные. . То же самое относится и к созданию нашего Learner. Мы будем использовать ту же функцию, что и раньше, с одним новым параметром,и будем готовы обучить нашу модель. . &#1054;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . Как обычно, мы можем использовать cnn_learner для создания нашего ученика. . learn = cnn_learner(dls, resnet18, y_range=(-1,1)) . y_range реализуется в fastai с помощью sigmoid_range, который определяется как: . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . Он устанавливается в качестве конечного слоя модели, если определен y_range. Подумайте о том, что делает эта функция и почему она заставляет модель выводить активации в диапазоне (lo,hi). . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . Мы не указали функцию потерь и это означает fastai выбирает по умолчанию. Давайте посмотрим, что он выбрал для нас: . dls.loss_func . FlattenedLoss of MSELoss() . Это имеет смысл, поскольку, когда координаты используются в качестве зависимой переменной, большую часть времени мы, вероятно, пытаемся предсказать что-то как можно более близкое; это в основном то, что делает MSELoss (mean squared error loss). Если вы хотите использовать другую функцию потерь, вы можете передать ее cnn_learner с помощью параметра loss_func. . Обратите внимание также, что мы не указали никаких метрик. Это потому, что MSE уже является необходимой метрикой для этой задачи (хотя она, вероятно, более интерпретируема после того, как мы возьмем квадратный корень). . Мы можем выбрать лучшую скорость обучения с помощью искателя скорости обучения: . learn.lr_find() . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.03981071710586548) . Мы попробуем LR 1e-2: . lr = 1e-2 learn.fine_tune(3, lr) . epoch train_loss valid_loss time . 0 | 0.050240 | 0.016628 | 01:12 | . epoch train_loss valid_loss time . 0 | 0.007083 | 0.002008 | 01:30 | . 1 | 0.002808 | 0.000214 | 01:31 | . 2 | 0.001404 | 0.000119 | 01:31 | . Обычно, когда мы запускаем это, мы получаем потерю около 0,0001, что соответствует средней ошибке предсказания координат: . math.sqrt(0.0001) . 0.01 . Это звучит очень точно! Но очень важно взглянуть на наши результаты с помощью Learner.show_results. Левая сторона-это фактические (истинные) координаты, а правая - предсказания нашей модели: . learn.show_results(ds_idx=1, nrows=3, figsize=(6,8)) . Просто удивительно, что всего за несколько минут вычислений мы создали такую точную модель ключевых точек и без какого-либо специального приложения для конкретной области. Это сила построения гибких API и использования трансфертного обучения! Особенно поразительно, что мы смогли так эффективно использовать трансферное обучение даже между совершенно разными задачами; наша предварительно обученная модель была обучена классификации изображений, и мы точно настроились на регрессию изображений. . &#1047;&#1072;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1077; . В задачах, которые на первый взгляд совершенно различны (классификация с одной меткой, классификация с несколькими метками и регрессия), мы в конечном итоге используем одну и ту же модель с разным количеством выходов. Функция потерь-это единственное, что меняется, поэтому важно дважды проверить, что вы используете правильную функцию потерь для своей проблемы. . fastai автоматически попытается выбрать правильный вариант из построенных вами данных, но если вы используете чистый PyTorch для создания своих загрузчиков данных, убедитесь, что вы хорошо проработали вопрос о том, какую функцию потерь, и помните, что вы, скорее всего, захотите: . nn.CrossEntropyLoss для классификации с одной меткой | nn.BCEWithLogitsLoss для классификации с множесством меток | nn.MSELoss для регрессии | .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2021/01/06/06-multicat.html",
            "relUrl": "/russian/fast.ai/solution/2021/01/06/06-multicat.html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Russian - Fastbook Chapter 6 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы к шестой части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, как и к осталььным частям прошу писать в коментариях - поправлю. . 1. Как множественная классификация может улучшить медвежий классификатор? . Позволило бы классифицировать отсутствие медведей. В противном случае модель множественной классификации будет предсказывать присутствие медведя, даже если его там нет (если только явно не добавлен отдельный класс). . 2. Как мы кодируем зависимую переменную в задаче классификации с несколькими метками? . Кодируется как one-hot encoded вектор. По сути, это означает, что у нас есть нулевой вектор одинаковой длины числу классов с единицой с индексом соответствующего класса. . 3. Как получить доступ к строкам и столбцам DataFrame, как если бы это была матрица? . Вы можете использовать .iloc. Например, df. iloc[10,10] выберет элемент в 10-й строке и 10-м столбце, как если бы DataFrame был матрицей. . 4. Как получить столбец по имени из DataFrame? . Это очень просто. Вы можете просто использовать указатель. Пример: df[‘column_name’] . **5. В чем разница между Dataset и DataLoader? . Dataset-это набор, который возвращает кортеж независимой и зависимой переменной для одного элемента. DataLoader - это функционально расширеный Dataset. Это итератор, который обеспечивает поток мини-пакетов, где каждый мини-пакет представляет собой пару пакетов из независимых и зависимых переменных. . 6. Что обычно содержиться в Dataset? . Обучающий и проверочный набор. . 7. Что обычно содержит DataLoader? . Тренировочный и проверочный dataloader. . **8. Что обычно лямбда делает в Python? . Лямбды-это сокращенный вариант для написания функций (написание однострочных функций). Он отлично подходит для быстрого прототипирования и итерации, но поскольку он не сериализуем, его нельзя использовать в развертывании и производстве. . 9. Каковы методы настройки того, как независимые и зависимые переменные создаются с помощью API блоков данных? . get_x and get_y . get_x используется для указания способа создания независимых переменных. | get_y используется для указания того, как маркируются точки данных | . 10. Почему softmax не является подходящей функцией активации для вывода при использовании one hot encoded целевого значения? . Softmax нужно, чтобы модель предсказывала только один класс, что может быть неверно в задаче многоклассовой классификации. В задачах классификации с несколькими метками входные данные могут иметь несколько меток или даже не иметь меток. . **11. Почему nll_loss не является подходящей функцией потерь при использовании one hot encoded целевого значения? . Опять же, nll_loss работает только тогда, когда модели нужно предсказать только один класс, чего здесь нет. . 12. В чем разница между nn.BCELoss и nn.BCEWithLogitsLoss? . nn.BCELoss не включает в себя начальную сигмоиду. Предполагается, что соответствующая функция активации (т.е. сигмовидная) уже была применена к предсказаниям. nn.BCEWithLogitsLoss с другой стороны применяет и сигмоиду и кросс-энтропию в одной функции. . 13. Почему мы не можем использовать обычную точность(accurancy) в задаче с несколькими метками? . Обычная точнасть предполагает, что конечным предсказанным моделью классом является класс с наибольшей активацией. Однако в задачах с несколькими метками может быть несколько меток. Поэтому порог для активаций должен быть установлен для выбора конечных прогнозируемых классов на основе активаций по сравнению с целевыми классами. . 14. Когда это нормально, чтобы настроить гипер-параметр в проверочном наборе? . Это нормально, когда отношение между гиперпараметром и наблюдаемой метрикой гладкая, что позволит исключить выбор некорректного выброса. . 15. Как реализован y_range в fastai? (Посмотрите, сможете ли вы реализовать его самостоятельно и протестировать без пика!) . y_range реализован с помощью sigmoid_range в fastai. . def sigmoid_range(x, lo, hi): return x.sigmoid() * (hi-lo) + lo . 16. Что такое проблема регрессии? Какую функцию потерь вы должны использовать для такой проблемы? . В регрессионной задаче зависимая переменная или метки, которые мы пытаемся предсказать, являются непрерывными значениями. Для таких задач используется функция среднеквадратичных потерь ошибок. . 17. Что вам нужно сделать, чтобы убедиться, что библиотека fastai применяет одно и то же увеличение данных к вашим входным изображениям и координатам целевой точки? . Вам нужно использовать правильный DataBlock. В данном случае это PointBlock. Этот DataBlock автоматически применяет увеличение данных к входным изображениям и координатам целевой точки. . 1. Почему мы сначала изменяем размер до большого размера на процессоре, а затем до меньшего размера на графическом процессоре? . Эта концепция известна как проклеивание (presizing). Прирост данных часто применяется к изображениям, и на самом деле это делается на графическом процессоре. Однако увеличение объема данных может привести к ухудшению качества и появлению артефактов, особенно на краях. Поэтому, чтобы свести к минимуму ухудшение качества данных, дополнения выполняются на более крупном изображении, а затем выполняется RandomResizeCrop изменение размера до требуемого размера изображения. . 2. Если вы не знакомы с регулярными выражениями, найдите учебник по регулярным выражениям и задач и выполните их. . Выполняется самостоятельно . 3. Каковы два наиболее распространенных способа предоставления данных для большинства наборов данных глубокого обучения? . Отдельные файлы, представляющие элементы данных, такие как текстовые документы или изображения. | Таблица данных, например в формате CSV, где каждая строка является элементом и может включать имена файлов, обеспечивающие связь между данными в таблице и данными в других форматах, таких как текстовые документы и изображения. | . 4. Посмотрите документацию на L и попробуйте использовать несколько новых методов. . Выполняется самостоятельно . 5. Посмотрите документацию для модуля Python pathlib и попробуйте использовать несколько методов класса Path. . Выполняется самостоятельно . 6. Приведите два примера того, как преобразования изображений могут ухудшить качество данных. . Вращение может являться причиной пустых областей в конечном изображении | Другие операции могут потребовать интерполяции, которая основана на исходных пикселях изображения и в результате более низкое качество изображения | . 7. Какой метод в fastai для просмотра данных в загрузчике данных (DataLoader)? . DataLoader.show_batch . 8. Какой метод в fastai, чтобы помочь вам отладить DataBlock? . DataBlock.summary . 9. Следует ли вам отложить обучение модели до тех пор, пока вы не очистите свои данные полностью? . Нет. Лучше всего первоначально создать базовую модель. . 10. Что за два метода объединены в кросс-энтропию в PyTorch? . Кросс энтропийные потери представляет собой комбинацию функции Softmax и отрицательной логарифмической потери правдоподобия. . 11. Каковы два свойства активаций, которые гарантирует softmax? Почему это так важно? . Выходные данные в сумме дают один и модель может предсказать только один класс. Кроме того, усиливаются небольшие изменения в выходных активациях, что полезно, поскольку это означает, что модель выберет метку с более высокой увереностью (хорошо для проблем с конкретными метками). . 12. Когда вы хотите, чтобы ваши активации не имели этих свойств? . Когда у вас есть проблемы с классификацией нескольких меток (возможно более одной метки). . 13. Вычислите столбцы exp и softmax. . ПРОПУЩЕНО . 14. Почему мы не можем использовать torch.where для создания функции потерь для наборов данных, где наша метка может иметь более двух категорий? . Потому что torch.where может выбирать только между двумя возможностями, в то время как для многоклассовой классификации у нас есть необходимость в выборе нескольких вариантов. . 15. Каково значение log (-2)? Почему? . Значение не определено. Логарифм является обратной экспоненциальной функцией, а экспоненциальная функция всегда положительна, независимо от того, какое значение передается. Таким образом, логарифм не определен для отрицательных значений. . **16. Каковы два хороших эмпирических правила для выбора скорости обучения при использовании искателя скорости обучения? . Любой из этих двух пунктов должен быть использован для скорости обучения: . на порядок меньше, чем там, где была достигнута минимальная потеря (то есть минимум, деленный на 10) | последняя точка, где потеря явно уменьшилась. | . 17. Какие два шага делает метод fine_tune? . Тренирует новую голову (со случайными весами) в течение одной эпохи | Разморозет все слои и тренирует их все для требуемого количества эпох | . 18. Как получить исходный код метода или функции в Jupyter notebook? . Использовать ?? после функции. Пример: DataBlock.summary?? . 19. Что такое дискриминационные показатели обучения? . Дискриминативные скорости обучения относятся к тренировочному трюку использования различных скоростей обучения для разных слоев модели. Это обычно используется в трансфертном обучении. Идея заключается в том, что при обучении предварительно подготовленной модели вы не хотите резко менять более ранние слои, поскольку она содержит информацию о простых объектах, таких как ребра и формы. Но более поздние слои могут быть изменены немного больше, поскольку они могут содержать информацию о чертах лица или других объектах, которые могут не иметь отношения к вашей задаче. Таким образом, более ранние слои имеют более низкую скорость обучения, а более поздние слои имеют более высокую скорость обучения. . 20. Как объект slice на языке Python интерпретируется при передаче в качестве скорости обучения в fastai? . Первое значение объекта среза - это скорость обучения для самого раннего слоя, а второе-скорость обучения для последнего слоя. Промежуточные слои будут иметь скорости обучения, которые мультипликативно равноудалены во всем этом диапазоне. . 21. Почему ранняя остановка является плохим выбором при использовании одного цикла тренировок? . Если используется ранняя остановка, обучение может не успеть достичь более низких значений скорости на графике, что могло бы способствовать совершенствованию модели. Поэтому рекомендуется переучивать модель и выбирать количество эпох исходя из того, где были найдены предыдущие лучшие результаты. . 22. В чем разница между resnet 50 и resnet101? . Числа 50 и 101 относятся к числу слоев в моделях. Таким образом, ResNet101-это более крупная модель с большим количеством слоев по сравнению с ResNet50. Эти варианты моделей обычно используются, поскольку существуют модели с предварительно подготовленными весами ImageNet. . 23. Что делает to_fp16? . Это позволяет проводить обучение со смешанной точностью, в котором для ускорения обучения используются менее точные числа. .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2021/01/04/fastai-chapter6-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2021/01/04/fastai-chapter6-solution.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Russian - Solution Lesson 5 on Fast.ai",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() . &#1050;&#1083;&#1072;&#1089;&#1089;&#1080;&#1092;&#1080;&#1082;&#1072;&#1094;&#1080;&#1103; &#1080;&#1079;&#1086;&#1073;&#1088;&#1072;&#1078;&#1077;&#1085;&#1080;&#1081; . &#1054;&#1090; &#1082;&#1086;&#1096;&#1077;&#1082; &#1080; &#1089;&#1086;&#1073;&#1072;&#1082; &#1082; &#1087;&#1086;&#1088;&#1086;&#1076;&#1072;&#1084; &#1078;&#1080;&#1074;&#1086;&#1090;&#1085;&#1099;&#1093; . Набор данных Pet уже загружен и необходимо получить путь к нему . from fastai.vision.all import * path = untar_data(URLs.PETS) . Чтобы увидеть, что находится в нашем наборе данных, мы можем использовать метод ls: . path.ls() . (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/american_bulldog_146.jpg&#39;),Path(&#39;images/japanese_chin_12.jpg&#39;),Path(&#39;images/Sphynx_247.jpg&#39;),Path(&#39;images/beagle_158.jpg&#39;),Path(&#39;images/beagle_76.jpg&#39;),Path(&#39;images/shiba_inu_207.jpg&#39;),Path(&#39;images/Siamese_56.jpg&#39;),Path(&#39;images/keeshond_194.jpg&#39;),Path(&#39;images/miniature_pinscher_89.jpg&#39;),Path(&#39;images/leonberger_42.jpg&#39;)...] . Чтобы мы могли протестировать наши регулярные выражения, давайте выберем одно из имен файлов: . fname = (path/&quot;images&quot;).ls()[0] . Давайте используем метод findall, чтобы попробовать регулярное выражение с именем файла объекта fname. Следующее регулярное выражение выбирает все символы до последнего символа подчеркивания если после него идут цифры и расширение файла JPEG.: . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;american_bulldog&#39;] . Для использования регулярных выражений для всего набора данных мы можем использовать класс RegexLabeller. . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . &#1048;&#1079;&#1084;&#1077;&#1085;&#1077;&#1085;&#1080;&#1077; &#1088;&#1072;&#1079;&#1084;&#1077;&#1088;&#1072; . Для реализации процесса изменения размера фактически используется Resize как преобразование элемента с большим размером, а RandomResizedCrop - как пакетное преобразование с меньшим размером. RandomResizedCrop будет использоваться, если вы включите параметр main_scale в своей функции aug_transforms, как это было сделано в вызове DataBlock в предыдущем разделе. Кроме того, вы можете использовать pad или squish вместо crop (по умолчанию) для первоначального изменения размера. . dblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_y=parent_label, item_tfms=Resize(460)) dls1 = dblock1.dataloaders([(Path.cwd()/&#39;images&#39;/&#39;grizzly.jpg&#39;)]*100, bs=8) dls1.train.get_idxs = lambda: Inf.ones x,y = dls1.valid.one_batch() _,axs = subplots(1, 2) x1 = TensorImage(x.clone()) x1 = x1.affine_coord(sz=224) x1 = x1.rotate(draw=30, p=1.) x1 = x1.zoom(draw=1.2, p=1.) x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.) tfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224), Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)]) x = Pipeline(tfms)(x) #x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode) TensorImage(x[0]).show(ctx=axs[0]) TensorImage(x1[0]).show(ctx=axs[1]); . &#1055;&#1088;&#1086;&#1074;&#1077;&#1088;&#1082;&#1072; &#1080; &#1086;&#1090;&#1083;&#1072;&#1076;&#1082;&#1072; &#1073;&#1083;&#1086;&#1082;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Вы можете вывести данные с помощью метода show_batch для проверки: . dls.show_batch(nrows=1, ncols=3) . Чтобы отладить это,мы рекомендуем вам использовать резюме.Например, одна из распространенных ошибок заключается в том, что вы забываете использовать преобразование размера, поэтому вы получаете изображения разных размеров и не можете их паковать. Вот как будет выглядеть резюме в этом случае (обратите внимание, что точный текст может измениться с момента написания, но это даст вам представление): . pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /storage/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /storage/data/oxford-iiit-pet/images/shiba_inu_98.jpg applying PILBase.create gives PILImage mode=RGB size=500x374 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /storage/data/oxford-iiit-pet/images/shiba_inu_98.jpg applying partial gives shiba_inu applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(33) Final sample: (PILImage mode=RGB size=500x374, TensorCategory(33)) Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImage mode=RGB size=500x374, TensorCategory(33)) applying ToTensor gives (TensorImage of size 3x374x500, TensorCategory(33)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Error! It&#39;s not possible to collate your items in a batch Could not collate the 0-th members of your tuples because got the following shapes torch.Size([3, 374, 500]),torch.Size([3, 375, 500]),torch.Size([3, 500, 424]),torch.Size([3, 351, 500]) . RuntimeError Traceback (most recent call last) &lt;ipython-input-12-ead0dd2a047d&gt; in &lt;module&gt; 3 splitter=RandomSplitter(seed=42), 4 get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) -&gt; 5 pets1.summary(path/&#34;images&#34;) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs) 188 why = _find_fail_collate(s) 189 print(&#34;Make sure all parts of your samples are tensors of the same size&#34; if why is None else why) --&gt; 190 raise e 191 192 if len([f for f in dls.train.after_batch.fs if f.name != &#39;noop&#39;])!=0: /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/block.py in summary(self, source, bs, show_batch, **kwargs) 182 print(&#34; nCollating items in a batch&#34;) 183 try: --&gt; 184 b = dls.train.create_batch(s) 185 b = retain_types(b, s[0] if is_listy(s) else s) 186 except Exception as e: /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/load.py in create_batch(self, b) 129 def retain(self, res, b): return retain_types(res, b[0] if is_listy(b) else b) 130 def create_item(self, s): return next(self.it) if s is None else self.dataset[s] --&gt; 131 def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b) 132 def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b) 133 def to(self, device): self.device = device /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/load.py in fa_collate(t) 46 b = t[0] 47 return (default_collate(t) if isinstance(b, _collate_types) &gt; 48 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 49 else default_collate(t)) 50 /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/load.py in &lt;listcomp&gt;(.0) 46 b = t[0] 47 return (default_collate(t) if isinstance(b, _collate_types) &gt; 48 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 49 else default_collate(t)) 50 /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/data/load.py in fa_collate(t) 45 &#34;A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s&#34; 46 b = t[0] &gt; 47 return (default_collate(t) if isinstance(b, _collate_types) 48 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 49 else default_collate(t)) /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch) 53 storage = elem.storage()._new_shared(numel) 54 out = elem.new(storage) &gt; 55 return torch.stack(batch, 0, out=out) 56 elif elem_type.__module__ == &#39;numpy&#39; and elem_type.__name__ != &#39;str_&#39; 57 and elem_type.__name__ != &#39;string_&#39;: /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs) 315 316 def __torch_function__(self, func, types, args=(), kwargs=None): --&gt; 317 with torch._C.DisableTorchFunction(): ret = _convert(func(*args, **(kwargs or {})), self.__class__) 318 if isinstance(ret, TensorBase): ret.set_meta(self, as_copy=True) 319 return ret RuntimeError: stack expects each tensor to be equal size, but got [3, 374, 500] at entry 0 and [3, 375, 500] at entry 1 . Для первоначального теста мы будем использовать простую модель: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.527914 | 0.350461 | 0.105548 | 11:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.513429 | 0.363308 | 0.108931 | 14:55 | . 1 | 0.307440 | 0.248480 | 0.075778 | 15:00 | . &#1050;&#1088;&#1086;&#1089;&#1089;-&#1069;&#1085;&#1090;&#1088;&#1086;&#1087;&#1080;&#1081;&#1085;&#1099;&#1077; &#1055;&#1086;&#1090;&#1077;&#1088;&#1080; (Cross-Entropy Loss) . &#1040;&#1082;&#1090;&#1080;&#1074;&#1072;&#1094;&#1080;&#1103; &#1080; &#1084;&#1077;&#1090;&#1082;&#1080; . Чтобы получить массив реальных данных из нашего загрузчика данных, мы можем использовать метод one_batch: . x,y = dls.one_batch() . Давайте посмотрим, что на самом деле содержится в нашей зависимой переменной: . y . TensorCategory([25, 30, 1, 17, 30, 18, 29, 9, 19, 14, 20, 14, 24, 6, 19, 17, 24, 20, 13, 4, 3, 1, 19, 1, 23, 2, 28, 32, 20, 12, 11, 1, 32, 6, 8, 35, 11, 34, 14, 33, 23, 27, 3, 11, 24, 2, 22, 7, 20, 26, 29, 28, 12, 1, 27, 27, 30, 0, 2, 13, 22, 29, 9, 13]) . Наш размер пакета равен 64, поэтому у нас есть 64 строки в этом Тензоре. Каждая строка представляет собой одно целое число от 0 до 36, представляющее 37 возможных пород домашних животных. Мы можем просматривать предсказания (то есть активации конечного слоя нашей нейронной сети) с помощью Learner.get_preds. Она возвращает прогнозы и цели по умолчанию, но поскольку у нас уже есть цели, мы можем игнорировать их, вставив специальную переменную _: . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . TensorImage([1.9830e-07, 1.8078e-07, 1.0011e-06, 2.7075e-08, 9.1111e-08, 1.0051e-08, 1.6212e-07, 8.9683e-08, 3.2831e-07, 1.4720e-07, 9.0234e-08, 1.2954e-08, 9.1324e-08, 1.3131e-07, 1.6930e-07, 5.0692e-08, 1.9956e-06, 1.9521e-08, 1.4544e-08, 4.4335e-07, 5.2613e-07, 4.8888e-06, 4.8523e-08, 2.5678e-08, 1.1946e-05, 9.9994e-01, 1.1523e-07, 3.8850e-06, 4.4177e-07, 2.3358e-07, 3.1694e-05, 1.0225e-07, 3.7125e-09, 9.5777e-07, 6.3361e-07, 1.7584e-06, 2.7162e-08]) . Фактические предсказания - это 37 вероятностей между 0 и 1, которые в сумме равняются 1: . len(preds[0]),preds[0].sum() . (37, TensorImage(1.)) . Softmax . В нашей классификационной модели мы используем функцию активации softmax в последнем слое, что гарантирует, что все значения активации находятся между 0 и 1 и что они в сумме дают 1. . Softmax аналогичен сигмовидной функции, которую мы использовали ранее. Как напоминание сигмоид выглядит так: . plot_function(torch.sigmoid, min=-4,max=4) . Теперь подумайте о том, что произойдет, если мы захотим поставить цель получить больше категорий чем две (например, наши 37 пород домашних животных). Это означает, что нам понадобится активация для каждой категории. Мы можем создать, например, нейронную сеть, которая предсказывает тройки и семерки используя для этого две активации, по одной для каждого класса-это будет хорошим первым шагом к созданию более общего подхода. Давайте просто используем некоторые случайные числа со стандартным отклонением 2 (по этому мы умножаем randn на 2) для этого примера, предполагая, что у нас есть 6 изображений и 2 возможные категории (где первый столбец представляет тройки, а второй-семерки): . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . Мы не можем просто взять сигмоиду этого непосредственно, так как мы не получаем что сумма значений в строке вероятности того, что это тройка плюс вероятность того, что это семерка равняется одному: . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . Итак, что же на самом деле означают эти активации в бинарном случае? Одна пара активаций просто указывает на относительную вероятность того, входное изображение является тройкой по сравнению с семеркой. Общая сумма, являются ли оба значения высокими или низкими не имеют значения. Все, что имеет значение, это то, что оно выше другого и на сколько. . В связи с тем, что у нас интерес немного другой - рассмотреть двухактивационную версию нашей нейроной сети, то эту проблему можно рассмотреть с другой стороны. Мы можем просто взять разницу между активациями нейронной сети, потому что это отражает, насколько мы более уверены в том, что на вход подается 3, а не 7, а затем взять сигмоиду этого: . (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . Второй столбец (вероятность того, что это будет 7) будет тогда просто тем значением, которое вычитается из 1. . Теперь нам нужен способ сделать все это, который будет также работать для более чем двух столбцов. Оказывается он есть и эта функция называемая softmax: . def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True) . Экспоненциальная функция (exp):буквально определяется как e**x, где e-специальное число, приблизительно равное 2,718. Это обратная функция натурального логарифма. Обратите внимание, что exp всегда положительный, и он очень быстро увеличивается! . Давайте проверим, что softmax возвращает те же значения, что и sigmoid для первого столбца, и эти значения вычитаются из 1 для второго столбца: . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . &#1051;&#1086;&#1075;&#1072;&#1088;&#1080;&#1092;&#1084;&#1080;&#1095;&#1077;&#1089;&#1082;&#1072;&#1103; &#1042;&#1077;&#1088;&#1086;&#1103;&#1090;&#1085;&#1086;&#1089;&#1090;&#1100; . Когда мы вычисляли потери для нашего MNIST примера в последней главе, то мы использовали функцию: . def mnist_loss(inputs, targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).mean() . Точно так же, как мы перешли от sigmoid к softmax, нам нужно перейти и для функции потерь, чтобы работать не только с двоичной классификацией. Она должна быть способна классифицировать любое количество категорий (в данном случае у нас есть 37 категорий). Наши активации, после softmax, находятся между 0 и 1 и суммируются до 1 для каждой строки в пакете предсказаний. Наши цели-целые числа от 0 до 36. . В двоичном случае мы использовали torch.where для выбора между inputs и (1-inputs). Когда мы рассматриваем двоичную классификацию как общую классификационную задачу с двумя категориями, это на самом деле становится еще проще, потому что (как мы видели в предыдущем разделе) у нас теперь есть два столбца, содержащие эквивалент inputs и (1-inputs). Итак, все, что нам нужно сделать, это выбрать из соответствующего столбца. Давайте попробуем реализовать это в PyTorch. Для нашего воображаемого примера троек и семерок предположим, что это наши метки: . targ = tensor([0,1,0,1,1,0]) . и следующие активации softmax . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Затем для каждого элемента targ мы можем использовать для выбора соответствующего столбца sm_acts с помощью тензорной индексации, например: . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . Чтобы точно увидеть, что здесь происходит, давайте сложим все столбцы вместе в таблицу. Здесь первые два столбца - это наши активации, затем у нас есть цели, индекс строки и, наконец, результат, показанный непосредственно выше: . from IPython.display import HTML df = pd.DataFrame(sm_acts, columns=[&quot;3&quot;,&quot;7&quot;]) df[&#39;targ&#39;] = targ df[&#39;idx&#39;] = idx df[&#39;loss&#39;] = sm_acts[range(6), targ] t = df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.996640 | 0.003360 | 1 | 3 | 0.003360 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . Глядя на эту таблицу, вы можете видеть, что последний столбец может быть вычислен, взяв столбцы targ и idx в качестве индексов в двухколоночной матрице, содержащей столбцы 3 и 7. Вот что на самом деле делает sm_acts[idx, targ]. . Действительно интересно то, что это на самом деле работает так же хорошо с более чем двумя колонками. Чтобы увидеть это, рассмотрим, что произойдет, если мы добавим столбец активации для каждой цифры (от 0 до 9), а targ будет содержать число от 0 до 9. Пока столбцы активации суммируются до 1 (а это так и будет, если мы используем softmax), у нас будет функция потерь, которая показывает, насколько хорошо мы предсказываем каждую цифру. . Мы только выбираем потери из столбца, содержащего правильную метку. Нам не нужно рассматривать другие столбцы, потому что при использовании softmax они складываются до 1 минус активация, соответствующая правильной метке. Поэтому, получая активацию для правильной метки как можно выше, мы также уменьшаем активацию остальных столбцов. . В PyTorch предоставлена функция, которая делает точно то же самое, что и sm_acts[range(n), targ] (за исключением того, что она принимает отрицательное значение), называемую nll_loss (NLL означает отрицательное логарифмическое правдоподобие): . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . &#1042;&#1086;&#1079;&#1100;&#1084;&#1077;&#1084; &#1083;&#1086;&#1075;&#1072;&#1088;&#1080;&#1092;&#1084; . Функция, которую мы видели в предыдущем разделе, довольно хорошо работает как функция потерь, но мы можем сделать ее немного лучше. Проблема в том, что мы используем вероятности, а вероятности не могут быть меньше 0 или больше 1. Это означает, что для нашей модели нет разницы между числом 0,99 и 0,999. Действительно, эти цифры очень близки друг к другу, но с другой стороны 0,999 в 10 раз более точное, чем 0,99. Итак, мы хотим преобразовать наши числа от 0 до 1, чтобы они лежали между отрицательной бесконечностью и положительной бесконечностью. Существует математическая функция, которая делает это: логарифм (доступен как torch.log). Он не определен для чисел меньше 0 и выглядит следующим образом: . plot_function(torch.log, min=0,max=4) . Когда мы сначала берем softmax, а затем логарифмическую вероятность этого, эта комбинация называется кросс-энтропийной потерей. В PyTorch она доступна как nn.CrossEntropyLoss(что на практике фактически делает log_softmax и nll_loss): . loss_func = nn.CrossEntropyLoss() . Как видите, это класс. Его создание дает вам объект, который ведет себя как функция: . loss_func(acts, targ) . tensor(1.8045) . Все функции потерь PyTorch предоставляются в двух формах: класс, только что показанный выше, а также простая функциональная форма, доступная в пространстве имен F: . F.cross_entropy(acts, targ) . tensor(1.8045) . Любой из них прекрасно работает и может быть использован в любой ситуации. Мы заметили, что большинство людей склонны использовать версию класса, и она чаще используется в официальных документах и примерах PyTorch, поэтому мы тоже будем использовать ее. . По умолчанию функции потерь PyTorch принимают среднее значение потерь всех элементов. Вы можете использовать reduction= &#39;none&#39;, чтобы отключить это: . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . &#1048;&#1085;&#1090;&#1077;&#1088;&#1087;&#1088;&#1077;&#1090;&#1072;&#1094;&#1080;&#1103; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . Очень трудно интерпретировать функции потерь напрямую, потому что они предназначены для того, чтобы быть вещами, которые компьютеры могут дифференцировать и оптимизировать, а не вещами, которые люди могут понять. Вот почему у нас есть метрики. Они не используются в процессе оптимизации, а просто помогают нам, бедным людям, понять, что происходит. В этом случае наша точность уже выглядит довольно хорошо! Так где же мы совершаем ошибки? . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . О боже—в этом случае матрицу ошибок очень трудно прочитать. У нас есть 37 различных пород домашних животных, а это значит, что у нас есть 37×37 записей в этой гигантской матрице! Вместо этого мы можем использовать метод most_confused, который просто показывает нам ячейки матрицы ошибок с наиболее неверными предсказаниями (здесь, по крайней мере, с 5 или более): . interp.most_confused(min_val=5) . [(&#39;Ragdoll&#39;, &#39;Birman&#39;, 9), (&#39;Bengal&#39;, &#39;Abyssinian&#39;, 5), (&#39;american_pit_bull_terrier&#39;, &#39;american_bulldog&#39;, 5), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 5)] . &#1059;&#1083;&#1091;&#1076;&#1096;&#1077;&#1085;&#1080;&#1077; &#1085;&#1072;&#1096;&#1077;&#1081; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . &#1048;&#1089;&#1082;&#1072;&#1090;&#1077;&#1083;&#1100; &#1089;&#1082;&#1086;&#1088;&#1086;&#1089;&#1090;&#1080; &#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1103; (The Learning Rate Finder) . Одна из самых важных вещей, которую мы можем сделать при обучении модели, - это убедиться, что у нас есть правильная скорость обучения. Если наша скорость обучения слишком низка, для обучения нашей модели может потребоваться много-много эпох. Это не только отнимает время, но и означает, что у нас могут возникнуть проблемы с переобучением, потому что каждый раз, когда мы полностью проходим через данные, мы даем нашей модели шанс запомнить их. . Так что давайте просто сделаем нашу скорость обучения действительно высокой, не так ли? Конечно, давайте попробуем и посмотрим, что получится: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 2.560477 | 3.031554 | 0.379567 | 11:09 | . epoch train_loss valid_loss error_rate time . 0 | 3.068019 | 1.584792 | 0.429635 | 15:03 | . Это выглядит не очень хорошо. Вот что произошло. Оптимизатор шагнул в правильном направлении, но шагнул так далеко, что полностью перешагнул минимальную потерю. Повторяя это несколько раз, он становится все дальше и дальше, а не ближе и ближе! . Что мы делаем, чтобы найти идеальную скорость обучения-не слишком высокую и не слишком низкую? В 2015 году исследователю Лесли Смиту пришла в голову блестящая идея, получившая название learning rate finder. Его идея состояла в том, чтобы начать с очень, очень маленькой скорости обучения, чего-то настолько маленького, что мы никогда не ожидали бы, что это будет слишком большим, чтобы справиться. Мы используем это для одной мини-партии, находим, каковы потери после этого, а затем увеличиваем скорость обучения на некоторый процент (например, удваивая ее каждый раз). Затем мы делаем еще одну мини-партию, отслеживаем потери и снова удваиваем скорость обучения. Мы продолжаем делать это до тех пор, пока потеря не станет хуже, а не лучше. Это тот момент, когда мы знаем, что зашли слишком далеко. Затем мы выбираем скорость обучения немного ниже этой точки. Наш совет-выбрать либо то, либо другое: . На порядок меньше, чем там, где была достигнута минимальная ошибка (то есть минимум, деленный на 10) | Последняя точка, где ошибки явно уменьшались | . Искатель скорости обучения вычисляет эти точки на кривой, чтобы помочь вам. Оба эти правила обычно дают примерно одинаковое значение. В первой главе мы не указывали скорость обучения, используя значение по умолчанию из библиотеки fastai (которое равно 1e-3): . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.20e-02, steepest point: 4.37e-03 . На этом графике мы видим, что в диапазоне от 1е-6 до 1е-3 на самом деле ничего не происходит, и модель не обучается. Затем ошибки начинают уменьшаться, пока не достигнут минимума, а затем снова увеличиваются. Мы не хотим, чтобы скорость обучения превышала 1е-1. Но 1е-1 уже то же высоко: на этом этапе мы вышли из периода, когда ошибки неуклонно снижались. . На этом графике скорости обучения кажется, что скорость обучения около 3e-3 была бы подходящей, поэтому давайте выберем ее: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.305556 | 0.348068 | 0.110284 | 11:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.561989 | 0.377429 | 0.108254 | 14:55 | . 1 | 0.334126 | 0.248589 | 0.073748 | 14:57 | . &#1056;&#1072;&#1079;&#1084;&#1086;&#1088;&#1072;&#1078;&#1080;&#1074;&#1072;&#1085;&#1080;&#1077; &#1080; Transfer Learning (&#1090;&#1088;&#1072;&#1085;&#1089;&#1092;&#1077;&#1088;&#1085;&#1086;&#1077; &#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077;) . Когда мы создаем модель из предварительно подготовленной сети, fastai автоматически замораживает все предварительно подготовленные слои для нас. Когда мы вызываем метод fine_tune fastai делает две вещи: . Обучает случайно добавленные слои для одной эпохи, а все остальные слои оставляет заморожеными | Размораживает все слои и тренирует их все на требуемое количество эпох | . Так что давайте попробуем сделать это вручную сами. Сначала мы будем обучать случайно добавленные слои для трех эпох, используя fit_one_cycle. Как уже упоминалось в . learn.fine_tune?? . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.195682 | 0.359648 | 0.116373 | 11:01 | . 1 | 0.536354 | 0.268985 | 0.083897 | 10:54 | . 2 | 0.337108 | 0.244635 | 0.077808 | 10:59 | . Затем мы разморозим модель: . learn.unfreeze() . и снова запустите lr_find, потому что наличие большего количества слоев для обучения и весов, которые уже были обучены в течение трех эпох, означает, что наша ранее найденная скорость обучения больше не подходит: . learn.lr_find() . SuggestedLRs(lr_min=1.3182566908653825e-05, lr_steep=6.309573450380412e-07) . Обратите внимание, что график немного отличается от того, когда у нас были случайные веса: у нас нет того резкого спуска, который указывает на то, что модель обучается. Это потому, что наша модель уже прошла обучение. Здесь мы имеем несколько плоскую область перед резким увеличением, и мы должны взять точку задолго до этого резкого увеличения—например, 1е-5. Точка с максимальным градиентом-это не то, что мы ищем здесь, и ее следует игнорировать. . Давайте тренироваться с подходящей скоростью обучения: . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.261118 | 0.245165 | 0.073072 | 15:25 | . 1 | 0.244202 | 0.230939 | 0.068336 | 14:58 | . 2 | 0.222367 | 0.231852 | 0.069012 | 14:53 | . 3 | 0.216706 | 0.226638 | 0.066306 | 14:52 | . 4 | 0.193296 | 0.225428 | 0.066982 | 13:08 | . 5 | 0.184676 | 0.225443 | 0.067659 | 14:22 | . Discriminative Learning Rates . fasta позволяет передавать объект slice Python в любом месте, где предполагается скорость обучения. Первое переданное значение будет скоростью обучения в самом раннем слое нейронной сети, а второе-скоростью обучения в последнем слое. Промежуточные слои будут иметь скорости обучения, которые мультипликативно равноудалены во всем этом диапазоне. Давайте воспользуемся этим подходом, чтобы повторить предыдущее обучение, но на этот раз мы установим только самый низкий уровень нашей сети на скорость обучения 1e-6; другие слои будут масштабироваться до 1e-4. Давайте немного потренируемся и посмотрим что получится: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.173580 | 0.367213 | 0.116373 | 00:36 | . 1 | 0.539504 | 0.270600 | 0.089986 | 00:35 | . 2 | 0.346232 | 0.240070 | 0.079161 | 00:35 | . epoch train_loss valid_loss error_rate time . 0 | 0.268828 | 0.231327 | 0.075101 | 00:44 | . 1 | 0.260505 | 0.225987 | 0.073748 | 00:44 | . 2 | 0.246831 | 0.228189 | 0.074425 | 00:44 | . 3 | 0.220918 | 0.224612 | 0.075101 | 00:43 | . 4 | 0.195331 | 0.215086 | 0.066982 | 00:44 | . 5 | 0.171444 | 0.211952 | 0.071719 | 00:43 | . 6 | 0.148516 | 0.216797 | 0.069689 | 00:44 | . 7 | 0.140547 | 0.210381 | 0.066306 | 00:44 | . 8 | 0.147587 | 0.211248 | 0.063599 | 00:44 | . 9 | 0.136450 | 0.208178 | 0.064276 | 00:43 | . 10 | 0.117142 | 0.208999 | 0.064953 | 00:43 | . 11 | 0.114411 | 0.209443 | 0.060893 | 00:44 | . Теперь тонкая настройка работает отлично! . fastai может показать нам график ошибок при обучении и валидации: . learn.recorder.plot_loss() . Как вы можете видеть, потеря тренировок становится все лучше и лучше. Но обратите внимание, что в конечном итоге улучшение потери валидации замедляется, а иногда даже ухудшается! Это точка, в которой модель начинает перестраиваться. В частности, модель становится слишком самоуверенной в своих предсказаниях. Но это не значит, что она становится обязательно менее точной. Взгляните на таблицу результатов обучения за каждую эпоху, и вы часто увидите, что точность продолжает улучшаться, даже когда потеря валидации становится хуже. В конце концов, важна ваша точность или, в более общем смысле, выбранные вами показатели, а не потери. Потеря-это просто функция, которую мы дали компьютеру, чтобы помочь нам оптимизировать. . Еще одно решение, которое вы должны принять при обучении модели, - это как долго тренироваться. Мы рассмотрим это в следующий раз. . Deeper Architectures . Недостаток более глубоких архитектур заключается в том, что их обучение занимает гораздо больше времени. Одна из техник, которая может значительно ускорить процесс, - это тренировка смешанной точности. Это относится к использованию менее точных чисел (Число́ полови́нной то́чности (half-precision floating point) также называемое fp16) там, где это возможно во время обучения. Когда мы пишем эти слова в начале 2020 года, почти все современные графические процессоры NVIDIA поддерживают специальную функцию, называемую тензорными ядрами, которая может значительно ускорить обучение нейронных сетей в 2-3 раза. Они также требуют гораздо меньше памяти GPU. Чтобы включить эту функцию в fastai, просто добавьте to_fp16() после создания вашего Learner (вам также нужно импортировать модуль). . Вы не можете заранее знать, какая архитектура лучше всего подходит для вашей конкретной проблемы—вам нужно попробовать потренироваться. Так что давайте теперь попробуем ResNet-50 со смешанной точностью: . from fastai.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . epoch train_loss valid_loss error_rate time . 0 | 1.418004 | 0.275086 | 0.090663 | 00:58 | . 1 | 0.594283 | 0.286878 | 0.095399 | 00:59 | . 2 | 0.433570 | 0.260211 | 0.084574 | 00:58 | . epoch train_loss valid_loss error_rate time . 0 | 0.283216 | 0.237705 | 0.075778 | 01:11 | . 1 | 0.300372 | 0.374852 | 0.104871 | 01:11 | . 2 | 0.263926 | 0.244888 | 0.069689 | 01:11 | . 3 | 0.150826 | 0.218107 | 0.062246 | 01:12 | . 4 | 0.082293 | 0.197206 | 0.059540 | 01:12 | . 5 | 0.052833 | 0.192704 | 0.058187 | 01:12 | . В этом случае мы не видим явного выигрыша от более глубокой модели. Это полезно помнить—большие модели не обязательно являются лучшими моделями для вашего конкретного случая! Обязательно попробуйте небольшие модели, прежде чем приступать к масштабированию. .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2020/12/21/05-pet-breeds.html",
            "relUrl": "/russian/fast.ai/solution/2020/12/21/05-pet-breeds.html",
            "date": " • Dec 21, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Russian - Fastbook Chapter 5 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы к пятой части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, как и к осталььным частям прошу писать в коментариях - поправлю. . 1. Почему мы сначала изменяем размер до большого размера на процессоре, а затем до меньшего размера на графическом процессоре? . Эта концепция известна как проклеивание (presizing). Прирост данных часто применяется к изображениям, и на самом деле это делается на графическом процессоре. Однако увеличение объема данных может привести к ухудшению качества и появлению артефактов, особенно на краях. Поэтому, чтобы свести к минимуму ухудшение качества данных, дополнения выполняются на более крупном изображении, а затем выполняется RandomResizeCrop изменение размера до требуемого размера изображения. . 2. Если вы не знакомы с регулярными выражениями, найдите учебник по регулярным выражениям и задач и выполните их. . Выполняется самостоятельно . 3. Каковы два наиболее распространенных способа предоставления данных для большинства наборов данных глубокого обучения? . Отдельные файлы, представляющие элементы данных, такие как текстовые документы или изображения. | Таблица данных, например в формате CSV, где каждая строка является элементом и может включать имена файлов, обеспечивающие связь между данными в таблице и данными в других форматах, таких как текстовые документы и изображения. | . 4. Посмотрите документацию на L и попробуйте использовать несколько новых методов. . Выполняется самостоятельно . 5. Посмотрите документацию для модуля Python pathlib и попробуйте использовать несколько методов класса Path. . Выполняется самостоятельно . 6. Приведите два примера того, как преобразования изображений могут ухудшить качество данных. . Вращение может являться причиной пустых областей в конечном изображении | Другие операции могут потребовать интерполяции, которая основана на исходных пикселях изображения и в результате более низкое качество изображения | . 7. Какой метод в fastai для просмотра данных в загрузчике данных (DataLoader)? . DataLoader.show_batch . 8. Какой метод в fastai, чтобы помочь вам отладить DataBlock? . DataBlock.summary . 9. Следует ли вам отложить обучение модели до тех пор, пока вы не очистите свои данные полностью? . Нет. Лучше всего первоначально создать базовую модель. . 10. Что за два метода объединены в кросс-энтропию в PyTorch? . Кросс энтропийные потери представляет собой комбинацию функции Softmax и отрицательной логарифмической потери правдоподобия. . 11. Каковы два свойства активаций, которые гарантирует softmax? Почему это так важно? . Выходные данные в сумме дают один и модель может предсказать только один класс. Кроме того, усиливаются небольшие изменения в выходных активациях, что полезно, поскольку это означает, что модель выберет метку с более высокой увереностью (хорошо для проблем с конкретными метками). . 12. Когда вы хотите, чтобы ваши активации не имели этих свойств? . Когда у вас есть проблемы с классификацией нескольких меток (возможно более одной метки). . 13. Вычислите столбцы exp и softmax. . ПРОПУЩЕНО . 14. Почему мы не можем использовать torch.where для создания функции потерь для наборов данных, где наша метка может иметь более двух категорий? . Потому что torch.where может выбирать только между двумя возможностями, в то время как для многоклассовой классификации у нас есть необходимость в выборе нескольких вариантов. . 15. Каково значение log (-2)? Почему? . Значение не определено. Логарифм является обратной экспоненциальной функцией, а экспоненциальная функция всегда положительна, независимо от того, какое значение передается. Таким образом, логарифм не определен для отрицательных значений. . **16. Каковы два хороших эмпирических правила для выбора скорости обучения при использовании искателя скорости обучения? . Любой из этих двух пунктов должен быть использован для скорости обучения: . на порядок меньше, чем там, где была достигнута минимальная потеря (то есть минимум, деленный на 10) | последняя точка, где потеря явно уменьшилась. | . 17. Какие два шага делает метод fine_tune? . Тренирует новую голову (со случайными весами) в течение одной эпохи | Разморозет все слои и тренирует их все для требуемого количества эпох | . 18. Как получить исходный код метода или функции в Jupyter notebook? . Использовать ?? после функции. Пример: DataBlock.summary?? . 19. Что такое дискриминационные показатели обучения? . Дискриминативные скорости обучения относятся к тренировочному трюку использования различных скоростей обучения для разных слоев модели. Это обычно используется в трансфертном обучении. Идея заключается в том, что при обучении предварительно подготовленной модели вы не хотите резко менять более ранние слои, поскольку она содержит информацию о простых объектах, таких как ребра и формы. Но более поздние слои могут быть изменены немного больше, поскольку они могут содержать информацию о чертах лица или других объектах, которые могут не иметь отношения к вашей задаче. Таким образом, более ранние слои имеют более низкую скорость обучения, а более поздние слои имеют более высокую скорость обучения. . 20. Как объект slice на языке Python интерпретируется при передаче в качестве скорости обучения в fastai? . Первое значение объекта среза - это скорость обучения для самого раннего слоя, а второе-скорость обучения для последнего слоя. Промежуточные слои будут иметь скорости обучения, которые мультипликативно равноудалены во всем этом диапазоне. . 21. Почему ранняя остановка является плохим выбором при использовании одного цикла тренировок? . Если используется ранняя остановка, обучение может не успеть достичь более низких значений скорости на графике, что могло бы способствовать совершенствованию модели. Поэтому рекомендуется переучивать модель и выбирать количество эпох исходя из того, где были найдены предыдущие лучшие результаты. . 22. В чем разница между resnet 50 и resnet101? . Числа 50 и 101 относятся к числу слоев в моделях. Таким образом, ResNet101-это более крупная модель с большим количеством слоев по сравнению с ResNet50. Эти варианты моделей обычно используются, поскольку существуют модели с предварительно подготовленными весами ImageNet. . 23. Что делает to_fp16? . Это позволяет проводить обучение со смешанной точностью, в котором для ускорения обучения используются менее точные числа. .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2020/12/20/fastai-chapter5-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2020/12/20/fastai-chapter5-solution.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Russian - Solution Lesson 4 on Fast.ai",
            "content": "Следующий блокнот с решением к третьему или четвертому уроку. Честно сказать, то это не я не определился, а просто у составителей третий урок называется &quot;этика&quot; и я ему не придал должного значения. Там только почитать. У данный блокнот относиться ко второй части третьего урока. . В этом уроке будет создаваться модель, которая может классифицировать изображение как 3 или 7. Для это загрузим образцы MNIST содержащие изображения только этих цифр: . MNIST содержит изображения рукописных цифр, собранные Национальным институтом стандартов и технологий и сведенные в набор данных машинного обучения Янном Лекуном и его коллегами. Лекун использовал MNIST в 1998 году в Lenet-5, первой компьютерной системе, продемонстрировавшей практически полезное распознавание рукописных цифр. Это был один из самых важных прорывов в истории искусственного интеллекта. . path = untar_data(URLs.MNIST_SAMPLE) . Посмотреть, что находится в каталоге можно при помощи метода ls, добавленного в fastai. Этот метод возвращает объект специального класса fastai под названием L, который помимо функциональных возможностей списков Python, также может многое другое. Одна из его удобных особенностей заключается в том, что при выводе на печать он отображает количество элементов перед их перечислением (если их больше 10, он просто показывает первые несколько): . path.ls() . (#3) [Path(&#39;labels.csv&#39;),Path(&#39;valid&#39;),Path(&#39;train&#39;)] . Набор значений MNIST имеет стандартную компоновку данных для машинного обучения: отдельные папки для обучающего набора и проверки (и/или тестового множества). Дальше посмотрим, что находится внутри тренировочного набора: . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . В нем есть папки, содержащие цифры 3 и 7. Благодаря тому, что изображения раскиданы по папкам - на языке машинного обучения они получили метки &quot;3&quot; и &quot;7&quot;. Давайте заглянем в одну из этих папок (сортировку использована для того, чтобы мы получили один и тот же порядок файлов): . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . Как и следовало ожидать в ней полно изображений. Дальше можно вывести одно из них: . im3_path = threes[1] im3 = Image.open(im3_path) im3 . Здесь мы используем класс Image из библиотеки Python Imaging Library (PIL), которая является наиболее широко используемым пакетом Python для работы с растровым изображением. Jupyter так же приспособлен к работе с PIL. . В компьютере все представлено в виде чисел. Чтобы просмотреть числа, составляющие это изображение, мы должны преобразовать его в массив NumPy или Тензор PyTorch. Например, вот как выглядит часть изображения, преобразованная в массив NumPy: . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . 4:10 указывает на то, что нам необходимы строки с индексом 4 (включено) до 10 (не включено) и то же самое для столбцов. NumPy индексирует сверху вниз и слева направо. Вот то же самое для Тензора PyTorch: . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . Можно взять только ту часть массива, в которой находятся часть цифр, отличных от нуля, а затем использовать Pandas DataFrame для использования заполнения градиентом оттенков серого ячеек, которое ясно покажет как создается изображение из значений пикселей: . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . &#1055;&#1077;&#1088;&#1074;&#1072;&#1103; &#1087;&#1086;&#1087;&#1099;&#1090;&#1082;&#1072;: &#1057;&#1093;&#1086;&#1076;&#1089;&#1090;&#1074;&#1086; &#1087;&#1080;&#1082;&#1089;&#1077;&#1083;&#1077;&#1081; . Создадим списки, содержащий все наши тензоры 3 и отдельно 7 вместе. После этого проверим их размеры. . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . Дальше проверим на одном из изображений, что оно выглядит нормально. Так значения переведены в тензоры, которые Jupyter , будет распечатывать как матрицы, а не изображения, то нужно использовать функцию show_image из fastai. . show_image(three_tensors[1]); . При помощи функции stack из PyTorch объединим все тензоры в один. Так как ряд операции в PyTorch, такие как например взятие среднего значения, требуют использование чисел с плавающей точкой, то приведем их дальше к типу float. Если полученные значения изображения с плавающей точкой, то обычно используют их в диапазоне от 0 до 1 для чего делим на 255. . Функция shape позволяет узнать размеры тензора. В результате получим 6131 изображений 28 на 28 пикселей. . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . Используя len можно получить размерность тензора (rank - количество осей) . len(stacked_threes.shape) . 3 . Также можно получить rank тензора с помощью ndim: . stacked_threes.ndim . 3 . Дальше находим &quot;идеальную&quot; 3. Для этого вычисляем среднее всех тензоров по оси 0 и выведемм его на печать . mean3 = stacked_threes.mean(0) show_image(mean3); . Сделаем то же самое для 7 . mean7 = stacked_sevens.mean(0) show_image(mean7); . Возьмем тройку . a_3 = stacked_threes[1] show_image(a_3); . Дальше измерим растояние двумя методами: . при помощи среднего значения суммы абсолютного значения разностей (абсолютное значение-это модуль числа). Называется средней абсолютной разницей или нормой L1 | при помощи среднего значения квадрата разностей и квадратного корня. Называется среднеквадратичной ошибкой (RMSE) или нормой L2. | . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . В обоих случаях расстояние между исследуемым 3 и &quot;идеальным&quot; 3 меньше, чем расстояние до идеального 7. Таким образом простая модель даст точный прогноз. . В PyTorch предоставлены обе эти функции как функции потерь (loss functions): . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . NumPy &#1084;&#1072;&#1089;&#1089;&#1080;&#1074; &#1080; PyTorch &#1090;&#1077;&#1085;&#1079;&#1086;&#1088; . Чтобы создать массив или тензор, передайте список (или список списков, или список списков списков и т. д.) в array() или tensor(): . data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) . arr # numpy . array([[1, 2, 3], [4, 5, 6]]) . tns # pytorch . tensor([[1, 2, 3], [4, 5, 6]]) . Все последующие операции показаны на тензорах, но синтаксис и результаты для массивов NumPy идентичны. . Выбрать вторую строку (индексация с 0): . tns[1] . tensor([4, 5, 6]) . или столбец, используя : для обозначения всех строк: . tns[:,1] . tensor([2, 5]) . Так же можно использовать питоновские срезы Python ([start:end]), чтобы выбрать часть строки или столбца: . tns[1,1:3] . tensor([5, 6]) . Можно использовать стандартные операторы, такие как +, -, *, /: . tns+1 . tensor([[2, 3, 4], [5, 6, 7]]) . Так же у тензора есть тип . tns.type() . &#39;torch.LongTensor&#39; . Тип будет автоматически меняться по мере необходимости, например, с int на float: . tns*1.5 . tensor([[1.5000, 3.0000, 4.5000], [6.0000, 7.5000, 9.0000]]) . &#1042;&#1099;&#1095;&#1080;&#1089;&#1083;&#1077;&#1085;&#1080;&#1077; &#1084;&#1077;&#1090;&#1088;&#1080;&#1082; . Первым шагом создаем тензоры для 3 и 7 из валиадационного каталога для последующей оценки полученной модели . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Дальше напишем функцию, которая вычисляет среднюю абсолютную ошибку, используя выражение, очень похожее на то, которое было в прошлом разделе: . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . После вычислим растояние валиадационного набора до идеальной 3 и на выходе получим вектор . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1290, 0.1223, 0.1380, ..., 0.1337, 0.1132, 0.1097]), torch.Size([1010])) . Как видно ошибка несоответствия размеров не возникает, так как в этот раз используется broadcasting - автоматическое расширение тензора с меньшим размером (rank). . Пример операции для тензоров с одинаковым количеством измеренийа после с разным: . tensor([1,2,3]) + tensor([1,1,1]) . tensor([2, 3, 4]) . (valid_3_tens-mean3).shape . torch.Size([1010, 28, 28]) . Дальше создадим функцию mnist_distance, которая будет выяснять является ли изображение 3 или нет используя следующую логику: если расстояние между рассматриваемой цифрой и идеальным 3 меньше расстояния до идеального 7, то это 3: . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . Проверим на нашем примере: . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . Дальше протестируем на всем наборе . is_3(valid_3_tens) . tensor([True, True, True, ..., True, True, True]) . Теперь мы можем вычислить точность для каждого числа из троек и семерок, взяв среднее значение этой функции для всех троек и обратную функцию для всех семерок: . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . &#1057;&#1090;&#1086;&#1093;&#1072;&#1089;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080;&#1081; &#1075;&#1088;&#1072;&#1076;&#1080;&#1077;&#1085;&#1090;&#1085;&#1099;&#1081; &#1089;&#1087;&#1091;&#1089;&#1082; (Stochastic Gradient Descent) (SGD) . Шаги, которые потребуются, чтобы превратить функцию по определению троек и семерек в классификатор машинного обучения: . Инициализируйте веса. | Для каждого изображения использовать эти веса, чтобы предсказать, будет ли оно 3 или 7. | Исходя из полученных прогнозов, рассчитайте, насколько качественная полученная модель (расчитать ее потерю). | Вычислите градиент, который измеряется для каждого веса и как изменение этого веса изменяет потерю | Изменение всех весов, основанное на прошлом вычислении. | Вернитесь к Шагу 2 и повторите процесс. | Повторяйте до тех пор, пока не решите остановить тренировочный процесс (например, потому что получили качественную модель или вы не хотите больше ждать). | gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop Прежде чем применить эти шаги к нашей задаче классификации изображений, давайте проиллюстрируем, как они выглядят в более простом случае. Сначала мы определим очень простую функцию, квадратичную-давайте представим, что это наша функция потерь, а x-весовой параметр функции: . def f(x): return x**2 . Вот график этой функции: . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastbook/__init__.py:73: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . Последовательность шагов, описанных ранее, начинается с выбора некоторого случайного значения параметра и вычисления величины потери: . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . &#1042;&#1099;&#1095;&#1080;&#1089;&#1083;&#1077;&#1085;&#1080;&#1077; &#1075;&#1088;&#1072;&#1076;&#1080;&#1077;&#1085;&#1090;&#1072; . Во-первых, давайте выберем тензорное значение, для которого нам нужен градиент. requiresgrad - показывает, что мы хотим вычислить градиенты относительно этой переменной при этом значении: . xt = tensor(3.).requires_grad_() . Теперь мы вычисляем нашу функцию с этим значением. Обратите внимание, как PyTorch печатает не только вычисленное значение, но и примечание, что у него есть градиентная функция, которую он будет использовать для вычисления наших градиентов, когда это необходимо: . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . Дальше мы говорим PyTorch вычислить градиенты для нас: . yt.backward() . backward - здесь относится к метод обратного распространения ошибки (англ. backpropagation) — метод вычисления градиента, который используется при обновлении весов многослойного перцептрона. . xt.grad . tensor(6.) . Если вы помните свои школьные годы и правила вычисления производной от x 2, то знаете что производная равна 2*x, а у нас x=3, поэтому градиент равен 2*3=6, что и рассчитал для нас PyTorch! . Теперь мы повторим предыдущие шаги, но с аргументом в виде вектора для нашей функции: . xt = tensor([3.,4.,10.]).requires_grad_() xt . tensor([ 3., 4., 10.], requires_grad=True) . И мы добавим сумму к нашей функции, чтобы она могла взять вектор (т. е. тензор ранга 1) и вернуть скаляр (т. е. тензор ранга 0): . def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad . tensor([ 6., 8., 20.]) . &#1054;&#1090; &#1085;&#1072;&#1095;&#1072;&#1083;&#1072; &#1076;&#1086; &#1082;&#1086;&#1085;&#1094;&#1072; &#1087;&#1088;&#1080;&#1084;&#1077;&#1088; &#1089;&#1090;&#1086;&#1093;&#1072;&#1089;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1086;&#1075;&#1086; &#1075;&#1088;&#1072;&#1076;&#1080;&#1077;&#1085;&#1090;&#1085;&#1086;&#1075;&#1086; &#1089;&#1087;&#1091;&#1089;&#1082;&#1072; . Рассматриваться будет простая модель на примере американских горок. Измеряется скорость: когда тележка залазит на вершину, то скорость падает, а когда спускается, то возрастает. Задача - построить модель изменения скорости со временем. Если бы скорость измерялась каждую секунду в течении 20 секунд, это это могли быть следующие величины: . time = torch.arange(0,20).float(); time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . Определим функцию, которая будет принимать параметры и время и выполнять их разделение внутри и вычесление . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . Сначала нам нужно определить, что подразумевается под &quot;лучшим&quot; результатом. Это определяетя точно при помощи функции потерь, которая будет возвращать значение на основе прогноза и целевызх значений, где более низкие значения функции соответствуют &quot;лучшим&quot; прогнозам. Для непрерывных данных обычно используется среднеквадратичная ошибка: . def mse(preds, targets): return ((preds-targets)**2).mean() . Step 1: &#1048;&#1085;&#1080;&#1094;&#1080;&#1083;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; &#1087;&#1072;&#1088;&#1072;&#1084;&#1077;&#1090;&#1088;&#1086;&#1074; . Сначала инициализируем параметры случайными значениями и сообщаем PyTorch, что хотим отслеживать их градиенты, используя requiresgrad: . params = torch.randn(3).requires_grad_() . Step 2: &#1042;&#1099;&#1095;&#1080;&#1089;&#1083;&#1077;&#1085;&#1080;&#1077; &#1087;&#1088;&#1077;&#1076;&#1089;&#1082;&#1072;&#1079;&#1072;&#1085;&#1080;&#1103; . preds = f(time, params) . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . Step 3: &#1042;&#1099;&#1095;&#1080;&#1089;&#1083;&#1077;&#1085;&#1080;&#1077; &#1087;&#1086;&#1090;&#1077;&#1088;&#1080; . loss = mse(preds, speed) loss . tensor(25823.8086, grad_fn=&lt;MeanBackward0&gt;) . Step 4: &#1042;&#1099;&#1095;&#1080;&#1089;&#1083;&#1077;&#1085;&#1080;&#1077; &#1075;&#1088;&#1072;&#1076;&#1080;&#1077;&#1085;&#1090;&#1072; . loss.backward() params.grad . tensor([-53195.8633, -3419.7148, -253.8908]) . params.grad * 1e-5 . tensor([-0.5320, -0.0342, -0.0025]) . Можно использовать эти градиенты для улучшения параметров. В качестве скорости обучения используется значение 1e-5 или 0.00001 . params . tensor([-0.7658, -0.7506, 1.3525], requires_grad=True) . Step 5: &#1069;&#1090;&#1072;&#1087; &#1089; &#1074;&#1077;&#1089;&#1072;&#1084;&#1080; . Теперь нужно обновить параметры на основе градиентов, которые только что рассчитали: . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . Посмотрим как изменилась потеря . preds = f(time,params) mse(preds, speed) . tensor(5435.5356, grad_fn=&lt;MeanBackward0&gt;) . Построим график . show_preds(preds) . Так как необходимо повторить это несколько раз, поэтому логично создадать функцию которое будет это все включать в свое тело . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . Step 6: &#1055;&#1086;&#1074;&#1090;&#1086;&#1088;&#1080;&#1090;&#1100; &#1087;&#1088;&#1086;&#1094;&#1077;&#1089;&#1089; . for i in range(10): apply_step(params) . 5435.53564453125 1577.44921875 847.3778076171875 709.2225341796875 683.0758056640625 678.1243896484375 677.1838989257812 677.0023803710938 676.9645385742188 676.9537353515625 . Построим график для гаждого шага, чтоб увидеть как приблежается функция к идеальной . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Step 7: &#1057;&#1058;&#1054;&#1055; . Решение остановиться после 10 эпох произвольно. На практике необходимо наблюдать за метриками, чтобы решить, когда остановиться. . MNIST &#1092;&#1091;&#1085;&#1082;&#1094;&#1080;&#1103; &#1087;&#1086;&#1090;&#1077;&#1088;&#1080; . В исследуемой модели зависимыми переменными яляются сам изображения. Необходимо объединить все их в один тензор и изменить rank 2(матрица) на rank 3(вектор).Это можно сделать при помощи команды view из PyTorch. Происходит изменение формы тензора без изменения его содержимого. -1-параметр когда не известно сколько строк необходимо, но точно известно количество столбцов. Другими словами сделайть ось настолько большой, насколько это необходимо для размещения всех данных: . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . Так как необходима метка для каждого изображения, то испольуется 1 для 3 и 0 для 7-ки: . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . Dataset в PyTorch должен возвращать кортеж (x, y) при индексации. Python предоставляет функцию zip, которая в сочетании с фунцией list обеспечивает простой способ получить эту функциональность: . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . Теперь необходимы веса для каждого пикселя (это шаг инициализации в семиступенчатом процессе): . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . Функция weights/pixels не будет достаточно гибкой, так как она всегда равна 0, когда пиксели равны 0. Так как функция для прямой линии выглядит следующим образом Y=w x+b, то необходимо еще случайным образом иницилизировать b: . bias = init_params(1) . Вычислим прогноз для одного изображения: . (train_x[0]*weights.T).sum() + bias . tensor([20.2336], grad_fn=&lt;AddBackward0&gt;) . В Python умножение матриц, представленных с помощью оператора @. . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[20.2336], [17.0644], [15.2384], ..., [18.3804], [23.8567], [28.6816]], grad_fn=&lt;AddBackward0&gt;) . Чтобы решить, представляет ли прогноз 3 или 7, можно просто проверить, больше ли он 0: . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[ True], [ True], [ True], ..., [False], [False], [False]]) . corrects.float().mean().item() . 0.4912068545818329 . Теперь проверим каково изменение точности для одного небольшого изменения веса: . weights[0] *= 1.0001 . preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.4912068545818329 . Для рассмотрения предположим, что есть три изображения, которые, как известно являются 3, 7 и 3 с вероятностью, что первое число 3 - 0.9, второе число 7 - 0.4 и третье число 3 с верояятностью 0.2. . trgts = tensor([1,0,1]) prds = tensor([0.9, 0.4, 0.2]) . Далее создадим первую функцию потерь, которая измеряет расстояние между прогнозом и целевым значением. В нем torch.where(a,b,c) то же самое, что [b[i] if a[i] else c[i] for i in range(len(a))], но работает с тензорами с большей скоростью . def mnist_loss(predictions, targets): return torch.where(targets==1, 1-predictions, predictions).mean() . Испытание для prds и trgts: . torch.where(trgts==1, 1-prds, prds) . tensor([0.1000, 0.4000, 0.8000]) . Поскольку необходимо скалярное значение, то при помощи mnist_loss получаемт среднее значение предыдущего тензора: . mnist_loss(prds,trgts) . tensor(0.4333) . Для примера изменим наш прогноз для одного значения с 0.2 на 0.8и в результате потери уменьшаться, что говорит о том, что это лучший прогноз: . mnist_loss(tensor([0.9, 0.4, 0.8]),trgts) . tensor(0.2333) . Sigmoid . Сигмоидная функция всегда выводит число от 0 до 1. Он определяется следующим образом . def sigmoid(x): return 1/(1+torch.exp(-x)) . В Pytorch она определена и нет необходимости создавать свою собственную. Она определена от 0 до 1 и вот как это выглядит: . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . Обновим функцию mnist_loss, чтобы применить сигмоиду к входным данным: . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . SGD &#1080; Mini-Batches . DataLoader может взять любой набор Python и превратить ее в итератор для многих пакетов, например: . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . Для обучения модели нам нужна не просто коллекция Python, а коллекция, содержащая независимые и зависимые переменные (то есть входные значения и результаты). Коллекция, содержащая кортежи независимых и зависимых переменных, известна в PyTorch как Dataset. Один из примеров . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . Когда передается Dataset в DataLoader, получается много пакетов, которые сами являются кортежами тензоров, представляющих независимые и зависимые переменные: . dl = DataLoader(ds, batch_size=6, shuffle=True) list(dl) . [(tensor([17, 18, 10, 22, 8, 14]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;, &#39;o&#39;)), (tensor([20, 15, 9, 13, 21, 12]), (&#39;u&#39;, &#39;p&#39;, &#39;j&#39;, &#39;n&#39;, &#39;v&#39;, &#39;m&#39;)), (tensor([ 7, 25, 6, 5, 11, 23]), (&#39;h&#39;, &#39;z&#39;, &#39;g&#39;, &#39;f&#39;, &#39;l&#39;, &#39;x&#39;)), (tensor([ 1, 3, 0, 24, 19, 16]), (&#39;b&#39;, &#39;d&#39;, &#39;a&#39;, &#39;y&#39;, &#39;t&#39;, &#39;q&#39;)), (tensor([2, 4]), (&#39;c&#39;, &#39;e&#39;))] . &#1057;&#1086;&#1077;&#1076;&#1080;&#1085;&#1080;&#1090;&#1100; &#1074;&#1089;&#1077; &#1074;&#1086;&#1077;&#1076;&#1080;&#1085;&#1086; . Повторно инициализируем параметры: . weights = init_params((28*28,1)) bias = init_params(1) . DataLoader создать из Dataset: . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . Сделаем то же самое для проверочного набора: . valid_dl = DataLoader(valid_dset, batch_size=256) . Создадим мини-пакеты размером в четыре для тестирования: . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . preds = linear1(batch) preds . tensor([[-2.1876], [-8.3973], [ 2.5000], [-4.9473]], grad_fn=&lt;AddBackward0&gt;) . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.7419, grad_fn=&lt;MeanBackward0&gt;) . Теперь можем вычислить градиенты: . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0061), tensor([-0.0420])) . Поместим все это в функцию: . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . и протестируем: . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0121), tensor([-0.0840])) . Теперь посмотрим, что произойдет, если вызвать функцию дважды . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0182), tensor([-0.1260])) . Градиенты изменились! Причина этого заключается в том, что loss.backward фактически добавляет градиенты потерь к градиентам, которые в данный момент хранятся. Итак, сначала необходимо установить текущие градиенты на 0: . weights.grad.zero_() bias.grad.zero_(); . Создадим функцию базового цикла обучения для эпохи: . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . Также проверим, как работает. Чтобы решить, представляет ли прогноз 3 или 7, можно просто проверить, больше ли он 0: . (preds&gt;0.0).float() == train_y[:4] . tensor([[False], [False], [ True], [False]]) . Получается функция для вычисления точности сверки: . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . Можно проверить как это работает: . batch_accuracy(linear1(batch), train_y[:4]) . tensor(0.2500) . а затем сложить пакеты вместе . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5263 . Далее потренируемся на одной эпохе и проверим измениться ли точность: . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.6663 . Затем еще несколько: . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8265 0.8899 0.9182 0.9275 0.9397 0.9466 0.9505 0.9525 0.9559 0.9578 0.9598 0.9608 0.9613 0.9618 0.9632 0.9637 0.9647 0.9657 0.9672 0.9677 . Получен неплохой результат. Далее оптимизация. . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1086;&#1087;&#1090;&#1080;&#1084;&#1080;&#1079;&#1072;&#1090;&#1086;&#1088;&#1072; . nn.Linear делает то же самое, что и init_params и linear вместе. Он содержит как веса, так и смещения в одном классе. Копируем модель из предыдущего раздела: . linear_model = nn.Linear(28*28,1) . Получить доступ к параметрам в модуле PyTorch, которые будут обучены, можно через метод parameters: . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . Можно использовать эту информацию для создания оптимизатора . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . Создается оптимизатор путем передачи параметров модели: . opt = BasicOptim(linear_model.parameters(), lr) . Теперь можно упростить цикл . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . Функцию проверки вообще менять не надо . validate_epoch(linear_model) . 0.4608 . Поместим тренировочнуюый цикл в функцию, чтобы упростить: . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . Получим тот же результат, что и в прошлом разделе . train_model(linear_model, 20) . 0.4932 0.7685 0.8554 0.9135 0.9345 0.9482 0.957 0.9633 0.9658 0.9677 0.9697 0.9716 0.9736 0.9746 0.976 0.977 0.9775 0.9775 0.978 0.9785 . В fastai имеется класс SGD, который делает то же самое, что и BasicOptim: . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.8179 0.8496 0.914 0.9345 0.9482 0.957 0.9619 0.9658 0.9672 0.9692 0.9712 0.9741 0.9751 0.976 0.9775 0.9775 0.978 0.9785 0.979 . В fastai также есть Learner.fit, который используется вместо train_model. Чтобы создать Learner, необходимо сначала создать DataLoaders, передав обучающие и проверочные данные: . dls = DataLoaders(dl, valid_dl) . Чтобы создать Learner без использования приложений (например, cnn_learner), нужно передать все элементы, которые были созданы в этой главе: DataLoaders, модель, функцию оптимизации (которой будут переданы параметры), функцию потерь и, возможно, метрики для вывода: . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . Теперь можно тренировать . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.636709 | 0.503144 | 0.495584 | 00:00 | . 1 | 0.429828 | 0.248517 | 0.777233 | 00:00 | . 2 | 0.161680 | 0.155361 | 0.861629 | 00:00 | . 3 | 0.072948 | 0.097722 | 0.917566 | 00:00 | . 4 | 0.040128 | 0.073205 | 0.936212 | 00:00 | . 5 | 0.027210 | 0.059466 | 0.950442 | 00:00 | . 6 | 0.021837 | 0.050799 | 0.957802 | 00:00 | . 7 | 0.019398 | 0.044980 | 0.964181 | 00:00 | . 8 | 0.018122 | 0.040853 | 0.966143 | 00:00 | . 9 | 0.017330 | 0.037788 | 0.968106 | 00:00 | . &#1044;&#1086;&#1073;&#1072;&#1074;&#1083;&#1077;&#1085;&#1080;&#1077; &#1085;&#1077;&#1083;&#1080;&#1085;&#1077;&#1081;&#1085;&#1086;&#1089;&#1090;&#1080; . Полное описание базовой нейроной сети выглядит следующим образом: . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . w1 и w2-тензоры с весами, а b1 и b2-тензоры со смещениями. Параметры изначально инициализируются случайным образом, как и в предыдущем разделе: . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . res.max(tensor(0.0)) - заменить каждое отрицательное число нулем и данная функция представлена в PyTorch - F.relu (rectified linear unit.). . plot_function(F.relu) . Три строки кода, которые представлены ниже - слои. Первый и третий - линейные слои, а вторая строка кода - нелинейность или функция активации. Скорость дальше немного повысим, а количество эпох увеличим: . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.333021 | 0.396112 | 0.512267 | 00:00 | . 1 | 0.152461 | 0.235238 | 0.797350 | 00:00 | . 2 | 0.083573 | 0.117471 | 0.911678 | 00:00 | . 3 | 0.054309 | 0.078720 | 0.940628 | 00:00 | . 4 | 0.040829 | 0.061228 | 0.956330 | 00:00 | . 5 | 0.034006 | 0.051490 | 0.963690 | 00:00 | . 6 | 0.030123 | 0.045381 | 0.966634 | 00:00 | . 7 | 0.027619 | 0.041218 | 0.968106 | 00:00 | . 8 | 0.025825 | 0.038200 | 0.969087 | 00:00 | . 9 | 0.024441 | 0.035901 | 0.969578 | 00:00 | . 10 | 0.023321 | 0.034082 | 0.971541 | 00:00 | . 11 | 0.022387 | 0.032598 | 0.972031 | 00:00 | . 12 | 0.021592 | 0.031353 | 0.974485 | 00:00 | . 13 | 0.020904 | 0.030284 | 0.975466 | 00:00 | . 14 | 0.020300 | 0.029352 | 0.975466 | 00:00 | . 15 | 0.019766 | 0.028526 | 0.975466 | 00:00 | . 16 | 0.019288 | 0.027788 | 0.976448 | 00:00 | . 17 | 0.018857 | 0.027124 | 0.977429 | 00:00 | . 18 | 0.018465 | 0.026523 | 0.978410 | 00:00 | . 19 | 0.018107 | 0.025977 | 0.978901 | 00:00 | . 20 | 0.017777 | 0.025479 | 0.978901 | 00:00 | . 21 | 0.017473 | 0.025022 | 0.979392 | 00:00 | . 22 | 0.017191 | 0.024601 | 0.980373 | 00:00 | . 23 | 0.016927 | 0.024213 | 0.980373 | 00:00 | . 24 | 0.016680 | 0.023855 | 0.981354 | 00:00 | . 25 | 0.016449 | 0.023521 | 0.981354 | 00:00 | . 26 | 0.016230 | 0.023211 | 0.981354 | 00:00 | . 27 | 0.016023 | 0.022922 | 0.981354 | 00:00 | . 28 | 0.015827 | 0.022653 | 0.981845 | 00:00 | . 29 | 0.015641 | 0.022401 | 0.981845 | 00:00 | . 30 | 0.015463 | 0.022165 | 0.981845 | 00:00 | . 31 | 0.015294 | 0.021944 | 0.983317 | 00:00 | . 32 | 0.015132 | 0.021736 | 0.982826 | 00:00 | . 33 | 0.014977 | 0.021541 | 0.982826 | 00:00 | . 34 | 0.014828 | 0.021357 | 0.982336 | 00:00 | . 35 | 0.014686 | 0.021184 | 0.982336 | 00:00 | . 36 | 0.014549 | 0.021019 | 0.982336 | 00:00 | . 37 | 0.014417 | 0.020864 | 0.982336 | 00:00 | . 38 | 0.014290 | 0.020716 | 0.982336 | 00:00 | . 39 | 0.014168 | 0.020576 | 0.982336 | 00:00 | . Процесс обучения записывается в learn.recorder, а таблица с результатамиа хранится в values.В связи с чем построить график точности можно следующим образом:: . plt.plot(L(learn.recorder.values).itemgot(2)); . И теперь можно посмотреть окончательную точность: . learn.recorder.values[-1][2] . 0.98233562707901 . &#1048;&#1076;&#1077;&#1084; &#1075;&#1083;&#1091;&#1073;&#1078;&#1077; . Вот что происходит, когда обучаем при помощи 18-слойной модели используя тот же самый подход, что и в предыдущем разделе. . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.089869 | 0.028326 | 0.990186 | 00:09 | . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1089;&#1086;&#1073;&#1089;&#1090;&#1074;&#1077;&#1085;&#1085;&#1086;&#1081; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; &#1076;&#1083;&#1103; &#1074;&#1089;&#1077;&#1093; &#1095;&#1080;&#1089;&#1077;&#1083; . datapath = untar_data(URLs.MNIST) . data = ImageDataLoaders.from_folder(path=datapath, train=&#39;training&#39;, test=&#39;testing&#39;, valid_pct=0.2) . learn = cnn_learner(data, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.097839 | 0.118689 | 0.984071 | 00:45 | .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2020/12/13/04-mnist-basics.html",
            "relUrl": "/russian/fast.ai/solution/2020/12/13/04-mnist-basics.html",
            "date": " • Dec 13, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Russian - Fastbook Chapter 4 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы к четвертой части курса Deep Learning 2020 на Fast.ai. Напоминаю, что третья часть отдана этики. . 1. Как представлено на компьютере изображение в градиентах серого? Как цветное изображение? . Изображения представлены массивами со значениями пикселей, представляющими содержимое изображения. Для изображений в оттенках серого используется 2-мерный массив с пикселями, представляющими значения в оттенках серого, в диапазоне от 0 до 256. Значение 0 - белый цвет, а значение 255 - черный, а между ними различные оттенки серого. Для цветных изображений обычно используются три цветовых канала (красный, зеленый, синий), причем для каждого канала используется отдельный 256-диапазонный 2D-массив. Значение пикселя 0 снова представляет белый цвет, а 255 - сплошной красный, зеленый или синий. Три 2-D массива образуют окончательный 3-D массив (тензор ранга 3), представляющий цветное изображение. . 2. Как структурированы файлы и папки в наборе данных MNIST_SAMPLE? Почему? . Есть две подпапки, train и valid, первая содержит данные для обучения модели, вторая содержит данные для проверки подтверждения модели после каждого шага обучения. Оценка модели на валидационном наборе служит двум целям: а) сообщить о такой интерпретируемой человеком метрике, как точность (в отличие от часто абстрактных функций потерь, используемых для обучения), б) облегчить обнаружение переобучения путем оценки модели на наборе данных, на котором она не была обучена (короче говоря, модель переобучения работает все лучше на обучающем наборе, но все меньше на валидационном наборе). Конечно, каждый практик мог генерировать свои собственные тренировочные / валидационые разделения данных. Общедоступные наборы данных обычно предварительно разделяются для упрощения сравнения результатов между реализациями/публикациями. . Каждая подпапка имеет две подпапки 3 и 7, которые содержат файлы ф формате jpg для соответствующего класса изображений. Это распространенный способ организации наборов данных, состоящих из изображений. Для полного набора данных MNIST существует 10 подпапок, по одной для изображений каждой цифры. . 3. Объясните, как работает подход ”пиксельного сходства (pixel similarity)” к классификации цифр. . В подходе “сходства пикселей” мы генерируем образец для каждого класса, который хотим идентифицировать. В нашем случае мы хотим отличить изображения трех от изображений семи. Мы определяем образец трех как среднее значение по пикселям всех трех в обучающем наборе. Аналогично для семерки. Вы можете визуализировать два образца и увидеть, что они на самом деле являются размытыми версиями чисел, которые они представляют. Чтобы определить, является ли ранее нерассматриваемое изображение 3 или 7, мы вычисляем его расстояние до двух образцов (здесь: средняя абсолютная разница в пикселях). Мы говорим, что новый образ-это 3, если его расстояние до образца трех меньше, чем ддля образца семи. . 4. Что такое представление списков (list comprehension)? Теперь создайте тот, который выбирает нечетные числа из списка и удваивает их. . Списки (массивы на других языках программирования) часто генерируются с помощью цикла for. Представление списков (list comprehension) - это Питонический способ конденсирования создания списка с помощью цикла for в одно выражение. редставление списков (list comprehension) также часто будет включать условия для фильтрации. . lst_in = range(10) lst_out = [2*el for el in lst_in if el%2==1] # is equivalent to: lst_out = [] for el in lst_in: if el%2==1: lst_out.append(2*el) . 5. Что такое “тензор ранга 3”? . Ранг тензора-это число измерений, которые он имеет. Простой способ определить ранг - это количество индексов, которые вам понадобятся для ссылки на число внутри тензора. Скаляр может быть представлен как тензор ранга 0 (без индекса), вектор может быть представлен как тензор ранга 1 (один индекс, например, v[i]), матрица может быть представлена как тензор ранга 2 (два индекса, например,a[i, j]), а тензор ранга 3-это кубоид или “стек матриц” (три индекса, например,b[i,j, k]). В частности, ранг тензора не зависит от его формы или размерности, например, тензор формы 2x2x2 и тензор формы 3x5x7 имеют ранг 3. Обратите внимание, что термин “ранг” имеет различные значения в контексте тензоров и матриц (где он относится к числу линейно независимых векторов столбцов). . 6. В чем разница между тензорным рангом и формой (shape)? . Ранг - это число осей или измерений в Тензоре; форма (shape)-размер каждой оси тензора. . Как вы получаете ранг от формы? . Длина формы тензора - это его ранг. . Итак, если у нас есть изображения папки 3 из набора данных MINST_SAMPLE в Тензоре под названием stacked_threes, и мы находим его форму вот так. . In [ ]: stacked_threes.shape Out[ ]: torch.Size([6131, 28, 28]) . Нам просто нужно найти его длину, чтобы узнать его ранг. Это делается следующим образом. . In [ ]: len(stacked_threes.shape) Out[ ]: 3 . Вы также можете получить ранг тензора непосредственно с помощью ndim. . In [ ]: stacked_threes.ndim Out[ ]: 3 . 7. Что такое норма RMSE и L1? . Среднеквадратичная ошибка (RMSE), также называемая нормой L2, и средняя абсолютная разность (MAE), также называемая нормой L1, являются двумя широко используемыми методами измерения “расстояния”. Простые вычитания не работают, потому что некоторые различия положительны, а другие отрицательны и в результате они отменяют друг друга. Поэтому для правильного измерения расстояний необходима функция, которая фокусируется на величинах разностей. Проще всего было бы сложить абсолютные значения разностей, что и есть MAE. RMSE берет среднее значение квадрата (делает все положительным), а затем берет квадратный корень (отменяет возведение в квадрат). . 8. Как вы можете выполнить вычисление на тысячах чисел одновременно, во много тысяч раз быстрее, чем цикл на Python? . Поскольку циклы в Python очень медленные, лучше всего представлять операции как операции массива, а не циклически перебирать отдельные элементы. Если это можно сделать, то использование NumPy или PyTorch будет в тысячи раз быстрее, так как они используют базовый код C, который намного быстрее, чем чистый Python. Еще лучше то, что PyTorch позволяет запускать операции на GPU, которые будут иметь значительное ускорение, если есть параллельные операции, которые можно выполнить. . 9. Создайте тензор 3x3 или массив, содержащий числа от 1 до 9. Удвоьте его. Выберите в правом нижнем углу 4 цифры. . In [ ]: a = torch.Tensor(list(range(1,10))).view(3,3); print(a) Out [ ]: tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) In [ ]: b = 2*a; print(b) Out [ ]: tensor([[ 2., 4., 6.], [ 8., 10., 12.], [14., 16., 18.]]) In [ ]: b[1:,1:] Out []: tensor([[10., 12.], [16., 18.]]) . 10. Что такое бродкастинг? . Бродкастинг (broadcasting) - Научные / числовые пакеты Python, такие как NumPy и PyTorch, часто реализуют бродкастинг, который часто облегчает написание кода. В случае PyTorch тензоры с меньшим рангом расширяются, чтобы иметь тот же размер, что и тензор большего ранга. Таким образом, операции могут выполняться между тензорами с различным рангом. . 11. Обычно метрики рассчитываются с использованием обучающего набора или набора проверки? Почему? . Метрики обычно рассчитываются на основе набора валидации. Поскольку набор валидации является данными, которые не использовались для обучения модели, оценка метрик в наборе валидации лучше для того, чтобы определить, есть ли какое-либо переобучение и насколько хорошо модель могла бы обобщить, если бы ей были даны аналогичные данные. . 12. Что такое SGD? . SGD, или стохастический градиентный спуск, - это алгоритм оптимизации. В частности, SGD-это алгоритм, который будет обновлять параметры модели для того, чтобы минимизировать заданную функцию потерь, которая была оценена по прогнозам и цели. Ключевая идея SGD (и многих алгоритмов оптимизации, если на то пошло) заключается в том, что градиент функции потерь дает представление о том, как эта функция потерь изменяется в пространстве параметров, которое мы можем использовать, чтобы определить, как лучше всего обновить параметры, чтобы минимизировать функцию потерь. Это то, что делает SGD. . 13. Почему SGD использует мини-пакеты? . Нам нужно вычислить нашу функцию потерь (и наш градиент) на одной или нескольких точках данных. Мы не можем рассчитывать на всех наборах данных из-за компьютерных ограничений и ограничений по времени. Однако если мы будем перебирать каждую точку данных, градиент будет неустойчивым и неточным и не пригодным для обучения. В качестве компромисса мы рассчитываем средние потери для небольшого подмножества набора данных за один раз. Это подмножество называется мини-пакетом. Использование мини-пакетов также более эффективно с вычислительной точки зрения, чем отдельные элементы на графическом процессоре. . 14. Какие 7 шагов в SGD машинного обучения? . Инициализируйте параметры-случайные значения часто работают лучше всего. | Рассчитать прогнозы-это делается на тренировочном наборе, по одному мини-пакету за раз. | Вычислить потери – вычисляется средняя потеря по минипакету** | Вычисление градиентов-это аппроксимация того, как должны изменяться параметры, чтобы минимизировать функцию потерь | Шаг Весов-обновление параметров на основе вычисленных Весов | Повторить процесс | Остановка-на практике это либо основано на временных ограничениях, либо обычно основано на том, когда потери в обучении/валидации и показатели перестают улучшаться. | 15. Как мы инициализируем веса в модели? . Случайные веса работают довольно хорошо. . 16. Что такое “потеря”? . Функция потерь будет возвращать значение, основанное на заданных прогнозах и целевых показателях, где более низкие значения соответствуют лучшим прогнозам модели. . 17. Почему мы не можем всегда использовать высокую скорость обучения? . Потери могут “отскакивать” вокруг (колебаться) или даже расходиться, так как оптимизатор делает слишком большие шаги и обновляет параметры быстрее, чем это должно быть. . 18. Что такое “градиент”? . Градиенты говорят нам, насколько мы должны изменить каждый вес, чтобы сделать нашу модель лучше. По сути, это мера того, как изменяется функция потерь при изменении Весов модели (производной). . 19. Вам нужно знать, как самостоятельно вычислять градиенты? . Ручной расчет градиентов не требуется, так как библиотеки глубокого обучения автоматически рассчитают градиенты для вас. Эта функция известна как автоматическая дифференциация. В PyTorch, если requires_grad=True, градиенты могут быть возвращены методом обратного вызова: a.backward() . 20. Почему мы не можем использовать точность как функцию потерь? . Функция потерь должна изменяться по мере корректировки Весов. Точность меняется только в том случае, если меняются предсказания модели. Таким образом, если в модели есть небольшие изменения, которые, скажем, повышают уверенность в предсказании, но не изменяют предсказание, точность все равно не изменится. Таким образом, градиенты будут равны нулю везде, кроме тех случаев, когда фактические прогнозы изменяются. Таким образом, модель не может учиться на градиентах, равных нулю, и веса модели не будут обновляться и не будут обучаться. Хорошая функция потерь дает немного лучшие потери, когда модель дает немного лучшие прогнозы. Немного лучшие предсказания означают, что модель более уверена в правильности предсказания. Например, предсказание 0,9 против 0,7 для вероятности того, что изображение MNIST является 3, было бы немного лучшим предсказанием. Функция потерь должна отражать это. . 21. Нарисуйте сигмовидную функцию. Что особенного в ее форме? . . Сигмоидная функция-это гладкая кривая у которой все значения лежат между 0 и 1. У функций потерь значения вероятности или доверительного уровня лежат между 0 и 1, поэтому на конце модели используется сигмоидная функция. . 22. В чем разница между потерями и метриками? . Ключевое различие заключается в том, что метрики служат для человеческого понимания, а потери - автоматизированного обучения. Чтобы потеря была полезна для обучения, она должна иметь значимую производную. Многие показатели, такие как например точность, не подходят. Метрики в свою очередь это цифры которые волнуют людей и отражают производительность модели. . 23. Что является функцией для вычисления новых весов с использованием скорости обучения? . Функция оптимизации шага . 24. Что класс DataLoader делает? . Класс DataLoader может взять любую коллекцию Python и превратить ее в итератор для пакетов. . 25. Напишите псевдокод, показывающий основные шаги, предпринятые каждой эпохой для SGD. . for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr . 26. Создайте функцию, которая при передаче двух аргументов [1,2,3,4] и ‘abcd’ возвращает [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)] . Что особенного в этой структуре выходных данных? . def func(a,b): return list(zip(a,b)) . Эта структура данных полезна для моделей машинного обучения, когда вам нужны списки кортежей, где каждый кортеж будет содержать входные данные и метку. . 27. Что делает view в PyTorch? . Он изменяет форму тензора, не изменяя его содержания. . **28. Какая функция у параметра “смещения(bias)” в нейронной сети? Зачем он нам нужен? . Без параметров смещения, если на вход подается нуль, выход всегда будет равен нулю. Поэтому использование параметров смещения добавляет модели дополнительную гибкость. . 29. Что оператор @ делает в python? . Это оператор умножения матриц. . 30. Что делает метод backward делает? . Этот метод возвращает текущие градиенты. . 31. Почему мы должны обнулять градиенты? . PyTorch будет добавлять градиенты переменных в любые из ранее сохраненных градиентов. Если функция цикла обучения вызывается несколько раз, не обнуляя градиенты, градиент текущих потерь будет добавлен к ранее сохраненному значению градиента. . 32. Какую информацию мы должны передать Learner? . Нам нужно передать DataLoader, модель, функцию оптимизации, функцию потерь и, возможно, метрики для вывода. . 33. Покажите python или псевдокод для основных шагов обучающего цикла. . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() for i in range(20): train_epoch(model, lr, params) . 34. Что такое “ReLU”? Нарисуйте график для значений от -2 до +2. . ReLU просто означает “заменить любые отрицательные числа нулем”. Это обычно используемая функция активации. . . 35. Что такое “функция активации”? . Функция активации-это еще одна функция, входящая в состав нейронной сети, цель которой-обеспечить нелинейность модели. Идея состоит в том, что без функции активации есть несколько линейных функций вида y=mx+b. Однако ряд линейных слоев эквивалентен одному линейному слою, поэтому наша модель может подогнать только линию к данным. Вводя нелинейность между линейными слоями,это уже не так. Каждый слой несколько отделен от остальных слоев, и теперь модель может соответствовать гораздо более сложным функциям. На самом деле можно математически доказать, что такая модель может решить любую вычислимую задачу с произвольно высокой точностью, если модель достаточно велика с соответствующими весами. Это известно как универсальная аппроксимационная теорема. . *36. В чем разница между *F.relu и nn.ReLU? . F.relu - это функция Python для активации relu. С другой стороны, nn.ReLU-это модуль PyTorch. Это означает, что класс Python может быть вызван как функция таким же образом, как и F.relu. . **37. Универсальная аппроксимационная теорема показывает, что любая функция может быть аппроксимирована настолько близко, насколько это необходимо, используя только одну нелинейность. Так почему же мы обычно используем больше? . Использование более чем одной нелинейности дает практические преимущества. Мы можем использовать более глубокую модель с меньшим количеством параметров, лучшей производительностью, более быстрым обучением и меньшими требованиями к вычислениям и памяти. .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2020/12/09/fastai-chapter4-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2020/12/09/fastai-chapter4-solution.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Russian - Fastbook Chapter 2 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы ко второй части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, как и в первой части прошу писать в коментариях - поправлю. . 1. Приведите пример где модель классификации медведя может работать плохо из-за отличий в структуре или стиле обучающей выборки. . Существует много случаев, когда модель классификации медведей может ошибаться, причинной чего так же может являться отсутсвия экземпляров в обучающих данных: . изображение частично закрыто | ночные фото | низкое разрешение | объект далеко от камеры | приоритет характеритик (например цвета) | . 2. Где у языковой модели в настоящее время наблюдаются сложности? . Языковые модели могут генерировать тексты, которые например имитируют ответы или стили различных авторов. Но при этом их не интерисует коректность полученного результатав человеческом смысле. То есть используя фактическую базу данных трудно получитьправильные ответы, но при этом он будет казаться убедительным. Это может быть очень опасно, так как обыватель может принять все за “чистую” монету. . 3. Каких негативных социальных последствий может быть причина модель генерации текстов? . Способность моделей генерации текста генерировать очень убедительные ответы может быть использовано в массовом масштабе для распространения дезинформации (“фейковых новостей”) и подогрева конфликтов. . При обучении модели усиливают некоторые характеристики (например, гендерную и расовую предвзятость) и в результате выдвет цепочку необъективных результатов. . 4. В ситуациях, когда модель может ошибаться, и это может принести значительный вред, что является хорошей альтернативой автоматизации процесса? . Прогнозы контролируются экспертами, которые оценивают результат и принимают решение о следующем шаге. Особенно это актуально для применения машинного обучения при постановке медицинских диагнозов. Например, модель машинного обучения для идентификации инсультов при компьютерной томографии может выявлять высокоприоритетные случаи, которые должны быть расмотрены незамедлительно, в то время как другие случаи расмотраваются врачами в порядке общей очереди. За счет этого повышвется эффективность работы. . 5. В какого типа табличных данных глубокое обучение эффективно? . Глубокое обучение хорошо подходит для анализа табличных данных, включающих естественный язык, или категориальные столбцы (содержащих большее количество дискретных величин, таких как почтовый индекс). . 6. Что является ключевым недостатком использования модели глубокого обучения для рекомендательных систем? . Машинное обучение для рекомендательных систем часто говорит только о том, какие продукты могут быть интересны пользователю, и мпри этом фактически не являться рекомендацией. Например, пользователю рекомендуются книги которые он уже прочел на основании того, что он уже купил книги этого автора или другие товары которые уже приобретены. . 7 Что такое трансмиссионный (Drivetrain) подход? . . 8. Как этапы подхода Drivetrain соотносятся с системой рекомендаций . Цель механизма рекомендаций состоит в том, чтобы стимулировать дополнительные продажи за счет удовлетворения потребностей клиента в их предпочтениях. Рычагом является ранжирование рекомендаций. Данные необходимо для создания новых рекомендаций и стимулирования новых продаж. Потребуется провести множество рандомизированных экспериментов, чтобы собрать данные из большого объема рекомендаций для широкого круга клиентов. Этот шаг, который делают немногие организации; но без этого него нет информации, необходимой для реальной оптимизации рекомендаций, основанных на вашей истинной цели (больше продаж!). . 9. Создайте модель распознавания образа, используя данные, которые вам интересны, и разверните ее в интернете. . Ввыполнить просто второй ноутбук. Можно взять свою выборку, но я остановился на варианте медведей как в работе. . 10. Что такое DataLoaders(загрузчик данных)? . Класс Dataloader-это класс, который передает данные в fastai модель. По сути, это класс, который хранит необходимые объекты Dataloader (обычно наборs train и validation). . 11. Какие четыре вещи нам нужны для fastai? чтоб создать DataLoaders? . с какими типами данных мы работаем | как получить список элементов | какие метки для этих элементы | как создать validation набор | . 12. Что делает параметр split в DataBlock? . Используется для разделения данных на подмножества (как правило обучающие и проверочные). Например, для случайного разбиения данных можно использовать предопределенный класс RandomSplitter из fastai, передавая в данный класс значение доли данных, используемых для проверки. . 13. Как мы можно получить при случайном разделении один и тот же набор для проверки? . Оказывается, наши компьютеры не могут действительно генерировать случайные числа. Вместо этого они используют процесс, известный как псевдослучайный генератор. Однако этот процесс можно контролировать с помощью случайного seed. Если установить его начальное значение, то генератор псевдослучайных чисел будет генерировать “случайные” числа одним и тем же образом и они будут совпадать для каждого запуска. Определяя seed мы можем генерировать случайное разбиеник, которое всегда дает один и тот же набор для проверки. . 14. Какие буквы чаще всего используются для обозначения независимых и зависимых переменных? . х для независимых и y для зависимых. . 15. В чем разница между crop, pad и squish Resize ()? Когда и какой из них использовать? . crop изображения гризли: crop(обрезка) - это метод Resize() по умолчанию. В результате обрезается изображение так, чтобы оно соответствовало квадратной форме требуемого размера заполняя всю ширину и высоту. Это может привести к потере некоторых важных деталей. Например, если мы пытаемся распознать породу собаки или кошки, мы можем в конечном итоге обрезать ключевую часть тела или лица, необходимую для различения похожих пород. . | pad изображения гризли: pad-это альтернативный метод Resize (), который заполняет недостающие участки изображения нулями (которые отображаются черным цветом при просмотре изображений). Если мы заполняем изображения, то у нас есть много пустого пространства, которое тратиться впустую при вычислении для нашей модели и приводит к более низкому эффективному разрешению для той части изображения, которую мы фактически используем. . | squish изображения гризли: squish-это еще один альтернативный метод изменения размера, который либо сжимает, либо растягивает изображение. Это приводит к тому, что изображение принимает нереалистичную форму, что приведет к модели, которая запоминает вещи иначе, чем они есть на самом деле, и как следствие приводит к снижению точности. . | . Выбор метода изменения размера зависит от задачи и набора данных. Например, если объекты в изображениях набора данных занимают все изображение целиком и обрезка может привести к потере ценной информации, то более правильным будет использование методов сжатие и заполнения. . Другим хорошим методом является RandomResizedCrop, в котором обрезка изображения происходит случайным образом. В каждую эпоху модель будет видеть различную часть одного и того же изображения, что будет учтено при обучении. . 16. Что такое приращение данных? Зачем это нужно? . Приращение данных - создание случайных вариаций входных данных таким образом, что они кажутся разными, но при этом не изменяется их значение. Примеры включают в себя переворачивание, поворот, деформацию перспективы, изменение яркости и т. д. Приращение данных полезно для модели, чтобы лучше понять основную концепцию того, какие есть объекты и как интерисующие объекты представлены в изображениях. Таким образом, увеличение объема данных позволяет моделям машинного обучения проыодить обобщение . Это особенно важно, когда маркировка данных может быть медленной и дорогостоящей. . 17. В чем разница между item_tfms и batch_tfms? . item_tfms — используется для определения типа преобразований, применяемых к каждому изображению на процессоре. | batch_tfms — используется для определения типа преобразований, применяемых к каждому пакету на графическом процессоре. | . 18. ЧТо такое матрица ошибок(confusion matrix)? . Матрица ошибок - это сравнения сделанных предсказаний относительно истинных значений. Строки матрицы представляют собой истинные значения, а столбцы-предсказаные. Таким образом, количество изображений в диагональных элементах представляет собой количество правильно классифицированных изображений, в то время как вне диагональные элементы являются неправильно классифицированными изображениями. Матрицы ошибок предоставляют полезную информацию о том, насколько хорошо работает модель и для каких значений она склонна делать ошибки. . 19. Что export делает? . export сохраняет как архитектуру, так и обученные параметры архитектуры нейронной сети. Он также сохраняет то, как определен DataLoaders. . 20. Как это называется, когда мы используем модель для получения прогнозов вместо обучения? . Inference(вывод) . **21. Что такое виджеты IPython? . Виджеты IPython - это совместные функции JavaScript и Python, которые позволяют нам создавать и взаимодействовать с компонентами GUI (графи́ческий интерфе́йс по́льзователя) непосредственно в ноутбуке Jupyter. Примером этого может служить кнопка загрузки, которая может быть создана с помощью питоновской функции widgets.FileUpload(). . 22. Когда можно использовать процессор для развертывания? Когда лучше GPU? . Графические процессоры лучше всего подходят для выполнения идентичной работы параллельно. Если вы будете анализировать отдельные фрагменты данных за один раз (например, одно изображение или одно предложение), то процессоры могут быть более экономичными, особенно при том, что рыночной конкуренции за процессорные серверы больше по сравнению с конкуренцией за графические сервера. Графические процессоры можно использовать, если вы собираете ответы пользователей в пакеты за один раз и выполняете вывод на основании этого пакета. В связи с чем это будет вызывать задержку по времени в ожиданиях прогнозов модели. Кроме того, существует много других задач, когда ощущаются преимущества использования графического процессора, такие как управление памятью и работа с очередями из пакетов. . 23. Каковы недостатки развертывания приложения на сервере, а не на клиентском устройстве, таком как телефон или ПК? . Приложение для работы потребует подключения к сети, и у него будет дополнительная по времени задержки из-за сети при отправке входных данных и возврате результатов. Кроме того, отправка личных данных на сетевой сервер может привести к проблемам персональной безопасности. . 24. Три проблемы, которые могут возникнуть при внедрении системы предупреждения о медведях на практике? . Модель, которую мы обучили, скорее всего, будет работать плохо в следующих случаях: . Обработка ночных изображений | Работа с изображениями с низким разрешением (например, некоторые изображения смартфонов) | Запаздывание получение результатов что приводит к их бесполезности | **25. Что такое “данные вне домена”(“out of domain data”)? . Данные, которые принципиально отличаются в каком-то аспекте по сравнению с обучающими данными модели. Например, детектор объектов, который был обучен исключительно с помощью дневных фотографий, получает фотографию, сделанную ночью. . **26. Что такое “сдвиг домена”(“domain shift”)? . Это происходит, когда тип данных постепенно меняется с течением времени. Например, страховая компания использует модель глубокого обучения как часть своего алгоритма ценообразования, но со временем ее клиенты будут отличаться, причем исходные данные обучения не будут репрезентативны текущим данным. . 27. Каковы 3 шага в процессе развертывания? . Ручной процесс (Manual process) – модель запускается параллельно и непосредственно не управляет никакими действиями. Люди проверяют выходные данные модели. | Развертывание с ограниченным охватом (Limited scope deployment) – Область применения модели ограничена и тщательно контролируется. Например, ограничение выполнено по географическому и временному принципу, и в свою очередь это строго контролируется. | Постепенное расширение (Gradual expansion) – охват модели постепенно увеличивается и в то же время системы отчетности внедряются для проверки любых существенных изменений в предпринимаемых действиях по сравнению с процессом, если бы он выполнялся вручную (т. е. модели должны работать аналогично людям, если только уже не ожидается, что они будут выполнять лучше). | 28. Для своего проекта проведите мысленный эксперимент “что произойдет, если он пройдет очень, очень хорошо?” . Выполняется самостоятельно. . **29. Заведите блог и напишите свой первый пост в блоге. Например напишите о том, что, по вашему мнению, может быть полезно глубокое обучение в интересующей вас области. . Выполняется самостоятельно. .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2020/11/20/fastai-chapter2-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2020/11/20/fastai-chapter2-solution.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Russian - Solution Lesson 2 on Fast.ai",
            "content": "В этом блокоте рассмотрено решение по второму и началу третьего урока на fast.ai. Сначало как обычно устанавливаются и подключаются необходиые библиотеки. . Первоначально загружаются изображения с помощью Bing Image Search. Для этого регистрируюсь в Microsoft (так как у меня уже есть учетка, то этого не потребовалось) для получения бесплатной учетной записи. Предоставляется ключ, который вставляю в место XXX . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;XXX&#39;) . search_images_bing . &lt;function fastbook.search_images_bing(key, term, min_sz=128)&gt; . Но в связи с изменениями формата запросов и поиска изображений при помощи Bing со стороны Microsoft запрос, который использовался в ферале 2020 года часто выдает ошибку. Из-за чего развернулась большая дискуссия на форуме fast.ai. Я предложил свое решение, но оно выглядело достаточно коряво. На основании его один из пользователей сделал новую функцию search_images_bing. . Успешно загрузжены URL-адреса 150 медведей гризли (или, по крайней мере, изображения, которые Bing Image Search находит для данного поискового запроса). Можно посмотреть на один из них: . dest = &#39;images/grizzly.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . Для того, чтобы скачать все фото согласно поисковому запросу и поместить их в отдельные папки используется функция download_images из пакета fastai: . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . В результате в папках по 150 изображений . fns = get_image_files(path) fns . (#441) [Path(&#39;bears/grizzly/00000006.jpg&#39;),Path(&#39;bears/grizzly/00000005.jpg&#39;),Path(&#39;bears/grizzly/00000008.jpg&#39;),Path(&#39;bears/grizzly/00000009.jpg&#39;),Path(&#39;bears/grizzly/00000002.jpg&#39;),Path(&#39;bears/grizzly/00000003.jpg&#39;),Path(&#39;bears/grizzly/00000010.jpg&#39;),Path(&#39;bears/grizzly/00000000.jpg&#39;),Path(&#39;bears/grizzly/00000012.jpg&#39;),Path(&#39;bears/grizzly/00000007.jpg&#39;)...] . После этого проверяю загруженные файлы на наличие поврежденных . failed = verify_images(fns) failed . (#14) [Path(&#39;bears/black/00000026.jpg&#39;),Path(&#39;bears/black/00000032.jpg&#39;),Path(&#39;bears/black/00000039.jpg&#39;),Path(&#39;bears/black/00000122.jpg&#39;),Path(&#39;bears/black/00000119.jpg&#39;),Path(&#39;bears/teddy/00000010.jpg&#39;),Path(&#39;bears/teddy/00000004.jpg&#39;),Path(&#39;bears/teddy/00000055.jpg&#39;),Path(&#39;bears/teddy/00000047.jpg&#39;),Path(&#39;bears/teddy/00000065.jpg&#39;)...] . Чтобы удалить все &quot;бракованные&quot; изображения использую функцию unlink для каждого из них. В связи с тем, что verify_images возвращает объект типа L и в нем есть метод map, то переданная функция исполниться для каждого элемента коллекции: . failed.map(Path.unlink); . Все объекты хранятся в классе DataLoader, которые ему передаются. В результате они будут доступны как train и valid.С помощью DataBlock можно настроить каждый этап создания загрузчиков данных. В него передаются следующие параметры: . blocks=(ImageBlock, CategoryBlock) - кортеж, где содержится информация какие тип использовать для независимых и зависимых переменных. Независимая переменная-это то, что используется для предсказания, а зависимая переменная - результат. В этом случае независимыми переменными являются фото, а зависимыми переменными - категории (тип медведя) для каждого фото; | get_items=get_image_files - говорим, что для DataLoader данными будут являться пути к файлам. Функция get_image_files получает расположение и возвращает список всех изображений расположенных по этому пути (по умолчанию рекурсивно); | splitter=RandomSplitter(valid_pct=0.2, seed=42) - разделить train и valid случайным образом. Но для того, чтоб каждый раз разбивка не менялась, то фиксируется seed; | get_y=parent_label - функция, которая должна создать метки (зависимые переменные). parent_label-это функция, которая просто получает имя папки, в которой находится файл; | item_tfms=Resize(128) - преобразовать размер всех изображений к одному. | . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . Дальше передаю путь, по которому можно найти изображения: . dls = bears.dataloaders(path) . Показать некоторые из элементов при помощи метода show_batch: . dls.valid.show_batch(max_n=4, nrows=1) . Примеры двух вариантов использования метода Resize: заполнить пустые мест изображения нулями (черными полями) или растянуть/сжать их: . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . Пример использования RandomResizedCrop вместо Resize - выбирается часть изображения, а остальная обрезается. Параметр min_scale определяет размер минимальной части изображения которую нужно выбирать каждый раз, а unique=True в функции show_batch - использование одного и то же изображения. . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Трансформация изображений (поворот, переворачивание, деформация перспективы, изменение яркости и контрастности) функцией aug_transforms и двойное увеличение . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Обучение модели с RandomResizedCrop в 224 px и aug_transforms: . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.084715 | 0.166342 | 0.047059 | 00:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.171819 | 0.092201 | 0.047059 | 00:10 | . 1 | 0.129211 | 0.048695 | 0.035294 | 00:10 | . 2 | 0.095478 | 0.040643 | 0.023529 | 00:10 | . 3 | 0.075971 | 0.046714 | 0.023529 | 00:10 | . Проверка при помощи матрицы ошибок - confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Следующим шагом показываются изображения с максимальными потерями (loss) при помощи функции plot_top_losses. В заголовке каждого фото по порядку приведено следующее: предсказание, фактическая (целевая метка), потеря и вероятность. Вероятность - это уровень достоверности, от нуля до единицы, который модель присвоила своему прогнозу: . interp.plot_top_losses(5, nrows=1) . fastai включает в себя удобный графический интерфейс для очистки данных под названием ImageClassifierCleaner, который позволяет выбрать категорию и набор обучения по сравнению с проверкой и просмотреть изображения с наибольшей потерей (по порядку), а также меню, позволяющие выбирать изображения для удаления или повторной маркировки: . cleaner = ImageClassifierCleaner(learn) cleaner . После выполнения ручной разметки будут возращены индексы элементов для изменения. Чтобы удалить (разорвать связь) все изображения, выбранные для удаления, и переместить изображения, для которых мы выбрали другую категорию, запускается следующий код . После необходимо запустить повторное обучение и в результате можно добиться очень неплохих результатов. . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1086;&#1085;&#1083;&#1072;&#1081;&#1085; &#1087;&#1088;&#1080;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1103; . Вызвать функцию export, чтоб fastai сохранил модель в файл под названием export.pkl: . learn.export() . Далее проверка, что файл существует, используя метод ls, который у fastai добавлен в класс Path . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . Чтобы создать вывод из экспортированного файла используется load_learner . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Проводится проверка на одном из изображений, которое уже использовалось ранее . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, TensorImage(1), TensorImage([3.8019e-06, 1.0000e+00, 1.0876e-07])) . В результате получили три значения: предсказанную категорию, индекс предсказанной категории и вероятности каждой категории. . Последние два основаны на порядке категорий в vocab DataLoaders: . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; Notebook App &#1087;&#1088;&#1080;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1103; &#1085;&#1072; &#1086;&#1089;&#1085;&#1086;&#1074;&#1077; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . Создать виджет загрузки файлов: . btn_upload = widgets.FileUpload() btn_upload . Теперь мы можем передать изображение, но по факту для упрощения загружаем файл: . img = PILImage.create(btn_upload.data[-1]) . Для показа изображениия используется виджет Output: . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . Тогда получаем прогнозы и используем Label чтобы их показать: . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Создаем кнопку, чтобы сделать классификацию: . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . Создаем обработчик событий click . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . Поместим все в коробку (VBox) для завершения GUI: . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . &#1056;&#1072;&#1079;&#1074;&#1086;&#1088;&#1086;&#1090; Notebook &#1074; &#1088;&#1077;&#1072;&#1083;&#1100;&#1085;&#1086;&#1077; &#1087;&#1088;&#1080;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1080; . Перед тем, как развернуть приложение, надо установить дополнительный пакет и сделать настройку. Но при настройке у меня выскакивала ошибка. На форуме нашел подсказку и привожу ниже уже исправленный вариант. . После этого я сделал сокращенный вариант своего блокнота и разместил все в репрозитории на github. Первые мои настройки приложения через Binder оканчивались неудачей. После пойска на форумах и на самом сайте попытался настройть через Heroku. Но и там не все так гладко - надо пересобирать с устаревшими библиотеками. Но как говориться - лень - двигатель прогресса: . Я взял в одном из репрозиториев файл requirements.txt (точнее скопировал из него четыре строчки с версиями библиотек). | Прочитал внимательно учебник и заметил ошибку, которую я допускал: помимо ссылки на github репрозиторий ОБЯЗАТЕЛЬНО в Path to notebook file необходимо File сменить на URL и прописать там следующий путь: /voila/render/bear_classifier.ipynb. Может можно как и по другому, но я не испытывал. | В результате у вас получается следующее. Если вдруг не запуститься, значит опять что-то перенастроили. Но в результате должен открываться пустой сайт с кнопкой для загрузке изображения. После загрузки появляется фото, ответ и его вероятность. .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2020/11/20/02-production.html",
            "relUrl": "/russian/fast.ai/solution/2020/11/20/02-production.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "English - Using machine learning to predict gold mining stock prices",
            "content": "As a basis, I took a notebook published on colab for oil. This notebook examines the analysis of gold prices and shares of gold mining companies using machine analysis methods: linear regression, cluster analysis, and random forest. I immediately warn you that this post does not attempt to show the current situation and predict the future direction. Just like the author for oil, this article does not aim to raise or refute the possibilities of machine learning for analyzing stock prices or other tools. I upgraded the code for gold research in order to encourage those who are interested in further reflection and listen to constructive criticism in their address. . pip install yfinance --upgrade --no-cache-dir . Collecting yfinance Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz Requirement already satisfied, skipping upgrade: pandas&gt;=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.4) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0) Requirement already satisfied, skipping upgrade: multitasking&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading https://files.pythonhosted.org/packages/64/28/0b761b64ecbd63d272ed0e7a6ae6e4402fc37886b59181bfdf274424d693/lxml-4.6.1-cp36-cp36m-manylinux1_x86_64.whl (5.5MB) |████████████████████████████████| 5.5MB 6.9MB/s Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.6.20) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... done Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=2b1a24b9e8937bf5603faead14841f6f3e045d79bd13bf0e76bc189db75a8640 Stored in directory: /tmp/pip-ephem-wheel-cache-5al_gsvn/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c Successfully built yfinance Installing collected packages: lxml, yfinance Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.1 yfinance-0.1.55 . import yfinance as yf import pandas as pd import numpy as np import seaborn as sns from sklearn import metrics import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression . 1. Loading data . For the price of gold, take the value of the exchange-traded investment Fund SPDR Gold Trust, whose shares are 100% backed by precious metal. The quotes will be compared with the prices of gold mining companies &#39; shares: . Newmont Goldcorp (NMM) | Barrick Gold (GOLD) | AngloGold Ashanti (AU) | Kinross Gold (KGC) | Newcrest Mining (ENC) | Polyus (PLZL) | Polymetal (POLY) | Seligdar (SELG) | . gold = pd.DataFrame(yf.download(&quot;GLD&quot;, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed . gold = gold.reset_index() gold.columns = [&quot;Date&quot;,&quot;gold_price&quot;] gold[&#39;Date&#39;] = pd.to_datetime(gold[&#39;Date&#39;]) gold.head() . Date gold_price . 0 2010-01-04 | 109.800003 | . 1 2010-01-05 | 109.699997 | . 2 2010-01-06 | 111.510002 | . 3 2010-01-07 | 110.820000 | . 4 2010-01-08 | 111.370003 | . It is necessary to move the price of gold, as we will be interested in how yesterday&#39;s price affected today&#39;s stock price. . gold[&quot;gold_price&quot;] = gold[&quot;gold_price&quot;].shift(1) . shares=[&quot;NMM.SG&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,&quot;PLZL.ME&quot;,&quot;POLY.ME&quot;,&quot;SELG.ME&quot;] data= yf.download(shares, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . data = data.reset_index() data.head() . Date AU GOLD KGC NCM.AX NMM.SG PLZL.ME POLY.ME SELG.ME . 0 2010-01-04 | 39.698944 | 34.561649 | 18.105721 | 33.237167 | 26.924570 | NaN | NaN | NaN | . 1 2010-01-05 | 40.320408 | 34.989510 | 18.594805 | 33.901924 | 27.116940 | NaN | NaN | NaN | . 2 2010-01-06 | 41.601028 | 35.733963 | 19.256504 | 33.901924 | 27.289278 | NaN | NaN | NaN | . 3 2010-01-07 | 41.130215 | 35.229092 | 19.352404 | 34.298923 | NaN | NaN | NaN | NaN | . 4 2010-01-08 | 41.601028 | 35.451572 | 19.601744 | 33.421829 | 27.702093 | NaN | NaN | NaN | . data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) . all_data=pd.DataFrame() . for index in range(len(shares)): stock=pd.DataFrame() # transform the data stock=data.loc[:, (&quot;Date&quot;,shares[index])] stock[&quot;Date&quot;]=stock[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) stock.columns=[&quot;Date&quot;,&quot;share_price&quot;] test=pd.DataFrame(gold) output=stock.merge(test,on=&quot;Date&quot;,how=&quot;left&quot;) #combining two data sets stock[&quot;gold_price&quot;]=output[&quot;gold_price&quot;] stock[&#39;share_price&#39;]=pd.to_numeric(stock[&#39;share_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&#39;gold_price&#39;]=pd.to_numeric(stock[&#39;gold_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&quot;year&quot;]=pd.to_datetime(stock[&quot;Date&quot;]).dt.year #Create a column with years for subsequent filtering stock[&quot;name&quot;]=shares[index] stock = stock.dropna() #delete all NAN lines #creating a column with a scaled share price scaler=MinMaxScaler() stock[&quot;share_price_scaled&quot;]=scaler.fit_transform(stock[&quot;share_price&quot;].to_frame()) #add data to the main dataframe all_data=all_data.append(stock) #add the data . all_data_15 = all_data[(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)] all_data_15.head() . Date share_price gold_price year name share_price_scaled . 1301 2015-01-02 | 14.269927 | 113.580002 | 2015 | NMM.SG | 0.052072 | . 1302 2015-01-05 | 14.845476 | 114.080002 | 2015 | NMM.SG | 0.071190 | . 1303 2015-01-06 | 15.601913 | 115.800003 | 2015 | NMM.SG | 0.096317 | . 1304 2015-01-07 | 15.645762 | 117.120003 | 2015 | NMM.SG | 0.097773 | . 1305 2015-01-08 | 15.517859 | 116.430000 | 2015 | NMM.SG | 0.093525 | . 2. Data analysis . It is best to start analyzing data by presenting it visually, which will help you understand it better. . 2.1 Chart of gold price changes . gold[[&#39;Date&#39;,&#39;gold_price&#39;]].set_index(&#39;Date&#39;).plot(color=&quot;green&quot;, linewidth=1.0) plt.show() . 2.2. Plotting the pairplot chart for the price of Polyus and Barrick Gold shares over the past five years . palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) g = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;POLY.ME&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) g.fig.suptitle(&quot;Polyuse&quot;, y=1.08) palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) f = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;GOLD&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) f.fig.suptitle(&#39;Barrick Gold&#39;, y=1.08) plt.show() . A paired graph allows you to see the distribution of data by showing the paired relationships in the data set and the univariate distribution of data for each variable. You can also use the palette to see how this data changed in different years. . The chart is particularly interesting for 2016 and 2019, as it looks like the price of the Pole stock, Barrick Gold and the price of gold are lined up along the same line. We can also conclude from the distribution charts that the price of gold and stocks moved gradually towards higher values. . 2.3 Violinplot for the gold price . plt.figure(figsize=(10,10)) sns.set_style(&quot;whitegrid&quot;) palette=sns.cubehelix_palette(5, start=2.8, rot=0, dark=0.2, light=0.8, reverse=False) sns.violinplot(x=&quot;year&quot;, y=&quot;gold_price&quot;, data=all_data_15[[&quot;gold_price&quot;,&quot;year&quot;]], inner=&quot;quart&quot;, palette=palette, trim=True) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Price gold&quot;) plt.show() . 2.4 Violinplot for multiple shares . sns.catplot(x=&quot;year&quot;, y=&quot;share_price_scaled&quot;, col=&#39;name&#39;, col_wrap=3,kind=&quot;violin&quot;, split=True, data=all_data_15,inner=&quot;quart&quot;, palette=palette, trim=True, height=4, aspect=1.2) sns.despine(left=True) . A large fluctuation in gold prices was noted according to the charts in 2016 and 2019. As you can see from the graphs in the following figure, some companies such as Newmont Mining, Barrick Gold, AngloGold Ashanti, Newcrest Mining and Polymetal were also affected. It should also be noted that all prices are marked in the range from 0 to 1 and this may lead to inaccuracies in the interpretation. . Next, we will build distribution charts for one Russian company - Polymetal and one foreign company - Barrick Gold . sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;POLY.ME&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . It is necessary to pay attention to the distribution of the share price for the two companies and it will become clear that the shape of the density graph is the same for them. . 2.5 Charts of the dependence of the share price of various companies on the price of gold . sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,line_kws={&#39;color&#39;: &#39;blue&#39;},scatter_kws={&#39;color&#39;: &#39;grey&#39;}).set(ylim=(0, 1)) plt.show() . In fact, you won&#39;t be able to see much on these charts, although some stocks seem to have a relationship. . The next step is to try to color the charts depending on the years. . palette=sns.cubehelix_palette(5, start=2, rot=0, dark=0, light=.95, reverse=False) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,hue=&quot;year&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,palette=palette,height=4).set(ylim=(0, 1)) plt.show() . Here the picture is a little better in the sense that some companies have a data cloud stretching along a straight line in some years, which may indicate the existence of a dependency. . 3 Machine learning and prediction . I will give a definition for machine learning from Wikipedia: Machine learning is a class of artificial intelligence methods that are characterized not by direct problem solving, but by learning in the process of applying solutions to many similar problems. To build such methods, we use mathematical statistics, numerical methods, optimization methods, probability theory, graph theory, and various techniques for working with data in digital form. . Usually, machine learning algorithms can be classified into the following categories: learning with a teacher and learning without a teacher. Here is their definition from one of the sites: . Supervised learning is one of the sections of machine learning dedicated to solving the following problem. There is a set of objects (situations) and the set of possible answers (responses, reactions). There is some relationship between responses and objects, but it is unknown. Only a finite set of use cases is known — the &quot;object, response&quot; pairs, called the training sample. Based on this data, you need to restore the dependency, that is, build an algorithm that can give a fairly accurate answer for any object. To measure the accuracy of responses, a quality functional is introduced in a certain way. see the Links) . Unsupervised learning is one of the sections of machine learning. Studies a wide class of data processing problems in which only descriptions of a set of objects (training sample) are known, and it is required to detect internal relationships, dependencies, and patterns that exist between objects. Learning without a teacher is often contrasted with learning with a teacher, when each training object is given a &quot;correct answer&quot;, and you need to find the relationship between the objects and the answers. see links) . The following machine learning methods will be discussed later: . Cluster analysis | Linear regression | Random forest | . Using these algorithms, you can evaluate overvalued or undervalued stocks relative to the price of gold and possible movement on the next day. I remind you that you must be very careful and use the conclusions from this post at your own risk. I also remind you that my main goal is to show the potential of machine learning for stock valuation. . 3.1. Cluster analysis for Barrick Gold stock . Clustering is the task of dividing a set of objects into groups called clusters. Each group should contain &quot;similar&quot; objects, and objects from different groups should be as different as possible. . from sklearn.cluster import KMeans poly=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;] # We need to scale also gold price, so clustering is not influenced by the relative size of one axis. poly=pd.DataFrame(poly) poly[&#39;gold_price_scaled&#39;] = scaler.fit_transform(poly[&quot;gold_price&quot;].to_frame()) poly[&quot;cluster&quot;] = KMeans(n_clusters=5, random_state=1).fit_predict(poly[[&quot;share_price_scaled&quot;,&quot;gold_price_scaled&quot;]]) # The 954 most common RGB monitor colors https://xkcd.com/color/rgb/ colors = [&quot;baby blue&quot;, &quot;amber&quot;, &quot;scarlet&quot;, &quot;grey&quot;,&quot;milk chocolate&quot;, &quot;windows blue&quot;] palette=sns.xkcd_palette(colors) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,ci=None,palette=palette, hue=&quot;cluster&quot;,fit_reg=0 ,data=poly) plt.show() . Cluster analysis is used in a large number of machine learning tasks. But I have given it only for informational purposes, since in this form it does not bring much benefit to our analysis. . 3.2. Linear regression between Barrick Gold shares and the gold price . Next, we will build a regular linear regression using training with a teacher. The goal is to estimate the forecast of data for the last 100 days of 2019 based on data from 2018/2019 (excluding estimated ones). Training data is the data used to build the model, and test data is the data that we will try to predict. . for sh in shares: print(sh) #Data Preparation share_18=pd.DataFrame() share_18=all_data_15[(all_data_15[&#39;name&#39;]==sh)] # Get data 2018/19 share_18=share_18[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Just using 1 variable for linear regression. Split the data into training/testing sets train = share_18[:-100] test = share_18[-100:] x_train=train[&quot;gold_price&quot;].to_frame() y_train=train[&#39;share_price&#39;].to_frame() x_test=test[&quot;gold_price&quot;].to_frame() y_test=test[&#39;share_price&#39;].to_frame() regr = LinearRegression() #Create linear regression object regr.fit(x_train,y_train) #Train the model using the training sets print(&quot;Coefficients: &quot;, float(regr.coef_)) print(np.corrcoef(x_train,y_train, rowvar=False)) y_pred = regr.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # Plot outputs using matplotlib plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . NMM.SG Coefficients: 0.6629423053739908 [[1. 0.790953] [0.790953 1. ]] Mean Absolute Error: 6.063058573972694 Mean Squared Error: 39.21188296210148 Root Mean Squared Error: 6.261939233344689 . GOLD Coefficients: 0.3355465472461071 [[1. 0.67139243] [0.67139243 1. ]] Mean Absolute Error: 3.3769293704374657 Mean Squared Error: 11.756813554455096 Root Mean Squared Error: 3.4288210152259473 . AU Coefficients: 0.31252669952857776 [[1. 0.67830589] [0.67830589 1. ]] Mean Absolute Error: 2.2471377544809683 Mean Squared Error: 5.789211153877581 Root Mean Squared Error: 2.4060779608893768 . KGC Coefficients: 0.10461302060876282 [[1. 0.78266367] [0.78266367 1. ]] Mean Absolute Error: 1.0583009847297946 Mean Squared Error: 1.1523726951635975 Root Mean Squared Error: 1.073486234268329 . NCM.AX Coefficients: 0.5623005799590818 [[1. 0.79891272] [0.79891272 1. ]] Mean Absolute Error: 2.0335289996635937 Mean Squared Error: 5.836462091267656 Root Mean Squared Error: 2.415877085297937 . PLZL.ME Coefficients: 103.84435014609612 [[1. 0.60373084] [0.60373084 1. ]] Mean Absolute Error: 1315.093426667142 Mean Squared Error: 1776892.2964767825 Root Mean Squared Error: 1333.0012364873419 . POLY.ME Coefficients: 10.772023429299809 [[1. 0.63694034] [0.63694034 1. ]] Mean Absolute Error: 69.33753863275061 Mean Squared Error: 6800.525447108329 Root Mean Squared Error: 82.46529844187995 . SELG.ME Coefficients: 0.15570348678870732 [[1. 0.51630147] [0.51630147 1. ]] Mean Absolute Error: 1.8096071903165585 Mean Squared Error: 4.039450515732427 Root Mean Squared Error: 2.009838430255633 . From the above charts, we can conclude that the price of gold predicts the price of shares of foreign companies on the next day quite well. In Russian companies, this picture looks much worse. Of course, there may be a false impression about Seligdar shares. But visual analysis of the chart allows you to discard this assumption. . 3.3 Random forest on Newmont Goldcorp shares against the price of gold and shares of gold companies . Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . The random forest algorithm accepts more than one variable in the input data to predict the output data. It works very efficiently on large amounts of data, can handle many input variables, has efficient methods for estimating missing data, and many other advantages. The main disadvantages are: . Random forests is slow to generate forecasts because it has many decision trees. Whenever it makes a forecast, all the trees in the forest must make a forecast for the same given input and then vote on it. This whole process takes a long time. | the Model is difficult to interpret compared to the decision tree, where you can easily make a decision by following the path in the tree. | One of the great advantages of a random forest is that it can be used for both classification and regression problems, which make up most of today&#39;s machine learning systems. I will talk about random forests in classification, since classification is sometimes considered a building block of machine learning. Below you can see what a random forest with two trees looks like: . In addition to the gold price, we will use other variables to forecast the Newmont Goldcorp share price. This will be the share prices of other foreign gold mining companies. I know it doesn&#39;t make a lot of sense, but we just want to see how to build this type of model. This will allow us to see the impact of each of them on the final forecast.Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . from sklearn.ensemble import RandomForestRegressor # 1.- Data Preparation nmm15=pd.DataFrame() nmm15=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NMM.SG&quot;) &amp; (all_data_15[&#39;year&#39;]&gt;2016 )] nmm15=nmm15[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Load share price of other variables nmm15[&#39;GOLD&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;GOLD&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;GOLD&#39;] = nmm15[&#39;GOLD&#39;].shift(1) nmm15[&#39;AU&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;AU&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;AU&#39;] = nmm15[&#39;AU&#39;].shift(1) nmm15[&#39;KGC&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;KGC&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;KGC&#39;] = nmm15[&#39;KGC&#39;].shift(1) nmm15[&#39;NCM.AX&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NCM.AX&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;NCM.AX&#39;] = nmm15[&#39;NCM.AX&#39;].shift(1) nmm15 = nmm15.drop(nmm15.index[0]) train = nmm15[:-100] test = nmm15[-100:] x_train=train[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]] y_train=train[&#39;share_price&#39;] x_test=test[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,]] y_test=test[&#39;share_price&#39;].to_frame() # 2.- Create Randomforest object usinig a max depth=5 regressor = RandomForestRegressor(n_estimators=200, max_depth=5 ) # 3.- Train data clf=regressor.fit(x_train, y_train) # 4.- Predict! y_pred=regressor.predict(x_test) y_pred_list = list(y_pred) y_pred=pd.DataFrame(y_pred) . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_pred=plt.scatter(nmm15[&quot;gold_price&quot;], regressor.predict(nmm15[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]]), color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train,plt_pred),(&quot;train data&quot;,&quot;prediction&quot;)) plt.show() . The resulting model looks really good in addition, we must remember that Random Forest has many more parameters to configure, but the key one is the maximum depth, which is unlimited by default. Next, we&#39;ll check how this model predicts or tests data. . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . y_pred = clf.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 1.410409517520304 Mean Squared Error: 3.0995744019029483 Root Mean Squared Error: 1.7605608202794212 . importances=regressor.feature_importances_ indices=list(x_train) print(&quot;Feature ranking:&quot;) for f in range(x_train.shape[1]): print(&quot;Feature %s (%f)&quot; % (indices[f], importances[f])) f, (ax1) = plt.subplots(1, 1, figsize=(8, 6), sharex=True) sns.barplot(indices, importances, palette=&quot;BrBG&quot;, ax=ax1) ax1.set_ylabel(&quot;Importance&quot;) . Feature ranking: Feature gold_price (0.627703) Feature GOLD (0.045197) Feature AU (0.040957) Feature KGC (0.038973) Feature NCM.AX (0.247171) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0, 0.5, &#39;Importance&#39;) . By the importance of the signs, it immediately becomes clear how strong the value of gold is. . In short, I hope I was able to reveal to you the beginnings of a project on using machine learning to study stock prices, and I hope to hear your comments. .",
            "url": "https://zmey56.github.io/blog//english/machine%20learning/algotrading/2020/11/17/ml-prediction-gold-shares.html",
            "relUrl": "/english/machine%20learning/algotrading/2020/11/17/ml-prediction-gold-shares.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Russian - Fastbook Chapter 1 questionnaire solutions",
            "content": "Решил собрать ответы на русском языке на вопросы к первой части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, то пешите в коментариях - поправлю. . 1. Что Вам нужно для изучения глубокого обучения: . Много математики - неправда | Много данных - неправда | Дорогой компьютер - неправда | Докторская Степень - неправда | . 2. Назовите пять областей, где глубокое обучение лучше всего представлено сейчас: . Любые пять из следующих: . Обработка естественного языка (NLP) – ответы на вопросы, обобщение и классификация документов и т. д . Компьютерное зрение – интерпретация съемок спутников и беспилотных аппаратов, распознавание и определение лиц, субтитры к изображениям и т. д. . Медицина – обнаружение аномалий в медицинских изображениях (например, КТ, рентген, МРТ), обнаружение особенностей на съемках тканей (патология), диагностика диабетической ретинопатии и т. д. . Биология – сворачиваемость белков, классификация, задачи геномики, классификация клеток и т. д. . Генерация изображения/улучшение – раскрашивание изображений, повышая разрешение изображения (супер-разрешение), удаление шума с изображения (шумоподавление), преобразование изображения в стиле известных художников (перемешывание стилей) и т. д. . Рекомендательные системы – веб-поиск, рекомендации по продуктам и т. д. . Игра – шахматы, го и т. д . Робототехника – управление объектами в зависимости от их местоположения . Другие приложения – финансовое и логистическое прогнозирование; преобразование текста в речь; многое, многое другое. . 3. Как называлось первое устройство, которое было основано на принципе искусственного нейрона? . Персептрон Mark I построенный Фрэнком Розенблаттом . 4. Какие существуют требования к “параллельной распределенной обработке” согласно книге? . Набор процессорных блоков | Состояние активации | Выходная функция для каждого блока | Закономерность связи между блоками | Правило распространения для распространения схем активации через сеть связей | Правило активации для объединения входов, воздействующих на блок, в текущем состоянии этого блока для получения нового уровня активации для блока | Правило обучения, в соответствии с которым схема связности модифицируется опытом | Среда, в которой должна работать система | . 5. Каковы были два теоретических постулата, которые сдерживали развитие нейронных сетей? . В 1969 году Марвин Мински и Сеймур Паперт продемонстрировали в своей книге “Персептроны”, что один слой искусственных нейронов не может выучить простые, критические математические функции, такие как логический элемент XOR. Хотя впоследствии они продемонстрировали в той же книге, что дополнительные слои могут решить эту проблему, было принято во внимание только первое утверждение, что явилось первой “зимой” для ИИ. . В 1980-х годах изучались модели с двумя слоями. Теоретически можно аппроксимировать любую математическую функцию, используя два слоя искусственных нейронов. Однако на практике эти сети были слишком большими и слишком медленными. Хотя было продемонстрировано, что добавление дополнительных слоев повышает производительность, это понимание не было признано, и началась вторая зима искусственного интеллекта. В последнее десятилетие, с увеличением доступности данных и улучшением компьютерного оборудования (как в производительности процессора, так и, что более важно, в производительности графического процессора), нейронные сети наконец-то оправдали свой потенциал. . 6. Что такое графический процессор (GPU)? . GPU расшифровывается как графический процессор (также известный как видеокарта). Стандартные компьютеры имеют различные компоненты, такие как процессоры, оперативная память и т. д. Процессоры, или центральные процессоры, являются основными блоками всех стандартных компьютеров, и они выполняют инструкции, которые составляют компьютерные программы. Графические процессоры, с другой стороны, являются специализированными устройствами, предназначенными для отображения графики, особенно 3D - графики в современных компьютерных играх. Аппаратная оптимизация, используемая в графических процессорах, позволяет ему обрабатывать тысячи задач одновременно. Кстати, эти оптимизации позволяют нам запускать и обучать нейронные сети в сотни раз быстрее, чем на обычных процессорах. . 7. Откройте блокнот и выполните ячейку, содержащую: 1+1 . Что же произойдет? . В блокноте Jupyter мы можем создавать ячейки кода и запускать код в интерактивном режиме. Когда мы выполняем ячейку, содержащую некоторый код (в данном случае: 1+1), код запускает Python, а выходные данные отображаются под ячейкой кода (в данном случае: 2). . 8. Проследите за каждой ячейкой урезанной версии записной книжки для этой главы. Прежде чем выполнить каждую ячейку, угадайте, что произойдет. . Это необходимо сделать самому. . 9. Заполните онлайн-приложение Jupyter Notebook. . Это необходимо сделать самому. . 10. Почему трудно использовать традиционные компьютерные программы для распознавания изображений на фотографии? . Для нас, людей, легко идентифицировать изображения на фотографиях, например, идентифицировать кошек и собак. Это происходит потому, что подсознательно наш мозг узнал, какие черты определяют кошку или собаку. Но трудно определить набор правил для компьютерной программы, которая так же сможет делать это успешно. Можете ли вы придумать универсальное правило, чтобы определить, содержит ли фотография кошку или собаку? Как бы вы закодировали это в виде компьютерной программы? Это очень трудно, потому что кошки, собаки или другие объекты имеют большое разнообразие форм, текстур, цветов и других особенностей, и это почти невозможно вручную закодировать в компьютерной программе. . 11. Что Сэмюэль имел в виду под “распределением веса” (Weight Assignment)? . “распределением веса” относится к текущим значениям параметров модели. Артур Сэмюэл далее упоминает “автоматическое средство проверки эффективности любого текущего распределение веса” и “механизм изменения значения веса таким образом, чтобы максимизировать производительность”. Это относится к оценке и обучению модели с целью получения набора значений параметров, которые максимизируют производительность модели. . 12. Какой термин мы обычно используем в глубоком обучении для тог, что Сэмюэл назвал “Весами”? . Вместо этого мы используем термин параметры. В глубоком обучении термин “вес” имеет другое значение. (Нейронная сеть имеет различные параметры, к которым мы подгоняем наши данные. Как показано в следующих главах, существуют два типа параметров нейронной сети - веса и смещение). . 13. Нарисуйте картинку, которая обобщает взгляд Артура Сэмюэля на модель машинного обучения. . . 14. Почему трудно принять, что модель глубокого обучения делает определенный прогноз? . Это хорошо изученная тема, известная как интерпретируемость моделей глубокого обучения. Модели глубокого обучения трудно понять отчасти из-за их “глубокой” природы. Представьте себе модель линейной регрессии. Просто у нас есть некоторые входные переменные/данные, которые умножаются на некоторые веса, давая нам на выход полученное значение. Мы можем понять, какие переменные более важны, а какие менее важны, основываясь на их весах. Аналогичная логика может применяться и для небольшой нейронной сети с 1-3 слоями. Однако глубокие нейронные сети имеют сотни, если не тысячи слоев. Трудно определить, какие факторы играют важную роль в определении конечного результата. Нейроны в сети взаимодействуют друг с другом, причем выходы из одних нейронов поступают в другие нейроны. В целом, из-за сложной природы моделей глубокого обучения очень трудно понять, почему нейронная сеть делает тот или иной прогноз. . 15. Как называется теорема о том, что нейронная сеть может решить любую математическую задачу с любой точностью? . Универсальная теорема аппроксимации утверждает, что нейронные сети теоретически могут решать любую математическую функцию. Однако важно понимать, что практически, в силу ограниченности имеющихся данных и компьютерного оборудования, обучить любую модель практически невозможно. Но мы можем подойти к решению очень близко! . 16. Что вам нужно для того, чтобы обучить модель? . Вам понадобится архитектура для задачи. Вам понадобятся данные для ввода в вашу модель. Для большинства случаев использования глубокого обучения вам понадобятся метки для ваших данных, чтобы сравнить предсказания вашей модели. Вам понадобится функция потерь, которая будет количественно измерять производительность вашей модели. И вам нужен способ обновить параметры модели, чтобы улучшить ее производительность (это называется оптимизация). . 17. Как цикл обратной связи может повлиять на внедрение прогностической полициской модели? . В прогностической полицейской модели мы могли бы получить положительную обратную связь, что привело бы к очень предвзятой модели с небольшой прогностической силой. Например, нам может понадобиться модель, которая предсказывала бы преступления, но мы используем информацию об арестах в качестве данных . Однако сами эти данные несколько искажены из-за предвзятости существующих полицейских процессов контроля. Обучение с этими данными приводит к необъективной модели. Правоохранительные органы могли бы использовать эту модель для определения того, где сосредоточить полицейскую деятельность, увеличивая число арестов в этих районах. Эти дополнительные аресты будут использоваться при обучении будущих итераций моделей, что приведет к еще более предвзятой модели. Этот цикл продолжается как положительная обратная связь. . 18. Всегда ли мы должны использовать изображения размером 224х224 пикселя в модели распознавания кошек? . Нет, мы этого не должны делаем. 224x224 обычно используется по историческим причинам. Вы можете увеличить размер и получить лучшую производительность, но заплатить скоростью и памятью. . 19. В чем разница между классификацией и регрессией? . Классификация ориентирована на предсказание класса или категории (например, типа домашнего животного). Регрессия ориентирована на предсказание числовой величины (например, возраста домашнего животного). . 20. Что такое проверочный набор (validation set)? Что такое тестовый набор(test set)? Зачем они нам нужны? . Проверочный набор - это часть данных, которая используется не для обучения модели, а для оценки модели во время обучения, чтобы предотвратить переобучение. Он гарантирует, что производительность модели не является результатом “мошейничиства” или запоминания набора данных, а скорее потому, что она изучает соответствующие возможности для прогнозирования. Однако вполне возможно, что мы также переобучаем проверочные данные. Это происходит потому, что разработчик модели также является частью процесса обучения, корректируя гиперпараметры и процедуры обучения в соответствии с значениями проверки. Поэтому для окончательной оценки модели используется другая неиспользованная часть набора данных-тестовый набор. Такое разбиение набора данных необходимо для обеспечения того, чтобы модель обобщалась на неиспользованных данных. . 21. Что сделает fastai, если вы не определите проверочный набор? . fastai автоматически создаст проверочный набор данных. Он случайным образом возьмет 20% данных и назначит их в качестве проверочного набора ( valid_pct = 0.2 . 22. Можем ли мы всегда использовать случайную выборку для проверочного набора? Почему или почему нет? . Хорошые проверочные и тестовые наборы должны быть репрезентативными для новых данных, которые модель будет использовать в будущем. Иногда это не так, если используется случайная выборка. Например, для данных временных рядов случайные выборки не имеют смысла. Вместо этого лучше определить различные периоды времени для тренировки, проверки и тестирования. . 23. Что такое переобучение? Приведите пример. . Переобучение является наиболее сложной проблемой, когда речь заходит о тренировки моделей машинного обучения. Переобучение относится к той ситуации, когда модель слишком близко подходит к ограниченному набору данных, но плохо работает на неиспользованных данных. Это особенно важно, когда речь заходит о нейронных сетях, потому что нейронные сети потенциально могут “запоминать” набор данных, на котором была обучена модель, и будут плохо работать с незадействованными данными, потому что они не “запоминали” основные истинные значения для этих данных. Вот почему необходима логичная структура проверки путем разделения данных на обучающие, проверочные и тестовые. . 24. Что такое метрика? В чем отличие от “потерь” (loss)? . Метрика - это функция, которая измеряет качество прогнозов модели с помощью набора валидации. Она похоже на потери, которая также является мерой производительности модели. Однако потери предназначены для алгоритма оптимизации (например, SGD) с целью эффективного обновления параметров модели, в то время как метрики являются интерпретируемым человеком показателями производительности. Иногда метрика также может быть хорошим выбором для потерь. . 25. Как могут помочь натренированные модели? . Предварительно натренированные модели обученые для задач, которые могут быть весьма схожи с текущей задачей. Например, предварительно обученные модели распознавания изображений часто обучались на наборе данных ImageNet, который содержит 1000 классов, ориентированных на множество различных типов визуальных объектов. Предварительно обученные модели полезны, потому что они уже научились обрабатывать множество простых функций, таких как распознавание краев и цветов. Однако, поскольку модель была обучена для другой задачи, чем решаемая, эта модель не может использоваться как есть. . 26. Что такое “голова”(head) модели? . При использовании предварительно обученной модели более поздние слои модели, которые были полезны для задачи, на которой первоначально обучалась модель, заменяются одним или несколькими новыми слоями с рандомизированными весами, подходящими по размеру для набора данных, с которым вы работаете. Эти новые слои называются “головой” модели. . 27. Какие особенности обнаруживаются в ранних слоях CNN и в более поздних слоях? . Более ранние слои изучают простые объекты, такие как диагональные, горизонтальные и вертикальные ребра. Более поздние слои изучают более продвинутые объекты, такие как автомобильные колеса, лепестки цветов и даже очертания животных. . 28. Являются ли модели изображений полезными только для фотографий? . Нет! Модели изображений могут быть полезны для других типов изображений, таких как чертежи, медицинские данные и т. д. | . Очень много информации можно представить в виде изображений. Например, звук может быть преобразован в спектрограмму, которая является визуальной интерпретацией звука. Временные ряды (например, финансовые данные) можно преобразовать в изображение, построив график. Более того, существуют различные преобразования, которые генерируют изображения из временных рядов и достигли хороших результатов для классификации временных рядов. Есть много других примеров, и, проявив творческий подход, вы можете сформулировать свою проблему как проблему классификации изображений и использовать предварительно подготовленные модели изображений для получения самых современных результатов! . 29. Что такое “архитектура”? . Архитектура-это шаблон или структура модели. Она определяет математическую модель, которую мы пытаемся обучить. . 30. Что такое сегментация? . По своей сути сегментация-это задача классификации по пикселям. Мы пытаемся предсказать метку для каждого отдельного пикселя изображения. В результате получаем шаблон части изображения соответствуют данной метке. . 31. Для чего используется y_range? Когда нам понадобится? . y_range используется для ограничения прогнозируемых значений, когда наша задача сосредоточена на предсказании числового значения в заданном диапазоне (например, прогнозирование рейтингов фильмов в диапазоне 0,5-5). . 32. Что такое “гиперпараметры”? . Обучающие модели требуют различных параметров, определяющих способ обучения модели. Например, нам нужно определить, как долго мы собираемся тренировать или скорость обучения (насколько быстро параметры модели могут изменяться). Такого рода параметры называются гиперпараметрами. . 33. Как лучше всего избежать неудач при использовании ИИ в организации? . Ключевые моменты, которые следует учитывать при использовании ИИ в организации: . Убедитесь, что обучающий, проверочный и тестовый набор правильно определены, чтобы соответствующим образом оценить модель. | Попробуйте построить простую базовую модель, которую будущая модель должны превзойти. Или даже этой простой базовой модели может быть достаточно в некоторых случаях. | .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2020/11/12/fastai-chapter1-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2020/11/12/fastai-chapter1-solution.html",
            "date": " • Nov 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alexander Gladkikh, and I made my own website on GitHub dedicated to my Hobbies: Machine learning, Deep Learning, and algorithmic trading. . I take part in kaggle competitions, have knowledge of R and Python (Pandas, NumPy, Scipy, Scikit-learn, XGBoost), Java . At the main work I participate in projects on the use of new technologies in the field of labor protection and ecology. . I have been engaged in technical analysis of financial markets for a long time. Familiar with software Amibroker, and Metatrader Quik (scripting). . At work I had to deal with the analysis of data in the performance of research in biology at the Institute and writing projects on environmental protection. . My degrees . Corporate Energy University, 2020 . Digital production technologies in the power industry . YANDEX, MIPT, 2019 . Machine learning and data analysis . City Business School, 2019 . MINI-MBA Professional .",
          "url": "https://zmey56.github.io/blog//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zmey56.github.io/blog//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}