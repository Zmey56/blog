{
  
    
        "post0": {
            "title": "Курсовой проект на классификацию по одобрению кредита",
            "content": "&#1055;&#1086;&#1089;&#1090;&#1072;&#1085;&#1086;&#1074;&#1082;&#1072; &#1079;&#1072;&#1076;&#1072;&#1095;&#1080; . Задача . Требуется, на основании имеющихся данных о клиентах банка, построить модель, используя обучающий датасет, для прогнозирования невыполнения долговых обязательств по текущему кредиту. Выполнить прогноз для примеров из тестового датасета. . Наименование файлов с данными . course_project_train.csv - обучающий датасет course_project_test.csv - тестовый датасет . Целевая переменная . Credit Default - факт невыполнения кредитных обязательств . Метрика качества . F1-score (sklearn.metrics.f1_score) . Требования к решению . Целевая метрика . F1 &gt; 0.5 | Метрика оценивается по качеству прогноза для главного класса (1 - просрочка по кредиту) | . Решение должно содержать . Тетрадка Jupyter Notebook с кодом Вашего решения, названная по образцу {ФИО}_solution.ipynb, пример SShirkin_solution.ipynb | Файл CSV с прогнозами целевой переменной для тестового датасета, названный по образцу {ФИО}_predictions.csv, пример SShirkin_predictions.csv | Рекомендации для файла с кодом (ipynb) . Файл должен содержать заголовки и комментарии (markdown) | Повторяющиеся операции лучше оформлять в виде функций | Не делать вывод большого количества строк таблиц (5-10 достаточно) | По возможности добавлять графики, описывающие данные (около 3-5) | Добавлять только лучшую модель, то есть не включать в код все варианты решения проекта | Скрипт проекта должен отрабатывать от начала и до конца (от загрузки данных до выгрузки предсказаний) | Весь проект должен быть в одном скрипте (файл ipynb). | Допускается применение библиотек Python и моделей машинного обучения, которые были в данном курсе. | Сроки сдачи . Cдать проект нужно в течение 5 дней после окончания последнего вебинара. Оценки работ, сданных до дедлайна, будут представлены в виде рейтинга, ранжированного по заданной метрике качества. Проекты, сданные после дедлайна или сданные повторно, не попадают в рейтинг, но можно будет узнать результат. . &#1055;&#1088;&#1080;&#1084;&#1077;&#1088;&#1085;&#1086;&#1077; &#1086;&#1087;&#1080;&#1089;&#1072;&#1085;&#1080;&#1077; &#1101;&#1090;&#1072;&#1087;&#1086;&#1074; &#1074;&#1099;&#1087;&#1086;&#1083;&#1085;&#1077;&#1085;&#1080;&#1103; &#1082;&#1091;&#1088;&#1089;&#1086;&#1074;&#1086;&#1075;&#1086; &#1087;&#1088;&#1086;&#1077;&#1082;&#1090;&#1072; . Построение модели классификации . Обзор обучающего датасета | Обработка выбросов | Обработка пропусков | Анализ данных | Отбор признаков | Балансировка классов | Подбор моделей, получение бейзлана | Выбор наилучшей модели, настройка гиперпараметров | Проверка качества, борьба с переобучением | Интерпретация результатов | Прогнозирование на тестовом датасете . Выполнить для тестового датасета те же этапы обработки и постронияния признаков | Спрогнозировать целевую переменную, используя модель, построенную на обучающем датасете | Прогнозы должны быть для всех примеров из тестового датасета (для всех строк) | Соблюдать исходный порядок примеров из тестового датасета | &#1054;&#1073;&#1079;&#1086;&#1088; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Описание датасета . Home Ownership - домовладение | Annual Income - годовой доход | Years in current job - количество лет на текущем месте работы | Tax Liens - налоговые обременения | Number of Open Accounts - количество открытых счетов | Years of Credit History - количество лет кредитной истории | Maximum Open Credit - наибольший открытый кредит | Number of Credit Problems - количество проблем с кредитом | Months since last delinquent - количество месяцев с последней просрочки платежа | Bankruptcies - банкротства | Purpose - цель кредита | Term - срок кредита | Current Loan Amount - текущая сумма кредита | Current Credit Balance - текущий кредитный баланс | Monthly Debt - ежемесячный долг | Credit Default - факт невыполнения кредитных обязательств (0 - погашен вовремя, 1 - просрочка) | . Подключение библиотек и скриптов . import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt import seaborn as sns . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split, learning_curve from sklearn.metrics import classification_report, f1_score, precision_score, recall_score #from sklearn.model_selection import train_test_split, ShuffleSplit, cross_val_score, from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import RandomizedSearchCV, KFold import xgboost as xgb, lightgbm as lgbm, catboost as catb from scipy.stats import uniform as sp_randFloat from scipy.stats import randint as sp_randInt . import warnings warnings.filterwarnings(&#39;ignore&#39;) sns.set(style=&#39;whitegrid&#39;) sns.set_context(&quot;paper&quot;, font_scale=1.5) pd.options.display.float_format = &#39;{:,.2f}&#39;.format pd.set_option(&#39;display.max_rows&#39;, 50) . def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred): print(&#39;TRAIN n n&#39; + classification_report(y_train_true, y_train_pred)) print(&#39;TEST n n&#39; + classification_report(y_test_true, y_test_pred)) print(&#39;CONFUSION MATRIX n&#39;) print(pd.crosstab(y_test_true, y_test_pred)) . def balance_df_by_target(df, target_name): target_counts = df[target_name].value_counts() major_class_name = target_counts.argmax() minor_class_name = target_counts.argmin() disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1 for i in range(disbalance_coeff): sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name]) df = df.append(sample, ignore_index=True) return df.sample(frac=1) . def show_learning_curve_plot(estimator, X, y, cv=3, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)): train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=&#39;f1&#39;, train_sizes=train_sizes, n_jobs=n_jobs) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure(figsize=(15,8)) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=&quot;r&quot;) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=&quot;g&quot;) plt.plot(train_sizes, train_scores_mean, &#39;o-&#39;, color=&quot;r&quot;, label=&quot;Training score&quot;) plt.plot(train_sizes, test_scores_mean, &#39;o-&#39;, color=&quot;g&quot;, label=&quot;Cross-validation score&quot;) plt.title(f&quot;Learning curves ({type(estimator).__name__})&quot;) plt.xlabel(&quot;Training examples&quot;) plt.ylabel(&quot;Score&quot;) plt.legend(loc=&quot;best&quot;) plt.grid() plt.show() . def show_proba_calibration_plots(y_predicted_probs, y_true_labels): preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels))) thresholds = [] precisions = [] recalls = [] f1_scores = [] for threshold in np.linspace(0.1, 0.9, 9): thresholds.append(threshold) precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs &gt; threshold)))) scores_table = pd.DataFrame({&#39;f1&#39;:f1_scores, &#39;precision&#39;:precisions, &#39;recall&#39;:recalls, &#39;probability&#39;:thresholds}).sort_values(&#39;f1&#39;, ascending=False).round(3) figure = plt.figure(figsize = (15, 5)) plt1 = figure.add_subplot(121) plt1.plot(thresholds, precisions, label=&#39;Precision&#39;, linewidth=4) plt1.plot(thresholds, recalls, label=&#39;Recall&#39;, linewidth=4) plt1.plot(thresholds, f1_scores, label=&#39;F1&#39;, linewidth=4) plt1.set_ylabel(&#39;Scores&#39;) plt1.set_xlabel(&#39;Probability threshold&#39;) plt1.set_title(&#39;Probabilities threshold calibration&#39;) plt1.legend(bbox_to_anchor=(0.25, 0.25)) plt1.table(cellText = scores_table.values, colLabels = scores_table.columns, colLoc = &#39;center&#39;, cellLoc = &#39;center&#39;, loc = &#39;bottom&#39;, bbox = [0, -1.3, 1, 1]) plt2 = figure.add_subplot(122) plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], label=&#39;Another class&#39;, color=&#39;royalblue&#39;, alpha=1) plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], label=&#39;Main class&#39;, color=&#39;darkcyan&#39;, alpha=0.8) plt2.set_ylabel(&#39;Number of examples&#39;) plt2.set_xlabel(&#39;Probabilities&#39;) plt2.set_title(&#39;Probability histogram&#39;) plt2.legend(bbox_to_anchor=(1, 1)) plt.show() . def show_feature_importances(feature_names, feature_importances, get_top=None): feature_importances = pd.DataFrame({&#39;feature&#39;: feature_names, &#39;importance&#39;: feature_importances}) feature_importances = feature_importances.sort_values(&#39;importance&#39;, ascending=False) plt.figure(figsize = (20, len(feature_importances) * 0.355)) sns.barplot(feature_importances[&#39;importance&#39;], feature_importances[&#39;feature&#39;]) plt.xlabel(&#39;Importance&#39;) plt.title(&#39;Importance of features&#39;) plt.show() if get_top is not None: return feature_importances[&#39;feature&#39;][:get_top].tolist() . def plot_feature_importance(importance,names,model_type): #Create arrays from feature importance and feature names feature_importance = np.array(importance) feature_names = np.array(names) #Create a DataFrame using a Dictionary data={&#39;feature_names&#39;:feature_names,&#39;feature_importance&#39;:feature_importance} fi_df = pd.DataFrame(data) #Sort the DataFrame in order decreasing feature importance fi_df.sort_values(by=[&#39;feature_importance&#39;], ascending=False,inplace=True) #Define size of bar plot plt.figure(figsize=(10,8)) #Plot Searborn bar chart sns.barplot(x=fi_df[&#39;feature_importance&#39;], y=fi_df[&#39;feature_names&#39;]) #Add chart labels plt.title(model_type + &#39;FEATURE IMPORTANCE&#39;) plt.xlabel(&#39;FEATURE IMPORTANCE&#39;) plt.ylabel(&#39;FEATURE NAMES&#39;) . Пути к директориям и файлам . TARGET_NAME = &#39;Credit Default&#39; TRAIN_DATASET_PATH = &#39;data/train.csv&#39; TEST_DATASET_PATH = &#39;data/test.csv&#39; SCALER_FILE_PATH = &#39;data/scaler.pkl&#39; TRAIN_PART_PATH = &#39;data/training_project_train_part.csv&#39; TEST_PART_PATH = &#39;data/training_project_test_part.csv&#39; . Загрузка данных . train_df = pd.read_csv(TRAIN_DATASET_PATH) train_df.head() . Home Ownership Annual Income Years in current job Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Purpose Term Current Loan Amount Current Credit Balance Monthly Debt Credit Score Credit Default . 0 Own Home | 482,087.00 | NaN | 0.00 | 11.00 | 26.30 | 685,960.00 | 1.00 | NaN | 1.00 | debt consolidation | Short Term | 99,999,999.00 | 47,386.00 | 7,914.00 | 749.00 | 0 | . 1 Own Home | 1,025,487.00 | 10+ years | 0.00 | 15.00 | 15.30 | 1,181,730.00 | 0.00 | NaN | 0.00 | debt consolidation | Long Term | 264,968.00 | 394,972.00 | 18,373.00 | 737.00 | 1 | . 2 Home Mortgage | 751,412.00 | 8 years | 0.00 | 11.00 | 35.00 | 1,182,434.00 | 0.00 | NaN | 0.00 | debt consolidation | Short Term | 99,999,999.00 | 308,389.00 | 13,651.00 | 742.00 | 0 | . 3 Own Home | 805,068.00 | 6 years | 0.00 | 8.00 | 22.50 | 147,400.00 | 1.00 | NaN | 1.00 | debt consolidation | Short Term | 121,396.00 | 95,855.00 | 11,338.00 | 694.00 | 0 | . 4 Rent | 776,264.00 | 8 years | 0.00 | 13.00 | 13.60 | 385,836.00 | 1.00 | NaN | 0.00 | debt consolidation | Short Term | 125,840.00 | 93,309.00 | 7,180.00 | 719.00 | 0 | . test_df = pd.read_csv(TEST_DATASET_PATH) test_df.head() . Home Ownership Annual Income Years in current job Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Purpose Term Current Loan Amount Current Credit Balance Monthly Debt Credit Score . 0 Rent | NaN | 4 years | 0.00 | 9.00 | 12.50 | 220,968.00 | 0.00 | 70.00 | 0.00 | debt consolidation | Short Term | 162,470.00 | 105,906.00 | 6,813.00 | NaN | . 1 Rent | 231,838.00 | 1 year | 0.00 | 6.00 | 32.70 | 55,946.00 | 0.00 | 8.00 | 0.00 | educational expenses | Short Term | 78,298.00 | 46,037.00 | 2,318.00 | 699.00 | . 2 Home Mortgage | 1,152,540.00 | 3 years | 0.00 | 10.00 | 13.70 | 204,600.00 | 0.00 | NaN | 0.00 | debt consolidation | Short Term | 200,178.00 | 146,490.00 | 18,729.00 | 7,260.00 | . 3 Home Mortgage | 1,220,313.00 | 10+ years | 0.00 | 16.00 | 17.00 | 456,302.00 | 0.00 | 70.00 | 0.00 | debt consolidation | Short Term | 217,382.00 | 213,199.00 | 27,559.00 | 739.00 | . 4 Home Mortgage | 2,340,952.00 | 6 years | 0.00 | 11.00 | 23.60 | 1,207,272.00 | 0.00 | NaN | 0.00 | debt consolidation | Long Term | 777,634.00 | 425,391.00 | 42,605.00 | 706.00 | . 1. &#1054;&#1073;&#1079;&#1086;&#1088; &#1086;&#1073;&#1091;&#1095;&#1072;&#1102;&#1097;&#1077;&#1075;&#1086; &#1076;&#1072;&#1090;&#1072;&#1089;&#1077;&#1090;&#1072; . print(train_df.shape, test_df.shape) . (7500, 17) (2500, 16) . train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7500 entries, 0 to 7499 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 Home Ownership 7500 non-null object 1 Annual Income 5943 non-null float64 2 Years in current job 7129 non-null object 3 Tax Liens 7500 non-null float64 4 Number of Open Accounts 7500 non-null float64 5 Years of Credit History 7500 non-null float64 6 Maximum Open Credit 7500 non-null float64 7 Number of Credit Problems 7500 non-null float64 8 Months since last delinquent 3419 non-null float64 9 Bankruptcies 7486 non-null float64 10 Purpose 7500 non-null object 11 Term 7500 non-null object 12 Current Loan Amount 7500 non-null float64 13 Current Credit Balance 7500 non-null float64 14 Monthly Debt 7500 non-null float64 15 Credit Score 5943 non-null float64 16 Credit Default 7500 non-null int64 dtypes: float64(12), int64(1), object(4) memory usage: 996.2+ KB . columns_name = train_df.columns . train_df.nunique(dropna=False) . Home Ownership 4 Annual Income 5479 Years in current job 12 Tax Liens 8 Number of Open Accounts 39 Years of Credit History 408 Maximum Open Credit 6963 Number of Credit Problems 8 Months since last delinquent 90 Bankruptcies 6 Purpose 15 Term 2 Current Loan Amount 5386 Current Credit Balance 6592 Monthly Debt 6716 Credit Score 269 Credit Default 2 dtype: int64 . columns_name . Index([&#39;Home Ownership&#39;, &#39;Annual Income&#39;, &#39;Years in current job&#39;, &#39;Tax Liens&#39;, &#39;Number of Open Accounts&#39;, &#39;Years of Credit History&#39;, &#39;Maximum Open Credit&#39;, &#39;Number of Credit Problems&#39;, &#39;Months since last delinquent&#39;, &#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;, &#39;Current Loan Amount&#39;, &#39;Current Credit Balance&#39;, &#39;Monthly Debt&#39;, &#39;Credit Score&#39;, &#39;Credit Default&#39;], dtype=&#39;object&#39;) . # Так же выделю анализируюмую переменную y = train_df[&#39;Credit Default&#39;] cat_col = [&#39;Home Ownership&#39;, &#39;Years in current job&#39;, &#39;Tax Liens&#39;, &#39;Number of Credit Problems&#39;,&#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;] num_col = [&#39;Annual Income&#39;, &#39;Number of Open Accounts&#39;, &#39;Maximum Open Credit&#39;, &#39;Years of Credit History&#39;, &#39;Months since last delinquent&#39;, &#39;Current Loan Amount&#39;, &#39;Current Credit Balance&#39;, &#39;Monthly Debt&#39;, &#39;Credit Score&#39;,] . cat_df = train_df[cat_col] cat_df = cat_df.astype(str) num_df = train_df[num_col] num_df = num_df.astype(float) . fig, ax = plt.subplots(4,2, figsize=(40,35)) sns.countplot(train_df[&#39;Home Ownership&#39;], ax=ax[0,0]) sns.countplot(train_df[&#39;Years in current job&#39;], ax=ax[0,1]) sns.countplot(train_df[&#39;Tax Liens&#39;], ax=ax[1,0]) sns.countplot(train_df[&#39;Number of Credit Problems&#39;], ax=ax[1,1]) sns.countplot(train_df[&#39;Bankruptcies&#39;], ax=ax[2,0]) sns.countplot(train_df[&#39;Purpose&#39;], ax=ax[2,1]) sns.countplot(train_df[&#39;Term&#39;], ax=ax[3,0]) fig.show() . for c in cat_df.columns: print (&quot;- %s &quot; % c) print (cat_df[c].value_counts()) . - Home Ownership Home Mortgage 3637 Rent 3204 Own Home 647 Have Mortgage 12 Name: Home Ownership, dtype: int64 - Years in current job 10+ years 2332 2 years 705 3 years 620 &lt; 1 year 563 5 years 516 1 year 504 4 years 469 6 years 426 7 years 396 nan 371 8 years 339 9 years 259 Name: Years in current job, dtype: int64 - Tax Liens 0.0 7366 1.0 83 2.0 30 3.0 10 4.0 6 6.0 2 5.0 2 7.0 1 Name: Tax Liens, dtype: int64 - Number of Credit Problems 0.0 6469 1.0 882 2.0 93 3.0 35 4.0 9 5.0 7 6.0 4 7.0 1 Name: Number of Credit Problems, dtype: int64 - Bankruptcies 0.0 6660 1.0 786 2.0 31 nan 14 3.0 7 4.0 2 Name: Bankruptcies, dtype: int64 - Purpose debt consolidation 5944 other 665 home improvements 412 business loan 129 buy a car 96 medical bills 71 major purchase 40 take a trip 37 buy house 34 small business 26 wedding 15 moving 11 educational expenses 10 vacation 8 renewable energy 2 Name: Purpose, dtype: int64 - Term Short Term 5556 Long Term 1944 Name: Term, dtype: int64 . h = num_df.hist(bins=25,figsize=(16,16),xlabelsize=&#39;10&#39;,ylabelsize=&#39;10&#39;,xrot=-15) sns.despine(left=True, bottom=True) [x.title.set_size(12) for x in h.ravel()]; [x.yaxis.tick_left() for x in h.ravel()]; . mask = np.zeros_like(num_df.corr(), dtype=np.bool) mask[np.triu_indices_from(mask)] = True f, ax = plt.subplots(figsize=(16, 12)) plt.title(&#39;Pearson Correlation Matrix&#39;,fontsize=25) sns.heatmap(num_df.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=&quot;BuGn&quot;, #&quot;BuGn_r&quot; to reverse linecolor=&#39;w&#39;,annot=True,annot_kws={&quot;size&quot;:8},mask=mask,cbar_kws={&quot;shrink&quot;: .9}); . 2. &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1087;&#1088;&#1086;&#1087;&#1091;&#1089;&#1082;&#1086;&#1074; . train_df.isna().sum() . Home Ownership 0 Annual Income 1557 Years in current job 371 Tax Liens 0 Number of Open Accounts 0 Years of Credit History 0 Maximum Open Credit 0 Number of Credit Problems 0 Months since last delinquent 4081 Bankruptcies 14 Purpose 0 Term 0 Current Loan Amount 0 Current Credit Balance 0 Monthly Debt 0 Credit Score 1557 Credit Default 0 dtype: int64 . train_df.describe() . Annual Income Tax Liens Number of Open Accounts Years of Credit History Maximum Open Credit Number of Credit Problems Months since last delinquent Bankruptcies Current Loan Amount Current Credit Balance Monthly Debt Credit Score Credit Default . count 5,943.00 | 7,500.00 | 7,500.00 | 7,500.00 | 7,500.00 | 7,500.00 | 3,419.00 | 7,486.00 | 7,500.00 | 7,500.00 | 7,500.00 | 5,943.00 | 7,500.00 | . mean 1,366,391.72 | 0.03 | 11.13 | 18.32 | 945,153.73 | 0.17 | 34.69 | 0.12 | 11,873,177.45 | 289,833.24 | 18,314.45 | 1,151.09 | 0.28 | . std 845,339.20 | 0.27 | 4.91 | 7.04 | 16,026,216.67 | 0.50 | 21.69 | 0.35 | 31,926,122.97 | 317,871.38 | 11,926.76 | 1,604.45 | 0.45 | . min 164,597.00 | 0.00 | 2.00 | 4.00 | 0.00 | 0.00 | 0.00 | 0.00 | 11,242.00 | 0.00 | 0.00 | 585.00 | 0.00 | . 25% 844,341.00 | 0.00 | 8.00 | 13.50 | 279,229.50 | 0.00 | 16.00 | 0.00 | 180,169.00 | 114,256.50 | 10,067.50 | 711.00 | 0.00 | . 50% 1,168,386.00 | 0.00 | 10.00 | 17.00 | 478,159.00 | 0.00 | 32.00 | 0.00 | 309,573.00 | 209,323.00 | 16,076.50 | 731.00 | 0.00 | . 75% 1,640,137.00 | 0.00 | 14.00 | 21.80 | 793,501.50 | 0.00 | 50.00 | 0.00 | 519,882.00 | 360,406.25 | 23,818.00 | 743.00 | 1.00 | . max 10,149,344.00 | 7.00 | 43.00 | 57.70 | 1,304,726,170.00 | 7.00 | 118.00 | 4.00 | 99,999,999.00 | 6,506,797.00 | 136,679.00 | 7,510.00 | 1.00 | . Первое заполню Annual Income при помощи медианных значений . median_income = train_df[&#39;Annual Income&#39;].median() train_df[&#39;Annual Income&#39;] = train_df[&#39;Annual Income&#39;].fillna(median_income) test_df[&#39;Annual Income&#39;] = test_df[&#39;Annual Income&#39;].fillna(median_income) . Дальше занимаюсь Years in current job. В данный момент они имеют текстовое значение. Так что выделю по частоте наиболее часто встречающиеся и заменю им пропущенное значение . max_YCJ = train_df[&#39;Years in current job&#39;].value_counts().index[0] train_df[&#39;Years in current job&#39;] = train_df[&#39;Years in current job&#39;].fillna(max_YCJ) test_df[&#39;Years in current job&#39;] = test_df[&#39;Years in current job&#39;].fillna(max_YCJ) . Так же заменю значения на более приемлемые для дальнейшего использования в расчетах - цифры. . train_df[&#39;Years in current job&#39;] = train_df[&#39;Years in current job&#39;].replace({&#39;10+ years&#39;:10,&#39;2 years&#39;:2, &#39;3 years&#39;:3, &#39;&lt; 1 year&#39;:0, &#39;5 years&#39;:5, &#39;1 year&#39;:1, &#39;4 years&#39;:4, &#39;6 years&#39;:6,&#39;7 years&#39;:7, &#39;8 years&#39;:8, &#39;9 years&#39;:9}) test_df[&#39;Years in current job&#39;] = test_df[&#39;Years in current job&#39;].replace({&#39;10+ years&#39;:10,&#39;2 years&#39;:2, &#39;3 years&#39;:3, &#39;&lt; 1 year&#39;:0, &#39;5 years&#39;:5, &#39;1 year&#39;:1, &#39;4 years&#39;:4, &#39;6 years&#39;:6,&#39;7 years&#39;:7, &#39;8 years&#39;:8, &#39;9 years&#39;:9}) . С Months since last delinquent поступлю так же, как и Annual Income - использую медиану. Но может имеет смысл вообще удалить данную колонку, так как большая часть значений отсутствует . median_delinquent = train_df[&#39;Months since last delinquent&#39;].median() train_df[&#39;Months since last delinquent&#39;] = train_df[&#39;Months since last delinquent&#39;].fillna(median_delinquent) test_df[&#39;Months since last delinquent&#39;] = test_df[&#39;Months since last delinquent&#39;].fillna(median_delinquent) . Для банкротов при проверки на частоту выясняется, что большинство не было банкротами. Соответственно пропущенные значения заменю на 0. . train_df[&#39;Bankruptcies&#39;].value_counts() . 0.00 6660 1.00 786 2.00 31 3.00 7 4.00 2 Name: Bankruptcies, dtype: int64 . train_df[&#39;Bankruptcies&#39;] = train_df[&#39;Bankruptcies&#39;].fillna(0.00) test_df[&#39;Bankruptcies&#39;] = test_df[&#39;Bankruptcies&#39;].fillna(0.00) . Осталась последняя колонка с NaN значениями - Credit Score. Ее заполню уже привычным методом - медианы . median_CS = train_df[&#39;Credit Score&#39;].median() train_df[&#39;Credit Score&#39;] = train_df[&#39;Credit Score&#39;].fillna(median_CS) test_df[&#39;Credit Score&#39;] = test_df[&#39;Credit Score&#39;].fillna(median_CS) . 3. &#1054;&#1073;&#1088;&#1072;&#1073;&#1086;&#1090;&#1082;&#1072; &#1074;&#1099;&#1073;&#1088;&#1086;&#1089;&#1086;&#1074; . Выбросы смотрю по графикам и частоте. . Категориальные колонки оставляю без изменений - &#39;Years in current job&#39;, &#39;Tax Liens&#39;,&#39;Number of Credit Problems&#39;,&#39;Bankruptcies&#39;, &#39;Purpose&#39;, &#39;Term&#39;. . Из подсчета по частоте в Home Ownership выбивается значение для Have Mortgage как слишком низкое. Вероятнее всего это ошибочная запись для Home Mortgage. . test_df[&#39;Home Ownership&#39;].value_counts() . Home Mortgage 1225 Rent 1020 Own Home 248 Have Mortgage 7 Name: Home Ownership, dtype: int64 . train_df.loc[train_df[&#39;Home Ownership&#39;] == &#39;Have Mortgage&#39;, &#39;Home Ownership&#39;] = &#39;Home Mortgage&#39; test_df.loc[test_df[&#39;Home Ownership&#39;] == &#39;Have Mortgage&#39;, &#39;Home Ownership&#39;] = &#39;Home Mortgage&#39; . Так как в DataFrame тестовом отсутствует такая цель, как возобновляемая энергия, то объединяю их с путишествиями и называю - другое . train_df[&#39;Purpose&#39;] = train_df[&#39;Purpose&#39;].replace({&#39;vacation&#39;:&#39;other&#39;,&#39;renewable energy&#39;:&#39;other&#39;}) test_df[&#39;Purpose&#39;] = test_df[&#39;Purpose&#39;].replace({&#39;vacation&#39;:&#39;other&#39;,&#39;renewable energy&#39;:&#39;other&#39;}) . Дальше убираю сильные выбросы по годовому доходу, выданным кредитам, наибольшему открытому кредиту, количеству лет кредитной истории, количеству месяцев с последней просрочки платежа, текущем кредитном балансе, ежемесячном долге и кредитной оценке. Если значение вылетает за три сигмы, то присваиваю ему среднее плюс три сигмы. . В текущей сумме кредита превышения больше трех сигм отсутствуют, так что оставляю без изменений. . mean_AI = train_df[&#39;Annual Income&#39;].mean() sigma_AI = train_df[&#39;Annual Income&#39;].std() train_df.loc[train_df[&#39;Annual Income&#39;] &gt; (mean_AI + 3 * sigma_AI), &#39;Annual Income&#39;] = (mean_AI + 3 * sigma_AI) test_df.loc[test_df[&#39;Annual Income&#39;] &gt; (mean_AI + 3 * sigma_AI), &#39;Annual Income&#39;] = (mean_AI + 3 * sigma_AI) . mean_NOA = train_df[&#39;Number of Open Accounts&#39;].mean() sigma_NOA = train_df[&#39;Number of Open Accounts&#39;].std() train_df.loc[train_df[&#39;Number of Open Accounts&#39;] &gt; (mean_NOA + 3 * sigma_NOA), &#39;Number of Open Accounts&#39;] = (mean_NOA + 3 * sigma_NOA) test_df.loc[test_df[&#39;Number of Open Accounts&#39;] &gt; (mean_NOA + 3 * sigma_NOA), &#39;Number of Open Accounts&#39;] = (mean_NOA + 3 * sigma_NOA) . mean_MOC = train_df[&#39;Maximum Open Credit&#39;].mean() sigma_MOC = train_df[&#39;Maximum Open Credit&#39;].std() train_df.loc[train_df[&#39;Maximum Open Credit&#39;] &gt; (mean_MOC + 3 * sigma_MOC), &#39;Maximum Open Credit&#39;] = (mean_MOC + 3 * sigma_MOC) test_df.loc[test_df[&#39;Maximum Open Credit&#39;] &gt; (mean_MOC + 3 * sigma_MOC), &#39;Maximum Open Credit&#39;] = (mean_MOC + 3 * sigma_MOC) . mean_YCH = train_df[&#39;Years of Credit History&#39;].mean() sigma_YCH = train_df[&#39;Years of Credit History&#39;].std() train_df.loc[train_df[&#39;Years of Credit History&#39;] &gt; (mean_YCH + 3 * sigma_YCH), &#39;Years of Credit History&#39;] = (mean_YCH + 3 * sigma_YCH) test_df.loc[test_df[&#39;Years of Credit History&#39;] &gt; (mean_YCH + 3 * sigma_YCH), &#39;Years of Credit History&#39;] = (mean_YCH + 3 * sigma_YCH) . mean_MSLD = train_df[&#39;Months since last delinquent&#39;].mean() sigma_MSLD = train_df[&#39;Months since last delinquent&#39;].std() train_df.loc[train_df[&#39;Months since last delinquent&#39;] &gt; (mean_MSLD + 3 * sigma_MSLD), &#39;Months since last delinquent&#39;] = (mean_MSLD + 3 * sigma_MSLD) test_df.loc[test_df[&#39;Months since last delinquent&#39;] &gt; (mean_MSLD + 3 * sigma_MSLD), &#39;Months since last delinquent&#39;] = (mean_MSLD + 3 * sigma_MSLD) . mean_CCB = train_df[&#39;Current Credit Balance&#39;].mean() sigma_CCB = train_df[&#39;Current Credit Balance&#39;].std() train_df.loc[train_df[&#39;Current Credit Balance&#39;] &gt; (mean_CCB + 3 * sigma_CCB), &#39;Current Credit Balance&#39;] = (mean_CCB + 3 * sigma_CCB) test_df.loc[test_df[&#39;Current Credit Balance&#39;] &gt; (mean_CCB + 3 * sigma_CCB), &#39;Current Credit Balance&#39;] = (mean_CCB + 3 * sigma_CCB) . mean_MD = train_df[&#39;Monthly Debt&#39;].mean() sigma_MD = train_df[&#39;Monthly Debt&#39;].std() train_df.loc[train_df[&#39;Monthly Debt&#39;] &gt; (mean_MD + 3 * sigma_MD), &#39;Monthly Debt&#39;] = (mean_MD + 3 * sigma_MD) test_df.loc[test_df[&#39;Monthly Debt&#39;] &gt; (mean_MD + 3 * sigma_MD), &#39;Monthly Debt&#39;] = (mean_MD + 3 * sigma_MD) . mean_CS = train_df[&#39;Credit Score&#39;].mean() sigma_CS = train_df[&#39;Credit Score&#39;].std() train_df.loc[train_df[&#39;Credit Score&#39;] &gt; (mean_CS + 3 * sigma_CS), &#39;Credit Score&#39;] = (mean_CS + 3 * sigma_CS) test_df.loc[test_df[&#39;Credit Score&#39;] &gt; (mean_CS + 3 * sigma_CS), &#39;Credit Score&#39;] = (mean_CS + 3 * sigma_CS) . 4. &#1055;&#1086;&#1076;&#1075;&#1086;&#1090;&#1086;&#1074;&#1082;&#1072; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1082; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079;&#1091; . Первым шагом сделаю таблицу с фиктивными значениями категориальных данных и нормализую числовые значения. После чего сохраню полученный DataFrame для дальнейшего анализа . cat_dum_train = pd.get_dummies(train_df[cat_col]) cat_dum_test = pd.get_dummies(test_df[cat_col]) . scaler = StandardScaler() num_norm_train = pd.DataFrame(scaler.fit_transform(train_df[num_col]), columns = num_col) num_norm_test = pd.DataFrame(scaler.transform(test_df[num_col]), columns = num_col) . train_new = pd.concat([cat_dum_train, num_norm_train], axis=1) test_new = pd.concat([cat_dum_test, num_norm_test], axis=1) . X_train, X_test, y_train, y_test = train_test_split(train_new, y, shuffle=True, test_size=0.2, random_state=56) . 5. &#1041;&#1072;&#1083;&#1072;&#1085;&#1089;&#1080;&#1088;&#1086;&#1074;&#1082;&#1072; &#1094;&#1077;&#1083;&#1077;&#1074;&#1086;&#1081; &#1087;&#1077;&#1088;&#1077;&#1084;&#1077;&#1085;&#1085;&#1086;&#1081; . y.value_counts() . 0 5387 1 2113 Name: Credit Default, dtype: int64 . df_for_balancing = pd.concat([X_train, y_train], axis=1) df_balanced = balance_df_by_target(df_for_balancing, TARGET_NAME) df_balanced[TARGET_NAME].value_counts() . 0 4292 1 3416 Name: Credit Default, dtype: int64 . X_train = df_balanced.drop(columns=TARGET_NAME) y_train = df_balanced[TARGET_NAME] . 6. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1080; &#1086;&#1094;&#1077;&#1085;&#1082;&#1072; &#1073;&#1072;&#1079;&#1086;&#1074;&#1099;&#1093; &#1084;&#1086;&#1076;&#1077;&#1083;&#1077;&#1081; . Логистическая регрессия . model_lr = LogisticRegression() model_lr.fit(X_train, y_train) y_train_pred = model_lr.predict(X_train) y_test_pred = model_lr.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.69 0.83 0.75 4292 1 0.71 0.54 0.61 3416 accuracy 0.70 7708 macro avg 0.70 0.68 0.68 7708 weighted avg 0.70 0.70 0.69 7708 TEST precision recall f1-score support 0 0.82 0.80 0.81 1095 1 0.50 0.53 0.51 405 accuracy 0.73 1500 macro avg 0.66 0.66 0.66 1500 weighted avg 0.73 0.73 0.73 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 879 216 1 192 213 . k ближайших соседей . model_knn = KNeighborsClassifier() model_knn.fit(X_train, y_train) y_train_pred = model_knn.predict(X_train) y_test_pred = model_knn.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.79 0.85 0.82 4292 1 0.80 0.72 0.76 3416 accuracy 0.80 7708 macro avg 0.80 0.79 0.79 7708 weighted avg 0.80 0.80 0.79 7708 TEST precision recall f1-score support 0 0.81 0.77 0.79 1095 1 0.44 0.50 0.47 405 accuracy 0.70 1500 macro avg 0.63 0.63 0.63 1500 weighted avg 0.71 0.70 0.70 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 839 256 1 201 204 . Бустинговые алгоритмы . XGBoost . model_xgb = xgb.XGBClassifier(random_state=56) model_xgb.fit(X_train, y_train) y_train_pred = model_xgb.predict(X_train) y_test_pred = model_xgb.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . [20:38:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. TRAIN precision recall f1-score support 0 0.97 0.97 0.97 4292 1 0.97 0.96 0.96 3416 accuracy 0.97 7708 macro avg 0.97 0.97 0.97 7708 weighted avg 0.97 0.97 0.97 7708 TEST precision recall f1-score support 0 0.81 0.81 0.81 1095 1 0.48 0.47 0.48 405 accuracy 0.72 1500 macro avg 0.64 0.64 0.64 1500 weighted avg 0.72 0.72 0.72 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 886 209 1 213 192 . LightGBM . model_lgbm = lgbm.LGBMClassifier(random_state=56) model_lgbm.fit(X_train, y_train) y_train_pred = model_lgbm.predict(X_train) y_test_pred = model_lgbm.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.91 0.95 0.93 4292 1 0.93 0.88 0.90 3416 accuracy 0.92 7708 macro avg 0.92 0.91 0.92 7708 weighted avg 0.92 0.92 0.92 7708 TEST precision recall f1-score support 0 0.81 0.84 0.83 1095 1 0.52 0.48 0.50 405 accuracy 0.74 1500 macro avg 0.67 0.66 0.66 1500 weighted avg 0.73 0.74 0.74 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 917 178 1 211 194 . CatBoost . model_catb = catb.CatBoostClassifier(silent=True, random_state=56) model_catb.fit(X_train, y_train) y_train_pred = model_catb.predict(X_train) y_test_pred = model_catb.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.89 0.95 0.92 4292 1 0.93 0.86 0.89 3416 accuracy 0.91 7708 macro avg 0.91 0.90 0.90 7708 weighted avg 0.91 0.91 0.91 7708 TEST precision recall f1-score support 0 0.81 0.83 0.82 1095 1 0.51 0.49 0.50 405 accuracy 0.74 1500 macro avg 0.66 0.66 0.66 1500 weighted avg 0.73 0.74 0.73 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 907 188 1 208 197 . На тесте у меня лучше результаты для классификатора CatBoost, для него и буду подбирать параметрры . 7. &#1042;&#1099;&#1073;&#1086;&#1088; &#1083;&#1091;&#1095;&#1096;&#1077;&#1081; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; &#1080; &#1087;&#1086;&#1076;&#1073;&#1086;&#1088; &#1075;&#1080;&#1087;&#1077;&#1088;&#1087;&#1072;&#1088;&#1072;&#1084;&#1077;&#1090;&#1088;&#1086;&#1074; . model_catb = catb.CatBoostClassifier(class_weights=[1, 3.5], silent=True, random_state=56) . params_1 = {&#39;n_estimators&#39;:[1500, 1800, 2100], &#39;max_depth&#39;:[1, 2, 3]} . %%time rs = RandomizedSearchCV(model_catb, params_1, scoring=&#39;f1&#39;, cv=cv, n_jobs=-1) rs.fit(train_new, y) . CPU times: user 17.6 s, sys: 4.03 s, total: 21.6 s Wall time: 1min 56s . RandomizedSearchCV(cv=KFold(n_splits=3, random_state=56, shuffle=True), estimator=&lt;catboost.core.CatBoostClassifier object at 0x7f8b78854dc0&gt;, n_jobs=-1, param_distributions={&#39;max_depth&#39;: [1, 2, 3], &#39;n_estimators&#39;: [1500, 1800, 2100]}, scoring=&#39;f1&#39;) . rs.best_params_ . {&#39;n_estimators&#39;: 1500, &#39;max_depth&#39;: 3} . rs.best_score_ . 0.5418973339546257 . Обучение и оценка финальной модели . %%time final_model = catb.CatBoostClassifier(n_estimators=1500, max_depth=3, silent=True, random_state=56) final_model.fit(X_train, y_train) y_train_pred = final_model.predict(X_train) y_test_pred = final_model.predict(X_test) get_classification_report(y_train, y_train_pred, y_test, y_test_pred) . TRAIN precision recall f1-score support 0 0.76 0.87 0.81 4292 1 0.80 0.66 0.73 3416 accuracy 0.78 7708 macro avg 0.78 0.77 0.77 7708 weighted avg 0.78 0.78 0.78 7708 TEST precision recall f1-score support 0 0.82 0.82 0.82 1095 1 0.52 0.52 0.52 405 accuracy 0.74 1500 macro avg 0.67 0.67 0.67 1500 weighted avg 0.74 0.74 0.74 1500 CONFUSION MATRIX col_0 0 1 Credit Default 0 899 196 1 194 211 CPU times: user 16.2 s, sys: 4.13 s, total: 20.4 s Wall time: 3.53 s . Получение результата . %%time final_model = catb.CatBoostClassifier(n_estimators=1500, max_depth=3, silent=True, random_state=56) final_model.fit(train_new, y) . CPU times: user 15.6 s, sys: 4.25 s, total: 19.8 s Wall time: 3.43 s . &lt;catboost.core.CatBoostClassifier at 0x7f8b795233a0&gt; . y_pred = final_model.predict(test_new) . result=pd.DataFrame({&#39;Id&#39;:np.arange(2500), &#39;Credit Default&#39;: y_pred}) . RESULT_PATH=&#39;solutions.csv&#39; result.to_csv(RESULT_PATH, index=False) . result.to_csv(&#39;solutions.csv&#39;, index=False) . В общей таблице рейтинга я не выбился далеко и получил результат 0.45695 .",
            "url": "https://zmey56.github.io/blog//course%20project/machine%20learning/classification/2021/11/10/course-project-classification.html",
            "relUrl": "/course%20project/machine%20learning/classification/2021/11/10/course-project-classification.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Анализ ETF с использованием Python",
            "content": "Как использовать библиотеки Python, такие как Pandas, Matplotlib и Seaborn, для получения информации из ежедневных данных о ценах и объемах c фондового рынка. . С проникновением аналитики во многие сферы нашей жизни она не могла обойти стороной финансы. В этой статье рассмотрим ее применение для анализа ETF с целью их анализа, в том числе и с применением визуализиции. . 1. &#1054; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Для анализа будем использовать данные ETF с валютным хейджом: FXCN, FXRL, FXIT, FXUS и FXRU. Временной ряд рассмотрим за три года с 2018 по 2020 года. Само исследование проведем в Google Colaboratory. . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import pandas as pd import numpy as np import matplotlib.pyplot as plt from google.colab import files import warnings warnings.filterwarnings(&quot;ignore&quot;) . Сначало необходимо загрузить данные, которые представлены в формате CSV. . uploaded = files.upload() for fn in uploaded.keys(): print(&#39;User uploaded file «{name}» with length {length} bytes&#39;.format(name=fn, length=len(uploaded[fn]))) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving FXGD.csv to FXGD (1).csv Saving FXRU.csv to FXRU (1).csv Saving FXUS.csv to FXUS (1).csv Saving FXIT.csv to FXIT (1).csv Saving FXRL.csv to FXRL (1).csv Saving FXCN.csv to FXCN (1).csv User uploaded file «FXGD.csv» with length 53856 bytes User uploaded file «FXRU.csv» with length 32600 bytes User uploaded file «FXUS.csv» with length 56015 bytes User uploaded file «FXIT.csv» with length 56137 bytes User uploaded file «FXRL.csv» with length 55985 bytes User uploaded file «FXCN.csv» with length 56038 bytes . После этого прочтем данные с диска. Дальше необходимо создать два двадатафрейма - один с ценами закрытия, а другой с объемами торговли: . fxgd =pd.read_csv(&#39;/content/FXGD.csv&#39;) fxrl =pd.read_csv(&#39;/content/FXRL.csv&#39;) fxit =pd.read_csv(&#39;/content/FXIT.csv&#39;) fxus =pd.read_csv(&#39;/content/FXUS.csv&#39;) fxru =pd.read_csv(&#39;/content/FXRU.csv&#39;, sep=&#39;;&#39;) fxcn =pd.read_csv(&#39;/content/FXCN.csv&#39;) . def changeDF(df): df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) name =[x for x in globals() if globals()[x] is df][0] df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;HIGH&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) df = df.set_index([&#39;date&#39;]) df.columns = [name+&#39;_cl&#39;, name + &#39;_vol&#39;] return df . # df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) # name =[x for x in globals() if globals()[x] is df][0] # df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;CLOSE&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) # df = df.set_index([&#39;date&#39;]) # df.columns = [name] # return df . fxgd_change = changeDF(fxgd) fxrl_change = changeDF(fxrl) fxit_change = changeDF(fxit) fxus_change = changeDF(fxus) fxru_change = changeDF(fxru) fxcn_change = changeDF(fxcn) . etf = pd.concat([fxgd_change, fxrl_change, fxit_change, fxus_change, fxru_change, fxcn_change], axis=1) . etf.head() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . date . 2018-01-03 529.0 | 4340 | 1950.5 | 443 | 3612.0 | 581 | 2738.0 | 1049 | 641.0 | 139.0 | 2635.0 | 2098 | . 2018-01-04 527.0 | 1489 | 1992.0 | 659 | 3641.0 | 647 | 2745.0 | 586 | 639.0 | 128.0 | 2655.0 | 1331 | . 2018-01-05 526.0 | 1911 | 2004.5 | 846 | 3646.0 | 876 | 2744.0 | 322 | 637.0 | 306.0 | 2640.0 | 1664 | . 2018-01-09 525.5 | 5044 | 2024.0 | 2570 | 3673.0 | 1833 | 2766.0 | 653 | 638.0 | 448.0 | 2670.0 | 2304 | . 2018-01-10 527.5 | 9808 | 2030.0 | 765 | 3660.0 | 2485 | 2758.0 | 407 | 637.0 | 369.0 | 2665.0 | 1910 | . C FXRU пришлось немного поработать в EXCEL, так как скачанные данные прибивили лишний ноль к значению. По этому при загрузке пришлось указывать явный разделитель. . Дальше проверим наш датасет на предмет наличия значений NULL . print(etf.isnull().sum()) . fxgd_cl 0 fxgd_vol 0 fxrl_cl 0 fxrl_vol 0 fxit_cl 0 fxit_vol 0 fxus_cl 0 fxus_vol 0 fxru_cl 4 fxru_vol 4 fxcn_cl 0 fxcn_vol 0 dtype: int64 . Выбросим их, чтоб не мешали в дальнейшем расчете: . etf.dropna(inplace=True, axis=0) . Дальше имеет смысл посмотреть тип значений: . etf.dtypes . fxgd_cl float64 fxgd_vol int64 fxrl_cl float64 fxrl_vol int64 fxit_cl float64 fxit_vol int64 fxus_cl float64 fxus_vol int64 fxru_cl float64 fxru_vol float64 fxcn_cl float64 fxcn_vol int64 dtype: object . И посмотрим размер датасета: . etf.shape . (752, 12) . Так же дальше интересно посмотреть как вели себя ETF в последние полгода. Это можно сделать при помощи функции describe: . etf[-120:].describe() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . count 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | . mean 973.690000 | 148832.116667 | 3121.208333 | 8971.225000 | 8860.925000 | 14784.708333 | 4758.458333 | 12685.116667 | 950.133333 | 32841.958333 | 3878.233333 | 17515.733333 | . std 36.967338 | 94656.673543 | 169.290817 | 6816.481138 | 572.529639 | 8391.294562 | 267.491739 | 7730.347512 | 28.046367 | 14373.882822 | 219.697987 | 14471.016798 | . min 878.000000 | 34678.000000 | 2848.500000 | 2907.000000 | 7513.000000 | 4769.000000 | 4140.000000 | 4392.000000 | 880.800000 | 11069.000000 | 3422.000000 | 4448.000000 | . 25% 946.100000 | 87124.250000 | 2998.000000 | 5388.500000 | 8466.500000 | 9388.750000 | 4578.000000 | 8902.000000 | 935.900000 | 21355.000000 | 3721.000000 | 8495.250000 | . 50% 985.900000 | 127780.500000 | 3083.000000 | 7584.500000 | 9051.500000 | 12681.500000 | 4807.500000 | 11277.000000 | 951.450000 | 29360.000000 | 3898.000000 | 12329.500000 | . 75% 1001.650000 | 175438.250000 | 3219.500000 | 10754.250000 | 9306.250000 | 17640.000000 | 4982.000000 | 13963.750000 | 971.050000 | 42583.750000 | 4064.500000 | 22652.250000 | . max 1033.600000 | 666819.000000 | 3488.500000 | 67809.000000 | 9776.000000 | 63506.000000 | 5157.000000 | 73672.000000 | 1012.800000 | 91275.000000 | 4312.000000 | 101084.000000 | . pct_chg_etf[:50].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | . mean 0.018909 | 0.128734 | 0.141815 | 0.037679 | 0.010791 | 0.088726 | . std 0.634123 | 0.840398 | 1.222263 | 1.014099 | 0.544054 | 1.462912 | . min -1.291513 | -1.680871 | -4.318305 | -3.804348 | -1.550388 | -4.403670 | . 25% -0.381599 | -0.359217 | -0.432242 | -0.326851 | -0.312745 | -0.562852 | . 50% -0.094162 | 0.266967 | 0.175959 | 0.073884 | 0.000000 | 0.190041 | . 75% 0.379604 | 0.689444 | 0.847795 | 0.543863 | 0.319361 | 1.135292 | . max 1.826923 | 2.127660 | 3.065569 | 2.749529 | 1.107595 | 2.909091 | . В результате видно в каких пределах в последние полгода ETF провели большую часть аремени с вероятностью 75%. . После построим графики движения цены во времени. . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf.index, etf[&#39;fxgd_cl&#39;], &#39;tab:blue&#39; ) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf.index, etf[&#39;fxrl_cl&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf.index, etf[&#39;fxit_cl&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf.index, etf[&#39;fxus_cl&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf.index, etf[&#39;fxru_cl&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf.index, etf[&#39;fxcn_cl&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . Ежедневное процентное изменение цены etf вычисляется на основе процентного изменения между ценами закрытия 2 последовательных дней. Предположим, что цена закрытия вчера составляла 500 рублей, а сегодня она закрылась по 550 рублей. Таким образом, процентное изменение составляет 10%. т. е. ((550-500) / 500)*100. Здесь нет никакой тайны! . Далее, мы введем новый столбец, обозначающий дневную доходность в цене etf. Вычислить можно с помощью встроенной функции pct_change() в python. Так же немного переставлю колонки, чтоб визуально лучше воспринималось. . etf.columns . Index([&#39;fxgd_cl&#39;, &#39;fxgd_vol&#39;, &#39;fxrl_cl&#39;, &#39;fxrl_vol&#39;, &#39;fxit_cl&#39;, &#39;fxit_vol&#39;, &#39;fxus_cl&#39;, &#39;fxus_vol&#39;, &#39;fxru_cl&#39;, &#39;fxru_vol&#39;, &#39;fxcn_cl&#39;, &#39;fxcn_vol&#39;], dtype=&#39;object&#39;) . etf_cl = etf[[&#39;fxgd_cl&#39;, &#39;fxrl_cl&#39;, &#39;fxit_cl&#39;, &#39;fxus_cl&#39;, &#39;fxru_cl&#39;, &#39;fxcn_cl&#39;]] etf_cl_pct = etf_cl.pct_change()*100 etf_cl_pct.columns = [&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;] etf_vol = etf[[&#39;fxgd_vol&#39;, &#39;fxrl_vol&#39;, &#39;fxit_vol&#39;, &#39;fxus_vol&#39;, &#39;fxru_vol&#39;, &#39;fxcn_vol&#39;]] etf_new = pd.concat([etf_cl, etf_vol, etf_cl_pct], axis = 1) . etf_new.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-03 529.0 | 1950.5 | 3612.0 | 2738.0 | 641.0 | 2635.0 | 4340 | 443 | 581 | 1049 | 139.0 | 2098 | NaN | NaN | NaN | NaN | NaN | NaN | . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . etf_new = etf_new.dropna() . Представим изменение ежедневной доходности в виде графика во времени: . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf_new.index, etf_new[&#39;fxgd_cl_pct&#39;], &#39;tab:blue&#39;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf_new.index, etf_new[&#39;fxrl_cl_pct&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf_new.index, etf_new[&#39;fxit_cl_pct&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf_new.index, etf_new[&#39;fxus_cl_pct&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf_new.index, etf_new[&#39;fxru_cl_pct&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf_new.index, etf_new[&#39;fxcn_cl_pct&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . В течение большей части времени доходность составляет от -2% до 2% со скачками без пересечения отметки в 6% с обеих сторон. Наиболее шумной выглядит ETF FXCN. . Так же можно проверить новостные статьи за те дни, когда наблюдался резкий рост/падение цен на etf и понять чем было обусловлено. . Построим гистограмму распределения ежедневных доходов: . import seaborn as sns sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(15,15)) sns.histplot(data=etf_new[&#39;fxgd_cl_pct&#39;], kde=True, color=&quot;orange&quot;, ax=axs[0, 0]) axs[0,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxrl_cl_pct&#39;], kde=True, color=&quot;olive&quot;, ax=axs[0, 1]) axs[0,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxit_cl_pct&#39;], kde=True, color=&quot;gold&quot;, ax=axs[1, 0]) axs[1,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxus_cl_pct&#39;], kde=True, color=&quot;grey&quot;, ax=axs[1, 1]) axs[1,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxru_cl_pct&#39;], kde=True, color=&quot;teal&quot;, ax=axs[2, 0]) axs[2,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxcn_cl_pct&#39;], kde=True, color=&quot;brown&quot;, ax=axs[2, 1]) axs[2,1].set_xlim(-10,10) plt.show() . etf_new[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | . mean 0.084329 | 0.084240 | 0.140564 | 0.089850 | 0.056374 | 0.065255 | . std 1.081425 | 1.163047 | 1.398492 | 1.141144 | 0.771131 | 1.414819 | . min -5.709816 | -8.065290 | -6.874365 | -8.567335 | -5.198422 | -5.273973 | . 25% -0.456676 | -0.444714 | -0.574001 | -0.444633 | -0.346166 | -0.752409 | . 50% 0.030111 | 0.126835 | 0.206940 | 0.137979 | 0.026178 | 0.128783 | . 75% 0.622500 | 0.718721 | 0.887283 | 0.645403 | 0.445645 | 0.899653 | . max 5.619982 | 7.784431 | 8.297990 | 6.079599 | 4.604008 | 6.554307 | . Гистограммы ежедневных доходностей центрированы вокруг среднего значения, которое для всех etf было больше нуля и говорит о положительном тренде. Видно, что доходность для всех ETF большую часть времени лежала в пределах от -2,5 до 2,5%. Наибольшую доходность показали - FXIT, а наименьшую - FXRU. . 2. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1090;&#1088;&#1077;&#1085;&#1076;&#1072; . Затем мы добавляем новый столбец &quot;Тренд&quot;, значения которого основаны на ежедневном процентном изменении, которое мы рассчитали выше. Тенденция определяется отношением снизу. Скопируем датасет в новый, с которым и продолжим работу. . def trend(x): if x &gt; -0.5 and x &lt;= 0.5: return &#39;Практически или без изменений&#39; elif x &gt; 0.5 and x &lt;= 1.5: return &#39;Небольшой позитив&#39; elif x &gt; -1.5 and x &lt;= -0.5: return &#39;Небольшой негатив&#39; elif x &gt; 1.5 and x &lt;= 2.5: return &#39;Позитив&#39; elif x &gt; -2.5 and x &lt;= -1.5: return &#39;Негатив&#39; elif x &gt; 2.5 and x &lt;= 5: return &#39;Значительный позитив&#39; elif x &gt; -5 and x &lt;= -2.5: return &#39;Значительный негатив&#39; elif x &gt; 5: return &#39;Максимальный позитив&#39; elif x &lt;= -5: return &#39;Максимальный негатив&#39; . etf_trend = etf_new.copy() . etf_trend.columns[12:] . Index([&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;], dtype=&#39;object&#39;) . for stock in etf_trend.columns[12:]: etf_trend[&quot;Trend_&quot; + str(stock)] = np.zeros(etf_trend[stock].count()) etf_trend[&quot;Trend_&quot;+ str(stock)] = etf_trend[stock].apply(lambda x:trend(x)) . etf_trend.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct Trend_fxgd_cl_pct Trend_fxrl_cl_pct Trend_fxit_cl_pct Trend_fxus_cl_pct Trend_fxru_cl_pct Trend_fxcn_cl_pct . date . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | Практически или без изменений | Позитив | Небольшой позитив | Практически или без изменений | Практически или без изменений | Небольшой позитив | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | Практически или без изменений | Небольшой позитив | Небольшой позитив | Небольшой позитив | Практически или без изменений | Небольшой позитив | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | . 2018-01-11 526.0 | 2042.0 | 3673.0 | 2755.0 | 635.0 | 2650.0 | 5548 | 1220 | 1282 | 968 | 326.0 | 1722 | -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts() . Практически или без изменений 351 Небольшой позитив 166 Небольшой негатив 141 Позитив 44 Негатив 25 Значительный позитив 13 Значительный негатив 7 Максимальный негатив 2 Максимальный позитив 2 Name: Trend_fxgd_cl_pct, dtype: int64 . Дальше можно взглянуть как вели себя акции акцииETF в последние 3 года. Для этого их изменения можно визуализировать при помощи круговых диаграмм, где каждый сектор представляет процент дней, в течение которых происходил каждый тренд. Для построения будем использовать функцию groupby() со столбцом тренда. . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(20,17)) axs[0, 0].pie(etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].pie(etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].pie(etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].pie(etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].pie(etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].pie(etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 1].set_title(&#39;FXCN&#39;) plt.show() . За рассматриваемый период с 2018 года по 2020 года большую часть времени ETF практически не изменялись, или изменялись незначительно при заданных параметрах. Так же важно отметить, что при небольших изменениях они как правило были позитивными. При более больших - это соотношение сохранялось кроме FXRU. . 3. &#1045;&#1078;&#1077;&#1076;&#1085;&#1077;&#1074;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1080; &#1086;&#1073;&#1098;&#1077;&#1084;&#1099; . Следующим шагом продолжим работу с объемами: . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) axs[0].stem(etf_trend.index[-253:], etf_trend[&#39;fxgd_cl_pct&#39;][-253:]) axs[0].plot((etf_trend[&#39;fxgd_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[0].set_title(&#39;FXGD&#39;) axs[1].stem(etf_trend.index[-253:], etf_trend[&#39;fxrl_cl_pct&#39;][-253:]) axs[1].plot((etf_trend[&#39;fxrl_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[1].set_title(&#39;FXRL&#39;) axs[2].stem(etf_trend.index[-253:], etf_trend[&#39;fxit_cl_pct&#39;][-253:]) axs[2].plot((etf_trend[&#39;fxit_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[2].set_title(&#39;FXIT&#39;) axs[3].stem(etf_trend.index[-253:], etf_trend[&#39;fxus_cl_pct&#39;][-253:]) axs[3].plot((etf_trend[&#39;fxus_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[3].set_title(&#39;FXUS&#39;) axs[4].stem(etf_trend.index[-253:], etf_trend[&#39;fxru_cl_pct&#39;][-253:]) axs[4].plot((etf_trend[&#39;fxru_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[4].set_title(&#39;FXRU&#39;) axs[5].stem(etf_trend.index[-253:], etf_trend[&#39;fxcn_cl_pct&#39;][-253:]) axs[5].plot((etf_trend[&#39;fxcn_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[5].set_title(&#39;FXCN&#39;) . Text(0.5, 1.0, &#39;FXCN&#39;) . Сопоставляя ежедневный объем торговли(зеленым цветом) с ежедневной доходностью(синим цветом), было отмечено, что часто для ETF характерно, что когда объем торгов высок, наблюдается сравнительно высокий рост или падение цены. Объем торгов ETF в сочетании с ростом или падениемы на данный инструмент является показателем доверия трейдеров и инвесторов к конкретному ETF. . 4. &#1050;&#1086;&#1088;&#1088;&#1077;&#1083;&#1103;&#1094;&#1080;&#1086;&#1085;&#1085;&#1099;&#1081; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079; ETF . Основное правило диверсификации - не клади все яйца в одну корзинку. По этому если мы решили собирать портфель из ETF, то они не должны быть сильно взаимосвязаны друг с другом. Математическим языком - коэффициент корреляции Пирсона между любой парой должен быть близок к 0. Смысл - они не должны падать синхронно, чтоб инвестиции не превратились в 0. . Проанализировать корреляцию между различными ETF можно с помощью парной диаграммы Seaborn. Для удобства оставим только процентные изменения за день в отдельном новом датафрейме. . pct_chg_etf = etf_new[etf_new.columns[12:]] . sns.set(style = &#39;ticks&#39;, font_scale = 1.25) sns.pairplot(pct_chg_etf) plt.show() . pct_chg_etf.corr() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . fxgd_cl_pct 1.000000 | -0.236301 | 0.107085 | 0.119773 | 0.590029 | 0.139992 | . fxrl_cl_pct -0.236301 | 1.000000 | 0.335061 | 0.300352 | -0.384120 | 0.232063 | . fxit_cl_pct 0.107085 | 0.335061 | 1.000000 | 0.895261 | 0.138016 | 0.641551 | . fxus_cl_pct 0.119773 | 0.300352 | 0.895261 | 1.000000 | 0.202682 | 0.610576 | . fxru_cl_pct 0.590029 | -0.384120 | 0.138016 | 0.202682 | 1.000000 | 0.197225 | . fxcn_cl_pct 0.139992 | 0.232063 | 0.641551 | 0.610576 | 0.197225 | 1.000000 | . На графике визуально можно увидеть наличие корреляции между различными ETF. Обратите внимание, что корреляционный анализ выполняется для ежедневного процентного изменения(дневной доходности) цены ETF, а не для их цены. . Из полученных графиков ясно видно, что следующие FXIT и FXUS не следует класть в одну корзину, так как между нми наблюдается сильная зависимость. Остальные могут быть включены в портфель, поскольку ни одна из двух оставшихся ETF не демонстрирует какой-либо существенной корреляции. . Но у визуального анализа есть существенный недостаток - он не предоставляет подробной информации о количественной оценки взаимосвязи, таких как значение R Пирсона и p нулевой гипотезы. В связи с чем при визуальном анализе остается под вопросом FXCN - есть ли у данного ETF сильная взаимосвязь с FXUS или нет. . Один из способов решения данного вопроса - построение графиков seaborn.jointplot с подробной информацией по значению R Пирсона (коэффициент корреляции Пирсона) для каждой пары ETF. Значение R Пирсона колеблется от -1 до 1. Отрицательное значение указывает на отрицательную линейную связь, в то время как положительное значение указывает на положительную связь. Значение R Пирсона ближе к 1 (или -1) указывает на сильную корреляцию, в то время как значение ближе к 0 указывает на слабую корреляцию. . Так же чем интересны данные графики - построение гистограмм распределения по краям, а так же значение p-value. . Но если рассматривать все пары, то нам потребуется большое количество графиков. По этому остановимся только на тех, которые вызывают сомнения: . pct_chg_etf.head() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-04 -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . 2018-01-11 -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | . from scipy.stats import stats a_1 = pct_chg_etf.fxit_cl_pct b_1 = pct_chg_etf.fxus_cl_pct b_2 = pct_chg_etf.fxcn_cl_pct g_1 = sns.jointplot(&#39;fxit_cl_pct&#39;, &#39;fxcn_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_1, p_1 = stats.pearsonr(a_1, b_1) g_1.ax_joint.annotate(f&#39;$ rho = {r_1:.3f}, p = {p_1:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_1.ax_joint.scatter(a_1, b_1) g_1.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxus&#39;, size=15) g_2 = sns.jointplot(&#39;fxus_cl_pct&#39;, &#39;fxit_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_2, p_2 = stats.pearsonr(a_1, b_2) g_2.ax_joint.annotate(f&#39;$ rho = {r_2:.3f}, p = {p_2:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_2.ax_joint.scatter(a_1, b_2) g_2.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxcn&#39;, size=15) plt.tight_layout() plt.show() . Первый гррафик подтвердил наличие сильной взаимосвязи между FXIT и FXUS, что говорит о нежелательности их брать в один портфель. В свою очередь корреляция между FXCN и FXIT оказалась ниже 0,7, что говорит о возможности совместного нахождения в одной корзине. . 5. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1074;&#1086;&#1083;&#1072;&#1090;&#1080;&#1083;&#1100;&#1085;&#1086;&#1089;&#1090;&#1080; . Волатильность-один из важнейших показателей на финансовых рынках. Говорят, что ценная бумага обладает высокой волатильностью, если ее стоимость может резко измениться за короткий промежуток времени. С другой стороны, более низкая волатильность означает, что стоимость имеет тенденцию быть относительно стабильной в течение определенного периода времени. Эти изменения обусловлены несколькими факторами, включая спрос и предложение, настроения, жадность, страх и т.д. Математически волатильность измеряется с помощью статистической меры, называемой &quot;стандартным отклонением&quot;, которая измеряет отклонение актива от его средней стоимости. . Произведем рассчет 5-дневной скользящей средней дневной доходности и стандартного отклонения. После этого построим график. Все это можно выполнить при помощи функций Pandas rolling() и std(). . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) for i, etf in enumerate(pct_chg_etf.columns): axs[i].plot(pct_chg_etf[etf].rolling(5).std()*np.sqrt(5)) axs[i].plot(pct_chg_etf[etf].rolling(7).mean()) axs[i].set_title(etf[:4], size=20) . volatility = pct_chg_etf[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;,&#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].rolling(5).std()*np.sqrt(5) . volatility[:150].plot(linewidth=4, figsize = (35, 15)) plt.legend(loc=2, prop={&#39;size&#39;: 16}) . &lt;matplotlib.legend.Legend at 0x7f6bad8d31d0&gt; . Как результат вы можете заметить, что наиболее сильная низкая волатильность характерна для ETF на российские акции - FXRU. Многие трейдеры и инвесторы ищут инвестиции с более высокой волатильностью, чтобы получать более высокую прибыль. Если финансовый инструмент не движется, он не только обладает низкой волатильностью, но и имеет низкий потенциал прибыли. С другой стороны, ценные бумаги с очень высоким уровнем волатильности могут иметь огромный потенциал прибыли, но риск также высок. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/09/18/data-analysis-visualization-finance.html",
            "relUrl": "/finance/investment/python/2021/09/18/data-analysis-visualization-finance.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Оптимизации портфеля с помощью Python и PyPortfolioOpt",
            "content": "В данной статье приводится пример вычисления оптимизированных весов активов в портфеле используя Портфельную теорию Марковица при помощи Python . &#1055;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1100;&#1085;&#1072;&#1103; &#1090;&#1077;&#1086;&#1088;&#1080;&#1103; &#1052;&#1072;&#1088;&#1082;&#1086;&#1074;&#1080;&#1094;&#1072; . Портфельная теория Марковица(далее ПТМ) (Modern portfolio theory) — разработанная Гарри Марковицем методика формирования инвестиционного портфеля, направленная на оптимальный выбор активов, исходя из требуемого соотношения доходность/риск. Сформулированные им в 1950-х годах идеи составляют основу современной портфельной теории. . Основные положения портфельной теории были сформулированы Гарри Марковицем при подготовке им докторской диссертации в 1950—1951 годах. . Рождением же портфельной теории Марковица считается опубликованная в «Финансовом журнале» в 1952 году статья «Выбор портфеля». В ней он впервые предложил математическую модель формирования оптимального портфеля и привёл методы построения портфелей при определённых условиях. Основная заслуга Марковица состояла в предложении вероятностной формализации понятий «доходность» и «риск», что позволило перевести задачу выбора оптимального портфеля на формальный математический язык. Надо отметить, что в годы создания теории Марковиц работал в RAND Corp., вместе с одним из основателей линейной и нелинейной оптимизации — Джорджем Данцигом и сам участвовал в решении указанных задач. Поэтому собственная теория, после необходимой формализации, хорошо ложилась в указанное русло. . Марковиц постоянно занимается усовершенствованием своей теории и в 1959 году выпускает первую посвящённую ей монографию «Выбор портфеля: эффективная диверсификация инвестиций». . В 1990 году, когда Марковицу вручают Нобелевскую премию, выходит книга «Средне-дисперсионный анализ при выборе портфеля и рынка капитала» ссылка. . 1. &#1054;&#1078;&#1080;&#1076;&#1072;&#1077;&#1084;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103;(Portfolio Expected Return) . Ожидаемая доходность портфеля будет зависеть от ожидаемой доходности каждого из активов, входящих в него. Такой подход позволяет снизить риск за счет диверсификации и одновременно максимизировать доход инвестора, поскольку убытки по одним инвестициям будут компенсированы доходом по другим. . Ожидаемая доходность портфеля представляет собой суммарную ожидаемую доходность входящих в него ценных бумаг, взвешенную с учетом их доли в портфеле. . . 2. &#1044;&#1080;&#1089;&#1087;&#1077;&#1088;&#1089;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; (Portfolio Variance ) . Дисперсия портфеля - это процесс, который определяет степень риска или волатильности, связанной с инвестиционным портфелем. Основная формула для расчета этой дисперсии фокусируется на взаимосвязи между так называемой дисперсией доходности и ковариацией, связанной с каждой из ценных бумаг, найденных в портфеле, а также с процентом или частью портфеля, который представляет каждая ценная бумага. . . 3. &#1050;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; &#1064;&#1072;&#1088;&#1087;&#1072; (Sharpe Ratio) . Коэффициент Шарпа измеряет доходность инвестиций по отношению к безрисковой ставке (казначейской ставке) и степени риска. В целом, более высокое значение коэффициента Шарпа указывает на лучшие и более прибыльные инвестиции. Таким образом, если сравнивать два портфеля с одинаковыми рисками, то при прочих равных условиях было бы лучше инвестировать в портфель с более высоким коэффициентом Шарпа. . . 4. &#1069;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1072;&#1103; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094;&#1072; (The Efficient Frontier ) . Определение и рисунок из Википедии: . Граница эффективности (англ. Efficient frontier) в портфельной теории Марковица — инвестиционный портфель, оптимизированный в отношении риска и доходности. Формально границей эффективности является набор портфелей, удовлетворяющих такому условию, что не существует другого портфеля с более высокой ожидаемой доходностью, но с таким же стандартным отклонением доходности. Понятие границы эффективности было впервые сформулировано Гарри Марковицем в 1952 году в модели Марковица. . Портфель может быть охарактеризован как «эффективный», если он имеет максимально возможный ожидаемый уровень доходности для своего уровня риска (который представлен стандартным отклонением доходности портфеля). Так, на график соотношения риска и доходности может быть нанесена любая возможная комбинация рискованных активов, и совокупность всех таких возможных портфелей определяет регион в этом пространстве. При отсутствии в портфеле безрискового актива граница эффективности определяется верхней (восходящей) частью гиперболы, ограничивающей область допустимых решений для всех соотношений активов в портфеле. . В случае же, если в портфель может быть включён безрисковый актив, граница эффективности вырождается в отрезок прямой линии, исходящий от значения доходности безрискового актива на оси ординат (ожидаемая доходность портфеля) и проходящий по касательной к границе области допустимых решений. Все портфели на отрезке между собственно безрисковым активом и точкой касания состоят из комбинации безрискового актива и рисковых активов, в то время как все портфели на линии выше и справа от точки касания образуются короткой позицией в безрисковом активе и инвестированием в рисковые активы. . . . &#1054;&#1087;&#1090;&#1080;&#1084;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; &#1085;&#1072; Python . 1. &#1048;&#1084;&#1087;&#1086;&#1088;&#1090; &#1085;&#1077;&#1086;&#1073;&#1093;&#1086;&#1076;&#1080;&#1084;&#1099;&#1093; &#1073;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1082; . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import matplotlib.pyplot as plt import numpy as np import pandas as pd import pandas_datareader as web from matplotlib.ticker import FuncFormatter . Непосредственно для анализа и оптимизации портфеля существует библиотека PyPortfolioOpt. Так как она не входит в стандартный набор, то ее необходимо установить. . !pip install PyPortfolioOpt #Installing the Portfolio Optimzation Library . Collecting PyPortfolioOpt Downloading https://files.pythonhosted.org/packages/46/55/7d39d78d554ee33a7317e345caf01339da11406c28f18bc48794fe967935/PyPortfolioOpt-1.4.1-py3-none-any.whl (56kB) |████████████████████████████████| 61kB 3.2MB/s Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.1.5) Requirement already satisfied: scipy&lt;2.0,&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.4.1) Collecting cvxpy&lt;2.0.0,&gt;=1.1.10 Downloading https://files.pythonhosted.org/packages/83/47/fd1e818b8da30ef18695a0fbf9b66611ab18506f0a44fc69480a75f4db1b/cvxpy-1.1.12.tar.gz (1.3MB) |████████████████████████████████| 1.3MB 7.9MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: numpy&lt;2.0,&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2018.9) Requirement already satisfied: ecos&gt;=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.0.7.post1) Requirement already satisfied: scs&gt;=1.1.6 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.1.3) Requirement already satisfied: osqp&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.6.2.post0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.19-&gt;PyPortfolioOpt) (1.15.0) Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp&gt;=0.4.1-&gt;cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.1.5.post0) Building wheels for collected packages: cvxpy Building wheel for cvxpy (PEP 517) ... done Created wheel for cvxpy: filename=cvxpy-1.1.12-cp37-cp37m-linux_x86_64.whl size=2731641 sha256=2c888a76787438c69d6a1dce26762a30ead689ee8b9da895efc81ad29620fbdf Stored in directory: /root/.cache/pip/wheels/9b/62/55/1da181c05c710c5d99bd560edebec3bd6a61cb69acef9dc00e Successfully built cvxpy Installing collected packages: cvxpy, PyPortfolioOpt Found existing installation: cvxpy 1.0.31 Uninstalling cvxpy-1.0.31: Successfully uninstalled cvxpy-1.0.31 Successfully installed PyPortfolioOpt-1.4.1 cvxpy-1.1.12 . Импортируем функции для дальнейшей работы: . from pypfopt.efficient_frontier import EfficientFrontier from pypfopt import risk_models from pypfopt import expected_returns from pypfopt.cla import CLA import pypfopt.plotting as pplt from matplotlib.ticker import FuncFormatter from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices . 2. &#1055;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1087;&#1086; &#1072;&#1082;&#1094;&#1080;&#1103;&#1084; &#1080;&#1079; &#1080;&#1085;&#1090;&#1077;&#1088;&#1085;&#1077;&#1090;&#1072; . Сначало установим опять пакет, который не входит в стандартный набор. Он позволяет получить данные по акциям с сайтя yahoo. . Тикеры, которые будут использоваться для анализа - одна из компаний входящих в лидеры в своем секторе. . !pip install yfinance --upgrade --no-cache-dir . Collecting yfinance Downloading https://files.pythonhosted.org/packages/a7/ee/315752b9ef281ba83c62aa7ec2e2074f85223da6e7e74efb4d3e11c0f510/yfinance-0.1.59.tar.gz Requirement already satisfied, skipping upgrade: pandas&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0) Requirement already satisfied, skipping upgrade: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB) |████████████████████████████████| 6.3MB 6.4MB/s Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... done Created wheel for yfinance: filename=yfinance-0.1.59-py2.py3-none-any.whl size=23442 sha256=519c6bb89355fc0fab0d0a1c7f12df703543e4aadbde996b33dcf9592621bb6e Stored in directory: /tmp/pip-ephem-wheel-cache-weglluyo/wheels/f8/2a/0f/4b5a86e1d52e451757eb6bc17fd899629f0925c777741b6d04 Successfully built yfinance Installing collected packages: lxml, yfinance Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.3 yfinance-0.1.59 . import yfinance as yf tickers = [&#39;LKOH.ME&#39;,&#39;GMKN.ME&#39;, &#39;DSKY.ME&#39;, &#39;NKNC.ME&#39;, &#39;MTSS.ME&#39;, &#39;IRAO.ME&#39;, &#39;SBER.ME&#39;, &#39;AFLT.ME&#39;] df_stocks= yf.download(tickers, start=&#39;2018-01-01&#39;, end=&#39;2020-12-31&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . df_stocks.head() . AFLT.ME DSKY.ME GMKN.ME IRAO.ME LKOH.ME MTSS.ME NKNC.ME SBER.ME . Date . 2018-01-03 127.199066 | 70.177948 | 8249.352539 | 3.025520 | 2844.152100 | 197.995163 | 33.301483 | 145.441605 | . 2018-01-04 134.899857 | 71.621933 | 8455.913086 | 3.181567 | 2910.237305 | 202.738434 | 33.173889 | 149.769119 | . 2018-01-05 133.450317 | 71.621933 | 8441.316406 | 3.160166 | 2967.178467 | 202.199417 | 33.237682 | 149.643677 | . 2018-01-09 136.349426 | 71.116539 | 8521.605469 | 3.103989 | 3016.222900 | 203.421158 | 33.556660 | 150.772598 | . 2018-01-10 135.262268 | 71.658035 | 8507.006836 | 3.087939 | 3026.613525 | 204.427307 | 33.811848 | 149.116821 | . Дальше необходимо проверить есть ли среди полученных значений NaN. В случае их наличия они будут мешать дальнейшему исследованию. Для того, чтобы это решить, необходимо рассмотреть или иную акцию, или заменить их для примера средней ценой между днем до и после значения NaN. . nullin_df = pd.DataFrame(df_stocks,columns=tickers) print(nullin_df.isnull().sum()) . LKOH.ME 0 GMKN.ME 0 DSKY.ME 0 NKNC.ME 0 MTSS.ME 0 IRAO.ME 0 SBER.ME 0 AFLT.ME 0 dtype: int64 . 3. &#1056;&#1072;&#1089;&#1095;&#1077;&#1090;&#1099; . Перейдем к расчетам по оптимизации портфеля и начнем с определения ожидаемой доходности и дисперсии портфеля. Далее сохраним значения весов портфеля с максимальным коэффициентом Шарпа и минимальной диспрсией. . # mu = expected_returns.mean_historical_return(df_stocks) # #Sample Variance of Portfolio # Sigma = risk_models.sample_cov(df_stocks) # #Max Sharpe Ratio - Tangent to the EF # ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks # sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks # sharpe_pwt=ef.clean_weights() # print(sharpe_pwt) #Годовая доходность mu = expected_returns.mean_historical_return(df_stocks) #Дисперсия портфеля Sigma = risk_models.sample_cov(df_stocks) . ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks sharpe_pwt=ef.clean_weights() print(sharpe_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.0), (&#39;DSKY.ME&#39;, 0.22606), (&#39;GMKN.ME&#39;, 0.48796), (&#39;IRAO.ME&#39;, 0.0), (&#39;LKOH.ME&#39;, 0.0), (&#39;MTSS.ME&#39;, 0.02953), (&#39;NKNC.ME&#39;, 0.25645), (&#39;SBER.ME&#39;, 0.0)]) . Необходимо обратить внимание, что если изменить weight_bounds=(0,1) на weight_bounds=(-1,1), то в портфеле будут учитываться и короткие позиции по акциям. . Дальше посмотрим общие характеристики по портфелю. . ef.portfolio_performance(verbose=True) . Expected annual return: 37.1% Annual volatility: 20.7% Sharpe Ratio: 1.70 . (0.37123023494063007, 0.20717177784552962, 1.695357536597058) . Теперь посмотрим портфель с минимальной волатильностью: . ef1 = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks minvol=ef1.min_volatility() minvol_pwt=ef1.clean_weights() print(minvol_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.02876), (&#39;DSKY.ME&#39;, 0.24503), (&#39;GMKN.ME&#39;, 0.10403), (&#39;IRAO.ME&#39;, 0.0938), (&#39;LKOH.ME&#39;, 0.01168), (&#39;MTSS.ME&#39;, 0.41967), (&#39;NKNC.ME&#39;, 0.09704), (&#39;SBER.ME&#39;, 0.0)]) . ef1.portfolio_performance(verbose=True, risk_free_rate = 0.27) . Expected annual return: 24.0% Annual volatility: 16.9% Sharpe Ratio: -0.18 . (0.239915644698749, 0.16885732511472468, -0.17816434839774456) . 4. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1075;&#1088;&#1072;&#1092;&#1080;&#1082;&#1072; &#1101;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1099;&#1093; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094; . Заключительным шагом является построение эффективной границы для визуального представления и расчет распределения активов. Тут встречается одна сложность, решить которую пока что мне не удалось - пакет создан для анализа в долларах и в результате в выводе присутствует их обозначение. Но с другой стороны наличия значка &quot;$&quot; не должно сильно мешать. . Анализ произведем для суммы в 100 000 рублей. . cl_obj = CLA(mu, Sigma) ax = pplt.plot_efficient_frontier(cl_obj, showfig = False) ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: &#39;{:.0%}&#39;.format(x))) ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: &#39;{:.0%}&#39;.format(y))) . Первым этапом посчитаем портфель с минимальной волатильностью: . latest_prices = get_latest_prices(df_stocks) # Allocate Portfolio Value in $ as required to show number of shares/stocks to buy, also bounds for shorting will affect allocation #Min Volatility Portfolio Allocation $10000 allocation_minv, rem_minv = DiscreteAllocation(minvol_pwt, latest_prices, total_portfolio_value=100000).lp_portfolio() print(allocation_minv) print(&quot;Leftover Fund value in$ after building minimum volatility portfolio is ${:.2f}&quot;.format(rem_minv)) print(&quot;Осталось денежных средств после построения портфеля с минимальной волатильностью составляет {:.2f} рублей&quot;.format(rem_minv)) print() . {&#39;AFLT.ME&#39;: 41, &#39;DSKY.ME&#39;: 181, &#39;IRAO.ME&#39;: 1765, &#39;LKOH.ME&#39;: 1, &#39;MTSS.ME&#39;: 127, &#39;NKNC.ME&#39;: 107} Leftover Fund value in$ after building minimum volatility portfolio is $6152.03 Осталось денежных средств после построения портфеля с минимальной волатильностью составляет 6152.03 рублей . Вторым шагом портфель с максимальным коэффициентом Шарпа: . latest_prices1 = get_latest_prices(df_stocks) allocation_shp, rem_shp = DiscreteAllocation(sharpe_pwt, latest_prices1, total_portfolio_value=100000).lp_portfolio() print(allocation_shp) print(&quot;Leftover Fund value in$ after building Max Sharpe ratio portfolio is ${:.2f}&quot;.format(rem_shp)) print(&quot;Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа {:.2f} рублей&quot;.format(rem_shp)) #allocation using integer programming via PyPortfolioOpt User Guide #Alex Putkov code used for guidance and reference in applying integer programming . {&#39;DSKY.ME&#39;: 167, &#39;GMKN.ME&#39;: 2, &#39;MTSS.ME&#39;: 9, &#39;NKNC.ME&#39;: 283} Leftover Fund value in$ after building Max Sharpe ratio portfolio is $1319.05 Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа 1319.05 рублей . В результтате нам предлагается купить для оптимального портфеля 167 акций Детского мира, 2 акции Норильского никеля, 9 акций МТС и 283 акцию Нижнекамскнефтехим. В результате у нас еще останется 1319 рублей. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/05/16/automating-portfolio-optimization.html",
            "relUrl": "/finance/investment/python/2021/05/16/automating-portfolio-optimization.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Использование API Fmp Cloud для отбора акций по дивидендам на Nasdaq с помощью Python",
            "content": "Акции с высокой дивидентной доходностью часто являются отличной инвестиционной стратегией для инвесторов, стремящихся получать приток денежных средств каждый год. В данной статье буден создан скрипт на Python для отбора их на бирже NASDAQ. . &#1063;&#1090;&#1086; &#1090;&#1072;&#1082;&#1086;&#1077; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1090;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100;? . Возьму определение из Википедии. Дивиде́ндная дохо́дность (англ. dividend yield) — это отношение величины годового дивиденда на акцию к цене акции. Данная величина выражается чаще всего в процентах. . Пример . При цене акции ОАО «Лукойл» 1124,37 рублей и дивиденде 28 рублей на акцию дивидендная доходность будет равна: . . Так же необходимо обратить внимание, что многие растущие компании, такие как для примера Amazon и Yandex, не выплачивают дивиденды, поскольку они реинвестируют всю прибыль в развитие бизнеса. Поэтому дивидендная доходность для этих фирм будет равна нулю. . &#1056;&#1072;&#1089;&#1095;&#1077;&#1090; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1076;&#1085;&#1086;&#1081; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1080; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Расчет дивидендной доходности является простой задачей, которую можно выполнить с помощью финансового API под названием fmpcloud и Python. Этот API предлагает несколько бесплатных запросов в день после регистрации. . Первым делом нужно извлечь список тикеров для всех акций, торгующихся на Nasdaq, по которым собираемся рассчитать дивидендную доходность. | import requests demo = &#39;ВАШ API КОД&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] for ticker in tickers: symbols.append(ticker[&#39;symbol&#39;]) . len(symbols) . 5500 . После необходимо пройтись по полученому списку акций и получить финансовую информацию по компании. Так же необходимо понимать, что получаем только последние данные, а не за все время существование компании. | DivYield = {} for company in symbols: try: companydata = requests.get(f&#39;https://fmpcloud.io/api/v3/profile/{company}?apikey={demo}&#39;) companydata = companydata.json() latest_Annual_Dividend = companydata[0][&#39;lastDiv&#39;] price = companydata[0][&#39;price&#39;] market_Capitalization = companydata[0][&#39;mktCap&#39;] name = companydata[0][&#39;companyName&#39;] exchange = companydata[0][&#39;exchange&#39;] dividend_Yield= latest_Annual_Dividend/price DivYield[company] = {} DivYield[company][&#39;Dividend_Yield&#39;] = dividend_Yield DivYield[company][&#39;latest_Price&#39;] = price DivYield[company][&#39;latest_Dividend&#39;] = latest_Annual_Dividend DivYield[company][&#39;market_Capit_in_M&#39;] = market_Capitalization/1000000 DivYield[company][&#39;company_Name&#39;] = name DivYield[company][&#39;exchange&#39;] = exchange except: pass . Сбор данных может занять значительное по продолжительности время. После их можно представить в виде отсортированного DataFrame, где сверху будут акций с высокой дивидендной доходностью. . import pandas as pd DivYield_dataframe = pd.DataFrame.from_dict(DivYield, orient=&#39;index&#39;) DivYield_dataframe = DivYield_dataframe.sort_values([&#39;Dividend_Yield&#39;], ascending=[False]) DivYield_dataframe.head(15) . Dividend_Yield latest_Price latest_Dividend market_Capit_in_M company_Name exchange . SNT 0.682238 | 4.1100 | 2.8040 | 95.258704 | Senstar Technologies Ltd. | Nasdaq Global Market | . TBK 0.151587 | 117.4900 | 17.8100 | 2951.818752 | Triumph Bancorp, Inc. | Nasdaq Global Select | . MGOAX 0.100728 | 16.4900 | 1.6610 | 1625.336832 | Victory Munder Mid-Cap Core Growth Fund Class A | Nasdaq Capital Market | . GLAD 0.067450 | 11.5641 | 0.7800 | 396.699520 | Gladstone Capital Corporation | Nasdaq Global Select | . NVEC 0.057954 | 69.0200 | 4.0000 | 333.589504 | NVE Corporation | Nasdaq Capital Market | . QYLG 0.054108 | 32.6199 | 1.7650 | 0.000000 | Global X NASDAQ 100 Covered Call &amp; Growth ETF | Nasdaq Global Market | . FUND 0.049811 | 8.7410 | 0.4354 | 256.320208 | Sprott Focus Trust, Inc. | Nasdaq Global Select | . SQQQ 0.044732 | 7.2655 | 0.3250 | 130697.036000 | ProShares UltraPro Short QQQ | Nasdaq Global Market | . MTEX 0.042901 | 39.1600 | 1.6800 | 74.116176 | Mannatech, Incorporated | Nasdaq Global Select | . ESHY 0.040237 | 21.0750 | 0.8480 | 0.000000 | Xtrackers J.P. Morgan ESG USD High Yield Corpo... | Nasdaq Global Select | . PBCT 0.038865 | 18.6800 | 0.7260 | 7994.199552 | People&#39;s United Financial, Inc. | Nasdaq Global Select | . NYMTP 0.038217 | 25.3290 | 0.9680 | 1337.495296 | New York Mortgage Trust, Inc. | Nasdaq Global Select | . MNSBP 0.037127 | 27.5000 | 1.0210 | 0.000000 | MainStreet Bancshares, Inc. | Nasdaq Global Select | . REG 0.033759 | 70.5000 | 2.3800 | 11976.117248 | Regency Centers Corporation | Nasdaq Global Select | . TTEC 0.032629 | 93.1700 | 3.0400 | 4377.620480 | TTEC Holdings, Inc. | Nasdaq Global Select | . &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1087;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1088;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1072; &#1080; &#1079;&#1072;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1077; . Предварительно проведем расчет средней дивидендной доходности по акциям которые платят дивиденды: . meanDivNasdaq = DivYield_dataframe[DivYield_dataframe[&#39;Dividend_Yield&#39;]&gt;0][&#39;Dividend_Yield&#39;].mean() print(&quot;Средняя дивидендная доходность по рынку Nasdaq равна &quot;, &quot;{:.2%}&quot;.format(meanDivNasdaq)) . Средняя дивидендная доходность по рынку Nasdaq равна 3.48% . Самой высокой дивидендной доходностью в полученных результатах у акций компании Senstar Technologies Ltd. — 68.22%. Так же замечено, что в системе похоже сидит баг - может выдавать акции по которым никогда не платили дивиденты. Так же по другим рынкам заметил, что в список могут включаться акции по которым перестали платить дивиденды давно. А так, как подписка Free ограничена по количеству запросов, то подстроить не удалось. Так же в том случае, если при проверке выясняется, что дивиденды платили недавно, то все равно необходимо быть осторожным при выборе компаний по данному показателю, так как он может являться результатом падения цены акций и как следствия ростом дивидендной доходности. Так же выплата высоких дивидендов может не сохраниться в будущем, тем более если у компании возникнут финансовые проблемы. . Основной смысл в следующем - анализ дивидендной доходности не должен быть единственным критерием. Я для одного из своих портфелей так же смотрю: EPS, EBITDA, FCF, срок выплаты дивидендов, капитализация компании, чистая рентабельность (отношение выручки к прибыли) и коэффициент Net Debt/EBITDA. . Но как говориться - все вышеприведенное не является инвестиционной рекомендацией и выбор остается за каждым самостоятельно. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/04/10/high-divident-stocks.html",
            "relUrl": "/finance/investment/python/2021/04/10/high-divident-stocks.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Using machine learning to predict gold mining stock prices",
            "content": "As a basis, I took a notebook published on colab for oil. This notebook examines the analysis of gold prices and shares of gold mining companies using machine analysis methods: linear regression, cluster analysis, and random forest. I immediately warn you that this post does not attempt to show the current situation and predict the future direction. Just like the author for oil, this article does not aim to raise or refute the possibilities of machine learning for analyzing stock prices or other tools. I upgraded the code for gold research in order to encourage those who are interested in further reflection and listen to constructive criticism in their address. . import yfinance as yf import pandas as pd import numpy as np import seaborn as sns from sklearn import metrics import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression . 1. Loading data . For the price of gold, take the value of the exchange-traded investment Fund SPDR Gold Trust, whose shares are 100% backed by precious metal. The quotes will be compared with the prices of gold mining companies &#39; shares: . Newmont Goldcorp (NMM) | Barrick Gold (GOLD) | AngloGold Ashanti (AU) | Kinross Gold (KGC) | Newcrest Mining (ENC) | Polyus (PLZL) | Polymetal (POLY) | Seligdar (SELG) | . gold = pd.DataFrame(yf.download(&quot;GLD&quot;, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed . gold = gold.reset_index() gold.columns = [&quot;Date&quot;,&quot;gold_price&quot;] gold[&#39;Date&#39;] = pd.to_datetime(gold[&#39;Date&#39;]) gold.head() . Date gold_price . 0 2010-01-04 | 109.800003 | . 1 2010-01-05 | 109.699997 | . 2 2010-01-06 | 111.510002 | . 3 2010-01-07 | 110.820000 | . 4 2010-01-08 | 111.370003 | . It is necessary to move the price of gold, as we will be interested in how yesterday&#39;s price affected today&#39;s stock price. . gold[&quot;gold_price&quot;] = gold[&quot;gold_price&quot;].shift(1) . shares=[&quot;NMM.SG&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,&quot;PLZL.ME&quot;,&quot;POLY.ME&quot;,&quot;SELG.ME&quot;] data= yf.download(shares, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . data = data.reset_index() data.head() . Date AU GOLD KGC NCM.AX NMM.SG PLZL.ME POLY.ME SELG.ME . 0 2010-01-04 | 39.698944 | 34.561649 | 18.105721 | 33.237167 | 26.924570 | NaN | NaN | NaN | . 1 2010-01-05 | 40.320408 | 34.989510 | 18.594805 | 33.901924 | 27.116940 | NaN | NaN | NaN | . 2 2010-01-06 | 41.601028 | 35.733963 | 19.256504 | 33.901924 | 27.289278 | NaN | NaN | NaN | . 3 2010-01-07 | 41.130215 | 35.229092 | 19.352404 | 34.298923 | NaN | NaN | NaN | NaN | . 4 2010-01-08 | 41.601028 | 35.451572 | 19.601744 | 33.421829 | 27.702093 | NaN | NaN | NaN | . data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) . all_data=pd.DataFrame() . for index in range(len(shares)): stock=pd.DataFrame() # transform the data stock=data.loc[:, (&quot;Date&quot;,shares[index])] stock[&quot;Date&quot;]=stock[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) stock.columns=[&quot;Date&quot;,&quot;share_price&quot;] test=pd.DataFrame(gold) output=stock.merge(test,on=&quot;Date&quot;,how=&quot;left&quot;) #combining two data sets stock[&quot;gold_price&quot;]=output[&quot;gold_price&quot;] stock[&#39;share_price&#39;]=pd.to_numeric(stock[&#39;share_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&#39;gold_price&#39;]=pd.to_numeric(stock[&#39;gold_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&quot;year&quot;]=pd.to_datetime(stock[&quot;Date&quot;]).dt.year #Create a column with years for subsequent filtering stock[&quot;name&quot;]=shares[index] stock = stock.dropna() #delete all NAN lines #creating a column with a scaled share price scaler=MinMaxScaler() stock[&quot;share_price_scaled&quot;]=scaler.fit_transform(stock[&quot;share_price&quot;].to_frame()) #add data to the main dataframe all_data=all_data.append(stock) #add the data . all_data_15 = all_data[(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)] all_data_15.head() . Date share_price gold_price year name share_price_scaled . 1301 2015-01-02 | 14.269927 | 113.580002 | 2015 | NMM.SG | 0.052072 | . 1302 2015-01-05 | 14.845476 | 114.080002 | 2015 | NMM.SG | 0.071190 | . 1303 2015-01-06 | 15.601913 | 115.800003 | 2015 | NMM.SG | 0.096317 | . 1304 2015-01-07 | 15.645762 | 117.120003 | 2015 | NMM.SG | 0.097773 | . 1305 2015-01-08 | 15.517859 | 116.430000 | 2015 | NMM.SG | 0.093525 | . 2. Data analysis . It is best to start analyzing data by presenting it visually, which will help you understand it better. . 2.1 Chart of gold price changes . gold[[&#39;Date&#39;,&#39;gold_price&#39;]].set_index(&#39;Date&#39;).plot(color=&quot;green&quot;, linewidth=1.0) plt.show() . 2.2. Plotting the pairplot chart for the price of Polyus and Barrick Gold shares over the past five years . palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) g = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;POLY.ME&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) g.fig.suptitle(&quot;Polyuse&quot;, y=1.08) palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) f = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;GOLD&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) f.fig.suptitle(&#39;Barrick Gold&#39;, y=1.08) plt.show() . A paired graph allows you to see the distribution of data by showing the paired relationships in the data set and the univariate distribution of data for each variable. You can also use the palette to see how this data changed in different years. . The chart is particularly interesting for 2016 and 2019, as it looks like the price of the Pole stock, Barrick Gold and the price of gold are lined up along the same line. We can also conclude from the distribution charts that the price of gold and stocks moved gradually towards higher values. . 2.3 Violinplot for the gold price . plt.figure(figsize=(10,10)) sns.set_style(&quot;whitegrid&quot;) palette=sns.cubehelix_palette(5, start=2.8, rot=0, dark=0.2, light=0.8, reverse=False) sns.violinplot(x=&quot;year&quot;, y=&quot;gold_price&quot;, data=all_data_15[[&quot;gold_price&quot;,&quot;year&quot;]], inner=&quot;quart&quot;, palette=palette, trim=True) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Price gold&quot;) plt.show() . 2.4 Violinplot for multiple shares . sns.catplot(x=&quot;year&quot;, y=&quot;share_price_scaled&quot;, col=&#39;name&#39;, col_wrap=3,kind=&quot;violin&quot;, split=True, data=all_data_15,inner=&quot;quart&quot;, palette=palette, trim=True, height=4, aspect=1.2) sns.despine(left=True) . A large fluctuation in gold prices was noted according to the charts in 2016 and 2019. As you can see from the graphs in the following figure, some companies such as Newmont Mining, Barrick Gold, AngloGold Ashanti, Newcrest Mining and Polymetal were also affected. It should also be noted that all prices are marked in the range from 0 to 1 and this may lead to inaccuracies in the interpretation. . Next, we will build distribution charts for one Russian company - Polymetal and one foreign company - Barrick Gold . sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;POLY.ME&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . It is necessary to pay attention to the distribution of the share price for the two companies and it will become clear that the shape of the density graph is the same for them. . 2.5 Charts of the dependence of the share price of various companies on the price of gold . sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,line_kws={&#39;color&#39;: &#39;blue&#39;},scatter_kws={&#39;color&#39;: &#39;grey&#39;}).set(ylim=(0, 1)) plt.show() . In fact, you won&#39;t be able to see much on these charts, although some stocks seem to have a relationship. . The next step is to try to color the charts depending on the years. . palette=sns.cubehelix_palette(5, start=2, rot=0, dark=0, light=.95, reverse=False) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,hue=&quot;year&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,palette=palette,height=4).set(ylim=(0, 1)) plt.show() . Here the picture is a little better in the sense that some companies have a data cloud stretching along a straight line in some years, which may indicate the existence of a dependency. . 3 Machine learning and prediction . I will give a definition for machine learning from Wikipedia: Machine learning is a class of artificial intelligence methods that are characterized not by direct problem solving, but by learning in the process of applying solutions to many similar problems. To build such methods, we use mathematical statistics, numerical methods, optimization methods, probability theory, graph theory, and various techniques for working with data in digital form. . Usually, machine learning algorithms can be classified into the following categories: learning with a teacher and learning without a teacher. Here is their definition from one of the sites: . Supervised learning is one of the sections of machine learning dedicated to solving the following problem. There is a set of objects (situations) and the set of possible answers (responses, reactions). There is some relationship between responses and objects, but it is unknown. Only a finite set of use cases is known — the &quot;object, response&quot; pairs, called the training sample. Based on this data, you need to restore the dependency, that is, build an algorithm that can give a fairly accurate answer for any object. To measure the accuracy of responses, a quality functional is introduced in a certain way. see the Links) . Unsupervised learning is one of the sections of machine learning. Studies a wide class of data processing problems in which only descriptions of a set of objects (training sample) are known, and it is required to detect internal relationships, dependencies, and patterns that exist between objects. Learning without a teacher is often contrasted with learning with a teacher, when each training object is given a &quot;correct answer&quot;, and you need to find the relationship between the objects and the answers. see links) . The following machine learning methods will be discussed later: . Cluster analysis | Linear regression | Random forest | . Using these algorithms, you can evaluate overvalued or undervalued stocks relative to the price of gold and possible movement on the next day. I remind you that you must be very careful and use the conclusions from this post at your own risk. I also remind you that my main goal is to show the potential of machine learning for stock valuation. . 3.1. Cluster analysis for Barrick Gold stock . Clustering is the task of dividing a set of objects into groups called clusters. Each group should contain &quot;similar&quot; objects, and objects from different groups should be as different as possible. . from sklearn.cluster import KMeans poly=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;] # We need to scale also gold price, so clustering is not influenced by the relative size of one axis. poly=pd.DataFrame(poly) poly[&#39;gold_price_scaled&#39;] = scaler.fit_transform(poly[&quot;gold_price&quot;].to_frame()) poly[&quot;cluster&quot;] = KMeans(n_clusters=5, random_state=1).fit_predict(poly[[&quot;share_price_scaled&quot;,&quot;gold_price_scaled&quot;]]) # The 954 most common RGB monitor colors https://xkcd.com/color/rgb/ colors = [&quot;baby blue&quot;, &quot;amber&quot;, &quot;scarlet&quot;, &quot;grey&quot;,&quot;milk chocolate&quot;, &quot;windows blue&quot;] palette=sns.xkcd_palette(colors) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,ci=None,palette=palette, hue=&quot;cluster&quot;,fit_reg=0 ,data=poly) plt.show() . Cluster analysis is used in a large number of machine learning tasks. But I have given it only for informational purposes, since in this form it does not bring much benefit to our analysis. . 3.2. Linear regression between Barrick Gold shares and the gold price . Next, we will build a regular linear regression using training with a teacher. The goal is to estimate the forecast of data for the last 100 days of 2019 based on data from 2018/2019 (excluding estimated ones). Training data is the data used to build the model, and test data is the data that we will try to predict. . for sh in shares: print(sh) #Data Preparation share_18=pd.DataFrame() share_18=all_data_15[(all_data_15[&#39;name&#39;]==sh)] # Get data 2018/19 share_18=share_18[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Just using 1 variable for linear regression. Split the data into training/testing sets train = share_18[:-100] test = share_18[-100:] x_train=train[&quot;gold_price&quot;].to_frame() y_train=train[&#39;share_price&#39;].to_frame() x_test=test[&quot;gold_price&quot;].to_frame() y_test=test[&#39;share_price&#39;].to_frame() regr = LinearRegression() #Create linear regression object regr.fit(x_train,y_train) #Train the model using the training sets print(&quot;Coefficients: &quot;, float(regr.coef_)) print(np.corrcoef(x_train,y_train, rowvar=False)) y_pred = regr.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # Plot outputs using matplotlib plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . NMM.SG Coefficients: 0.6629423053739908 [[1. 0.790953] [0.790953 1. ]] Mean Absolute Error: 6.063058573972694 Mean Squared Error: 39.21188296210148 Root Mean Squared Error: 6.261939233344689 . GOLD Coefficients: 0.3355465472461071 [[1. 0.67139243] [0.67139243 1. ]] Mean Absolute Error: 3.3769293704374657 Mean Squared Error: 11.756813554455096 Root Mean Squared Error: 3.4288210152259473 . AU Coefficients: 0.31252669952857776 [[1. 0.67830589] [0.67830589 1. ]] Mean Absolute Error: 2.2471377544809683 Mean Squared Error: 5.789211153877581 Root Mean Squared Error: 2.4060779608893768 . KGC Coefficients: 0.10461302060876282 [[1. 0.78266367] [0.78266367 1. ]] Mean Absolute Error: 1.0583009847297946 Mean Squared Error: 1.1523726951635975 Root Mean Squared Error: 1.073486234268329 . NCM.AX Coefficients: 0.5623005799590818 [[1. 0.79891272] [0.79891272 1. ]] Mean Absolute Error: 2.0335289996635937 Mean Squared Error: 5.836462091267656 Root Mean Squared Error: 2.415877085297937 . PLZL.ME Coefficients: 103.84435014609612 [[1. 0.60373084] [0.60373084 1. ]] Mean Absolute Error: 1315.093426667142 Mean Squared Error: 1776892.2964767825 Root Mean Squared Error: 1333.0012364873419 . POLY.ME Coefficients: 10.772023429299809 [[1. 0.63694034] [0.63694034 1. ]] Mean Absolute Error: 69.33753863275061 Mean Squared Error: 6800.525447108329 Root Mean Squared Error: 82.46529844187995 . SELG.ME Coefficients: 0.15570348678870732 [[1. 0.51630147] [0.51630147 1. ]] Mean Absolute Error: 1.8096071903165585 Mean Squared Error: 4.039450515732427 Root Mean Squared Error: 2.009838430255633 . From the above charts, we can conclude that the price of gold predicts the price of shares of foreign companies on the next day quite well. In Russian companies, this picture looks much worse. Of course, there may be a false impression about Seligdar shares. But visual analysis of the chart allows you to discard this assumption. . 3.3 Random forest on Newmont Goldcorp shares against the price of gold and shares of gold companies . Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . The random forest algorithm accepts more than one variable in the input data to predict the output data. It works very efficiently on large amounts of data, can handle many input variables, has efficient methods for estimating missing data, and many other advantages. The main disadvantages are: . Random forests is slow to generate forecasts because it has many decision trees. Whenever it makes a forecast, all the trees in the forest must make a forecast for the same given input and then vote on it. This whole process takes a long time. | the Model is difficult to interpret compared to the decision tree, where you can easily make a decision by following the path in the tree. | One of the great advantages of a random forest is that it can be used for both classification and regression problems, which make up most of today&#39;s machine learning systems. I will talk about random forests in classification, since classification is sometimes considered a building block of machine learning. Below you can see what a random forest with two trees looks like: . In addition to the gold price, we will use other variables to forecast the Newmont Goldcorp share price. This will be the share prices of other foreign gold mining companies. I know it doesn&#39;t make a lot of sense, but we just want to see how to build this type of model. This will allow us to see the impact of each of them on the final forecast.Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . from sklearn.ensemble import RandomForestRegressor # 1.- Data Preparation nmm15=pd.DataFrame() nmm15=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NMM.SG&quot;) &amp; (all_data_15[&#39;year&#39;]&gt;2016 )] nmm15=nmm15[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Load share price of other variables nmm15[&#39;GOLD&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;GOLD&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;GOLD&#39;] = nmm15[&#39;GOLD&#39;].shift(1) nmm15[&#39;AU&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;AU&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;AU&#39;] = nmm15[&#39;AU&#39;].shift(1) nmm15[&#39;KGC&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;KGC&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;KGC&#39;] = nmm15[&#39;KGC&#39;].shift(1) nmm15[&#39;NCM.AX&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NCM.AX&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;NCM.AX&#39;] = nmm15[&#39;NCM.AX&#39;].shift(1) nmm15 = nmm15.drop(nmm15.index[0]) train = nmm15[:-100] test = nmm15[-100:] x_train=train[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]] y_train=train[&#39;share_price&#39;] x_test=test[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,]] y_test=test[&#39;share_price&#39;].to_frame() # 2.- Create Randomforest object usinig a max depth=5 regressor = RandomForestRegressor(n_estimators=200, max_depth=5 ) # 3.- Train data clf=regressor.fit(x_train, y_train) # 4.- Predict! y_pred=regressor.predict(x_test) y_pred_list = list(y_pred) y_pred=pd.DataFrame(y_pred) . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_pred=plt.scatter(nmm15[&quot;gold_price&quot;], regressor.predict(nmm15[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]]), color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train,plt_pred),(&quot;train data&quot;,&quot;prediction&quot;)) plt.show() . The resulting model looks really good in addition, we must remember that Random Forest has many more parameters to configure, but the key one is the maximum depth, which is unlimited by default. Next, we&#39;ll check how this model predicts or tests data. . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . y_pred = clf.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 1.410409517520304 Mean Squared Error: 3.0995744019029483 Root Mean Squared Error: 1.7605608202794212 . importances=regressor.feature_importances_ indices=list(x_train) print(&quot;Feature ranking:&quot;) for f in range(x_train.shape[1]): print(&quot;Feature %s (%f)&quot; % (indices[f], importances[f])) f, (ax1) = plt.subplots(1, 1, figsize=(8, 6), sharex=True) sns.barplot(indices, importances, palette=&quot;BrBG&quot;, ax=ax1) ax1.set_ylabel(&quot;Importance&quot;) . Feature ranking: Feature gold_price (0.627703) Feature GOLD (0.045197) Feature AU (0.040957) Feature KGC (0.038973) Feature NCM.AX (0.247171) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0, 0.5, &#39;Importance&#39;) . By the importance of the signs, it immediately becomes clear how strong the value of gold is. . In short, I hope I was able to reveal to you the beginnings of a project on using machine learning to study stock prices, and I hope to hear your comments. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "relUrl": "/finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Использование метода Монте-Карло для создания портфеля",
            "content": "Начинающие (да и не только) инвесторы часто задаются вопросом о том, как отобрать для себя идеальное соотношение активов входящих в портфель. Часто (или не очень, но знаю про двух точно) у некоторых брокеров эту функцию выполняет торговый робот. Но заложенные в них алгоритмы не раскрываются. . В этом посте будет рассмотрено то, как оптимизировать портфель при помощи Python и симуляции Монте Карло. Под оптимизацией портфеля понимается такое соотношение весов, которое будет удовлетворять одному из условий: . Портфель с минимальным уровнем риском при желаемой доходности; | Портфель с максимальной доходностью при установленном риске; | Портфель с максимальным значением доходности | . Для расчета возьмем девять акций, которые рекомендовал торговый робот одного из брокеров на начало января 2020 года и так же он устанавливал по ним оптимальные веса в портфеле: &#39;ATVI&#39;,&#39;BA&#39;,&#39;CNP&#39;,&#39;CMA&#39;, &#39;STZ&#39;,&#39;GPN&#39;,&#39;MPC&#39;,&#39;NEM&#39; и &#39;PKI&#39;. Для анализа будет взяты данные по акциям за последние три года. . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt # Получаем данные по акциям ticker = [&#39;ATVI&#39;,&#39;BA&#39;,&#39;CNP&#39;,&#39;CMA&#39;, &#39;STZ&#39;,&#39;GPN&#39;,&#39;MPC&#39;,&#39;NEM&#39;, &#39;PKI&#39;] stock = yf.download(ticker,&#39;2017-01-01&#39;, &#39;2019-01-31&#39;) . [*********************100%***********************] 9 of 9 completed . Если сложить долю всех акций, входящих в портфель, то сумма должна стремиться к единице (а лучше быть равна). Дальше как обычно проведем подготовку данных для расчетов: . all_adj_close = stock[[&#39;Adj Close&#39;]] # ежедневная доходность all_returns = all_adj_close.pct_change() # узнаем среднюю доходность и получаем ковариационную матрицу mean_returns = all_returns.mean() cov_matrix = all_returns.cov() . Теперь можно провести расчет для весов предложенных торговым роботом и узнать доходность данного портфеля за последник три года и стандартное отклонение. . robot = np.array([0.0441, 0.1030, 0.1086, 0.2070, 0.1525, 0.0714, 0.0647, 0.1828, 0.0661]) # доходность, стандартное отклонение и коэффициент Шарпо portfolio_return_robot = np.sum(mean_returns * robot) portfolio_std_dev_robot = np.sqrt(np.dot(robot.T,np.dot(cov_matrix, robot))) sharpo_robot = portfolio_return_robot/portfolio_std_dev_robot # объединим полученные значения в таблицу и выведем ее robot_result = np.array([portfolio_return_robot, portfolio_std_dev_robot, sharpo_robot]) robot_result = np.array([portfolio_return_robot, portfolio_std_dev_robot, sharpo_robot]) robot_result = np.concatenate((robot_result, robot), axis=0) robot_sim_result = pd.DataFrame(robot_result, columns=[&#39;Robot&#39;], index=[&#39;ret&#39;,&#39;stdev&#39;,&#39;sharpe&#39;,ticker[0],ticker[1],ticker[2],ticker[3],ticker[4],ticker[5],ticker[6],ticker[7],ticker[8]]) print(robot_sim_result) . Robot ret 0.000852 stdev 0.008635 sharpe 0.098683 ATVI 0.044100 BA 0.103000 CNP 0.108600 CMA 0.207000 STZ 0.152500 GPN 0.071400 MPC 0.064700 NEM 0.182800 PKI 0.066100 . Симуляция Монте-Карло . Первоначально небольшое вступительное слово о том, как используется метод Монте-Карла для оптимизации портфеля . Сначала акциям задаются случайные веса, после чего производится расчет доходности и стандартного отклонения. Полученные значения сохраняются. Следующим шагом случайным образом меняются веса (главное не забывать, что их сумма должна составлять единицу) и все повторяется — расчет и сохранение полученного значения. Количество итераций зависит от времени, мощностей компьютера для расчета и рисков, который готов принять инвестор. В этот раз попробуем провести 10000 расчетов для выявления портфеля с минимальным убытком и максимальным значением коэффициента Шарпа. . num_iterations = 10000 simulation_res = np.zeros((4+len(ticker)-1,num_iterations)) # сама итерация for i in range(num_iterations): #Выбрать случайные веса и нормализовать, чтоб сумма равнялась 1 weights = np.array(np.random.random(9)) weights /= np.sum(weights) #Вычислить доходность и стандартное отклонение portfolio_return = np.sum(mean_returns * weights) portfolio_std_dev = np.sqrt(np.dot(weights.T,np.dot(cov_matrix, weights))) #Сохранить все полученные значения в массив simulation_res[0,i] = portfolio_return simulation_res[1,i] = portfolio_std_dev #Вычислить коэффициент Шарпа и сохранить simulation_res[2,i] = simulation_res[0,i] / simulation_res[1,i] #Сохранить веса for j in range(len(weights)): simulation_res[j+3,i] = weights[j] # сохраняем полученный массив в DataFrame для построения данных и анализа. sim_frame = pd.DataFrame(simulation_res.T,columns=[&#39;ret&#39;,&#39;stdev&#39;,&#39;sharpe&#39;,ticker[0],ticker[1],ticker[2],ticker[3],ticker[4],ticker[5],ticker[6],ticker[7],ticker[8]]) . Теперь можно рассчитать портфель с максимальным коэффициентом Шарпа или минимальным риска. . max_sharpe = sim_frame.iloc[sim_frame[&#39;sharpe&#39;].idxmax()] # узнать минимальное стандартное отклонение min_std = sim_frame.iloc[sim_frame[&#39;stdev&#39;].idxmin()] print (&quot;The portfolio for max Sharpe Ratio: n&quot;, max_sharpe) print (&quot;The portfolio for min risk: n&quot;, min_std) . The portfolio for max Sharpe Ratio: ret 0.001258 stdev 0.010977 sharpe 0.114567 ATVI 0.065653 BA 0.377049 CNP 0.028034 CMA 0.102441 STZ 0.149974 GPN 0.091727 MPC 0.020134 NEM 0.155008 PKI 0.009980 Name: 9001, dtype: float64 The portfolio for min risk: ret 0.000546 stdev 0.007489 sharpe 0.072923 ATVI 0.020115 BA 0.021464 CNP 0.179417 CMA 0.256539 STZ 0.035221 GPN 0.100015 MPC 0.209400 NEM 0.076111 PKI 0.101716 Name: 9578, dtype: float64 . Ну а самое важное представление можно получить, когда данные визуализируешь: . fig, ax = plt.subplots(figsize=(10, 10)) #Создать разноцветный график scatter plot для различных значений коэффициента Шарпо по оси x и стандартного отклонения по оси y plt.scatter(sim_frame.stdev,sim_frame.ret,c=sim_frame.sharpe,cmap=&#39;RdYlBu&#39;) plt.xlabel(&#39;Standard Deviation&#39;) plt.ylabel(&#39;Returns&#39;) plt.ylim(0,.0015) plt.xlim(0.007,0.012) plt.scatter(max_sharpe[1],max_sharpe[0],marker=(5,1,0),color=&#39;r&#39;,s=600) plt.scatter(min_std[1],min_std[0],marker=(5,1,0),color=&#39;b&#39;,s=600) plt.scatter(portfolio_std_dev_robot, portfolio_return_robot,marker=(5,1,0),color=&#39;g&#39;,s=600) plt.show() . Портфель с максимальным коэффициентом Шарпа показан красной звездой, синей — с минимальным стандартным отклонением и зеленой — предложенный роботом. Как видно — портфель, предложенный роботом, не совпадает с этими показателями, но на каком остановиться портфеле — выбор остается за инвестором. А я постараюсь в конце года вернуться к сравнению портфелей. А сейчас все три портфеля находятся в просадке. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/05/03/using-monte-carlo-method-create-portfolio.html",
            "relUrl": "/finance/investment/python/2020/05/03/using-monte-carlo-method-create-portfolio.html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Общий финансовый анализ на Python (Часть 3)",
            "content": "После всех вычислений, приведенных в прошлых двух публикациях, можно углубиться в статистический анализ и рассмотреть метод наименьших квадратов. Для этой цели используется библиотека statsmodels, которая позволяет пользователям исследовать данные, оценивать статистические модели и выполнять статистические тесты. За основу были взяты эта статья и эта статья. Само описание используемой функции на английском доступно по следующей ссылке. . Сначала немного теории: . О линейной регрессии . Линейная регрессия используется в качестве прогнозирующей модели, когда предполагается линейная зависимость между зависимой переменной (переменная, которую мы пытаемся предсказать) и независимой переменной (переменная и/или переменные, используемые для предсказания). . В самом простой случае при рассмотрении используется одна переменная на основании которой мы пытаемся предсказать другую. Формула в этом случае имеет следующий вид: . Y = C + M*X . Y - зависимая переменная (результат / прогноз / оценка) | C - Константа (Y-Intercept) | M - Наклон линии регрессии (угловой коэффициент или градиент оценённой линии; она представляет собой величину, на которую Y увеличивается в среднем, если мы увеличиваем X на одну единицу) | X - независимая переменная (предиктор, используемый в прогнозе Y) | . В действительности так же может существовать связь между зависимой переменной и несколькими независимыми переменными. Для этих типов моделей (при условии линейности) мы можем использовать множественную линейную регрессию следующего вида: . Y = C + M1X1 + M2X2 + … . Бета — коэффициент . Про данный коэффициент написано уже много, для примера на этой странице . Коротко, если не вдаваться в подробности, то можно его охарактеризовать следующим образом: . Акции c бета-коэффициентом: . ноль указывает на отсутствие корреляции между акцией и индексом | единица указывает на то, что акция имеет ту же волатильность, что и индекс | больше одного — указывает на большую доходность (а следовательно и риски) акции, чем индекс | менее единицы — менее волатильная акция, чем индекса | . Другими словами, если акция увеличится на 14%, в то время как рынок вырос всего на 10%, то бета-коэффициент акции составит 1,4. Как правило на рынках с более высоким бета-коэффициентом можно предположить лучшие условия для вознаграждения (а следовательно и для риска). . . Практика . Следующий код Python включает в себя пример линейной регрессии, где входной переменной является доходность по Индексу МосБиржи, а оцениваемая переменная — доходность по акциям Аэрофлот. . Для того, чтобы отсутствовала необходимость вспоминать как загружать данные и приводить данные к форме, необходимой для расчета — код приводиться с момента загрузки данных и до получения результатов. Вот полный синтаксис для выполнения линейной регрессии в Python с использованием statsmodels: . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt import statsmodels.api as sm #Загружаю данные ticker = [&#39;AFLT.ME&#39;,&#39;IMOEX.ME&#39;] stock = yf.download(ticker) # Выделение скорректированой цены закрытия all_adj_close = stock[[&#39;Adj Close&#39;]] # Вычисление доходности all_returns = np.log(all_adj_close / all_adj_close.shift(1)) # Выделение доходности по акциям aflt_returns = all_returns[&#39;Adj Close&#39;][[&#39;AFLT.ME&#39;]].fillna(0) # Выделение доходности по индексу МосБиржи moex_returns = all_returns[&#39;Adj Close&#39;][[&#39;IMOEX.ME&#39;]].fillna(0) # Создание нового DataFrame return_data = pd.concat([aflt_returns, moex_returns], axis=1)[1:] return_data.columns = [&#39;AFLT.ME&#39;, &#39;IMOEX.ME&#39;] # Добавляем столбец единиц и определяем X и y X = sm.add_constant(return_data[&#39;IMOEX.ME&#39;]) y = return_data[&#39;AFLT.ME&#39;] # Создание модели model_moex = sm.OLS(y,X).fit() # Вывод результатов print(model_moex.summary()) . [*********************100%***********************] 2 of 2 completed OLS Regression Results ============================================================================== Dep. Variable: AFLT.ME R-squared: 0.135 Model: OLS Adj. R-squared: 0.135 Method: Least Squares F-statistic: 454.5 Date: Mon, 18 Oct 2021 Prob (F-statistic): 7.89e-94 Time: 19:33:22 Log-Likelihood: 7260.3 No. Observations: 2909 AIC: -1.452e+04 Df Residuals: 2907 BIC: -1.450e+04 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] const -7.886e-05 0.000 -0.213 0.831 -0.001 0.001 IMOEX.ME 0.8101 0.038 21.320 0.000 0.736 0.885 ============================================================================== Omnibus: 651.369 Durbin-Watson: 1.857 Prob(Omnibus): 0.000 Jarque-Bera (JB): 22810.853 Skew: -0.295 Prob(JB): 0.00 Kurtosis: 16.706 Cond. No. 103. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . На сайте yahoo и Мосбиржи бета коэффициент отличается незначительно в большую сторону. Но надо честно признаться, что расчет для некоторых других акций с российской биржи показал более значительные отличия, но в пределах интервала. . . Тот же анализ для акции FB и индекса SP500. Здесь вычисление, как в оригинале, проводится через месячную доходность. . sp_500 = yf.download(&#39;^GSPC&#39;, start=&quot;2012-05-01&quot;) fb = yf.download(&#39;FB&#39;) # Пересчет в месячную доходность fb = fb.resample(&#39;BM&#39;).apply(lambda x: x[-1]) sp_500 = sp_500.resample(&#39;BM&#39;).apply(lambda x: x[-1]) monthly_prices = pd.concat([fb[&#39;Close&#39;], sp_500[&#39;Close&#39;]], axis=1) monthly_prices.columns = [&#39;FB&#39;, &#39;^GSPC&#39;] monthly_returns = monthly_prices.pct_change(1) clean_monthly_returns = monthly_returns.dropna(axis=0) X = clean_monthly_returns[&#39;^GSPC&#39;] y = clean_monthly_returns[&#39;FB&#39;] X1 = sm.add_constant(X) model_fb_sp_500 = sm.OLS(y, X1) results_fb_sp_500 = model_fb_sp_500.fit() print(results_fb_sp_500.summary()) . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed OLS Regression Results ============================================================================== Dep. Variable: FB R-squared: 0.181 Model: OLS Adj. R-squared: 0.174 Method: Least Squares F-statistic: 24.59 Date: Mon, 18 Oct 2021 Prob (F-statistic): 2.56e-06 Time: 19:47:24 Log-Likelihood: 110.11 No. Observations: 113 AIC: -216.2 Df Residuals: 111 BIC: -210.8 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] const 0.0131 0.009 1.446 0.151 -0.005 0.031 ^GSPC 1.1515 0.232 4.959 0.000 0.691 1.612 ============================================================================== Omnibus: 26.778 Durbin-Watson: 1.820 Prob(Omnibus): 0.000 Jarque-Bera (JB): 104.750 Skew: 0.671 Prob(JB): 1.79e-23 Kurtosis: 7.522 Cond. No. 26.8 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . . В этом случае все совпало и подтвердило возможность использование statsmodels для определения коэффициента бета. . Ну и в качестве бонуса — если Вы хотите получить только бета — коэффициент и остальную статистику вы хотите оставить в стороне, то предлагается еще один код для его расчета: . from scipy import stats slope, intercept, r_value, p_value, std_err = stats.linregress(X, y) print(slope) . 1.1515416131106546 . Правда это не означает, что всю остальные получаемые значения надо игнорировать, но для их понимания понадобятся знание статистики. Приведу небольшую выдержку из получаемых значений: . R-squared, который является коэффициентом детерминации и принимает значения от 0 до 1. Чем ближе значение коэффициента к 1, тем сильнее зависимость; | Adj. R-squared — скорректированный R-squared на основании числа наблюдений и числа степеней свободы; | std err — стандартная ошибка оценки коэффициента; | P&gt;|t| — р-значение Величина менее 0,05 считается статистически значимой; | 0.025 и 0.975 — нижнее и верхнее значения доверительного интервала. | и т.д. | . На этом пока что все. Конечно, представляет интерес поискать зависимость между различными величинами для того, чтобы через одну предсказать другую и получить профит. В одном из иностранных источников встретилось предсказание индекса через процентную ставку и уровень безработицы. Но если изменение процентной ставки в России можно взять с сайта Центробанка, то другие пока продолжаю искать. К сожалению, на сайте Росстата не удалось найти актуальные. Это заключительная публикация в рамках статей общего финансового анализа. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/04/12/general-financial-analysis-python-part-3.html",
            "relUrl": "/finance/investment/python/2020/04/12/general-financial-analysis-python-part-3.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Общий финансовый анализ на Python (Часть 2)",
            "content": "Скользящее окно (Moving Windows) . В заголовке я привел дословный перевод. Если кто меня поправит, и другой термин более применим — то спасибо. . Смысл скользящего окна– с каждым новым значением функция пересчитывается за заданный период времени. Этих функций большое количество. Для примера: rolling.mean(), rolling.std(), которые чаще всего и используют при анализе движения акций. rolling.mean() — это обычная скользящая средняя, которая сглаживает краткосрочные колебания и позволяет визуализировать общую тенденцию. . adj_close_px = sber[&#39;Adj Close&#39;] # Вычисляю скользящую среднию moving_avg = adj_close_px.rolling(window=40).mean() # Вывожу результат print(moving_avg[-10:]) . Date 2021-10-04 329.640254 2021-10-05 330.483754 2021-10-06 331.295254 2021-10-07 332.290504 2021-10-08 333.397253 2021-10-11 334.841753 2021-10-12 335.932753 2021-10-13 336.992754 2021-10-14 338.135253 2021-10-15 339.242503 Name: Adj Close, dtype: float64 . График, который позволит понять то, что получается в результате работы данной функции: . sber[&#39;40&#39;] = adj_close_px.rolling(window=40).mean() # Вычисление длинной скользящей средней sber[&#39;252&#39;] = adj_close_px.rolling(window=252).mean() # Построение полученных значений sber[[&#39;Adj Close&#39;, &#39;40&#39;, &#39;252&#39;]].plot(figsize=(20,20)) plt.show() . Как видно rolling.mean() справляется с поставленной задачей. Функция сглаживает краткосрочные колебания и позволяет увидеть долгосрочный тренд на основании которого можно принять решение: цена выше рассматриваемой скользящей средней — берем акцию, ниже — продаем акцию — если просто и я бы не советовал следовать этому методу. Как правило помимо скользящих средних используются и другие индикаторы, которые могут подтвердить правильность принимаемого решения. Каждый должен самостоятельно принять решение в зависимости от стиля торговли. . Волатильность . Волатильность акций — это изменение дисперсии доходности акций в течение определенного периода времени. Обычно сравнивают волатильность одной акции с другой, чтобы получить представление о том, какая может иметь больший риск, или с рыночным индексом, чтобы понять волатильность акций относительно рынка. Как правило, чем выше волатильность, тем рискованнее инвестиции в эту акцию. Необходимо отметить, что она не является постоянной и изменяется с течением времени. Это можно увидеть опять же при помощи функции rolling.std(), входящей в пакет pandas. Пример расчета изменения волатильности: . min_periods = 60 # Вычисляю волатильность vol = daily_pct_change.rolling(min_periods).std() * np.sqrt(min_periods) # Строю график vol.plot(figsize=(15, 5)) plt.show() . Прошу обратить внимание, что в отличие от прошлой недели у меня появилось еще два значения — индекс московской биржи (IMOEX.ME) и РБК (RBCM.ME). Их значения мне потребуются в следующей публикации про метод наименьших квадратов. А на сегодня все. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/03/29/general-financial-analysis-python-part-2.html",
            "relUrl": "/finance/investment/python/2020/03/29/general-financial-analysis-python-part-2.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Общий финансовый анализ на Python (Часть 1)",
            "content": "В прошлой статье рассмотрено как можно получить информацию по финансовым инструментам. Дальше будет опубликовано несколько статей о том, что первоначально можно делать с полученными данными, как проводить анализ и составлять стратегию. Материалы составлены на основании публикаций в иностранных источниках и курсах на одной из онлайн платформ. . В этой статье будет рассмотрено, как рассчитывать доходность, волатильность и построить один из основных индикаторов. . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt sber = yf.download(&#39;SBER.ME&#39;,&#39;2016-01-01&#39;) . [*********************100%***********************] 1 of 1 completed . Доходность . Данная величина представляет собой процентное изменение стоимости акции за один торговый день. Оно не учитывает дивиденды и комиссии. Его легко рассчитать используя функцию pct_change () из пакета Pandas. . Как правило используют лог доходность, так как она позволяет лучше понять и исследовать изменения с течением времени. . daily_close = sber[[&#39;Adj Close&#39;]] # Дневная доходность daily_pct_change = daily_close.pct_change() # Заменить NA значения на 0 daily_pct_change.fillna(0, inplace=True) print(daily_pct_change.head()) # Дневная лог доходность daily_log_returns = np.log(daily_close.pct_change()+1) print(daily_log_returns.head()) . Adj Close Date 2016-01-04 0.000000 2016-01-05 0.008979 2016-01-06 -0.020629 2016-01-11 -0.060093 2016-01-12 0.007470 Adj Close Date 2016-01-04 NaN 2016-01-05 0.008939 2016-01-06 -0.020845 2016-01-11 -0.061974 2016-01-12 0.007442 . Чтобы из полученных данных узнать недельную и/или месячную доходность, используют функцию resample(). . monthly = sber.resample(&#39;BM&#39;).apply(lambda x: x[-1]) # Месячная доходность print(monthly.pct_change().tail()) # Пересчитать `sber` по кварталам и взять среднее значение за квартал quarter = sber.resample(&quot;4M&quot;).mean() # Квартальную доходность print(quarter.pct_change().tail()) . Open High Low Close Adj Close Volume Date 2021-06-30 -0.010055 -0.018202 -0.017745 -0.015593 -0.015593 0.363875 2021-07-30 -0.012117 -0.000782 -0.007253 0.001604 0.001604 0.589072 2021-08-31 0.086022 0.077640 0.086870 0.072213 0.072213 -0.344803 2021-09-30 0.004505 0.035464 0.006936 0.033522 0.033522 1.449116 2021-10-29 0.145282 0.105815 0.123980 0.093531 0.093531 -0.536555 Open High Low Close Adj Close Volume Date 2020-09-30 0.056380 0.052326 0.063663 0.059189 0.231932 -0.288569 2021-01-31 0.127438 0.128327 0.125520 0.126951 0.263348 0.268118 2021-05-31 0.159379 0.156331 0.162285 0.159680 0.174569 -0.295427 2021-09-30 0.101638 0.099245 0.104338 0.101912 0.156688 -0.429262 2022-01-31 0.156123 0.168087 0.150327 0.160703 0.160703 0.702144 . Функция pct_change () удобна для использования, но в свою очередь скрывает то, как получается значение. Схожее вычисление, которое поможет понять механизм, можно выполнить при помощи shift() из пакета из пакета Pandas. Дневная цена закрытия делится на прошлую (сдвинутую на один) цену и из полученного значения вычитается единица. Но есть один незначительный минус – первое значение в результате получается NA. . Расчет доходности основан на формуле: . . Дальше строится диаграмма распределения доходности и рассчитывается основная статистика: . Для значений по российским акциям есть небольшая тонкость. К названию акцию добавляется точка и заглавными буквами ME. Спасибо знатоки на смартлабе подсказали. . daily_pct_change = daily_close / daily_close.shift(1) - 1 print(daily_pct_change.head()) . Adj Close Date 2016-01-04 NaN 2016-01-05 0.008979 2016-01-06 -0.020629 2016-01-11 -0.060093 2016-01-12 0.007470 . daily_pct_change.hist(bins=50) plt.show() # Общая статистика print(daily_pct_change.describe()) . Adj Close count 1454.000000 mean 0.001442 std 0.018295 min -0.161417 25% -0.008009 50% 0.000708 75% 0.011137 max 0.128987 . Распределение выглядит очень симметрично и нормально распределённым вокруг значения 0,00. Для получения других значений статистики используется функция description (). В результате видно, что среднее значение немного больше нуля, а стандартное отклонение составляет практически 0,02. . Кумулятивная доходность . Кумулятивная дневная прибыль полезна для определения стоимости инвестиций через определенные промежуток времени. Ее можно рассчитать, как приводится в коде ниже. . cum_daily_return = (1 + daily_pct_change).cumprod() print(cum_daily_return.tail()) . Adj Close Date 2021-10-11 6.651933 2021-10-12 6.504360 2021-10-13 6.531676 2021-10-14 6.467939 2021-10-15 6.371218 . cum_daily_return.plot(figsize=(8,5)) plt.show() . Можно пересчитать доходность в месячном периоде: . cum_monthly_return = cum_daily_return.resample(&quot;M&quot;).mean() print(cum_monthly_return.tail()) . Adj Close Date 2021-06-30 5.354601 2021-07-31 5.182009 2021-08-31 5.584808 2021-09-30 5.653339 2021-10-31 6.318509 . Знание того, как рассчитать доходность, является ценным при анализе акции. Но еще большую ценность оно представляет при сравнении с другими акциями. . Возьмем некоторые акции (выбор их совершенно случайный) и построим их диаграмму. . ticker = [&#39;AFLT.ME&#39;,&#39;DSKY.ME&#39;,&#39;IRAO.ME&#39;,&#39;PIKK.ME&#39;, &#39;PLZL.ME&#39;,&#39;SBER.ME&#39;,&#39;ENRU.ME&#39;] stock = yf.download(ticker,&#39;2018-01-01&#39;) # Дневная доходность в `daily_close_px` daily_pct_change = stock[&#39;Adj Close&#39;].pct_change() # Распределение daily_pct_change.hist(bins=50, sharex=True, figsize=(20,8)) plt.show() . [*********************100%***********************] 7 of 7 completed . Еще один полезный график —матрица рассеяния. Ее можно легко построить при помощи функции scatter_matrix (), входящей в библиотеку pandas. В качестве аргументов используется daily_pct_change и устанавливается параметр Ядерной оценки плотности — Kernel Density Estimation. Кроме того, можно установить прозрачность с помощью параметра alpha и размер графика с помощью параметра figsize. . from pandas.plotting import scatter_matrix # Матрица рассеивания `daily_pct_change` scatter_matrix(daily_pct_change, diagonal=&#39;kde&#39;, alpha=0.1,figsize=(20,20)) plt.show() . На этом пока все. В следующей статье будет рассмотрено вычисление волатильности, средней и использование метода наименьших квадратов. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/03/14/general-financial-analysis-python-part-1.html",
            "relUrl": "/finance/investment/python/2020/03/14/general-financial-analysis-python-part-1.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Получение котировок акций при помощи Python (перевод)",
            "content": "Представляю вашему вниманию перевод статьи «Historical Stock Price Data in Python» автора Ishan Shah. . Статья о том, как получить ежедневные исторические данные по акциям, используя yfinance, и минутные данные, используя alpha vantage. . Как вы знаете, акции относятся к очень волатильному инструменту и очень важно тщательно анализировать поведение цены, прежде чем принимать какие-либо торговые решения. Ну а сначала надо получить данные и python может помочь в этом. . Биржевые данные могут быть загружены при помощи различных пакетов. В этой статье будут рассмотрены yahoo finance. . Его можно установить при помощи команды pip install yfinance. Приведенный ниже код показывает, как получить данные для AAPL с 2016 по 2019 год и построить скорректированную цену закрытия (скорректированная цена закрытия на дивиденды и сплиты) на графике . import yfinance as yf # Get the data for the stock AAPL data = yf.download(&#39;AAPL&#39;,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;) # Import the plotting library import matplotlib.pyplot as plt %matplotlib inline # Plot the close price of the AAPL data[&#39;Adj Close&#39;].plot() plt.show() . [*********************100%***********************] 1 of 1 completed . Ну а если необходимо получить по нескольким акциям, то необходимо внести небольшое дополнение в код. Для хранения значений используется DataFrame. При помощи пакета matplotlib и полученных данных можно построить график дневной доходности. . import pandas as pd tickers_list = [&#39;AAPL&#39;, &#39;WMT&#39;, &#39;IBM&#39;, &#39;MU&#39;, &#39;BA&#39;, &#39;AXP&#39;] # Import pandas data = pd.DataFrame(columns=tickers_list) # Fetch the data for ticker in tickers_list: data[ticker] = yf.download(ticker,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;)[&#39;Adj Close&#39;] # Print first 5 rows of the data data.head() . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . AAPL WMT IBM MU BA AXP . Date . 2015-12-31 24.266079 | 54.044613 | 106.805664 | 14.140234 | 129.673157 | 63.341213 | . 2016-01-04 24.286833 | 54.185669 | 105.509621 | 14.309997 | 126.005119 | 61.556175 | . 2016-01-05 23.678221 | 55.472874 | 105.431984 | 14.799313 | 126.516304 | 60.609035 | . 2016-01-06 23.214846 | 56.028313 | 104.904243 | 14.200150 | 124.507416 | 58.925949 | . 2016-01-07 22.235073 | 57.333141 | 103.111473 | 13.640932 | 119.287819 | 58.395412 | . # Plot all the close prices ((data.pct_change()+1).cumprod()).plot(figsize=(10, 7)) # Show the legend plt.legend() # Define the label for the title of the figure plt.title(&quot;Adjusted Close Price&quot;, fontsize=16) # Define the labels for x-axis and y-axis plt.ylabel(&#39;Price&#39;, fontsize=14) plt.xlabel(&#39;Year&#39;, fontsize=14) # Plot the grid lines plt.grid(which=&quot;major&quot;, color=&#39;k&#39;, linestyle=&#39;-.&#39;, linewidth=0.5) plt.show() . Для значений по российским акциям есть небольшая тонкость. К названию акцию добавляется точка и заглавными буквами ME. Спасибо знатоки на смартлабе подсказали. . tickers_list_rus = [&#39;TTLK.ME&#39;, &#39;GMKN.ME&#39;, &#39;LSRG.ME&#39;, &#39;TATNP.ME&#39;, &#39;MSTT.ME&#39;,&#39;YNDX.ME&#39;] data_rus = pd.DataFrame(columns=tickers_list_rus) . for ticker in tickers_list_rus: data_rus[ticker] = yf.download(ticker,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . data_rus.head() . TTLK.ME GMKN.ME LSRG.ME TATNP.ME MSTT.ME YNDX.ME . Date . 2016-01-04 0.091873 | 5647.466797 | 410.957520 | 112.794388 | 52.599274 | 1064.099976 | . 2016-01-05 0.092520 | 5750.643066 | 422.438416 | 112.513115 | 52.870754 | 1133.900024 | . 2016-01-06 0.092844 | 5690.508789 | 430.975464 | 111.613007 | 52.802883 | 1112.000000 | . 2016-01-11 0.092844 | 5443.646973 | 409.485657 | 110.150345 | 52.056313 | 987.000000 | . 2016-01-12 0.095755 | 5413.896484 | 400.948547 | 108.575157 | 51.581223 | 999.000000 | . ((data_rus.pct_change()+1).cumprod()).plot(figsize=(10, 7)) # Show the legend plt.legend() # Define the label for the title of the figure plt.title(&quot;Adjusted Close Price&quot;, fontsize=16) # Define the labels for x-axis and y-axis plt.ylabel(&#39;Price&#39;, fontsize=14) plt.xlabel(&#39;Year&#39;, fontsize=14) # Plot the grid lines plt.grid(which=&quot;major&quot;, color=&#39;k&#39;, linestyle=&#39;-.&#39;, linewidth=0.5) plt.show() . В дальнейшем эти данные можно проанализировать, создать торговую стратегию и оценить эффективность при помощи пакета pyfolio. В нем можно оценить коэффициент Шарпа, коэффициент Сортино, максимальную просадку и многие другие необходимые показатели. . Надеюсь, что мой перевод оригинальной статьи будет для Вас полезен. Код был проверен и все работает. Но пока для меня остался вопрос в возможности использования Alpha vantage для российского рынка. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/02/09/getting-stock-quotes-using-python.html",
            "relUrl": "/finance/investment/python/2020/02/09/getting-stock-quotes-using-python.html",
            "date": " • Feb 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alexander Gladkikh, and I made my own website on GitHub dedicated to my Hobbies: Machine learning, Deep Learning, and algorithmic trading. . I take part in kaggle competitions, have knowledge of R and Python (Pandas, NumPy, Scipy, Scikit-learn, XGBoost), Java . At the main work I participate in projects on the use of new technologies in the field of labor protection and ecology. . I have been engaged in technical analysis of financial markets for a long time. Familiar with software Amibroker, and Metatrader Quik (scripting). . At work I had to deal with the analysis of data in the performance of research in biology at the Institute and writing projects on environmental protection. . My degrees . Corporate Energy University, 2020 . Digital production technologies in the power industry . YANDEX, MIPT, 2019 . Machine learning and data analysis . City Business School, 2019 . MINI-MBA Professional .",
          "url": "https://zmey56.github.io/blog//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zmey56.github.io/blog//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}