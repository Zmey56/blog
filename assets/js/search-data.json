{
  
    
        "post0": {
            "title": "Использование API Fmp Cloud для отбора акций по дивидендам на Nasdaq с помощью Python",
            "content": "Акции с высокой дивидентной доходностью часто являются отличной инвестиционной стратегией для инвесторов, стремящихся получать приток денежных средств каждый год. В данной статье буден создан скрипт на Python для отбора их на бирже NASDAQ. . &#1063;&#1090;&#1086; &#1090;&#1072;&#1082;&#1086;&#1077; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1090;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100;? . Возьму определение из Википедии. Дивиде́ндная дохо́дность (англ. dividend yield) — это отношение величины годового дивиденда на акцию к цене акции. Данная величина выражается чаще всего в процентах. . Пример . При цене акции ОАО «Лукойл» 1124,37 рублей и дивиденде 28 рублей на акцию дивидендная доходность будет равна: . . Так же необходимо обратить внимание, что многие растущие компании, такие как для примера Amazon и Yandex, не выплачивают дивиденды, поскольку они реинвестируют всю прибыль в развитие бизнеса. Поэтому дивидендная доходность для этих фирм будет равна нулю. . &#1056;&#1072;&#1089;&#1095;&#1077;&#1090; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1076;&#1085;&#1086;&#1081; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1080; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Расчет дивидендной доходности является простой задачей, которую можно выполнить с помощью финансового API под названием fmpcloud и Python. Этот API предлагает несколько бесплатных запросов в день после регистрации. . Первым делом нужно извлечь список тикеров для всех акций, торгующихся на Nasdaq, по которым собираемся рассчитать дивидендную доходность. | import requests demo = &#39;Ваш API CODE&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] #for ticker in tickers: # symbols.append(ticker[&#39;symbol&#39;]) . import requests demo = &#39;39b9d9eeb3ba3fe57e039284db7ed2c0&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] for ticker in tickers: symbols.append(ticker[&#39;symbol&#39;]) print(symbols) #[&#39;SMMCW&#39;, &#39;VOD&#39;, &#39;TRMD&#39;, &#39;TRMB&#39;, &#39;NBL&#39;, &#39;EMMA&#39;,...] . TypeError Traceback (most recent call last) &lt;ipython-input-3-4cfc3d43ef34&gt; in &lt;module&gt; 8 symbols = [] 9 for ticker in tickers: &gt; 10 symbols.append(ticker[&#39;symbol&#39;]) 11 print(symbols) 12 #[&#39;SMMCW&#39;, &#39;VOD&#39;, &#39;TRMD&#39;, &#39;TRMB&#39;, &#39;NBL&#39;, &#39;EMMA&#39;,...] TypeError: string indices must be integers . len(symbols) . 0 . После необходимо пройтись по полученому списку акций и получить финансовую информацию по компании. Так же необходимо понимать, что получаем только последние данные, а не за все время существование компании. | DivYield = {} for company in symbols: try: companydata = requests.get(f&#39;https://fmpcloud.io/api/v3/profile/{company}?apikey={demo}&#39;) companydata = companydata.json() latest_Annual_Dividend = companydata[0][&#39;lastDiv&#39;] price = companydata[0][&#39;price&#39;] market_Capitalization = companydata[0][&#39;mktCap&#39;] name = companydata[0][&#39;companyName&#39;] exchange = companydata[0][&#39;exchange&#39;] dividend_Yield= latest_Annual_Dividend/price DivYield[company] = {} DivYield[company][&#39;Dividend_Yield&#39;] = dividend_Yield DivYield[company][&#39;latest_Price&#39;] = price DivYield[company][&#39;latest_Dividend&#39;] = latest_Annual_Dividend DivYield[company][&#39;market_Capit_in_M&#39;] = market_Capitalization/1000000 DivYield[company][&#39;company_Name&#39;] = name DivYield[company][&#39;exchange&#39;] = exchange except: pass . Сбор данных может занять значительное по продолжительности время. После их можно представить в виде отсортированного DataFrame, где сверху будут акций с высокой дивидендной доходностью. . import pandas as pd DivYield_dataframe = pd.DataFrame.from_dict(DivYield, orient=&#39;index&#39;) DivYield_dataframe = DivYield_dataframe.sort_values([&#39;Dividend_Yield&#39;], ascending=[False]) DivYield_dataframe.head(15) . &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1087;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1088;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1072; &#1080; &#1079;&#1072;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1077; . Предварительно проведем расчет средней дивидендной доходности по акциям которые платят дивиденды: . meanDivNasdaq = DivYield_dataframe[DivYield_dataframe[&#39;Dividend_Yield&#39;]&gt;0][&#39;Dividend_Yield&#39;].mean() print(&quot;Средняя дивидендная доходность по рынку Nasdaq равна &quot;, &quot;{:.2%}&quot;.format(meanDivNasdaq)) . Самой высокой дивидендной доходностью в полученных результатах у акций компании Triumph Bancorp Inc — 21,57%. Правда по ним никогда не платили дивиденды. Так что в системе похоже сидит баг. Так же по другим рынкам заметил, что в список могут включаться акции по которым перестали платить дивиденды давно. А так, как подписка Free ограничена по количеству запросов, то подстроить ее не удалось. Так же в том случае, если при проверке выясняется, что дивиденды платили недавно, то все равно необходимо быть осторожным при выборе компаний по данному показателю, так как он может являться результатом падения цены акций и как следствия ростом дивидендной доходности. Так же выплата высоких дивидендов может не сохраниться в будущем, тем более если у компании возникнут финансовые проблемы. . Основной смысл в следующем - анализ дивидендной доходности не должен быть единственным критерием. Я для одного из своих портфелей так же смотрю: EPS, EBITDA, FCF, срок выплаты дивидендов, капитализация компании, чистая рентабельность (отношение выручки к прибыли) и коэффициент Net Debt/EBITDA. . Но как говориться - все вышеприведенное не является инвестиционной рекомендацией и выбор остается за каждым самостоятельно. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/10/24/_2021-04-10-high-divident-stocks.html",
            "relUrl": "/finance/investment/python/2021/10/24/_2021-04-10-high-divident-stocks.html",
            "date": " • Oct 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Анализ ETF с использованием Python",
            "content": "Как использовать библиотеки Python, такие как Pandas, Matplotlib и Seaborn, для получения информации из ежедневных данных о ценах и объемах c фондового рынка. . С проникновением аналитики во многие сферы нашей жизни она не могла обойти стороной финансы. В этой статье рассмотрим ее применение для анализа ETF с целью их анализа, в том числе и с применением визуализиции. . 1. &#1054; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; . Для анализа будем использовать данные ETF с валютным хейджом: FXCN, FXRL, FXIT, FXUS и FXRU. Временной ряд рассмотрим за три года с 2018 по 2020 года. Само исследование проведем в Google Colaboratory. . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import pandas as pd import numpy as np import matplotlib.pyplot as plt from google.colab import files import warnings warnings.filterwarnings(&quot;ignore&quot;) . Сначало необходимо загрузить данные, которые представлены в формате CSV. . uploaded = files.upload() for fn in uploaded.keys(): print(&#39;User uploaded file «{name}» with length {length} bytes&#39;.format(name=fn, length=len(uploaded[fn]))) . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving FXGD.csv to FXGD (1).csv Saving FXRU.csv to FXRU (1).csv Saving FXUS.csv to FXUS (1).csv Saving FXIT.csv to FXIT (1).csv Saving FXRL.csv to FXRL (1).csv Saving FXCN.csv to FXCN (1).csv User uploaded file «FXGD.csv» with length 53856 bytes User uploaded file «FXRU.csv» with length 32600 bytes User uploaded file «FXUS.csv» with length 56015 bytes User uploaded file «FXIT.csv» with length 56137 bytes User uploaded file «FXRL.csv» with length 55985 bytes User uploaded file «FXCN.csv» with length 56038 bytes . После этого прочтем данные с диска. Дальше необходимо создать два двадатафрейма - один с ценами закрытия, а другой с объемами торговли: . fxgd =pd.read_csv(&#39;/content/FXGD.csv&#39;) fxrl =pd.read_csv(&#39;/content/FXRL.csv&#39;) fxit =pd.read_csv(&#39;/content/FXIT.csv&#39;) fxus =pd.read_csv(&#39;/content/FXUS.csv&#39;) fxru =pd.read_csv(&#39;/content/FXRU.csv&#39;, sep=&#39;;&#39;) fxcn =pd.read_csv(&#39;/content/FXCN.csv&#39;) . def changeDF(df): df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) name =[x for x in globals() if globals()[x] is df][0] df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;HIGH&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) df = df.set_index([&#39;date&#39;]) df.columns = [name+&#39;_cl&#39;, name + &#39;_vol&#39;] return df . # df[&#39;date&#39;] = pd.to_datetime(df[&#39;&lt;DATE&gt;&#39;].astype(str), dayfirst=True) # name =[x for x in globals() if globals()[x] is df][0] # df = df.drop([&#39;&lt;DATE&gt;&#39;,&#39;&lt;TIME&gt;&#39;, &#39;&lt;OPEN&gt;&#39;, &#39;&lt;CLOSE&gt;&#39;, &#39;&lt;LOW&gt;&#39;], axis=1) # df = df.set_index([&#39;date&#39;]) # df.columns = [name] # return df . fxgd_change = changeDF(fxgd) fxrl_change = changeDF(fxrl) fxit_change = changeDF(fxit) fxus_change = changeDF(fxus) fxru_change = changeDF(fxru) fxcn_change = changeDF(fxcn) . etf = pd.concat([fxgd_change, fxrl_change, fxit_change, fxus_change, fxru_change, fxcn_change], axis=1) . etf.head() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . date . 2018-01-03 529.0 | 4340 | 1950.5 | 443 | 3612.0 | 581 | 2738.0 | 1049 | 641.0 | 139.0 | 2635.0 | 2098 | . 2018-01-04 527.0 | 1489 | 1992.0 | 659 | 3641.0 | 647 | 2745.0 | 586 | 639.0 | 128.0 | 2655.0 | 1331 | . 2018-01-05 526.0 | 1911 | 2004.5 | 846 | 3646.0 | 876 | 2744.0 | 322 | 637.0 | 306.0 | 2640.0 | 1664 | . 2018-01-09 525.5 | 5044 | 2024.0 | 2570 | 3673.0 | 1833 | 2766.0 | 653 | 638.0 | 448.0 | 2670.0 | 2304 | . 2018-01-10 527.5 | 9808 | 2030.0 | 765 | 3660.0 | 2485 | 2758.0 | 407 | 637.0 | 369.0 | 2665.0 | 1910 | . C FXRU пришлось немного поработать в EXCEL, так как скачанные данные прибивили лишний ноль к значению. По этому при загрузке пришлось указывать явный разделитель. . Дальше проверим наш датасет на предмет наличия значений NULL . print(etf.isnull().sum()) . fxgd_cl 0 fxgd_vol 0 fxrl_cl 0 fxrl_vol 0 fxit_cl 0 fxit_vol 0 fxus_cl 0 fxus_vol 0 fxru_cl 4 fxru_vol 4 fxcn_cl 0 fxcn_vol 0 dtype: int64 . Выбросим их, чтоб не мешали в дальнейшем расчете: . etf.dropna(inplace=True, axis=0) . Дальше имеет смысл посмотреть тип значений: . etf.dtypes . fxgd_cl float64 fxgd_vol int64 fxrl_cl float64 fxrl_vol int64 fxit_cl float64 fxit_vol int64 fxus_cl float64 fxus_vol int64 fxru_cl float64 fxru_vol float64 fxcn_cl float64 fxcn_vol int64 dtype: object . И посмотрим размер датасета: . etf.shape . (752, 12) . Так же дальше интересно посмотреть как вели себя ETF в последние полгода. Это можно сделать при помощи функции describe: . etf[-120:].describe() . fxgd_cl fxgd_vol fxrl_cl fxrl_vol fxit_cl fxit_vol fxus_cl fxus_vol fxru_cl fxru_vol fxcn_cl fxcn_vol . count 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | 120.000000 | . mean 973.690000 | 148832.116667 | 3121.208333 | 8971.225000 | 8860.925000 | 14784.708333 | 4758.458333 | 12685.116667 | 950.133333 | 32841.958333 | 3878.233333 | 17515.733333 | . std 36.967338 | 94656.673543 | 169.290817 | 6816.481138 | 572.529639 | 8391.294562 | 267.491739 | 7730.347512 | 28.046367 | 14373.882822 | 219.697987 | 14471.016798 | . min 878.000000 | 34678.000000 | 2848.500000 | 2907.000000 | 7513.000000 | 4769.000000 | 4140.000000 | 4392.000000 | 880.800000 | 11069.000000 | 3422.000000 | 4448.000000 | . 25% 946.100000 | 87124.250000 | 2998.000000 | 5388.500000 | 8466.500000 | 9388.750000 | 4578.000000 | 8902.000000 | 935.900000 | 21355.000000 | 3721.000000 | 8495.250000 | . 50% 985.900000 | 127780.500000 | 3083.000000 | 7584.500000 | 9051.500000 | 12681.500000 | 4807.500000 | 11277.000000 | 951.450000 | 29360.000000 | 3898.000000 | 12329.500000 | . 75% 1001.650000 | 175438.250000 | 3219.500000 | 10754.250000 | 9306.250000 | 17640.000000 | 4982.000000 | 13963.750000 | 971.050000 | 42583.750000 | 4064.500000 | 22652.250000 | . max 1033.600000 | 666819.000000 | 3488.500000 | 67809.000000 | 9776.000000 | 63506.000000 | 5157.000000 | 73672.000000 | 1012.800000 | 91275.000000 | 4312.000000 | 101084.000000 | . pct_chg_etf[:50].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | 50.000000 | . mean 0.018909 | 0.128734 | 0.141815 | 0.037679 | 0.010791 | 0.088726 | . std 0.634123 | 0.840398 | 1.222263 | 1.014099 | 0.544054 | 1.462912 | . min -1.291513 | -1.680871 | -4.318305 | -3.804348 | -1.550388 | -4.403670 | . 25% -0.381599 | -0.359217 | -0.432242 | -0.326851 | -0.312745 | -0.562852 | . 50% -0.094162 | 0.266967 | 0.175959 | 0.073884 | 0.000000 | 0.190041 | . 75% 0.379604 | 0.689444 | 0.847795 | 0.543863 | 0.319361 | 1.135292 | . max 1.826923 | 2.127660 | 3.065569 | 2.749529 | 1.107595 | 2.909091 | . В результате видно в каких пределах в последние полгода ETF провели большую часть аремени с вероятностью 75%. . После построим графики движения цены во времени. . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf.index, etf[&#39;fxgd_cl&#39;], &#39;tab:blue&#39; ) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf.index, etf[&#39;fxrl_cl&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf.index, etf[&#39;fxit_cl&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf.index, etf[&#39;fxus_cl&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf.index, etf[&#39;fxru_cl&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf.index, etf[&#39;fxcn_cl&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . Ежедневное процентное изменение цены etf вычисляется на основе процентного изменения между ценами закрытия 2 последовательных дней. Предположим, что цена закрытия вчера составляла 500 рублей, а сегодня она закрылась по 550 рублей. Таким образом, процентное изменение составляет 10%. т. е. ((550-500) / 500)*100. Здесь нет никакой тайны! . Далее, мы введем новый столбец, обозначающий дневную доходность в цене etf. Вычислить можно с помощью встроенной функции pct_change() в python. Так же немного переставлю колонки, чтоб визуально лучше воспринималось. . etf.columns . Index([&#39;fxgd_cl&#39;, &#39;fxgd_vol&#39;, &#39;fxrl_cl&#39;, &#39;fxrl_vol&#39;, &#39;fxit_cl&#39;, &#39;fxit_vol&#39;, &#39;fxus_cl&#39;, &#39;fxus_vol&#39;, &#39;fxru_cl&#39;, &#39;fxru_vol&#39;, &#39;fxcn_cl&#39;, &#39;fxcn_vol&#39;], dtype=&#39;object&#39;) . etf_cl = etf[[&#39;fxgd_cl&#39;, &#39;fxrl_cl&#39;, &#39;fxit_cl&#39;, &#39;fxus_cl&#39;, &#39;fxru_cl&#39;, &#39;fxcn_cl&#39;]] etf_cl_pct = etf_cl.pct_change()*100 etf_cl_pct.columns = [&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;] etf_vol = etf[[&#39;fxgd_vol&#39;, &#39;fxrl_vol&#39;, &#39;fxit_vol&#39;, &#39;fxus_vol&#39;, &#39;fxru_vol&#39;, &#39;fxcn_vol&#39;]] etf_new = pd.concat([etf_cl, etf_vol, etf_cl_pct], axis = 1) . etf_new.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-03 529.0 | 1950.5 | 3612.0 | 2738.0 | 641.0 | 2635.0 | 4340 | 443 | 581 | 1049 | 139.0 | 2098 | NaN | NaN | NaN | NaN | NaN | NaN | . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . etf_new = etf_new.dropna() . Представим изменение ежедневной доходности в виде графика во времени: . fig, axs = plt.subplots(3, 2, figsize=(15,15)) axs[0, 0].plot(etf_new.index, etf_new[&#39;fxgd_cl_pct&#39;], &#39;tab:blue&#39;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].plot(etf_new.index, etf_new[&#39;fxrl_cl_pct&#39;], &#39;tab:orange&#39;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].plot(etf_new.index, etf_new[&#39;fxit_cl_pct&#39;], &#39;tab:green&#39;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].plot(etf_new.index, etf_new[&#39;fxus_cl_pct&#39;], &#39;tab:red&#39;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].plot(etf_new.index, etf_new[&#39;fxru_cl_pct&#39;], &#39;tab:grey&#39;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].plot(etf_new.index, etf_new[&#39;fxcn_cl_pct&#39;], &#39;tab:purple&#39;) axs[2, 1].set_title(&#39;FXCN&#39;) for ax in axs.flat: ax.set(xlabel=&#39;Data&#39;, ylabel=&#39;Price&#39;) for ax in axs.flat: ax.label_outer() . В течение большей части времени доходность составляет от -2% до 2% со скачками без пересечения отметки в 6% с обеих сторон. Наиболее шумной выглядит ETF FXCN. . Так же можно проверить новостные статьи за те дни, когда наблюдался резкий рост/падение цен на etf и понять чем было обусловлено. . Построим гистограмму распределения ежедневных доходов: . import seaborn as sns sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(15,15)) sns.histplot(data=etf_new[&#39;fxgd_cl_pct&#39;], kde=True, color=&quot;orange&quot;, ax=axs[0, 0]) axs[0,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxrl_cl_pct&#39;], kde=True, color=&quot;olive&quot;, ax=axs[0, 1]) axs[0,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxit_cl_pct&#39;], kde=True, color=&quot;gold&quot;, ax=axs[1, 0]) axs[1,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxus_cl_pct&#39;], kde=True, color=&quot;grey&quot;, ax=axs[1, 1]) axs[1,1].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxru_cl_pct&#39;], kde=True, color=&quot;teal&quot;, ax=axs[2, 0]) axs[2,0].set_xlim(-10,10) sns.histplot(data=etf_new[&#39;fxcn_cl_pct&#39;], kde=True, color=&quot;brown&quot;, ax=axs[2, 1]) axs[2,1].set_xlim(-10,10) plt.show() . etf_new[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].describe() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . count 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | 751.000000 | . mean 0.084329 | 0.084240 | 0.140564 | 0.089850 | 0.056374 | 0.065255 | . std 1.081425 | 1.163047 | 1.398492 | 1.141144 | 0.771131 | 1.414819 | . min -5.709816 | -8.065290 | -6.874365 | -8.567335 | -5.198422 | -5.273973 | . 25% -0.456676 | -0.444714 | -0.574001 | -0.444633 | -0.346166 | -0.752409 | . 50% 0.030111 | 0.126835 | 0.206940 | 0.137979 | 0.026178 | 0.128783 | . 75% 0.622500 | 0.718721 | 0.887283 | 0.645403 | 0.445645 | 0.899653 | . max 5.619982 | 7.784431 | 8.297990 | 6.079599 | 4.604008 | 6.554307 | . Гистограммы ежедневных доходностей центрированы вокруг среднего значения, которое для всех etf было больше нуля и говорит о положительном тренде. Видно, что доходность для всех ETF большую часть времени лежала в пределах от -2,5 до 2,5%. Наибольшую доходность показали - FXIT, а наименьшую - FXRU. . 2. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1090;&#1088;&#1077;&#1085;&#1076;&#1072; . Затем мы добавляем новый столбец &quot;Тренд&quot;, значения которого основаны на ежедневном процентном изменении, которое мы рассчитали выше. Тенденция определяется отношением снизу. Скопируем датасет в новый, с которым и продолжим работу. . def trend(x): if x &gt; -0.5 and x &lt;= 0.5: return &#39;Практически или без изменений&#39; elif x &gt; 0.5 and x &lt;= 1.5: return &#39;Небольшой позитив&#39; elif x &gt; -1.5 and x &lt;= -0.5: return &#39;Небольшой негатив&#39; elif x &gt; 1.5 and x &lt;= 2.5: return &#39;Позитив&#39; elif x &gt; -2.5 and x &lt;= -1.5: return &#39;Негатив&#39; elif x &gt; 2.5 and x &lt;= 5: return &#39;Значительный позитив&#39; elif x &gt; -5 and x &lt;= -2.5: return &#39;Значительный негатив&#39; elif x &gt; 5: return &#39;Максимальный позитив&#39; elif x &lt;= -5: return &#39;Максимальный негатив&#39; . etf_trend = etf_new.copy() . etf_trend.columns[12:] . Index([&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;, &#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;], dtype=&#39;object&#39;) . for stock in etf_trend.columns[12:]: etf_trend[&quot;Trend_&quot; + str(stock)] = np.zeros(etf_trend[stock].count()) etf_trend[&quot;Trend_&quot;+ str(stock)] = etf_trend[stock].apply(lambda x:trend(x)) . etf_trend.head() . fxgd_cl fxrl_cl fxit_cl fxus_cl fxru_cl fxcn_cl fxgd_vol fxrl_vol fxit_vol fxus_vol fxru_vol fxcn_vol fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct Trend_fxgd_cl_pct Trend_fxrl_cl_pct Trend_fxit_cl_pct Trend_fxus_cl_pct Trend_fxru_cl_pct Trend_fxcn_cl_pct . date . 2018-01-04 527.0 | 1992.0 | 3641.0 | 2745.0 | 639.0 | 2655.0 | 1489 | 659 | 647 | 586 | 128.0 | 1331 | -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | Практически или без изменений | Позитив | Небольшой позитив | Практически или без изменений | Практически или без изменений | Небольшой позитив | . 2018-01-05 526.0 | 2004.5 | 3646.0 | 2744.0 | 637.0 | 2640.0 | 1911 | 846 | 876 | 322 | 306.0 | 1664 | -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . 2018-01-09 525.5 | 2024.0 | 3673.0 | 2766.0 | 638.0 | 2670.0 | 5044 | 2570 | 1833 | 653 | 448.0 | 2304 | -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | Практически или без изменений | Небольшой позитив | Небольшой позитив | Небольшой позитив | Практически или без изменений | Небольшой позитив | . 2018-01-10 527.5 | 2030.0 | 3660.0 | 2758.0 | 637.0 | 2665.0 | 9808 | 765 | 2485 | 407 | 369.0 | 1910 | 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | Практически или без изменений | . 2018-01-11 526.0 | 2042.0 | 3673.0 | 2755.0 | 635.0 | 2650.0 | 5548 | 1220 | 1282 | 968 | 326.0 | 1722 | -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | Практически или без изменений | Небольшой позитив | Практически или без изменений | Практически или без изменений | Практически или без изменений | Небольшой негатив | . etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts() . Практически или без изменений 351 Небольшой позитив 166 Небольшой негатив 141 Позитив 44 Негатив 25 Значительный позитив 13 Значительный негатив 7 Максимальный негатив 2 Максимальный позитив 2 Name: Trend_fxgd_cl_pct, dtype: int64 . Дальше можно взглянуть как вели себя акции акцииETF в последние 3 года. Для этого их изменения можно визуализировать при помощи круговых диаграмм, где каждый сектор представляет процент дней, в течение которых происходил каждый тренд. Для построения будем использовать функцию groupby() со столбцом тренда. . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(3, 2, figsize=(20,17)) axs[0, 0].pie(etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxgd_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 0].set_title(&#39;FXGD&#39;) axs[0, 1].pie(etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxrl_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[0, 1].set_title(&#39;FXRL&#39;) axs[1, 0].pie(etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxit_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 0].set_title(&#39;FXIT&#39;) axs[1, 1].pie(etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxus_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[1, 1].set_title(&#39;FXUS&#39;) axs[2, 0].pie(etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxru_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 0].set_title(&#39;FXRU&#39;) axs[2, 1].pie(etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts(), labels = etf_trend[&#39;Trend_fxcn_cl_pct&#39;].value_counts().index, autopct=&quot;%.1f%%&quot;) axs[2, 1].set_title(&#39;FXCN&#39;) plt.show() . За рассматриваемый период с 2018 года по 2020 года большую часть времени ETF практически не изменялись, или изменялись незначительно при заданных параметрах. Так же важно отметить, что при небольших изменениях они как правило были позитивными. При более больших - это соотношение сохранялось кроме FXRU. . 3. &#1045;&#1078;&#1077;&#1076;&#1085;&#1077;&#1074;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1080; &#1086;&#1073;&#1098;&#1077;&#1084;&#1099; . Следующим шагом продолжим работу с объемами: . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) axs[0].stem(etf_trend.index[-253:], etf_trend[&#39;fxgd_cl_pct&#39;][-253:]) axs[0].plot((etf_trend[&#39;fxgd_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[0].set_title(&#39;FXGD&#39;) axs[1].stem(etf_trend.index[-253:], etf_trend[&#39;fxrl_cl_pct&#39;][-253:]) axs[1].plot((etf_trend[&#39;fxrl_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[1].set_title(&#39;FXRL&#39;) axs[2].stem(etf_trend.index[-253:], etf_trend[&#39;fxit_cl_pct&#39;][-253:]) axs[2].plot((etf_trend[&#39;fxit_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[2].set_title(&#39;FXIT&#39;) axs[3].stem(etf_trend.index[-253:], etf_trend[&#39;fxus_cl_pct&#39;][-253:]) axs[3].plot((etf_trend[&#39;fxus_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[3].set_title(&#39;FXUS&#39;) axs[4].stem(etf_trend.index[-253:], etf_trend[&#39;fxru_cl_pct&#39;][-253:]) axs[4].plot((etf_trend[&#39;fxru_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[4].set_title(&#39;FXRU&#39;) axs[5].stem(etf_trend.index[-253:], etf_trend[&#39;fxcn_cl_pct&#39;][-253:]) axs[5].plot((etf_trend[&#39;fxcn_vol&#39;]/10000)[-253:], color = &#39;green&#39;, alpha = 0.5) axs[5].set_title(&#39;FXCN&#39;) . Text(0.5, 1.0, &#39;FXCN&#39;) . Сопоставляя ежедневный объем торговли(зеленым цветом) с ежедневной доходностью(синим цветом), было отмечено, что часто для ETF характерно, что когда объем торгов высок, наблюдается сравнительно высокий рост или падение цены. Объем торгов ETF в сочетании с ростом или падениемы на данный инструмент является показателем доверия трейдеров и инвесторов к конкретному ETF. . 4. &#1050;&#1086;&#1088;&#1088;&#1077;&#1083;&#1103;&#1094;&#1080;&#1086;&#1085;&#1085;&#1099;&#1081; &#1072;&#1085;&#1072;&#1083;&#1080;&#1079; ETF . Основное правило диверсификации - не клади все яйца в одну корзинку. По этому если мы решили собирать портфель из ETF, то они не должны быть сильно взаимосвязаны друг с другом. Математическим языком - коэффициент корреляции Пирсона между любой парой должен быть близок к 0. Смысл - они не должны падать синхронно, чтоб инвестиции не превратились в 0. . Проанализировать корреляцию между различными ETF можно с помощью парной диаграммы Seaborn. Для удобства оставим только процентные изменения за день в отдельном новом датафрейме. . pct_chg_etf = etf_new[etf_new.columns[12:]] . sns.set(style = &#39;ticks&#39;, font_scale = 1.25) sns.pairplot(pct_chg_etf) plt.show() . pct_chg_etf.corr() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . fxgd_cl_pct 1.000000 | -0.236301 | 0.107085 | 0.119773 | 0.590029 | 0.139992 | . fxrl_cl_pct -0.236301 | 1.000000 | 0.335061 | 0.300352 | -0.384120 | 0.232063 | . fxit_cl_pct 0.107085 | 0.335061 | 1.000000 | 0.895261 | 0.138016 | 0.641551 | . fxus_cl_pct 0.119773 | 0.300352 | 0.895261 | 1.000000 | 0.202682 | 0.610576 | . fxru_cl_pct 0.590029 | -0.384120 | 0.138016 | 0.202682 | 1.000000 | 0.197225 | . fxcn_cl_pct 0.139992 | 0.232063 | 0.641551 | 0.610576 | 0.197225 | 1.000000 | . На графике визуально можно увидеть наличие корреляции между различными ETF. Обратите внимание, что корреляционный анализ выполняется для ежедневного процентного изменения(дневной доходности) цены ETF, а не для их цены. . Из полученных графиков ясно видно, что следующие FXIT и FXUS не следует класть в одну корзину, так как между нми наблюдается сильная зависимость. Остальные могут быть включены в портфель, поскольку ни одна из двух оставшихся ETF не демонстрирует какой-либо существенной корреляции. . Но у визуального анализа есть существенный недостаток - он не предоставляет подробной информации о количественной оценки взаимосвязи, таких как значение R Пирсона и p нулевой гипотезы. В связи с чем при визуальном анализе остается под вопросом FXCN - есть ли у данного ETF сильная взаимосвязь с FXUS или нет. . Один из способов решения данного вопроса - построение графиков seaborn.jointplot с подробной информацией по значению R Пирсона (коэффициент корреляции Пирсона) для каждой пары ETF. Значение R Пирсона колеблется от -1 до 1. Отрицательное значение указывает на отрицательную линейную связь, в то время как положительное значение указывает на положительную связь. Значение R Пирсона ближе к 1 (или -1) указывает на сильную корреляцию, в то время как значение ближе к 0 указывает на слабую корреляцию. . Так же чем интересны данные графики - построение гистограмм распределения по краям, а так же значение p-value. . Но если рассматривать все пары, то нам потребуется большое количество графиков. По этому остановимся только на тех, которые вызывают сомнения: . pct_chg_etf.head() . fxgd_cl_pct fxrl_cl_pct fxit_cl_pct fxus_cl_pct fxru_cl_pct fxcn_cl_pct . date . 2018-01-04 -0.378072 | 2.127660 | 0.802879 | 0.255661 | -0.312012 | 0.759013 | . 2018-01-05 -0.189753 | 0.627510 | 0.137325 | -0.036430 | -0.312989 | -0.564972 | . 2018-01-09 -0.095057 | 0.972811 | 0.740538 | 0.801749 | 0.156986 | 1.136364 | . 2018-01-10 0.380590 | 0.296443 | -0.353934 | -0.289226 | -0.156740 | -0.187266 | . 2018-01-11 -0.284360 | 0.591133 | 0.355191 | -0.108774 | -0.313972 | -0.562852 | . from scipy.stats import stats a_1 = pct_chg_etf.fxit_cl_pct b_1 = pct_chg_etf.fxus_cl_pct b_2 = pct_chg_etf.fxcn_cl_pct g_1 = sns.jointplot(&#39;fxit_cl_pct&#39;, &#39;fxcn_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_1, p_1 = stats.pearsonr(a_1, b_1) g_1.ax_joint.annotate(f&#39;$ rho = {r_1:.3f}, p = {p_1:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_1.ax_joint.scatter(a_1, b_1) g_1.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxus&#39;, size=15) g_2 = sns.jointplot(&#39;fxus_cl_pct&#39;, &#39;fxit_cl_pct&#39;, pct_chg_etf, kind = &#39;scatter&#39;) r_2, p_2 = stats.pearsonr(a_1, b_2) g_2.ax_joint.annotate(f&#39;$ rho = {r_2:.3f}, p = {p_2:.3f}$&#39;, xy=(0.1, 0.9), xycoords=&#39;axes fraction&#39;, ha=&#39;left&#39;, va=&#39;center&#39;, bbox={&#39;boxstyle&#39;: &#39;round&#39;, &#39;fc&#39;: &#39;powderblue&#39;, &#39;ec&#39;: &#39;navy&#39;}) g_2.ax_joint.scatter(a_1, b_2) g_2.set_axis_labels(xlabel=&#39;fxit&#39;, ylabel=&#39;fxcn&#39;, size=15) plt.tight_layout() plt.show() . Первый гррафик подтвердил наличие сильной взаимосвязи между FXIT и FXUS, что говорит о нежелательности их брать в один портфель. В свою очередь корреляция между FXCN и FXIT оказалась ниже 0,7, что говорит о возможности совместного нахождения в одной корзине. . 5. &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1074;&#1086;&#1083;&#1072;&#1090;&#1080;&#1083;&#1100;&#1085;&#1086;&#1089;&#1090;&#1080; . Волатильность-один из важнейших показателей на финансовых рынках. Говорят, что ценная бумага обладает высокой волатильностью, если ее стоимость может резко измениться за короткий промежуток времени. С другой стороны, более низкая волатильность означает, что стоимость имеет тенденцию быть относительно стабильной в течение определенного периода времени. Эти изменения обусловлены несколькими факторами, включая спрос и предложение, настроения, жадность, страх и т.д. Математически волатильность измеряется с помощью статистической меры, называемой &quot;стандартным отклонением&quot;, которая измеряет отклонение актива от его средней стоимости. . Произведем рассчет 5-дневной скользящей средней дневной доходности и стандартного отклонения. После этого построим график. Все это можно выполнить при помощи функций Pandas rolling() и std(). . sns.set(style=&quot;darkgrid&quot;) fig, axs = plt.subplots(6, 1, figsize=(30,35)) for i, etf in enumerate(pct_chg_etf.columns): axs[i].plot(pct_chg_etf[etf].rolling(5).std()*np.sqrt(5)) axs[i].plot(pct_chg_etf[etf].rolling(7).mean()) axs[i].set_title(etf[:4], size=20) . volatility = pct_chg_etf[[&#39;fxgd_cl_pct&#39;, &#39;fxrl_cl_pct&#39;, &#39;fxit_cl_pct&#39;, &#39;fxus_cl_pct&#39;,&#39;fxru_cl_pct&#39;, &#39;fxcn_cl_pct&#39;]].rolling(5).std()*np.sqrt(5) . volatility[:150].plot(linewidth=4, figsize = (35, 15)) plt.legend(loc=2, prop={&#39;size&#39;: 16}) . &lt;matplotlib.legend.Legend at 0x7f6bad8d31d0&gt; . Как результат вы можете заметить, что наиболее сильная низкая волатильность характерна для ETF на российские акции - FXRU. Многие трейдеры и инвесторы ищут инвестиции с более высокой волатильностью, чтобы получать более высокую прибыль. Если финансовый инструмент не движется, он не только обладает низкой волатильностью, но и имеет низкий потенциал прибыли. С другой стороны, ценные бумаги с очень высоким уровнем волатильности могут иметь огромный потенциал прибыли, но риск также высок. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/09/18/data-analysis-visualization-finance.html",
            "relUrl": "/finance/investment/python/2021/09/18/data-analysis-visualization-finance.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Оптимизации портфеля с помощью Python и PyPortfolioOpt",
            "content": "В данной статье приводится пример вычисления оптимизированных весов активов в портфеле используя Портфельную теорию Марковица при помощи Python . &#1055;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1100;&#1085;&#1072;&#1103; &#1090;&#1077;&#1086;&#1088;&#1080;&#1103; &#1052;&#1072;&#1088;&#1082;&#1086;&#1074;&#1080;&#1094;&#1072; . Портфельная теория Марковица(далее ПТМ) (Modern portfolio theory) — разработанная Гарри Марковицем методика формирования инвестиционного портфеля, направленная на оптимальный выбор активов, исходя из требуемого соотношения доходность/риск. Сформулированные им в 1950-х годах идеи составляют основу современной портфельной теории. . Основные положения портфельной теории были сформулированы Гарри Марковицем при подготовке им докторской диссертации в 1950—1951 годах. . Рождением же портфельной теории Марковица считается опубликованная в «Финансовом журнале» в 1952 году статья «Выбор портфеля». В ней он впервые предложил математическую модель формирования оптимального портфеля и привёл методы построения портфелей при определённых условиях. Основная заслуга Марковица состояла в предложении вероятностной формализации понятий «доходность» и «риск», что позволило перевести задачу выбора оптимального портфеля на формальный математический язык. Надо отметить, что в годы создания теории Марковиц работал в RAND Corp., вместе с одним из основателей линейной и нелинейной оптимизации — Джорджем Данцигом и сам участвовал в решении указанных задач. Поэтому собственная теория, после необходимой формализации, хорошо ложилась в указанное русло. . Марковиц постоянно занимается усовершенствованием своей теории и в 1959 году выпускает первую посвящённую ей монографию «Выбор портфеля: эффективная диверсификация инвестиций». . В 1990 году, когда Марковицу вручают Нобелевскую премию, выходит книга «Средне-дисперсионный анализ при выборе портфеля и рынка капитала» ссылка. . 1. &#1054;&#1078;&#1080;&#1076;&#1072;&#1077;&#1084;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103;(Portfolio Expected Return) . Ожидаемая доходность портфеля будет зависеть от ожидаемой доходности каждого из активов, входящих в него. Такой подход позволяет снизить риск за счет диверсификации и одновременно максимизировать доход инвестора, поскольку убытки по одним инвестициям будут компенсированы доходом по другим. . Ожидаемая доходность портфеля представляет собой суммарную ожидаемую доходность входящих в него ценных бумаг, взвешенную с учетом их доли в портфеле. . . 2. &#1044;&#1080;&#1089;&#1087;&#1077;&#1088;&#1089;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; (Portfolio Variance ) . Дисперсия портфеля - это процесс, который определяет степень риска или волатильности, связанной с инвестиционным портфелем. Основная формула для расчета этой дисперсии фокусируется на взаимосвязи между так называемой дисперсией доходности и ковариацией, связанной с каждой из ценных бумаг, найденных в портфеле, а также с процентом или частью портфеля, который представляет каждая ценная бумага. . . 3. &#1050;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; &#1064;&#1072;&#1088;&#1087;&#1072; (Sharpe Ratio) . Коэффициент Шарпа измеряет доходность инвестиций по отношению к безрисковой ставке (казначейской ставке) и степени риска. В целом, более высокое значение коэффициента Шарпа указывает на лучшие и более прибыльные инвестиции. Таким образом, если сравнивать два портфеля с одинаковыми рисками, то при прочих равных условиях было бы лучше инвестировать в портфель с более высоким коэффициентом Шарпа. . . 4. &#1069;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1072;&#1103; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094;&#1072; (The Efficient Frontier ) . Определение и рисунок из Википедии: . Граница эффективности (англ. Efficient frontier) в портфельной теории Марковица — инвестиционный портфель, оптимизированный в отношении риска и доходности. Формально границей эффективности является набор портфелей, удовлетворяющих такому условию, что не существует другого портфеля с более высокой ожидаемой доходностью, но с таким же стандартным отклонением доходности. Понятие границы эффективности было впервые сформулировано Гарри Марковицем в 1952 году в модели Марковица. . Портфель может быть охарактеризован как «эффективный», если он имеет максимально возможный ожидаемый уровень доходности для своего уровня риска (который представлен стандартным отклонением доходности портфеля). Так, на график соотношения риска и доходности может быть нанесена любая возможная комбинация рискованных активов, и совокупность всех таких возможных портфелей определяет регион в этом пространстве. При отсутствии в портфеле безрискового актива граница эффективности определяется верхней (восходящей) частью гиперболы, ограничивающей область допустимых решений для всех соотношений активов в портфеле. . В случае же, если в портфель может быть включён безрисковый актив, граница эффективности вырождается в отрезок прямой линии, исходящий от значения доходности безрискового актива на оси ординат (ожидаемая доходность портфеля) и проходящий по касательной к границе области допустимых решений. Все портфели на отрезке между собственно безрисковым активом и точкой касания состоят из комбинации безрискового актива и рисковых активов, в то время как все портфели на линии выше и справа от точки касания образуются короткой позицией в безрисковом активе и инвестированием в рисковые активы. . . . &#1054;&#1087;&#1090;&#1080;&#1084;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; &#1087;&#1086;&#1088;&#1090;&#1092;&#1077;&#1083;&#1103; &#1085;&#1072; Python . 1. &#1048;&#1084;&#1087;&#1086;&#1088;&#1090; &#1085;&#1077;&#1086;&#1073;&#1093;&#1086;&#1076;&#1080;&#1084;&#1099;&#1093; &#1073;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1082; . Как обычно в начале импортируем все необходимые библиотеки для дальнейшей работы . import matplotlib.pyplot as plt import numpy as np import pandas as pd import pandas_datareader as web from matplotlib.ticker import FuncFormatter . Непосредственно для анализа и оптимизации портфеля существует библиотека PyPortfolioOpt. Так как она не входит в стандартный набор, то ее необходимо установить. . !pip install PyPortfolioOpt #Installing the Portfolio Optimzation Library . Collecting PyPortfolioOpt Downloading https://files.pythonhosted.org/packages/46/55/7d39d78d554ee33a7317e345caf01339da11406c28f18bc48794fe967935/PyPortfolioOpt-1.4.1-py3-none-any.whl (56kB) |████████████████████████████████| 61kB 3.2MB/s Requirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.1.5) Requirement already satisfied: scipy&lt;2.0,&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.4.1) Collecting cvxpy&lt;2.0.0,&gt;=1.1.10 Downloading https://files.pythonhosted.org/packages/83/47/fd1e818b8da30ef18695a0fbf9b66611ab18506f0a44fc69480a75f4db1b/cvxpy-1.1.12.tar.gz (1.3MB) |████████████████████████████████| 1.3MB 7.9MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: numpy&lt;2.0,&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from PyPortfolioOpt) (1.19.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;PyPortfolioOpt) (2018.9) Requirement already satisfied: ecos&gt;=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.0.7.post1) Requirement already satisfied: scs&gt;=1.1.6 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (2.1.3) Requirement already satisfied: osqp&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.6.2.post0) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.19-&gt;PyPortfolioOpt) (1.15.0) Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp&gt;=0.4.1-&gt;cvxpy&lt;2.0.0,&gt;=1.1.10-&gt;PyPortfolioOpt) (0.1.5.post0) Building wheels for collected packages: cvxpy Building wheel for cvxpy (PEP 517) ... done Created wheel for cvxpy: filename=cvxpy-1.1.12-cp37-cp37m-linux_x86_64.whl size=2731641 sha256=2c888a76787438c69d6a1dce26762a30ead689ee8b9da895efc81ad29620fbdf Stored in directory: /root/.cache/pip/wheels/9b/62/55/1da181c05c710c5d99bd560edebec3bd6a61cb69acef9dc00e Successfully built cvxpy Installing collected packages: cvxpy, PyPortfolioOpt Found existing installation: cvxpy 1.0.31 Uninstalling cvxpy-1.0.31: Successfully uninstalled cvxpy-1.0.31 Successfully installed PyPortfolioOpt-1.4.1 cvxpy-1.1.12 . Импортируем функции для дальнейшей работы: . from pypfopt.efficient_frontier import EfficientFrontier from pypfopt import risk_models from pypfopt import expected_returns from pypfopt.cla import CLA import pypfopt.plotting as pplt from matplotlib.ticker import FuncFormatter from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices . 2. &#1055;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077; &#1076;&#1072;&#1085;&#1085;&#1099;&#1093; &#1087;&#1086; &#1072;&#1082;&#1094;&#1080;&#1103;&#1084; &#1080;&#1079; &#1080;&#1085;&#1090;&#1077;&#1088;&#1085;&#1077;&#1090;&#1072; . Сначало установим опять пакет, который не входит в стандартный набор. Он позволяет получить данные по акциям с сайтя yahoo. . Тикеры, которые будут использоваться для анализа - одна из компаний входящих в лидеры в своем секторе. . !pip install yfinance --upgrade --no-cache-dir . Collecting yfinance Downloading https://files.pythonhosted.org/packages/a7/ee/315752b9ef281ba83c62aa7ec2e2074f85223da6e7e74efb4d3e11c0f510/yfinance-0.1.59.tar.gz Requirement already satisfied, skipping upgrade: pandas&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0) Requirement already satisfied, skipping upgrade: multitasking&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading https://files.pythonhosted.org/packages/30/c0/d0526314971fc661b083ab135747dc68446a3022686da8c16d25fcf6ef07/lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3MB) |████████████████████████████████| 6.3MB 6.4MB/s Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.12.5) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... done Created wheel for yfinance: filename=yfinance-0.1.59-py2.py3-none-any.whl size=23442 sha256=519c6bb89355fc0fab0d0a1c7f12df703543e4aadbde996b33dcf9592621bb6e Stored in directory: /tmp/pip-ephem-wheel-cache-weglluyo/wheels/f8/2a/0f/4b5a86e1d52e451757eb6bc17fd899629f0925c777741b6d04 Successfully built yfinance Installing collected packages: lxml, yfinance Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.3 yfinance-0.1.59 . import yfinance as yf tickers = [&#39;LKOH.ME&#39;,&#39;GMKN.ME&#39;, &#39;DSKY.ME&#39;, &#39;NKNC.ME&#39;, &#39;MTSS.ME&#39;, &#39;IRAO.ME&#39;, &#39;SBER.ME&#39;, &#39;AFLT.ME&#39;] df_stocks= yf.download(tickers, start=&#39;2018-01-01&#39;, end=&#39;2020-12-31&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . df_stocks.head() . AFLT.ME DSKY.ME GMKN.ME IRAO.ME LKOH.ME MTSS.ME NKNC.ME SBER.ME . Date . 2018-01-03 127.199066 | 70.177948 | 8249.352539 | 3.025520 | 2844.152100 | 197.995163 | 33.301483 | 145.441605 | . 2018-01-04 134.899857 | 71.621933 | 8455.913086 | 3.181567 | 2910.237305 | 202.738434 | 33.173889 | 149.769119 | . 2018-01-05 133.450317 | 71.621933 | 8441.316406 | 3.160166 | 2967.178467 | 202.199417 | 33.237682 | 149.643677 | . 2018-01-09 136.349426 | 71.116539 | 8521.605469 | 3.103989 | 3016.222900 | 203.421158 | 33.556660 | 150.772598 | . 2018-01-10 135.262268 | 71.658035 | 8507.006836 | 3.087939 | 3026.613525 | 204.427307 | 33.811848 | 149.116821 | . Дальше необходимо проверить есть ли среди полученных значений NaN. В случае их наличия они будут мешать дальнейшему исследованию. Для того, чтобы это решить, необходимо рассмотреть или иную акцию, или заменить их для примера средней ценой между днем до и после значения NaN. . nullin_df = pd.DataFrame(df_stocks,columns=tickers) print(nullin_df.isnull().sum()) . LKOH.ME 0 GMKN.ME 0 DSKY.ME 0 NKNC.ME 0 MTSS.ME 0 IRAO.ME 0 SBER.ME 0 AFLT.ME 0 dtype: int64 . 3. &#1056;&#1072;&#1089;&#1095;&#1077;&#1090;&#1099; . Перейдем к расчетам по оптимизации портфеля и начнем с определения ожидаемой доходности и дисперсии портфеля. Далее сохраним значения весов портфеля с максимальным коэффициентом Шарпа и минимальной диспрсией. . # mu = expected_returns.mean_historical_return(df_stocks) # #Sample Variance of Portfolio # Sigma = risk_models.sample_cov(df_stocks) # #Max Sharpe Ratio - Tangent to the EF # ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks # sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks # sharpe_pwt=ef.clean_weights() # print(sharpe_pwt) #Годовая доходность mu = expected_returns.mean_historical_return(df_stocks) #Дисперсия портфеля Sigma = risk_models.sample_cov(df_stocks) . ef = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks sharpe_pfolio=ef.max_sharpe() #May use add objective to ensure minimum zero weighting to individual stocks sharpe_pwt=ef.clean_weights() print(sharpe_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.0), (&#39;DSKY.ME&#39;, 0.22606), (&#39;GMKN.ME&#39;, 0.48796), (&#39;IRAO.ME&#39;, 0.0), (&#39;LKOH.ME&#39;, 0.0), (&#39;MTSS.ME&#39;, 0.02953), (&#39;NKNC.ME&#39;, 0.25645), (&#39;SBER.ME&#39;, 0.0)]) . Необходимо обратить внимание, что если изменить weight_bounds=(0,1) на weight_bounds=(-1,1), то в портфеле будут учитываться и короткие позиции по акциям. . Дальше посмотрим общие характеристики по портфелю. . ef.portfolio_performance(verbose=True) . Expected annual return: 37.1% Annual volatility: 20.7% Sharpe Ratio: 1.70 . (0.37123023494063007, 0.20717177784552962, 1.695357536597058) . Теперь посмотрим портфель с минимальной волатильностью: . ef1 = EfficientFrontier(mu, Sigma, weight_bounds=(0,1)) #weight bounds in negative allows shorting of stocks minvol=ef1.min_volatility() minvol_pwt=ef1.clean_weights() print(minvol_pwt) . OrderedDict([(&#39;AFLT.ME&#39;, 0.02876), (&#39;DSKY.ME&#39;, 0.24503), (&#39;GMKN.ME&#39;, 0.10403), (&#39;IRAO.ME&#39;, 0.0938), (&#39;LKOH.ME&#39;, 0.01168), (&#39;MTSS.ME&#39;, 0.41967), (&#39;NKNC.ME&#39;, 0.09704), (&#39;SBER.ME&#39;, 0.0)]) . ef1.portfolio_performance(verbose=True, risk_free_rate = 0.27) . Expected annual return: 24.0% Annual volatility: 16.9% Sharpe Ratio: -0.18 . (0.239915644698749, 0.16885732511472468, -0.17816434839774456) . 4. &#1055;&#1086;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085;&#1080;&#1077; &#1075;&#1088;&#1072;&#1092;&#1080;&#1082;&#1072; &#1101;&#1092;&#1092;&#1077;&#1082;&#1090;&#1080;&#1074;&#1085;&#1099;&#1093; &#1075;&#1088;&#1072;&#1085;&#1080;&#1094; . Заключительным шагом является построение эффективной границы для визуального представления и расчет распределения активов. Тут встречается одна сложность, решить которую пока что мне не удалось - пакет создан для анализа в долларах и в результате в выводе присутствует их обозначение. Но с другой стороны наличия значка &quot;$&quot; не должно сильно мешать. . Анализ произведем для суммы в 100 000 рублей. . cl_obj = CLA(mu, Sigma) ax = pplt.plot_efficient_frontier(cl_obj, showfig = False) ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: &#39;{:.0%}&#39;.format(x))) ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: &#39;{:.0%}&#39;.format(y))) . Первым этапом посчитаем портфель с минимальной волатильностью: . latest_prices = get_latest_prices(df_stocks) # Allocate Portfolio Value in $ as required to show number of shares/stocks to buy, also bounds for shorting will affect allocation #Min Volatility Portfolio Allocation $10000 allocation_minv, rem_minv = DiscreteAllocation(minvol_pwt, latest_prices, total_portfolio_value=100000).lp_portfolio() print(allocation_minv) print(&quot;Leftover Fund value in$ after building minimum volatility portfolio is ${:.2f}&quot;.format(rem_minv)) print(&quot;Осталось денежных средств после построения портфеля с минимальной волатильностью составляет {:.2f} рублей&quot;.format(rem_minv)) print() . {&#39;AFLT.ME&#39;: 41, &#39;DSKY.ME&#39;: 181, &#39;IRAO.ME&#39;: 1765, &#39;LKOH.ME&#39;: 1, &#39;MTSS.ME&#39;: 127, &#39;NKNC.ME&#39;: 107} Leftover Fund value in$ after building minimum volatility portfolio is $6152.03 Осталось денежных средств после построения портфеля с минимальной волатильностью составляет 6152.03 рублей . Вторым шагом портфель с максимальным коэффициентом Шарпа: . latest_prices1 = get_latest_prices(df_stocks) allocation_shp, rem_shp = DiscreteAllocation(sharpe_pwt, latest_prices1, total_portfolio_value=100000).lp_portfolio() print(allocation_shp) print(&quot;Leftover Fund value in$ after building Max Sharpe ratio portfolio is ${:.2f}&quot;.format(rem_shp)) print(&quot;Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа {:.2f} рублей&quot;.format(rem_shp)) #allocation using integer programming via PyPortfolioOpt User Guide #Alex Putkov code used for guidance and reference in applying integer programming . {&#39;DSKY.ME&#39;: 167, &#39;GMKN.ME&#39;: 2, &#39;MTSS.ME&#39;: 9, &#39;NKNC.ME&#39;: 283} Leftover Fund value in$ after building Max Sharpe ratio portfolio is $1319.05 Осталось денежных средств после построения портфеля с максимальным коэффициентом Шарпа 1319.05 рублей . В результтате нам предлагается купить для оптимального портфеля 167 акций Детского мира, 2 акции Норильского никеля, 9 акций МТС и 283 акцию Нижнекамскнефтехим. В результате у нас еще останется 1319 рублей. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/05/16/automating-portfolio-optimization.html",
            "relUrl": "/finance/investment/python/2021/05/16/automating-portfolio-optimization.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Использование API Fmp Cloud для отбора акций по дивидендам на Nasdaq с помощью Python",
            "content": "Акции с высокой дивидентной доходностью часто являются отличной инвестиционной стратегией для инвесторов, стремящихся получать приток денежных средств каждый год. В данной статье буден создан скрипт на Python для отбора их на бирже NASDAQ. . &#1063;&#1090;&#1086; &#1090;&#1072;&#1082;&#1086;&#1077; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1090;&#1085;&#1072;&#1103; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1100;? . Возьму определение из Википедии. Дивиде́ндная дохо́дность (англ. dividend yield) — это отношение величины годового дивиденда на акцию к цене акции. Данная величина выражается чаще всего в процентах. . Пример . При цене акции ОАО «Лукойл» 1124,37 рублей и дивиденде 28 рублей на акцию дивидендная доходность будет равна: . . Так же необходимо обратить внимание, что многие растущие компании, такие как для примера Amazon и Yandex, не выплачивают дивиденды, поскольку они реинвестируют всю прибыль в развитие бизнеса. Поэтому дивидендная доходность для этих фирм будет равна нулю. . &#1056;&#1072;&#1089;&#1095;&#1077;&#1090; &#1076;&#1080;&#1074;&#1080;&#1076;&#1077;&#1085;&#1076;&#1085;&#1086;&#1081; &#1076;&#1086;&#1093;&#1086;&#1076;&#1085;&#1086;&#1089;&#1090;&#1080; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; Python . Расчет дивидендной доходности является простой задачей, которую можно выполнить с помощью финансового API под названием fmpcloud и Python. Этот API предлагает несколько бесплатных запросов в день после регистрации. . Первым делом нужно извлечь список тикеров для всех акций, торгующихся на Nasdaq, по которым собираемся рассчитать дивидендную доходность. | import requests demo = &#39;39b9d9eeb3ba3fe57e039284db7ed2c0&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] for ticker in tickers: symbols.append(ticker[&#39;symbol&#39;]) . . import requests demo = &#39;39b9d9eeb3ba3fe57e039284db7ed2c0&#39; tickers = requests.get(f&#39;https://fmpcloud.io/api/v3/symbol/available-nasdaq?apikey={demo}&#39;) tickers = tickers.json() symbols = [] for ticker in tickers: symbols.append(ticker[&#39;symbol&#39;]) #print(symbols) #[&#39;SMMCW&#39;, &#39;VOD&#39;, &#39;TRMD&#39;, &#39;TRMB&#39;, &#39;NBL&#39;, &#39;EMMA&#39;,...] . len(symbols) . 5500 . После необходимо пройтись по полученому списку акций и получить финансовую информацию по компании. Так же необходимо понимать, что получаем только последние данные, а не за все время существование компании. | DivYield = {} for company in symbols: try: companydata = requests.get(f&#39;https://fmpcloud.io/api/v3/profile/{company}?apikey={demo}&#39;) companydata = companydata.json() latest_Annual_Dividend = companydata[0][&#39;lastDiv&#39;] price = companydata[0][&#39;price&#39;] market_Capitalization = companydata[0][&#39;mktCap&#39;] name = companydata[0][&#39;companyName&#39;] exchange = companydata[0][&#39;exchange&#39;] dividend_Yield= latest_Annual_Dividend/price DivYield[company] = {} DivYield[company][&#39;Dividend_Yield&#39;] = dividend_Yield DivYield[company][&#39;latest_Price&#39;] = price DivYield[company][&#39;latest_Dividend&#39;] = latest_Annual_Dividend DivYield[company][&#39;market_Capit_in_M&#39;] = market_Capitalization/1000000 DivYield[company][&#39;company_Name&#39;] = name DivYield[company][&#39;exchange&#39;] = exchange except: pass . Сбор данных может занять значительное по продолжительности время. После их можно представить в виде отсортированного DataFrame, где сверху будут акций с высокой дивидендной доходностью. . import pandas as pd DivYield_dataframe = pd.DataFrame.from_dict(DivYield, orient=&#39;index&#39;) DivYield_dataframe = DivYield_dataframe.sort_values([&#39;Dividend_Yield&#39;], ascending=[False]) DivYield_dataframe.head(15) . &#1040;&#1085;&#1072;&#1083;&#1080;&#1079; &#1087;&#1086;&#1083;&#1091;&#1095;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1088;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1072; &#1080; &#1079;&#1072;&#1082;&#1083;&#1102;&#1095;&#1077;&#1085;&#1080;&#1077; . Предварительно проведем расчет средней дивидендной доходности по акциям которые платят дивиденды: . meanDivNasdaq = DivYield_dataframe[DivYield_dataframe[&#39;Dividend_Yield&#39;]&gt;0][&#39;Dividend_Yield&#39;].mean() print(&quot;Средняя дивидендная доходность по рынку Nasdaq равна &quot;, &quot;{:.2%}&quot;.format(meanDivNasdaq)) . Самой высокой дивидендной доходностью в полученных результатах у акций компании Triumph Bancorp Inc — 21,57%. Правда по ним никогда не платили дивиденды. Так что в системе похоже сидит баг. Так же по другим рынкам заметил, что в список могут включаться акции по которым перестали платить дивиденды давно. А так, как подписка Free ограничена по количеству запросов, то подстроить ее не удалось. Так же в том случае, если при проверке выясняется, что дивиденды платили недавно, то все равно необходимо быть осторожным при выборе компаний по данному показателю, так как он может являться результатом падения цены акций и как следствия ростом дивидендной доходности. Так же выплата высоких дивидендов может не сохраниться в будущем, тем более если у компании возникнут финансовые проблемы. . Основной смысл в следующем - анализ дивидендной доходности не должен быть единственным критерием. Я для одного из своих портфелей так же смотрю: EPS, EBITDA, FCF, срок выплаты дивидендов, капитализация компании, чистая рентабельность (отношение выручки к прибыли) и коэффициент Net Debt/EBITDA. . Но как говориться - все вышеприведенное не является инвестиционной рекомендацией и выбор остается за каждым самостоятельно. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2021/04/10/high-divident-stocks.html",
            "relUrl": "/finance/investment/python/2021/04/10/high-divident-stocks.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Using machine learning to predict gold mining stock prices",
            "content": "As a basis, I took a notebook published on colab for oil. This notebook examines the analysis of gold prices and shares of gold mining companies using machine analysis methods: linear regression, cluster analysis, and random forest. I immediately warn you that this post does not attempt to show the current situation and predict the future direction. Just like the author for oil, this article does not aim to raise or refute the possibilities of machine learning for analyzing stock prices or other tools. I upgraded the code for gold research in order to encourage those who are interested in further reflection and listen to constructive criticism in their address. . import yfinance as yf import pandas as pd import numpy as np import seaborn as sns from sklearn import metrics import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression . 1. Loading data . For the price of gold, take the value of the exchange-traded investment Fund SPDR Gold Trust, whose shares are 100% backed by precious metal. The quotes will be compared with the prices of gold mining companies &#39; shares: . Newmont Goldcorp (NMM) | Barrick Gold (GOLD) | AngloGold Ashanti (AU) | Kinross Gold (KGC) | Newcrest Mining (ENC) | Polyus (PLZL) | Polymetal (POLY) | Seligdar (SELG) | . gold = pd.DataFrame(yf.download(&quot;GLD&quot;, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed . gold = gold.reset_index() gold.columns = [&quot;Date&quot;,&quot;gold_price&quot;] gold[&#39;Date&#39;] = pd.to_datetime(gold[&#39;Date&#39;]) gold.head() . Date gold_price . 0 2010-01-04 | 109.800003 | . 1 2010-01-05 | 109.699997 | . 2 2010-01-06 | 111.510002 | . 3 2010-01-07 | 110.820000 | . 4 2010-01-08 | 111.370003 | . It is necessary to move the price of gold, as we will be interested in how yesterday&#39;s price affected today&#39;s stock price. . gold[&quot;gold_price&quot;] = gold[&quot;gold_price&quot;].shift(1) . shares=[&quot;NMM.SG&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,&quot;PLZL.ME&quot;,&quot;POLY.ME&quot;,&quot;SELG.ME&quot;] data= yf.download(shares, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . data = data.reset_index() data.head() . Date AU GOLD KGC NCM.AX NMM.SG PLZL.ME POLY.ME SELG.ME . 0 2010-01-04 | 39.698944 | 34.561649 | 18.105721 | 33.237167 | 26.924570 | NaN | NaN | NaN | . 1 2010-01-05 | 40.320408 | 34.989510 | 18.594805 | 33.901924 | 27.116940 | NaN | NaN | NaN | . 2 2010-01-06 | 41.601028 | 35.733963 | 19.256504 | 33.901924 | 27.289278 | NaN | NaN | NaN | . 3 2010-01-07 | 41.130215 | 35.229092 | 19.352404 | 34.298923 | NaN | NaN | NaN | NaN | . 4 2010-01-08 | 41.601028 | 35.451572 | 19.601744 | 33.421829 | 27.702093 | NaN | NaN | NaN | . data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) . all_data=pd.DataFrame() . for index in range(len(shares)): stock=pd.DataFrame() # transform the data stock=data.loc[:, (&quot;Date&quot;,shares[index])] stock[&quot;Date&quot;]=stock[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) stock.columns=[&quot;Date&quot;,&quot;share_price&quot;] test=pd.DataFrame(gold) output=stock.merge(test,on=&quot;Date&quot;,how=&quot;left&quot;) #combining two data sets stock[&quot;gold_price&quot;]=output[&quot;gold_price&quot;] stock[&#39;share_price&#39;]=pd.to_numeric(stock[&#39;share_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&#39;gold_price&#39;]=pd.to_numeric(stock[&#39;gold_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&quot;year&quot;]=pd.to_datetime(stock[&quot;Date&quot;]).dt.year #Create a column with years for subsequent filtering stock[&quot;name&quot;]=shares[index] stock = stock.dropna() #delete all NAN lines #creating a column with a scaled share price scaler=MinMaxScaler() stock[&quot;share_price_scaled&quot;]=scaler.fit_transform(stock[&quot;share_price&quot;].to_frame()) #add data to the main dataframe all_data=all_data.append(stock) #add the data . all_data_15 = all_data[(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)] all_data_15.head() . Date share_price gold_price year name share_price_scaled . 1301 2015-01-02 | 14.269927 | 113.580002 | 2015 | NMM.SG | 0.052072 | . 1302 2015-01-05 | 14.845476 | 114.080002 | 2015 | NMM.SG | 0.071190 | . 1303 2015-01-06 | 15.601913 | 115.800003 | 2015 | NMM.SG | 0.096317 | . 1304 2015-01-07 | 15.645762 | 117.120003 | 2015 | NMM.SG | 0.097773 | . 1305 2015-01-08 | 15.517859 | 116.430000 | 2015 | NMM.SG | 0.093525 | . 2. Data analysis . It is best to start analyzing data by presenting it visually, which will help you understand it better. . 2.1 Chart of gold price changes . gold[[&#39;Date&#39;,&#39;gold_price&#39;]].set_index(&#39;Date&#39;).plot(color=&quot;green&quot;, linewidth=1.0) plt.show() . 2.2. Plotting the pairplot chart for the price of Polyus and Barrick Gold shares over the past five years . palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) g = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;POLY.ME&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) g.fig.suptitle(&quot;Polyuse&quot;, y=1.08) palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) f = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;GOLD&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) f.fig.suptitle(&#39;Barrick Gold&#39;, y=1.08) plt.show() . A paired graph allows you to see the distribution of data by showing the paired relationships in the data set and the univariate distribution of data for each variable. You can also use the palette to see how this data changed in different years. . The chart is particularly interesting for 2016 and 2019, as it looks like the price of the Pole stock, Barrick Gold and the price of gold are lined up along the same line. We can also conclude from the distribution charts that the price of gold and stocks moved gradually towards higher values. . 2.3 Violinplot for the gold price . plt.figure(figsize=(10,10)) sns.set_style(&quot;whitegrid&quot;) palette=sns.cubehelix_palette(5, start=2.8, rot=0, dark=0.2, light=0.8, reverse=False) sns.violinplot(x=&quot;year&quot;, y=&quot;gold_price&quot;, data=all_data_15[[&quot;gold_price&quot;,&quot;year&quot;]], inner=&quot;quart&quot;, palette=palette, trim=True) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Price gold&quot;) plt.show() . 2.4 Violinplot for multiple shares . sns.catplot(x=&quot;year&quot;, y=&quot;share_price_scaled&quot;, col=&#39;name&#39;, col_wrap=3,kind=&quot;violin&quot;, split=True, data=all_data_15,inner=&quot;quart&quot;, palette=palette, trim=True, height=4, aspect=1.2) sns.despine(left=True) . A large fluctuation in gold prices was noted according to the charts in 2016 and 2019. As you can see from the graphs in the following figure, some companies such as Newmont Mining, Barrick Gold, AngloGold Ashanti, Newcrest Mining and Polymetal were also affected. It should also be noted that all prices are marked in the range from 0 to 1 and this may lead to inaccuracies in the interpretation. . Next, we will build distribution charts for one Russian company - Polymetal and one foreign company - Barrick Gold . sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;POLY.ME&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . It is necessary to pay attention to the distribution of the share price for the two companies and it will become clear that the shape of the density graph is the same for them. . 2.5 Charts of the dependence of the share price of various companies on the price of gold . sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,line_kws={&#39;color&#39;: &#39;blue&#39;},scatter_kws={&#39;color&#39;: &#39;grey&#39;}).set(ylim=(0, 1)) plt.show() . In fact, you won&#39;t be able to see much on these charts, although some stocks seem to have a relationship. . The next step is to try to color the charts depending on the years. . palette=sns.cubehelix_palette(5, start=2, rot=0, dark=0, light=.95, reverse=False) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,hue=&quot;year&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,palette=palette,height=4).set(ylim=(0, 1)) plt.show() . Here the picture is a little better in the sense that some companies have a data cloud stretching along a straight line in some years, which may indicate the existence of a dependency. . 3 Machine learning and prediction . I will give a definition for machine learning from Wikipedia: Machine learning is a class of artificial intelligence methods that are characterized not by direct problem solving, but by learning in the process of applying solutions to many similar problems. To build such methods, we use mathematical statistics, numerical methods, optimization methods, probability theory, graph theory, and various techniques for working with data in digital form. . Usually, machine learning algorithms can be classified into the following categories: learning with a teacher and learning without a teacher. Here is their definition from one of the sites: . Supervised learning is one of the sections of machine learning dedicated to solving the following problem. There is a set of objects (situations) and the set of possible answers (responses, reactions). There is some relationship between responses and objects, but it is unknown. Only a finite set of use cases is known — the &quot;object, response&quot; pairs, called the training sample. Based on this data, you need to restore the dependency, that is, build an algorithm that can give a fairly accurate answer for any object. To measure the accuracy of responses, a quality functional is introduced in a certain way. see the Links) . Unsupervised learning is one of the sections of machine learning. Studies a wide class of data processing problems in which only descriptions of a set of objects (training sample) are known, and it is required to detect internal relationships, dependencies, and patterns that exist between objects. Learning without a teacher is often contrasted with learning with a teacher, when each training object is given a &quot;correct answer&quot;, and you need to find the relationship between the objects and the answers. see links) . The following machine learning methods will be discussed later: . Cluster analysis | Linear regression | Random forest | . Using these algorithms, you can evaluate overvalued or undervalued stocks relative to the price of gold and possible movement on the next day. I remind you that you must be very careful and use the conclusions from this post at your own risk. I also remind you that my main goal is to show the potential of machine learning for stock valuation. . 3.1. Cluster analysis for Barrick Gold stock . Clustering is the task of dividing a set of objects into groups called clusters. Each group should contain &quot;similar&quot; objects, and objects from different groups should be as different as possible. . from sklearn.cluster import KMeans poly=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;] # We need to scale also gold price, so clustering is not influenced by the relative size of one axis. poly=pd.DataFrame(poly) poly[&#39;gold_price_scaled&#39;] = scaler.fit_transform(poly[&quot;gold_price&quot;].to_frame()) poly[&quot;cluster&quot;] = KMeans(n_clusters=5, random_state=1).fit_predict(poly[[&quot;share_price_scaled&quot;,&quot;gold_price_scaled&quot;]]) # The 954 most common RGB monitor colors https://xkcd.com/color/rgb/ colors = [&quot;baby blue&quot;, &quot;amber&quot;, &quot;scarlet&quot;, &quot;grey&quot;,&quot;milk chocolate&quot;, &quot;windows blue&quot;] palette=sns.xkcd_palette(colors) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,ci=None,palette=palette, hue=&quot;cluster&quot;,fit_reg=0 ,data=poly) plt.show() . Cluster analysis is used in a large number of machine learning tasks. But I have given it only for informational purposes, since in this form it does not bring much benefit to our analysis. . 3.2. Linear regression between Barrick Gold shares and the gold price . Next, we will build a regular linear regression using training with a teacher. The goal is to estimate the forecast of data for the last 100 days of 2019 based on data from 2018/2019 (excluding estimated ones). Training data is the data used to build the model, and test data is the data that we will try to predict. . for sh in shares: print(sh) #Data Preparation share_18=pd.DataFrame() share_18=all_data_15[(all_data_15[&#39;name&#39;]==sh)] # Get data 2018/19 share_18=share_18[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Just using 1 variable for linear regression. Split the data into training/testing sets train = share_18[:-100] test = share_18[-100:] x_train=train[&quot;gold_price&quot;].to_frame() y_train=train[&#39;share_price&#39;].to_frame() x_test=test[&quot;gold_price&quot;].to_frame() y_test=test[&#39;share_price&#39;].to_frame() regr = LinearRegression() #Create linear regression object regr.fit(x_train,y_train) #Train the model using the training sets print(&quot;Coefficients: &quot;, float(regr.coef_)) print(np.corrcoef(x_train,y_train, rowvar=False)) y_pred = regr.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # Plot outputs using matplotlib plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . NMM.SG Coefficients: 0.6629423053739908 [[1. 0.790953] [0.790953 1. ]] Mean Absolute Error: 6.063058573972694 Mean Squared Error: 39.21188296210148 Root Mean Squared Error: 6.261939233344689 . GOLD Coefficients: 0.3355465472461071 [[1. 0.67139243] [0.67139243 1. ]] Mean Absolute Error: 3.3769293704374657 Mean Squared Error: 11.756813554455096 Root Mean Squared Error: 3.4288210152259473 . AU Coefficients: 0.31252669952857776 [[1. 0.67830589] [0.67830589 1. ]] Mean Absolute Error: 2.2471377544809683 Mean Squared Error: 5.789211153877581 Root Mean Squared Error: 2.4060779608893768 . KGC Coefficients: 0.10461302060876282 [[1. 0.78266367] [0.78266367 1. ]] Mean Absolute Error: 1.0583009847297946 Mean Squared Error: 1.1523726951635975 Root Mean Squared Error: 1.073486234268329 . NCM.AX Coefficients: 0.5623005799590818 [[1. 0.79891272] [0.79891272 1. ]] Mean Absolute Error: 2.0335289996635937 Mean Squared Error: 5.836462091267656 Root Mean Squared Error: 2.415877085297937 . PLZL.ME Coefficients: 103.84435014609612 [[1. 0.60373084] [0.60373084 1. ]] Mean Absolute Error: 1315.093426667142 Mean Squared Error: 1776892.2964767825 Root Mean Squared Error: 1333.0012364873419 . POLY.ME Coefficients: 10.772023429299809 [[1. 0.63694034] [0.63694034 1. ]] Mean Absolute Error: 69.33753863275061 Mean Squared Error: 6800.525447108329 Root Mean Squared Error: 82.46529844187995 . SELG.ME Coefficients: 0.15570348678870732 [[1. 0.51630147] [0.51630147 1. ]] Mean Absolute Error: 1.8096071903165585 Mean Squared Error: 4.039450515732427 Root Mean Squared Error: 2.009838430255633 . From the above charts, we can conclude that the price of gold predicts the price of shares of foreign companies on the next day quite well. In Russian companies, this picture looks much worse. Of course, there may be a false impression about Seligdar shares. But visual analysis of the chart allows you to discard this assumption. . 3.3 Random forest on Newmont Goldcorp shares against the price of gold and shares of gold companies . Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . The random forest algorithm accepts more than one variable in the input data to predict the output data. It works very efficiently on large amounts of data, can handle many input variables, has efficient methods for estimating missing data, and many other advantages. The main disadvantages are: . Random forests is slow to generate forecasts because it has many decision trees. Whenever it makes a forecast, all the trees in the forest must make a forecast for the same given input and then vote on it. This whole process takes a long time. | the Model is difficult to interpret compared to the decision tree, where you can easily make a decision by following the path in the tree. | One of the great advantages of a random forest is that it can be used for both classification and regression problems, which make up most of today&#39;s machine learning systems. I will talk about random forests in classification, since classification is sometimes considered a building block of machine learning. Below you can see what a random forest with two trees looks like: . In addition to the gold price, we will use other variables to forecast the Newmont Goldcorp share price. This will be the share prices of other foreign gold mining companies. I know it doesn&#39;t make a lot of sense, but we just want to see how to build this type of model. This will allow us to see the impact of each of them on the final forecast.Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . from sklearn.ensemble import RandomForestRegressor # 1.- Data Preparation nmm15=pd.DataFrame() nmm15=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NMM.SG&quot;) &amp; (all_data_15[&#39;year&#39;]&gt;2016 )] nmm15=nmm15[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Load share price of other variables nmm15[&#39;GOLD&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;GOLD&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;GOLD&#39;] = nmm15[&#39;GOLD&#39;].shift(1) nmm15[&#39;AU&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;AU&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;AU&#39;] = nmm15[&#39;AU&#39;].shift(1) nmm15[&#39;KGC&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;KGC&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;KGC&#39;] = nmm15[&#39;KGC&#39;].shift(1) nmm15[&#39;NCM.AX&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NCM.AX&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;NCM.AX&#39;] = nmm15[&#39;NCM.AX&#39;].shift(1) nmm15 = nmm15.drop(nmm15.index[0]) train = nmm15[:-100] test = nmm15[-100:] x_train=train[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]] y_train=train[&#39;share_price&#39;] x_test=test[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,]] y_test=test[&#39;share_price&#39;].to_frame() # 2.- Create Randomforest object usinig a max depth=5 regressor = RandomForestRegressor(n_estimators=200, max_depth=5 ) # 3.- Train data clf=regressor.fit(x_train, y_train) # 4.- Predict! y_pred=regressor.predict(x_test) y_pred_list = list(y_pred) y_pred=pd.DataFrame(y_pred) . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_pred=plt.scatter(nmm15[&quot;gold_price&quot;], regressor.predict(nmm15[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]]), color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train,plt_pred),(&quot;train data&quot;,&quot;prediction&quot;)) plt.show() . The resulting model looks really good in addition, we must remember that Random Forest has many more parameters to configure, but the key one is the maximum depth, which is unlimited by default. Next, we&#39;ll check how this model predicts or tests data. . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . y_pred = clf.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 1.410409517520304 Mean Squared Error: 3.0995744019029483 Root Mean Squared Error: 1.7605608202794212 . importances=regressor.feature_importances_ indices=list(x_train) print(&quot;Feature ranking:&quot;) for f in range(x_train.shape[1]): print(&quot;Feature %s (%f)&quot; % (indices[f], importances[f])) f, (ax1) = plt.subplots(1, 1, figsize=(8, 6), sharex=True) sns.barplot(indices, importances, palette=&quot;BrBG&quot;, ax=ax1) ax1.set_ylabel(&quot;Importance&quot;) . Feature ranking: Feature gold_price (0.627703) Feature GOLD (0.045197) Feature AU (0.040957) Feature KGC (0.038973) Feature NCM.AX (0.247171) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0, 0.5, &#39;Importance&#39;) . By the importance of the signs, it immediately becomes clear how strong the value of gold is. . In short, I hope I was able to reveal to you the beginnings of a project on using machine learning to study stock prices, and I hope to hear your comments. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "relUrl": "/finance/investment/python/2020/11/17/ml-prediction-gold-shares.html",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Общий финансовый анализ на Python (Часть 3)",
            "content": "После всех вычислений, приведенных в прошлых двух публикациях, можно углубиться в статистический анализ и рассмотреть метод наименьших квадратов. Для этой цели используется библиотека statsmodels, которая позволяет пользователям исследовать данные, оценивать статистические модели и выполнять статистические тесты. За основу были взяты эта статья и эта статья. Само описание используемой функции на английском доступно по следующей ссылке. . Сначала немного теории: . О линейной регрессии . Линейная регрессия используется в качестве прогнозирующей модели, когда предполагается линейная зависимость между зависимой переменной (переменная, которую мы пытаемся предсказать) и независимой переменной (переменная и/или переменные, используемые для предсказания). . В самом простой случае при рассмотрении используется одна переменная на основании которой мы пытаемся предсказать другую. Формула в этом случае имеет следующий вид: . Y = C + M*X . Y - зависимая переменная (результат / прогноз / оценка) | C - Константа (Y-Intercept) | M - Наклон линии регрессии (угловой коэффициент или градиент оценённой линии; она представляет собой величину, на которую Y увеличивается в среднем, если мы увеличиваем X на одну единицу) | X - независимая переменная (предиктор, используемый в прогнозе Y) | . В действительности так же может существовать связь между зависимой переменной и несколькими независимыми переменными. Для этих типов моделей (при условии линейности) мы можем использовать множественную линейную регрессию следующего вида: . Y = C + M1X1 + M2X2 + … . Бета — коэффициент . Про данный коэффициент написано уже много, для примера на этой странице . Коротко, если не вдаваться в подробности, то можно его охарактеризовать следующим образом: . Акции c бета-коэффициентом: . ноль указывает на отсутствие корреляции между акцией и индексом | единица указывает на то, что акция имеет ту же волатильность, что и индекс | больше одного — указывает на большую доходность (а следовательно и риски) акции, чем индекс | менее единицы — менее волатильная акция, чем индекса | . Другими словами, если акция увеличится на 14%, в то время как рынок вырос всего на 10%, то бета-коэффициент акции составит 1,4. Как правило на рынках с более высоким бета-коэффициентом можно предположить лучшие условия для вознаграждения (а следовательно и для риска). . . Практика . Следующий код Python включает в себя пример линейной регрессии, где входной переменной является доходность по Индексу МосБиржи, а оцениваемая переменная — доходность по акциям Аэрофлот. . Для того, чтобы отсутствовала необходимость вспоминать как загружать данные и приводить данные к форме, необходимой для расчета — код приводиться с момента загрузки данных и до получения результатов. Вот полный синтаксис для выполнения линейной регрессии в Python с использованием statsmodels: . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt import statsmodels.api as sm #Загружаю данные ticker = [&#39;AFLT.ME&#39;,&#39;IMOEX.ME&#39;] stock = yf.download(ticker) # Выделение скорректированой цены закрытия all_adj_close = stock[[&#39;Adj Close&#39;]] # Вычисление доходности all_returns = np.log(all_adj_close / all_adj_close.shift(1)) # Выделение доходности по акциям aflt_returns = all_returns[&#39;Adj Close&#39;][[&#39;AFLT.ME&#39;]].fillna(0) # Выделение доходности по индексу МосБиржи moex_returns = all_returns[&#39;Adj Close&#39;][[&#39;IMOEX.ME&#39;]].fillna(0) # Создание нового DataFrame return_data = pd.concat([aflt_returns, moex_returns], axis=1)[1:] return_data.columns = [&#39;AFLT.ME&#39;, &#39;IMOEX.ME&#39;] # Добавляем столбец единиц и определяем X и y X = sm.add_constant(return_data[&#39;IMOEX.ME&#39;]) y = return_data[&#39;AFLT.ME&#39;] # Создание модели model_moex = sm.OLS(y,X).fit() # Вывод результатов print(model_moex.summary()) . [*********************100%***********************] 2 of 2 completed OLS Regression Results ============================================================================== Dep. Variable: AFLT.ME R-squared: 0.135 Model: OLS Adj. R-squared: 0.135 Method: Least Squares F-statistic: 454.5 Date: Mon, 18 Oct 2021 Prob (F-statistic): 7.89e-94 Time: 19:33:22 Log-Likelihood: 7260.3 No. Observations: 2909 AIC: -1.452e+04 Df Residuals: 2907 BIC: -1.450e+04 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] const -7.886e-05 0.000 -0.213 0.831 -0.001 0.001 IMOEX.ME 0.8101 0.038 21.320 0.000 0.736 0.885 ============================================================================== Omnibus: 651.369 Durbin-Watson: 1.857 Prob(Omnibus): 0.000 Jarque-Bera (JB): 22810.853 Skew: -0.295 Prob(JB): 0.00 Kurtosis: 16.706 Cond. No. 103. ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . На сайте yahoo и Мосбиржи бета коэффициент отличается незначительно в большую сторону. Но надо честно признаться, что расчет для некоторых других акций с российской биржи показал более значительные отличия, но в пределах интервала. . . Тот же анализ для акции FB и индекса SP500. Здесь вычисление, как в оригинале, проводится через месячную доходность. . sp_500 = yf.download(&#39;^GSPC&#39;, start=&quot;2012-05-01&quot;) fb = yf.download(&#39;FB&#39;) # Пересчет в месячную доходность fb = fb.resample(&#39;BM&#39;).apply(lambda x: x[-1]) sp_500 = sp_500.resample(&#39;BM&#39;).apply(lambda x: x[-1]) monthly_prices = pd.concat([fb[&#39;Close&#39;], sp_500[&#39;Close&#39;]], axis=1) monthly_prices.columns = [&#39;FB&#39;, &#39;^GSPC&#39;] monthly_returns = monthly_prices.pct_change(1) clean_monthly_returns = monthly_returns.dropna(axis=0) X = clean_monthly_returns[&#39;^GSPC&#39;] y = clean_monthly_returns[&#39;FB&#39;] X1 = sm.add_constant(X) model_fb_sp_500 = sm.OLS(y, X1) results_fb_sp_500 = model_fb_sp_500.fit() print(results_fb_sp_500.summary()) . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed OLS Regression Results ============================================================================== Dep. Variable: FB R-squared: 0.181 Model: OLS Adj. R-squared: 0.174 Method: Least Squares F-statistic: 24.59 Date: Mon, 18 Oct 2021 Prob (F-statistic): 2.56e-06 Time: 19:47:24 Log-Likelihood: 110.11 No. Observations: 113 AIC: -216.2 Df Residuals: 111 BIC: -210.8 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P&gt;|t| [0.025 0.975] const 0.0131 0.009 1.446 0.151 -0.005 0.031 ^GSPC 1.1515 0.232 4.959 0.000 0.691 1.612 ============================================================================== Omnibus: 26.778 Durbin-Watson: 1.820 Prob(Omnibus): 0.000 Jarque-Bera (JB): 104.750 Skew: 0.671 Prob(JB): 1.79e-23 Kurtosis: 7.522 Cond. No. 26.8 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . . В этом случае все совпало и подтвердило возможность использование statsmodels для определения коэффициента бета. . Ну и в качестве бонуса — если Вы хотите получить только бета — коэффициент и остальную статистику вы хотите оставить в стороне, то предлагается еще один код для его расчета: . from scipy import stats slope, intercept, r_value, p_value, std_err = stats.linregress(X, y) print(slope) . 1.1515416131106546 . Правда это не означает, что всю остальные получаемые значения надо игнорировать, но для их понимания понадобятся знание статистики. Приведу небольшую выдержку из получаемых значений: . R-squared, который является коэффициентом детерминации и принимает значения от 0 до 1. Чем ближе значение коэффициента к 1, тем сильнее зависимость; | Adj. R-squared — скорректированный R-squared на основании числа наблюдений и числа степеней свободы; | std err — стандартная ошибка оценки коэффициента; | P&gt;|t| — р-значение Величина менее 0,05 считается статистически значимой; | 0.025 и 0.975 — нижнее и верхнее значения доверительного интервала. | и т.д. | . На этом пока что все. Конечно, представляет интерес поискать зависимость между различными величинами для того, чтобы через одну предсказать другую и получить профит. В одном из иностранных источников встретилось предсказание индекса через процентную ставку и уровень безработицы. Но если изменение процентной ставки в России можно взять с сайта Центробанка, то другие пока продолжаю искать. К сожалению, на сайте Росстата не удалось найти актуальные. Это заключительная публикация в рамках статей общего финансового анализа. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/04/12/general-financial-analysis-python-part-3.html",
            "relUrl": "/finance/investment/python/2020/04/12/general-financial-analysis-python-part-3.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Общий финансовый анализ на Python (Часть 2)",
            "content": "Скользящее окно (Moving Windows) . В заголовке я привел дословный перевод. Если кто меня поправит, и другой термин более применим — то спасибо. . Смысл скользящего окна– с каждым новым значением функция пересчитывается за заданный период времени. Этих функций большое количество. Для примера: rolling.mean(), rolling.std(), которые чаще всего и используют при анализе движения акций. rolling.mean() — это обычная скользящая средняя, которая сглаживает краткосрочные колебания и позволяет визуализировать общую тенденцию. . adj_close_px = sber[&#39;Adj Close&#39;] # Вычисляю скользящую среднию moving_avg = adj_close_px.rolling(window=40).mean() # Вывожу результат print(moving_avg[-10:]) . Date 2021-10-04 329.640254 2021-10-05 330.483754 2021-10-06 331.295254 2021-10-07 332.290504 2021-10-08 333.397253 2021-10-11 334.841753 2021-10-12 335.932753 2021-10-13 336.992754 2021-10-14 338.135253 2021-10-15 339.242503 Name: Adj Close, dtype: float64 . График, который позволит понять то, что получается в результате работы данной функции: . sber[&#39;40&#39;] = adj_close_px.rolling(window=40).mean() # Вычисление длинной скользящей средней sber[&#39;252&#39;] = adj_close_px.rolling(window=252).mean() # Построение полученных значений sber[[&#39;Adj Close&#39;, &#39;40&#39;, &#39;252&#39;]].plot(figsize=(20,20)) plt.show() . Как видно rolling.mean() справляется с поставленной задачей. Функция сглаживает краткосрочные колебания и позволяет увидеть долгосрочный тренд на основании которого можно принять решение: цена выше рассматриваемой скользящей средней — берем акцию, ниже — продаем акцию — если просто и я бы не советовал следовать этому методу. Как правило помимо скользящих средних используются и другие индикаторы, которые могут подтвердить правильность принимаемого решения. Каждый должен самостоятельно принять решение в зависимости от стиля торговли. . Волатильность . Волатильность акций — это изменение дисперсии доходности акций в течение определенного периода времени. Обычно сравнивают волатильность одной акции с другой, чтобы получить представление о том, какая может иметь больший риск, или с рыночным индексом, чтобы понять волатильность акций относительно рынка. Как правило, чем выше волатильность, тем рискованнее инвестиции в эту акцию. Необходимо отметить, что она не является постоянной и изменяется с течением времени. Это можно увидеть опять же при помощи функции rolling.std(), входящей в пакет pandas. Пример расчета изменения волатильности: . min_periods = 60 # Вычисляю волатильность vol = daily_pct_change.rolling(min_periods).std() * np.sqrt(min_periods) # Строю график vol.plot(figsize=(15, 5)) plt.show() . Прошу обратить внимание, что в отличие от прошлой недели у меня появилось еще два значения — индекс московской биржи (IMOEX.ME) и РБК (RBCM.ME). Их значения мне потребуются в следующей публикации про метод наименьших квадратов. А на сегодня все. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/03/29/general-financial-analysis-python-part-2.html",
            "relUrl": "/finance/investment/python/2020/03/29/general-financial-analysis-python-part-2.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Общий финансовый анализ на Python (Часть 1)",
            "content": "В прошлой статье рассмотрено как можно получить информацию по финансовым инструментам. Дальше будет опубликовано несколько статей о том, что первоначально можно делать с полученными данными, как проводить анализ и составлять стратегию. Материалы составлены на основании публикаций в иностранных источниках и курсах на одной из онлайн платформ. . В этой статье будет рассмотрено, как рассчитывать доходность, волатильность и построить один из основных индикаторов. . import pandas as pd import yfinance as yf import numpy as np import matplotlib.pyplot as plt sber = yf.download(&#39;SBER.ME&#39;,&#39;2016-01-01&#39;) . [*********************100%***********************] 1 of 1 completed . Доходность . Данная величина представляет собой процентное изменение стоимости акции за один торговый день. Оно не учитывает дивиденды и комиссии. Его легко рассчитать используя функцию pct_change () из пакета Pandas. . Как правило используют лог доходность, так как она позволяет лучше понять и исследовать изменения с течением времени. . daily_close = sber[[&#39;Adj Close&#39;]] # Дневная доходность daily_pct_change = daily_close.pct_change() # Заменить NA значения на 0 daily_pct_change.fillna(0, inplace=True) print(daily_pct_change.head()) # Дневная лог доходность daily_log_returns = np.log(daily_close.pct_change()+1) print(daily_log_returns.head()) . Adj Close Date 2016-01-04 0.000000 2016-01-05 0.008979 2016-01-06 -0.020629 2016-01-11 -0.060093 2016-01-12 0.007470 Adj Close Date 2016-01-04 NaN 2016-01-05 0.008939 2016-01-06 -0.020845 2016-01-11 -0.061974 2016-01-12 0.007442 . Чтобы из полученных данных узнать недельную и/или месячную доходность, используют функцию resample(). . monthly = sber.resample(&#39;BM&#39;).apply(lambda x: x[-1]) # Месячная доходность print(monthly.pct_change().tail()) # Пересчитать `sber` по кварталам и взять среднее значение за квартал quarter = sber.resample(&quot;4M&quot;).mean() # Квартальную доходность print(quarter.pct_change().tail()) . Open High Low Close Adj Close Volume Date 2021-06-30 -0.010055 -0.018202 -0.017745 -0.015593 -0.015593 0.363875 2021-07-30 -0.012117 -0.000782 -0.007253 0.001604 0.001604 0.589072 2021-08-31 0.086022 0.077640 0.086870 0.072213 0.072213 -0.344803 2021-09-30 0.004505 0.035464 0.006936 0.033522 0.033522 1.449116 2021-10-29 0.145282 0.105815 0.123980 0.093531 0.093531 -0.536555 Open High Low Close Adj Close Volume Date 2020-09-30 0.056380 0.052326 0.063663 0.059189 0.231932 -0.288569 2021-01-31 0.127438 0.128327 0.125520 0.126951 0.263348 0.268118 2021-05-31 0.159379 0.156331 0.162285 0.159680 0.174569 -0.295427 2021-09-30 0.101638 0.099245 0.104338 0.101912 0.156688 -0.429262 2022-01-31 0.156123 0.168087 0.150327 0.160703 0.160703 0.702144 . Функция pct_change () удобна для использования, но в свою очередь скрывает то, как получается значение. Схожее вычисление, которое поможет понять механизм, можно выполнить при помощи shift() из пакета из пакета Pandas. Дневная цена закрытия делится на прошлую (сдвинутую на один) цену и из полученного значения вычитается единица. Но есть один незначительный минус – первое значение в результате получается NA. . Расчет доходности основан на формуле: . . Дальше строится диаграмма распределения доходности и рассчитывается основная статистика: . Для значений по российским акциям есть небольшая тонкость. К названию акцию добавляется точка и заглавными буквами ME. Спасибо знатоки на смартлабе подсказали. . daily_pct_change = daily_close / daily_close.shift(1) - 1 print(daily_pct_change.head()) . Adj Close Date 2016-01-04 NaN 2016-01-05 0.008979 2016-01-06 -0.020629 2016-01-11 -0.060093 2016-01-12 0.007470 . daily_pct_change.hist(bins=50) plt.show() # Общая статистика print(daily_pct_change.describe()) . Adj Close count 1454.000000 mean 0.001442 std 0.018295 min -0.161417 25% -0.008009 50% 0.000708 75% 0.011137 max 0.128987 . Распределение выглядит очень симметрично и нормально распределённым вокруг значения 0,00. Для получения других значений статистики используется функция description (). В результате видно, что среднее значение немного больше нуля, а стандартное отклонение составляет практически 0,02. . Кумулятивная доходность . Кумулятивная дневная прибыль полезна для определения стоимости инвестиций через определенные промежуток времени. Ее можно рассчитать, как приводится в коде ниже. . cum_daily_return = (1 + daily_pct_change).cumprod() print(cum_daily_return.tail()) . Adj Close Date 2021-10-11 6.651933 2021-10-12 6.504360 2021-10-13 6.531676 2021-10-14 6.467939 2021-10-15 6.371218 . cum_daily_return.plot(figsize=(8,5)) plt.show() . Можно пересчитать доходность в месячном периоде: . cum_monthly_return = cum_daily_return.resample(&quot;M&quot;).mean() print(cum_monthly_return.tail()) . Adj Close Date 2021-06-30 5.354601 2021-07-31 5.182009 2021-08-31 5.584808 2021-09-30 5.653339 2021-10-31 6.318509 . Знание того, как рассчитать доходность, является ценным при анализе акции. Но еще большую ценность оно представляет при сравнении с другими акциями. . Возьмем некоторые акции (выбор их совершенно случайный) и построим их диаграмму. . ticker = [&#39;AFLT.ME&#39;,&#39;DSKY.ME&#39;,&#39;IRAO.ME&#39;,&#39;PIKK.ME&#39;, &#39;PLZL.ME&#39;,&#39;SBER.ME&#39;,&#39;ENRU.ME&#39;] stock = yf.download(ticker,&#39;2018-01-01&#39;) # Дневная доходность в `daily_close_px` daily_pct_change = stock[&#39;Adj Close&#39;].pct_change() # Распределение daily_pct_change.hist(bins=50, sharex=True, figsize=(20,8)) plt.show() . [*********************100%***********************] 7 of 7 completed . Еще один полезный график —матрица рассеяния. Ее можно легко построить при помощи функции scatter_matrix (), входящей в библиотеку pandas. В качестве аргументов используется daily_pct_change и устанавливается параметр Ядерной оценки плотности — Kernel Density Estimation. Кроме того, можно установить прозрачность с помощью параметра alpha и размер графика с помощью параметра figsize. . from pandas.plotting import scatter_matrix # Матрица рассеивания `daily_pct_change` scatter_matrix(daily_pct_change, diagonal=&#39;kde&#39;, alpha=0.1,figsize=(20,20)) plt.show() . На этом пока все. В следующей статье будет рассмотрено вычисление волатильности, средней и использование метода наименьших квадратов. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/03/14/general-financial-analysis-python-part-1.html",
            "relUrl": "/finance/investment/python/2020/03/14/general-financial-analysis-python-part-1.html",
            "date": " • Mar 14, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Получение котировок акций при помощи Python (перевод)",
            "content": "Представляю вашему вниманию перевод статьи «Historical Stock Price Data in Python» автора Ishan Shah. . Статья о том, как получить ежедневные исторические данные по акциям, используя yfinance, и минутные данные, используя alpha vantage. . Как вы знаете, акции относятся к очень волатильному инструменту и очень важно тщательно анализировать поведение цены, прежде чем принимать какие-либо торговые решения. Ну а сначала надо получить данные и python может помочь в этом. . Биржевые данные могут быть загружены при помощи различных пакетов. В этой статье будут рассмотрены yahoo finance. . Его можно установить при помощи команды pip install yfinance. Приведенный ниже код показывает, как получить данные для AAPL с 2016 по 2019 год и построить скорректированную цену закрытия (скорректированная цена закрытия на дивиденды и сплиты) на графике . import yfinance as yf # Get the data for the stock AAPL data = yf.download(&#39;AAPL&#39;,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;) # Import the plotting library import matplotlib.pyplot as plt %matplotlib inline # Plot the close price of the AAPL data[&#39;Adj Close&#39;].plot() plt.show() . [*********************100%***********************] 1 of 1 completed . Ну а если необходимо получить по нескольким акциям, то необходимо внести небольшое дополнение в код. Для хранения значений используется DataFrame. При помощи пакета matplotlib и полученных данных можно построить график дневной доходности. . import pandas as pd tickers_list = [&#39;AAPL&#39;, &#39;WMT&#39;, &#39;IBM&#39;, &#39;MU&#39;, &#39;BA&#39;, &#39;AXP&#39;] # Import pandas data = pd.DataFrame(columns=tickers_list) # Fetch the data for ticker in tickers_list: data[ticker] = yf.download(ticker,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;)[&#39;Adj Close&#39;] # Print first 5 rows of the data data.head() . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . AAPL WMT IBM MU BA AXP . Date . 2015-12-31 24.266079 | 54.044613 | 106.805664 | 14.140234 | 129.673157 | 63.341213 | . 2016-01-04 24.286833 | 54.185669 | 105.509621 | 14.309997 | 126.005119 | 61.556175 | . 2016-01-05 23.678221 | 55.472874 | 105.431984 | 14.799313 | 126.516304 | 60.609035 | . 2016-01-06 23.214846 | 56.028313 | 104.904243 | 14.200150 | 124.507416 | 58.925949 | . 2016-01-07 22.235073 | 57.333141 | 103.111473 | 13.640932 | 119.287819 | 58.395412 | . # Plot all the close prices ((data.pct_change()+1).cumprod()).plot(figsize=(10, 7)) # Show the legend plt.legend() # Define the label for the title of the figure plt.title(&quot;Adjusted Close Price&quot;, fontsize=16) # Define the labels for x-axis and y-axis plt.ylabel(&#39;Price&#39;, fontsize=14) plt.xlabel(&#39;Year&#39;, fontsize=14) # Plot the grid lines plt.grid(which=&quot;major&quot;, color=&#39;k&#39;, linestyle=&#39;-.&#39;, linewidth=0.5) plt.show() . Для значений по российским акциям есть небольшая тонкость. К названию акцию добавляется точка и заглавными буквами ME. Спасибо знатоки на смартлабе подсказали. . tickers_list_rus = [&#39;TTLK.ME&#39;, &#39;GMKN.ME&#39;, &#39;LSRG.ME&#39;, &#39;TATNP.ME&#39;, &#39;MSTT.ME&#39;,&#39;YNDX.ME&#39;] data_rus = pd.DataFrame(columns=tickers_list_rus) . for ticker in tickers_list_rus: data_rus[ticker] = yf.download(ticker,&#39;2016-01-01&#39;,&#39;2019-08-01&#39;)[&#39;Adj Close&#39;] . [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed [*********************100%***********************] 1 of 1 completed . data_rus.head() . TTLK.ME GMKN.ME LSRG.ME TATNP.ME MSTT.ME YNDX.ME . Date . 2016-01-04 0.091873 | 5647.466797 | 410.957520 | 112.794388 | 52.599274 | 1064.099976 | . 2016-01-05 0.092520 | 5750.643066 | 422.438416 | 112.513115 | 52.870754 | 1133.900024 | . 2016-01-06 0.092844 | 5690.508789 | 430.975464 | 111.613007 | 52.802883 | 1112.000000 | . 2016-01-11 0.092844 | 5443.646973 | 409.485657 | 110.150345 | 52.056313 | 987.000000 | . 2016-01-12 0.095755 | 5413.896484 | 400.948547 | 108.575157 | 51.581223 | 999.000000 | . ((data_rus.pct_change()+1).cumprod()).plot(figsize=(10, 7)) # Show the legend plt.legend() # Define the label for the title of the figure plt.title(&quot;Adjusted Close Price&quot;, fontsize=16) # Define the labels for x-axis and y-axis plt.ylabel(&#39;Price&#39;, fontsize=14) plt.xlabel(&#39;Year&#39;, fontsize=14) # Plot the grid lines plt.grid(which=&quot;major&quot;, color=&#39;k&#39;, linestyle=&#39;-.&#39;, linewidth=0.5) plt.show() . В дальнейшем эти данные можно проанализировать, создать торговую стратегию и оценить эффективность при помощи пакета pyfolio. В нем можно оценить коэффициент Шарпа, коэффициент Сортино, максимальную просадку и многие другие необходимые показатели. . Надеюсь, что мой перевод оригинальной статьи будет для Вас полезен. Код был проверен и все работает. Но пока для меня остался вопрос в возможности использования Alpha vantage для российского рынка. .",
            "url": "https://zmey56.github.io/blog//finance/investment/python/2020/02/09/getting-stock-quotes-using-python.html",
            "relUrl": "/finance/investment/python/2020/02/09/getting-stock-quotes-using-python.html",
            "date": " • Feb 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alexander Gladkikh, and I made my own website on GitHub dedicated to my Hobbies: Machine learning, Deep Learning, and algorithmic trading. . I take part in kaggle competitions, have knowledge of R and Python (Pandas, NumPy, Scipy, Scikit-learn, XGBoost), Java . At the main work I participate in projects on the use of new technologies in the field of labor protection and ecology. . I have been engaged in technical analysis of financial markets for a long time. Familiar with software Amibroker, and Metatrader Quik (scripting). . At work I had to deal with the analysis of data in the performance of research in biology at the Institute and writing projects on environmental protection. . My degrees . Corporate Energy University, 2020 . Digital production technologies in the power industry . YANDEX, MIPT, 2019 . Machine learning and data analysis . City Business School, 2019 . MINI-MBA Professional .",
          "url": "https://zmey56.github.io/blog//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zmey56.github.io/blog//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}