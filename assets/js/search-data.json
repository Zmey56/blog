{
  
    
        "post0": {
            "title": "English - Using machine learning to predict gold mining stock prices",
            "content": "As a basis, I took a notebook published on colab for oil. This notebook examines the analysis of gold prices and shares of gold mining companies using machine analysis methods: linear regression, cluster analysis, and random forest. I immediately warn you that this post does not attempt to show the current situation and predict the future direction. Just like the author for oil, this article does not aim to raise or refute the possibilities of machine learning for analyzing stock prices or other tools. I upgraded the code for gold research in order to encourage those who are interested in further reflection and listen to constructive criticism in their address. . pip install yfinance --upgrade --no-cache-dir . Collecting yfinance Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz Requirement already satisfied, skipping upgrade: pandas&gt;=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.4) Requirement already satisfied, skipping upgrade: numpy&gt;=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0) Requirement already satisfied, skipping upgrade: multitasking&gt;=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9) Collecting lxml&gt;=4.5.1 Downloading https://files.pythonhosted.org/packages/64/28/0b761b64ecbd63d272ed0e7a6ae6e4402fc37886b59181bfdf274424d693/lxml-4.6.1-cp36-cp36m-manylinux1_x86_64.whl (5.5MB) |████████████████████████████████| 5.5MB 6.9MB/s Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=0.24-&gt;yfinance) (2.8.1) Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (1.24.3) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2020.6.20) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (2.10) Requirement already satisfied, skipping upgrade: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.20-&gt;yfinance) (3.0.4) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24-&gt;yfinance) (1.15.0) Building wheels for collected packages: yfinance Building wheel for yfinance (setup.py) ... done Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=2b1a24b9e8937bf5603faead14841f6f3e045d79bd13bf0e76bc189db75a8640 Stored in directory: /tmp/pip-ephem-wheel-cache-5al_gsvn/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c Successfully built yfinance Installing collected packages: lxml, yfinance Found existing installation: lxml 4.2.6 Uninstalling lxml-4.2.6: Successfully uninstalled lxml-4.2.6 Successfully installed lxml-4.6.1 yfinance-0.1.55 . import yfinance as yf import pandas as pd import numpy as np import seaborn as sns from sklearn import metrics import matplotlib.pyplot as plt from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression . 1. Loading data . For the price of gold, take the value of the exchange-traded investment Fund SPDR Gold Trust, whose shares are 100% backed by precious metal. The quotes will be compared with the prices of gold mining companies &#39; shares: . Newmont Goldcorp (NMM) | Barrick Gold (GOLD) | AngloGold Ashanti (AU) | Kinross Gold (KGC) | Newcrest Mining (ENC) | Polyus (PLZL) | Polymetal (POLY) | Seligdar (SELG) | . gold = pd.DataFrame(yf.download(&quot;GLD&quot;, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;]) . [*********************100%***********************] 1 of 1 completed . gold = gold.reset_index() gold.columns = [&quot;Date&quot;,&quot;gold_price&quot;] gold[&#39;Date&#39;] = pd.to_datetime(gold[&#39;Date&#39;]) gold.head() . Date gold_price . 0 2010-01-04 | 109.800003 | . 1 2010-01-05 | 109.699997 | . 2 2010-01-06 | 111.510002 | . 3 2010-01-07 | 110.820000 | . 4 2010-01-08 | 111.370003 | . It is necessary to move the price of gold, as we will be interested in how yesterday&#39;s price affected today&#39;s stock price. . gold[&quot;gold_price&quot;] = gold[&quot;gold_price&quot;].shift(1) . shares=[&quot;NMM.SG&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,&quot;PLZL.ME&quot;,&quot;POLY.ME&quot;,&quot;SELG.ME&quot;] data= yf.download(shares, start=&quot;2010-01-01&quot;, end=&quot;2019-12-31&quot;)[&#39;Adj Close&#39;] . [*********************100%***********************] 8 of 8 completed . data = data.reset_index() data.head() . Date AU GOLD KGC NCM.AX NMM.SG PLZL.ME POLY.ME SELG.ME . 0 2010-01-04 | 39.698944 | 34.561649 | 18.105721 | 33.237167 | 26.924570 | NaN | NaN | NaN | . 1 2010-01-05 | 40.320408 | 34.989510 | 18.594805 | 33.901924 | 27.116940 | NaN | NaN | NaN | . 2 2010-01-06 | 41.601028 | 35.733963 | 19.256504 | 33.901924 | 27.289278 | NaN | NaN | NaN | . 3 2010-01-07 | 41.130215 | 35.229092 | 19.352404 | 34.298923 | NaN | NaN | NaN | NaN | . 4 2010-01-08 | 41.601028 | 35.451572 | 19.601744 | 33.421829 | 27.702093 | NaN | NaN | NaN | . data[&#39;Date&#39;] = pd.to_datetime(data[&#39;Date&#39;]) . all_data=pd.DataFrame() . for index in range(len(shares)): stock=pd.DataFrame() # transform the data stock=data.loc[:, (&quot;Date&quot;,shares[index])] stock[&quot;Date&quot;]=stock[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) stock.columns=[&quot;Date&quot;,&quot;share_price&quot;] test=pd.DataFrame(gold) output=stock.merge(test,on=&quot;Date&quot;,how=&quot;left&quot;) #combining two data sets stock[&quot;gold_price&quot;]=output[&quot;gold_price&quot;] stock[&#39;share_price&#39;]=pd.to_numeric(stock[&#39;share_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&#39;gold_price&#39;]=pd.to_numeric(stock[&#39;gold_price&#39;], errors=&#39;coerce&#39;).dropna(0) stock[&quot;year&quot;]=pd.to_datetime(stock[&quot;Date&quot;]).dt.year #Create a column with years for subsequent filtering stock[&quot;name&quot;]=shares[index] stock = stock.dropna() #delete all NAN lines #creating a column with a scaled share price scaler=MinMaxScaler() stock[&quot;share_price_scaled&quot;]=scaler.fit_transform(stock[&quot;share_price&quot;].to_frame()) #add data to the main dataframe all_data=all_data.append(stock) #add the data . all_data_15 = all_data[(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)] all_data_15.head() . Date share_price gold_price year name share_price_scaled . 1301 2015-01-02 | 14.269927 | 113.580002 | 2015 | NMM.SG | 0.052072 | . 1302 2015-01-05 | 14.845476 | 114.080002 | 2015 | NMM.SG | 0.071190 | . 1303 2015-01-06 | 15.601913 | 115.800003 | 2015 | NMM.SG | 0.096317 | . 1304 2015-01-07 | 15.645762 | 117.120003 | 2015 | NMM.SG | 0.097773 | . 1305 2015-01-08 | 15.517859 | 116.430000 | 2015 | NMM.SG | 0.093525 | . 2. Data analysis . It is best to start analyzing data by presenting it visually, which will help you understand it better. . 2.1 Chart of gold price changes . gold[[&#39;Date&#39;,&#39;gold_price&#39;]].set_index(&#39;Date&#39;).plot(color=&quot;green&quot;, linewidth=1.0) plt.show() . 2.2. Plotting the pairplot chart for the price of Polyus and Barrick Gold shares over the past five years . palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) g = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;POLY.ME&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) g.fig.suptitle(&quot;Polyuse&quot;, y=1.08) palette=sns.cubehelix_palette(18, start=2, rot=0, dark=0, light=.95, reverse=False) f = sns.pairplot(all_data[(all_data[&#39;name&#39;]==&quot;GOLD&quot;)&amp;(all_data[&#39;year&#39;]&gt;2014)&amp;(all_data[&#39;year&#39;]&lt;2020)]. drop([&quot;share_price_scaled&quot;],axis=1), hue=&quot;year&quot;,height=4) f.fig.suptitle(&#39;Barrick Gold&#39;, y=1.08) plt.show() . A paired graph allows you to see the distribution of data by showing the paired relationships in the data set and the univariate distribution of data for each variable. You can also use the palette to see how this data changed in different years. . The chart is particularly interesting for 2016 and 2019, as it looks like the price of the Pole stock, Barrick Gold and the price of gold are lined up along the same line. We can also conclude from the distribution charts that the price of gold and stocks moved gradually towards higher values. . 2.3 Violinplot for the gold price . plt.figure(figsize=(10,10)) sns.set_style(&quot;whitegrid&quot;) palette=sns.cubehelix_palette(5, start=2.8, rot=0, dark=0.2, light=0.8, reverse=False) sns.violinplot(x=&quot;year&quot;, y=&quot;gold_price&quot;, data=all_data_15[[&quot;gold_price&quot;,&quot;year&quot;]], inner=&quot;quart&quot;, palette=palette, trim=True) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Price gold&quot;) plt.show() . 2.4 Violinplot for multiple shares . sns.catplot(x=&quot;year&quot;, y=&quot;share_price_scaled&quot;, col=&#39;name&#39;, col_wrap=3,kind=&quot;violin&quot;, split=True, data=all_data_15,inner=&quot;quart&quot;, palette=palette, trim=True, height=4, aspect=1.2) sns.despine(left=True) . A large fluctuation in gold prices was noted according to the charts in 2016 and 2019. As you can see from the graphs in the following figure, some companies such as Newmont Mining, Barrick Gold, AngloGold Ashanti, Newcrest Mining and Polymetal were also affected. It should also be noted that all prices are marked in the range from 0 to 1 and this may lead to inaccuracies in the interpretation. . Next, we will build distribution charts for one Russian company - Polymetal and one foreign company - Barrick Gold . sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;POLY.ME&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) sns.jointplot(&quot;gold_price&quot;, &quot;share_price&quot;,data=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;],kind=&quot;kde&quot;, height=6,ratio=2,color=&quot;red&quot;).plot_joint(sns.kdeplot, zorder=0, n_levels=20) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . It is necessary to pay attention to the distribution of the share price for the two companies and it will become clear that the shape of the density graph is the same for them. . 2.5 Charts of the dependence of the share price of various companies on the price of gold . sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,line_kws={&#39;color&#39;: &#39;blue&#39;},scatter_kws={&#39;color&#39;: &#39;grey&#39;}).set(ylim=(0, 1)) plt.show() . In fact, you won&#39;t be able to see much on these charts, although some stocks seem to have a relationship. . The next step is to try to color the charts depending on the years. . palette=sns.cubehelix_palette(5, start=2, rot=0, dark=0, light=.95, reverse=False) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,hue=&quot;year&quot;, col=&quot;name&quot;,ci=None, col_wrap=3, data=all_data_15, order=1,palette=palette,height=4).set(ylim=(0, 1)) plt.show() . Here the picture is a little better in the sense that some companies have a data cloud stretching along a straight line in some years, which may indicate the existence of a dependency. . 3 Machine learning and prediction . I will give a definition for machine learning from Wikipedia: Machine learning is a class of artificial intelligence methods that are characterized not by direct problem solving, but by learning in the process of applying solutions to many similar problems. To build such methods, we use mathematical statistics, numerical methods, optimization methods, probability theory, graph theory, and various techniques for working with data in digital form. . Usually, machine learning algorithms can be classified into the following categories: learning with a teacher and learning without a teacher. Here is their definition from one of the sites: . Supervised learning is one of the sections of machine learning dedicated to solving the following problem. There is a set of objects (situations) and the set of possible answers (responses, reactions). There is some relationship between responses and objects, but it is unknown. Only a finite set of use cases is known — the &quot;object, response&quot; pairs, called the training sample. Based on this data, you need to restore the dependency, that is, build an algorithm that can give a fairly accurate answer for any object. To measure the accuracy of responses, a quality functional is introduced in a certain way. see the Links) . Unsupervised learning is one of the sections of machine learning. Studies a wide class of data processing problems in which only descriptions of a set of objects (training sample) are known, and it is required to detect internal relationships, dependencies, and patterns that exist between objects. Learning without a teacher is often contrasted with learning with a teacher, when each training object is given a &quot;correct answer&quot;, and you need to find the relationship between the objects and the answers. see links) . The following machine learning methods will be discussed later: . Cluster analysis | Linear regression | Random forest | . Using these algorithms, you can evaluate overvalued or undervalued stocks relative to the price of gold and possible movement on the next day. I remind you that you must be very careful and use the conclusions from this post at your own risk. I also remind you that my main goal is to show the potential of machine learning for stock valuation. . 3.1. Cluster analysis for Barrick Gold stock . Clustering is the task of dividing a set of objects into groups called clusters. Each group should contain &quot;similar&quot; objects, and objects from different groups should be as different as possible. . from sklearn.cluster import KMeans poly=all_data_15[all_data_15[&#39;name&#39;]==&quot;GOLD&quot;] # We need to scale also gold price, so clustering is not influenced by the relative size of one axis. poly=pd.DataFrame(poly) poly[&#39;gold_price_scaled&#39;] = scaler.fit_transform(poly[&quot;gold_price&quot;].to_frame()) poly[&quot;cluster&quot;] = KMeans(n_clusters=5, random_state=1).fit_predict(poly[[&quot;share_price_scaled&quot;,&quot;gold_price_scaled&quot;]]) # The 954 most common RGB monitor colors https://xkcd.com/color/rgb/ colors = [&quot;baby blue&quot;, &quot;amber&quot;, &quot;scarlet&quot;, &quot;grey&quot;,&quot;milk chocolate&quot;, &quot;windows blue&quot;] palette=sns.xkcd_palette(colors) sns.lmplot(x=&quot;gold_price&quot;, y=&quot;share_price_scaled&quot;,ci=None,palette=palette, hue=&quot;cluster&quot;,fit_reg=0 ,data=poly) plt.show() . Cluster analysis is used in a large number of machine learning tasks. But I have given it only for informational purposes, since in this form it does not bring much benefit to our analysis. . 3.2. Linear regression between Barrick Gold shares and the gold price . Next, we will build a regular linear regression using training with a teacher. The goal is to estimate the forecast of data for the last 100 days of 2019 based on data from 2018/2019 (excluding estimated ones). Training data is the data used to build the model, and test data is the data that we will try to predict. . for sh in shares: print(sh) #Data Preparation share_18=pd.DataFrame() share_18=all_data_15[(all_data_15[&#39;name&#39;]==sh)] # Get data 2018/19 share_18=share_18[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Just using 1 variable for linear regression. Split the data into training/testing sets train = share_18[:-100] test = share_18[-100:] x_train=train[&quot;gold_price&quot;].to_frame() y_train=train[&#39;share_price&#39;].to_frame() x_test=test[&quot;gold_price&quot;].to_frame() y_test=test[&#39;share_price&#39;].to_frame() regr = LinearRegression() #Create linear regression object regr.fit(x_train,y_train) #Train the model using the training sets print(&quot;Coefficients: &quot;, float(regr.coef_)) print(np.corrcoef(x_train,y_train, rowvar=False)) y_pred = regr.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # Plot outputs using matplotlib plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . NMM.SG Coefficients: 0.6629423053739908 [[1. 0.790953] [0.790953 1. ]] Mean Absolute Error: 6.063058573972694 Mean Squared Error: 39.21188296210148 Root Mean Squared Error: 6.261939233344689 . GOLD Coefficients: 0.3355465472461071 [[1. 0.67139243] [0.67139243 1. ]] Mean Absolute Error: 3.3769293704374657 Mean Squared Error: 11.756813554455096 Root Mean Squared Error: 3.4288210152259473 . AU Coefficients: 0.31252669952857776 [[1. 0.67830589] [0.67830589 1. ]] Mean Absolute Error: 2.2471377544809683 Mean Squared Error: 5.789211153877581 Root Mean Squared Error: 2.4060779608893768 . KGC Coefficients: 0.10461302060876282 [[1. 0.78266367] [0.78266367 1. ]] Mean Absolute Error: 1.0583009847297946 Mean Squared Error: 1.1523726951635975 Root Mean Squared Error: 1.073486234268329 . NCM.AX Coefficients: 0.5623005799590818 [[1. 0.79891272] [0.79891272 1. ]] Mean Absolute Error: 2.0335289996635937 Mean Squared Error: 5.836462091267656 Root Mean Squared Error: 2.415877085297937 . PLZL.ME Coefficients: 103.84435014609612 [[1. 0.60373084] [0.60373084 1. ]] Mean Absolute Error: 1315.093426667142 Mean Squared Error: 1776892.2964767825 Root Mean Squared Error: 1333.0012364873419 . POLY.ME Coefficients: 10.772023429299809 [[1. 0.63694034] [0.63694034 1. ]] Mean Absolute Error: 69.33753863275061 Mean Squared Error: 6800.525447108329 Root Mean Squared Error: 82.46529844187995 . SELG.ME Coefficients: 0.15570348678870732 [[1. 0.51630147] [0.51630147 1. ]] Mean Absolute Error: 1.8096071903165585 Mean Squared Error: 4.039450515732427 Root Mean Squared Error: 2.009838430255633 . From the above charts, we can conclude that the price of gold predicts the price of shares of foreign companies on the next day quite well. In Russian companies, this picture looks much worse. Of course, there may be a false impression about Seligdar shares. But visual analysis of the chart allows you to discard this assumption. . 3.3 Random forest on Newmont Goldcorp shares against the price of gold and shares of gold companies . Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . The random forest algorithm accepts more than one variable in the input data to predict the output data. It works very efficiently on large amounts of data, can handle many input variables, has efficient methods for estimating missing data, and many other advantages. The main disadvantages are: . Random forests is slow to generate forecasts because it has many decision trees. Whenever it makes a forecast, all the trees in the forest must make a forecast for the same given input and then vote on it. This whole process takes a long time. | the Model is difficult to interpret compared to the decision tree, where you can easily make a decision by following the path in the tree. | One of the great advantages of a random forest is that it can be used for both classification and regression problems, which make up most of today&#39;s machine learning systems. I will talk about random forests in classification, since classification is sometimes considered a building block of machine learning. Below you can see what a random forest with two trees looks like: . In addition to the gold price, we will use other variables to forecast the Newmont Goldcorp share price. This will be the share prices of other foreign gold mining companies. I know it doesn&#39;t make a lot of sense, but we just want to see how to build this type of model. This will allow us to see the impact of each of them on the final forecast.Random forest is a machine learning algorithm that uses a Committee (ensemble) of decision trees. The main idea is to use a large ensemble of decision trees, each of which in itself gives a very low quality of classification, but due to their large number, the result is good. . from sklearn.ensemble import RandomForestRegressor # 1.- Data Preparation nmm15=pd.DataFrame() nmm15=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NMM.SG&quot;) &amp; (all_data_15[&#39;year&#39;]&gt;2016 )] nmm15=nmm15[[&quot;share_price&quot;,&quot;gold_price&quot;]].reset_index() # Load share price of other variables nmm15[&#39;GOLD&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;GOLD&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;GOLD&#39;] = nmm15[&#39;GOLD&#39;].shift(1) nmm15[&#39;AU&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;AU&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;AU&#39;] = nmm15[&#39;AU&#39;].shift(1) nmm15[&#39;KGC&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;KGC&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;KGC&#39;] = nmm15[&#39;KGC&#39;].shift(1) nmm15[&#39;NCM.AX&#39;]=all_data_15[(all_data_15[&#39;name&#39;]==&quot;NCM.AX&quot;)][-980:].reset_index()[&#39;share_price&#39;] nmm15[&#39;NCM.AX&#39;] = nmm15[&#39;NCM.AX&#39;].shift(1) nmm15 = nmm15.drop(nmm15.index[0]) train = nmm15[:-100] test = nmm15[-100:] x_train=train[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]] y_train=train[&#39;share_price&#39;] x_test=test[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;,]] y_test=test[&#39;share_price&#39;].to_frame() # 2.- Create Randomforest object usinig a max depth=5 regressor = RandomForestRegressor(n_estimators=200, max_depth=5 ) # 3.- Train data clf=regressor.fit(x_train, y_train) # 4.- Predict! y_pred=regressor.predict(x_test) y_pred_list = list(y_pred) y_pred=pd.DataFrame(y_pred) . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_pred=plt.scatter(nmm15[&quot;gold_price&quot;], regressor.predict(nmm15[[&quot;gold_price&quot;,&quot;GOLD&quot;,&quot;AU&quot;,&quot;KGC&quot;,&quot;NCM.AX&quot;]]), color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train,plt_pred),(&quot;train data&quot;,&quot;prediction&quot;)) plt.show() . The resulting model looks really good in addition, we must remember that Random Forest has many more parameters to configure, but the key one is the maximum depth, which is unlimited by default. Next, we&#39;ll check how this model predicts or tests data. . plt_train=plt.scatter(x_train[&quot;gold_price&quot;],y_train, color=&#39;grey&#39;) plt_test=plt.scatter(x_test[&quot;gold_price&quot;],y_test, color=&#39;green&#39;) plt_pred=plt.scatter(x_test[&quot;gold_price&quot;], y_pred, color=&#39;black&#39;) plt.xlabel(&quot;gold_price&quot;) plt.ylabel(&quot;share_price&quot;) plt.legend((plt_train, plt_test,plt_pred),(&quot;train data&quot;, &quot;test data&quot;,&quot;prediction&quot;)) plt.show() . y_pred = clf.predict(x_test) print(&#39;Mean Absolute Error:&#39;, metrics.mean_absolute_error(y_test, y_pred)) print(&#39;Mean Squared Error:&#39;, metrics.mean_squared_error(y_test, y_pred)) print(&#39;Root Mean Squared Error:&#39;, np.sqrt(metrics.mean_squared_error(y_test, y_pred))) . Mean Absolute Error: 1.410409517520304 Mean Squared Error: 3.0995744019029483 Root Mean Squared Error: 1.7605608202794212 . importances=regressor.feature_importances_ indices=list(x_train) print(&quot;Feature ranking:&quot;) for f in range(x_train.shape[1]): print(&quot;Feature %s (%f)&quot; % (indices[f], importances[f])) f, (ax1) = plt.subplots(1, 1, figsize=(8, 6), sharex=True) sns.barplot(indices, importances, palette=&quot;BrBG&quot;, ax=ax1) ax1.set_ylabel(&quot;Importance&quot;) . Feature ranking: Feature gold_price (0.627703) Feature GOLD (0.045197) Feature AU (0.040957) Feature KGC (0.038973) Feature NCM.AX (0.247171) . /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . Text(0, 0.5, &#39;Importance&#39;) . By the importance of the signs, it immediately becomes clear how strong the value of gold is. . In short, I hope I was able to reveal to you the beginnings of a project on using machine learning to study stock prices, and I hope to hear your comments. .",
            "url": "https://zmey56.github.io/blog//english/machine%20learning/algotrading/2020/12/12/_11_17_ml_prediction_gold_shares.html",
            "relUrl": "/english/machine%20learning/algotrading/2020/12/12/_11_17_ml_prediction_gold_shares.html",
            "date": " • Dec 12, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Russian - Fastbook Chapter 4 questionnaire solutions",
            "content": "Ответы на русском языке на вопросы к четвертой части курса Deep Learning 2020 на Fast.ai. Напоминаю, что третья часть отдана этики. . 1. Как представлено на компьютере изображение в градиентах серого? Как цветное изображение? . Изображения представлены массивами со значениями пикселей, представляющими содержимое изображения. Для изображений в оттенках серого используется 2-мерный массив с пикселями, представляющими значения в оттенках серого, в диапазоне от 0 до 256. Значение 0 - белый цвет, а значение 255 - черный, а между ними различные оттенки серого. Для цветных изображений обычно используются три цветовых канала (красный, зеленый, синий), причем для каждого канала используется отдельный 256-диапазонный 2D-массив. Значение пикселя 0 снова представляет белый цвет, а 255 - сплошной красный, зеленый или синий. Три 2-D массива образуют окончательный 3-D массив (тензор ранга 3), представляющий цветное изображение. . 2. Как структурированы файлы и папки в наборе данных MNIST_SAMPLE? Почему? . Есть две подпапки, train и valid, первая содержит данные для обучения модели, вторая содержит данные для проверки подтверждения модели после каждого шага обучения. Оценка модели на валидационном наборе служит двум целям: а) сообщить о такой интерпретируемой человеком метрике, как точность (в отличие от часто абстрактных функций потерь, используемых для обучения), б) облегчить обнаружение переобучения путем оценки модели на наборе данных, на котором она не была обучена (короче говоря, модель переобучения работает все лучше на обучающем наборе, но все меньше на валидационном наборе). Конечно, каждый практик мог генерировать свои собственные тренировочные / валидационые разделения данных. Общедоступные наборы данных обычно предварительно разделяются для упрощения сравнения результатов между реализациями/публикациями. . Каждая подпапка имеет две подпапки 3 и 7, которые содержат файлы ф формате jpg для соответствующего класса изображений. Это распространенный способ организации наборов данных, состоящих из изображений. Для полного набора данных MNIST существует 10 подпапок, по одной для изображений каждой цифры. . 3. Объясните, как работает подход ”пиксельного сходства (pixel similarity)” к классификации цифр. . В подходе “сходства пикселей” мы генерируем образец для каждого класса, который хотим идентифицировать. В нашем случае мы хотим отличить изображения трех от изображений семи. Мы определяем образец трех как среднее значение по пикселям всех трех в обучающем наборе. Аналогично для семерки. Вы можете визуализировать два образца и увидеть, что они на самом деле являются размытыми версиями чисел, которые они представляют. Чтобы определить, является ли ранее нерассматриваемое изображение 3 или 7, мы вычисляем его расстояние до двух образцов (здесь: средняя абсолютная разница в пикселях). Мы говорим, что новый образ-это 3, если его расстояние до образца трех меньше, чем ддля образца семи. . 4. Что такое представление списков (list comprehension)? Теперь создайте тот, который выбирает нечетные числа из списка и удваивает их. . Списки (массивы на других языках программирования) часто генерируются с помощью цикла for. Представление списков (list comprehension) - это Питонический способ конденсирования создания списка с помощью цикла for в одно выражение. редставление списков (list comprehension) также часто будет включать условия для фильтрации. . lst_in = range(10) lst_out = [2*el for el in lst_in if el%2==1] # is equivalent to: lst_out = [] for el in lst_in: if el%2==1: lst_out.append(2*el) . 5. Что такое “тензор ранга 3”? . Ранг тензора-это число измерений, которые он имеет. Простой способ определить ранг - это количество индексов, которые вам понадобятся для ссылки на число внутри тензора. Скаляр может быть представлен как тензор ранга 0 (без индекса), вектор может быть представлен как тензор ранга 1 (один индекс, например, v[i]), матрица может быть представлена как тензор ранга 2 (два индекса, например,a[i, j]), а тензор ранга 3-это кубоид или “стек матриц” (три индекса, например,b[i,j, k]). В частности, ранг тензора не зависит от его формы или размерности, например, тензор формы 2x2x2 и тензор формы 3x5x7 имеют ранг 3. Обратите внимание, что термин “ранг” имеет различные значения в контексте тензоров и матриц (где он относится к числу линейно независимых векторов столбцов). . 6. В чем разница между тензорным рангом и формой (shape)? . Ранг - это число осей или измерений в Тензоре; форма (shape)-размер каждой оси тензора. . Как вы получаете ранг от формы? . Длина формы тензора - это его ранг. . Итак, если у нас есть изображения папки 3 из набора данных MINST_SAMPLE в Тензоре под названием stacked_threes, и мы находим его форму вот так. . In [ ]: stacked_threes.shape Out[ ]: torch.Size([6131, 28, 28]) . Нам просто нужно найти его длину, чтобы узнать его ранг. Это делается следующим образом. . In [ ]: len(stacked_threes.shape) Out[ ]: 3 . Вы также можете получить ранг тензора непосредственно с помощью ndim. . In [ ]: stacked_threes.ndim Out[ ]: 3 . 7. Что такое норма RMSE и L1? . Среднеквадратичная ошибка (RMSE), также называемая нормой L2, и средняя абсолютная разность (MAE), также называемая нормой L1, являются двумя широко используемыми методами измерения “расстояния”. Простые вычитания не работают, потому что некоторые различия положительны, а другие отрицательны и в результате они отменяют друг друга. Поэтому для правильного измерения расстояний необходима функция, которая фокусируется на величинах разностей. Проще всего было бы сложить абсолютные значения разностей, что и есть MAE. RMSE берет среднее значение квадрата (делает все положительным), а затем берет квадратный корень (отменяет возведение в квадрат). . 8. Как вы можете выполнить вычисление на тысячах чисел одновременно, во много тысяч раз быстрее, чем цикл на Python? . Поскольку циклы в Python очень медленные, лучше всего представлять операции как операции массива, а не циклически перебирать отдельные элементы. Если это можно сделать, то использование NumPy или PyTorch будет в тысячи раз быстрее, так как они используют базовый код C, который намного быстрее, чем чистый Python. Еще лучше то, что PyTorch позволяет запускать операции на GPU, которые будут иметь значительное ускорение, если есть параллельные операции, которые можно выполнить. . 9. Создайте тензор 3x3 или массив, содержащий числа от 1 до 9. Удвоьте его. Выберите в правом нижнем углу 4 цифры. . In [ ]: a = torch.Tensor(list(range(1,10))).view(3,3); print(a) Out [ ]: tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]) In [ ]: b = 2*a; print(b) Out [ ]: tensor([[ 2., 4., 6.], [ 8., 10., 12.], [14., 16., 18.]]) In [ ]: b[1:,1:] Out []: tensor([[10., 12.], [16., 18.]]) . 10. Что такое бродкастинг? . Бродкастинг (broadcasting) - Научные / числовые пакеты Python, такие как NumPy и PyTorch, часто реализуют бродкастинг, который часто облегчает написание кода. В случае PyTorch тензоры с меньшим рангом расширяются, чтобы иметь тот же размер, что и тензор большего ранга. Таким образом, операции могут выполняться между тензорами с различным рангом. . 11. Обычно метрики рассчитываются с использованием обучающего набора или набора проверки? Почему? . Метрики обычно рассчитываются на основе набора валидации. Поскольку набор валидации является данными, которые не использовались для обучения модели, оценка метрик в наборе валидации лучше для того, чтобы определить, есть ли какое-либо переобучение и насколько хорошо модель могла бы обобщить, если бы ей были даны аналогичные данные. . 12. Что такое SGD? . SGD, или стохастический градиентный спуск, - это алгоритм оптимизации. В частности, SGD-это алгоритм, который будет обновлять параметры модели для того, чтобы минимизировать заданную функцию потерь, которая была оценена по прогнозам и цели. Ключевая идея SGD (и многих алгоритмов оптимизации, если на то пошло) заключается в том, что градиент функции потерь дает представление о том, как эта функция потерь изменяется в пространстве параметров, которое мы можем использовать, чтобы определить, как лучше всего обновить параметры, чтобы минимизировать функцию потерь. Это то, что делает SGD. . 13. Почему SGD использует мини-пакеты? . Нам нужно вычислить нашу функцию потерь (и наш градиент) на одной или нескольких точках данных. Мы не можем рассчитывать на всех наборах данных из-за компьютерных ограничений и ограничений по времени. Однако если мы будем перебирать каждую точку данных, градиент будет неустойчивым и неточным и не пригодным для обучения. В качестве компромисса мы рассчитываем средние потери для небольшого подмножества набора данных за один раз. Это подмножество называется мини-пакетом. Использование мини-пакетов также более эффективно с вычислительной точки зрения, чем отдельные элементы на графическом процессоре. . 14. Какие 7 шагов в SGD машинного обучения? . Инициализируйте параметры-случайные значения часто работают лучше всего. | Рассчитать прогнозы-это делается на тренировочном наборе, по одному мини-пакету за раз. | Вычислить потери – вычисляется средняя потеря по минипакету** | Вычисление градиентов-это аппроксимация того, как должны изменяться параметры, чтобы минимизировать функцию потерь | Шаг Весов-обновление параметров на основе вычисленных Весов | Повторить процесс | Остановка-на практике это либо основано на временных ограничениях, либо обычно основано на том, когда потери в обучении/валидации и показатели перестают улучшаться. | 15. Как мы инициализируем веса в модели? . Случайные веса работают довольно хорошо. . 16. Что такое “потеря”? . Функция потерь будет возвращать значение, основанное на заданных прогнозах и целевых показателях, где более низкие значения соответствуют лучшим прогнозам модели. . 17. Почему мы не можем всегда использовать высокую скорость обучения? . Потери могут “отскакивать” вокруг (колебаться) или даже расходиться, так как оптимизатор делает слишком большие шаги и обновляет параметры быстрее, чем это должно быть. . 18. Что такое “градиент”? . Градиенты говорят нам, насколько мы должны изменить каждый вес, чтобы сделать нашу модель лучше. По сути, это мера того, как изменяется функция потерь при изменении Весов модели (производной). . 19. Вам нужно знать, как самостоятельно вычислять градиенты? . Ручной расчет градиентов не требуется, так как библиотеки глубокого обучения автоматически рассчитают градиенты для вас. Эта функция известна как автоматическая дифференциация. В PyTorch, если requires_grad=True, градиенты могут быть возвращены методом обратного вызова: a.backward() . 20. Почему мы не можем использовать точность как функцию потерь? . Функция потерь должна изменяться по мере корректировки Весов. Точность меняется только в том случае, если меняются предсказания модели. Таким образом, если в модели есть небольшие изменения, которые, скажем, повышают уверенность в предсказании, но не изменяют предсказание, точность все равно не изменится. Таким образом, градиенты будут равны нулю везде, кроме тех случаев, когда фактические прогнозы изменяются. Таким образом, модель не может учиться на градиентах, равных нулю, и веса модели не будут обновляться и не будут обучаться. Хорошая функция потерь дает немного лучшие потери, когда модель дает немного лучшие прогнозы. Немного лучшие предсказания означают, что модель более уверена в правильности предсказания. Например, предсказание 0,9 против 0,7 для вероятности того, что изображение MNIST является 3, было бы немного лучшим предсказанием. Функция потерь должна отражать это. . 21. Нарисуйте сигмовидную функцию. Что особенного в ее форме? . . Сигмоидная функция-это гладкая кривая у которой все значения лежат между 0 и 1. У функций потерь значения вероятности или доверительного уровня лежат между 0 и 1, поэтому на конце модели используется сигмоидная функция. . 22. В чем разница между потерями и метриками? . Ключевое различие заключается в том, что метрики служат для человеческого понимания, а потери - автоматизированного обучения. Чтобы потеря была полезна для обучения, она должна иметь значимую производную. Многие показатели, такие как например точность, не подходят. Метрики в свою очередь это цифры которые волнуют людей и отражают производительность модели. . 23. Что является функцией для вычисления новых весов с использованием скорости обучения? . Функция оптимизации шага . 24. Что класс DataLoader делает? . Класс DataLoader может взять любую коллекцию Python и превратить ее в итератор для пакетов. . 25. Напишите псевдокод, показывающий основные шаги, предпринятые каждой эпохой для SGD. . for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr . 26. Создайте функцию, которая при передаче двух аргументов [1,2,3,4] и ‘abcd’ возвращает [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)] . Что особенного в этой структуре выходных данных? . def func(a,b): return list(zip(a,b)) . Эта структура данных полезна для моделей машинного обучения, когда вам нужны списки кортежей, где каждый кортеж будет содержать входные данные и метку. . 27. Что делает view в PyTorch? . Он изменяет форму тензора, не изменяя его содержания. . **28. Какая функция у параметра “смещения(bias)” в нейронной сети? Зачем он нам нужен? . Без параметров смещения, если на вход подается нуль, выход всегда будет равен нулю. Поэтому использование параметров смещения добавляет модели дополнительную гибкость. . 29. Что оператор @ делает в python? . Это оператор умножения матриц. . 30. Что делает метод backward делает? . Этот метод возвращает текущие градиенты. . 31. Почему мы должны обнулять градиенты? . PyTorch будет добавлять градиенты переменных в любые из ранее сохраненных градиентов. Если функция цикла обучения вызывается несколько раз, не обнуляя градиенты, градиент текущих потерь будет добавлен к ранее сохраненному значению градиента. . 32. Какую информацию мы должны передать Learner? . Нам нужно передать DataLoader, модель, функцию оптимизации, функцию потерь и, возможно, метрики для вывода. . 33. Покажите python или псевдокод для основных шагов обучающего цикла. . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() for i in range(20): train_epoch(model, lr, params) . 34. Что такое “ReLU”? Нарисуйте график для значений от -2 до +2. . ReLU просто означает “заменить любые отрицательные числа нулем”. Это обычно используемая функция активации. . . 35. Что такое “функция активации”? . Функция активации-это еще одна функция, входящая в состав нейронной сети, цель которой-обеспечить нелинейность модели. Идея состоит в том, что без функции активации есть несколько линейных функций вида y=mx+b. Однако ряд линейных слоев эквивалентен одному линейному слою, поэтому наша модель может подогнать только линию к данным. Вводя нелинейность между линейными слоями,это уже не так. Каждый слой несколько отделен от остальных слоев, и теперь модель может соответствовать гораздо более сложным функциям. На самом деле можно математически доказать, что такая модель может решить любую вычислимую задачу с произвольно высокой точностью, если модель достаточно велика с соответствующими весами. Это известно как универсальная аппроксимационная теорема. . *36. В чем разница между *F.relu и nn.ReLU? . F.relu - это функция Python для активации relu. С другой стороны, nn.ReLU-это модуль PyTorch. Это означает, что класс Python может быть вызван как функция таким же образом, как и F.relu. . **37. Универсальная аппроксимационная теорема показывает, что любая функция может быть аппроксимирована настолько близко, насколько это необходимо, используя только одну нелинейность. Так почему же мы обычно используем больше? . Использование более чем одной нелинейности дает практические преимущества. Мы можем использовать более глубокую модель с меньшим количеством параметров, лучшей производительностью, более быстрым обучением и меньшими требованиями к вычислениям и памяти. .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2020/12/09/fastai-chapter4-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2020/12/09/fastai-chapter4-solution.html",
            "date": " • Dec 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Russian - Solution Lesson 2 on Fast.ai",
            "content": "В этом блокоте рассмотрено решение по второму и началу третьего урока на fast.ai. Сначало как обычно устанавливаются и подключаются необходиые библиотеки. . Первоначально загружаются изображения с помощью Bing Image Search. Для этого регистрируюсь в Microsoft (так как у меня уже есть учетка, то этого не потребовалось) для получения бесплатной учетной записи. Предоставляется ключ, который вставляю в место XXX . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;XXX&#39;) . search_images_bing . &lt;function fastbook.search_images_bing(key, term, min_sz=128)&gt; . Но в связи с изменениями формата запросов и поиска изображений при помощи Bing со стороны Microsoft запрос, который использовался в ферале 2020 года часто выдает ошибку. Из-за чего развернулась большая дискуссия на форуме fast.ai. Я предложил свое решение, но оно выглядело достаточно коряво. На основании его один из пользователей сделал новую функцию search_images_bing. . Успешно загрузжены URL-адреса 150 медведей гризли (или, по крайней мере, изображения, которые Bing Image Search находит для данного поискового запроса). Можно посмотреть на один из них: . dest = &#39;images/grizzly.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . Для того, чтобы скачать все фото согласно поисковому запросу и поместить их в отдельные папки используется функция download_images из пакета fastai: . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . В результате в папках по 150 изображений . fns = get_image_files(path) fns . (#441) [Path(&#39;bears/grizzly/00000006.jpg&#39;),Path(&#39;bears/grizzly/00000005.jpg&#39;),Path(&#39;bears/grizzly/00000008.jpg&#39;),Path(&#39;bears/grizzly/00000009.jpg&#39;),Path(&#39;bears/grizzly/00000002.jpg&#39;),Path(&#39;bears/grizzly/00000003.jpg&#39;),Path(&#39;bears/grizzly/00000010.jpg&#39;),Path(&#39;bears/grizzly/00000000.jpg&#39;),Path(&#39;bears/grizzly/00000012.jpg&#39;),Path(&#39;bears/grizzly/00000007.jpg&#39;)...] . После этого проверяю загруженные файлы на наличие поврежденных . failed = verify_images(fns) failed . (#14) [Path(&#39;bears/black/00000026.jpg&#39;),Path(&#39;bears/black/00000032.jpg&#39;),Path(&#39;bears/black/00000039.jpg&#39;),Path(&#39;bears/black/00000122.jpg&#39;),Path(&#39;bears/black/00000119.jpg&#39;),Path(&#39;bears/teddy/00000010.jpg&#39;),Path(&#39;bears/teddy/00000004.jpg&#39;),Path(&#39;bears/teddy/00000055.jpg&#39;),Path(&#39;bears/teddy/00000047.jpg&#39;),Path(&#39;bears/teddy/00000065.jpg&#39;)...] . Чтобы удалить все &quot;бракованные&quot; изображения использую функцию unlink для каждого из них. В связи с тем, что verify_images возвращает объект типа L и в нем есть метод map, то переданная функция исполниться для каждого элемента коллекции: . failed.map(Path.unlink); . Все объекты хранятся в классе DataLoader, которые ему передаются. В результате они будут доступны как train и valid.С помощью DataBlock можно настроить каждый этап создания загрузчиков данных. В него передаются следующие параметры: . blocks=(ImageBlock, CategoryBlock) - кортеж, где содержится информация какие тип использовать для независимых и зависимых переменных. Независимая переменная-это то, что используется для предсказания, а зависимая переменная - результат. В этом случае независимыми переменными являются фото, а зависимыми переменными - категории (тип медведя) для каждого фото; | get_items=get_image_files - говорим, что для DataLoader данными будут являться пути к файлам. Функция get_image_files получает расположение и возвращает список всех изображений расположенных по этому пути (по умолчанию рекурсивно); | splitter=RandomSplitter(valid_pct=0.2, seed=42) - разделить train и valid случайным образом. Но для того, чтоб каждый раз разбивка не менялась, то фиксируется seed; | get_y=parent_label - функция, которая должна создать метки (зависимые переменные). parent_label-это функция, которая просто получает имя папки, в которой находится файл; | item_tfms=Resize(128) - преобразовать размер всех изображений к одному. | . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . Дальше передаю путь, по которому можно найти изображения: . dls = bears.dataloaders(path) . Показать некоторые из элементов при помощи метода show_batch: . dls.valid.show_batch(max_n=4, nrows=1) . Примеры двух вариантов использования метода Resize: заполнить пустые мест изображения нулями (черными полями) или растянуть/сжать их: . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, nrows=1) . Пример использования RandomResizedCrop вместо Resize - выбирается часть изображения, а остальная обрезается. Параметр min_scale определяет размер минимальной части изображения которую нужно выбирать каждый раз, а unique=True в функции show_batch - использование одного и то же изображения. . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=4, nrows=1, unique=True) . Трансформация изображений (поворот, переворачивание, деформация перспективы, изменение яркости и контрастности) функцией aug_transforms и двойное увеличение . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . Обучение модели с RandomResizedCrop в 224 px и aug_transforms: . bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.084715 | 0.166342 | 0.047059 | 00:10 | . epoch train_loss valid_loss error_rate time . 0 | 0.171819 | 0.092201 | 0.047059 | 00:10 | . 1 | 0.129211 | 0.048695 | 0.035294 | 00:10 | . 2 | 0.095478 | 0.040643 | 0.023529 | 00:10 | . 3 | 0.075971 | 0.046714 | 0.023529 | 00:10 | . Проверка при помощи матрицы ошибок - confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Следующим шагом показываются изображения с максимальными потерями (loss) при помощи функции plot_top_losses. В заголовке каждого фото по порядку приведено следующее: предсказание, фактическая (целевая метка), потеря и вероятность. Вероятность - это уровень достоверности, от нуля до единицы, который модель присвоила своему прогнозу: . interp.plot_top_losses(5, nrows=1) . fastai включает в себя удобный графический интерфейс для очистки данных под названием ImageClassifierCleaner, который позволяет выбрать категорию и набор обучения по сравнению с проверкой и просмотреть изображения с наибольшей потерей (по порядку), а также меню, позволяющие выбирать изображения для удаления или повторной маркировки: . cleaner = ImageClassifierCleaner(learn) cleaner . После выполнения ручной разметки будут возращены индексы элементов для изменения. Чтобы удалить (разорвать связь) все изображения, выбранные для удаления, и переместить изображения, для которых мы выбрали другую категорию, запускается следующий код . После необходимо запустить повторное обучение и в результате можно добиться очень неплохих результатов. . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; &#1086;&#1085;&#1083;&#1072;&#1081;&#1085; &#1087;&#1088;&#1080;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1103; . Вызвать функцию export, чтоб fastai сохранил модель в файл под названием export.pkl: . learn.export() . Далее проверка, что файл существует, используя метод ls, который у fastai добавлен в класс Path . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . Чтобы создать вывод из экспортированного файла используется load_learner . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Проводится проверка на одном из изображений, которое уже использовалось ранее . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, TensorImage(1), TensorImage([3.8019e-06, 1.0000e+00, 1.0876e-07])) . В результате получили три значения: предсказанную категорию, индекс предсказанной категории и вероятности каждой категории. . Последние два основаны на порядке категорий в vocab DataLoaders: . learn_inf.dls.vocab . [&#39;black&#39;, &#39;grizzly&#39;, &#39;teddy&#39;] . &#1057;&#1086;&#1079;&#1076;&#1072;&#1085;&#1080;&#1077; Notebook App &#1087;&#1088;&#1080;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1103; &#1085;&#1072; &#1086;&#1089;&#1085;&#1086;&#1074;&#1077; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080; . Создать виджет загрузки файлов: . btn_upload = widgets.FileUpload() btn_upload . Теперь мы можем передать изображение, но по факту для упрощения загружаем файл: . img = PILImage.create(btn_upload.data[-1]) . Для показа изображениия используется виджет Output: . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . Тогда получаем прогнозы и используем Label чтобы их показать: . pred,pred_idx,probs = learn_inf.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Создаем кнопку, чтобы сделать классификацию: . btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . Создаем обработчик событий click . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . Поместим все в коробку (VBox) для завершения GUI: . VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . &#1056;&#1072;&#1079;&#1074;&#1086;&#1088;&#1086;&#1090; Notebook &#1074; &#1088;&#1077;&#1072;&#1083;&#1100;&#1085;&#1086;&#1077; &#1087;&#1088;&#1080;&#1083;&#1086;&#1078;&#1077;&#1085;&#1080;&#1080; . Перед тем, как развернуть приложение, надо установить дополнительный пакет и сделать настройку. Но при настройке у меня выскакивала ошибка. На форуме нашел подсказку и привожу ниже уже исправленный вариант. . После этого я сделал сокращенный вариант своего блокнота и разместил все в репрозитории на github. Первые мои настройки приложения через Binder оканчивались неудачей. После пойска на форумах и на самом сайте попытался настройть через Heroku. Но и там не все так гладко - надо пересобирать с устаревшими библиотеками. Но как говориться - лень - двигатель прогресса: . Я взял в одном из репрозиториев файл requirements.txt (точнее скопировал из него четыре строчки с версиями библиотек). | Прочитал внимательно учебник и заметил ошибку, которую я допускал: помимо ссылки на github репрозиторий ОБЯЗАТЕЛЬНО в Path to notebook file необходимо File сменить на URL и прописать там следующий путь: /voila/render/bear_classifier.ipynb. Может можно как и по другому, но я не испытывал. | В результате у вас получается следующее. Если вдруг не запуститься, значит опять что-то перенастроили. Но в результате должен открываться пустой сайт с кнопкой для загрузке изображения. После загрузки появляется фото, ответ и его вероятность. .",
            "url": "https://zmey56.github.io/blog//russian/fast.ai/solution/2020/11/20/02-production.html",
            "relUrl": "/russian/fast.ai/solution/2020/11/20/02-production.html",
            "date": " • Nov 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Russian - Fastbook Chapter 1 questionnaire solutions",
            "content": "Решил собрать ответы на русском языке на вопросы к первой части курса Deep Learning 2020 на Fast.ai. Если есть притензии к переводу, то пешите в коментариях - поправлю. . 1. Что Вам нужно для изучения глубокого обучения: . Много математики - неправда | Много данных - неправда | Дорогой компьютер - неправда | Докторская Степень - неправда | . 2. Назовите пять областей, где глубокое обучение лучше всего представлено сейчас: . Любые пять из следующих: . Обработка естественного языка (NLP) – ответы на вопросы, обобщение и классификация документов и т. д . Компьютерное зрение – интерпретация съемок спутников и беспилотных аппаратов, распознавание и определение лиц, субтитры к изображениям и т. д. . Медицина – обнаружение аномалий в медицинских изображениях (например, КТ, рентген, МРТ), обнаружение особенностей на съемках тканей (патология), диагностика диабетической ретинопатии и т. д. . Биология – сворачиваемость белков, классификация, задачи геномики, классификация клеток и т. д. . Генерация изображения/улучшение – раскрашивание изображений, повышая разрешение изображения (супер-разрешение), удаление шума с изображения (шумоподавление), преобразование изображения в стиле известных художников (перемешывание стилей) и т. д. . Рекомендательные системы – веб-поиск, рекомендации по продуктам и т. д. . Игра – шахматы, го и т. д . Робототехника – управление объектами в зависимости от их местоположения . Другие приложения – финансовое и логистическое прогнозирование; преобразование текста в речь; многое, многое другое. . 3. Как называлось первое устройство, которое было основано на принципе искусственного нейрона? . Персептрон Mark I построенный Фрэнком Розенблаттом . 4. Какие существуют требования к “параллельной распределенной обработке” согласно книге? . Набор процессорных блоков | Состояние активации | Выходная функция для каждого блока | Закономерность связи между блоками | Правило распространения для распространения схем активации через сеть связей | Правило активации для объединения входов, воздействующих на блок, в текущем состоянии этого блока для получения нового уровня активации для блока | Правило обучения, в соответствии с которым схема связности модифицируется опытом | Среда, в которой должна работать система | . 5. Каковы были два теоретических постулата, которые сдерживали развитие нейронных сетей? . В 1969 году Марвин Мински и Сеймур Паперт продемонстрировали в своей книге “Персептроны”, что один слой искусственных нейронов не может выучить простые, критические математические функции, такие как логический элемент XOR. Хотя впоследствии они продемонстрировали в той же книге, что дополнительные слои могут решить эту проблему, было принято во внимание только первое утверждение, что явилось первой “зимой” для ИИ. . В 1980-х годах изучались модели с двумя слоями. Теоретически можно аппроксимировать любую математическую функцию, используя два слоя искусственных нейронов. Однако на практике эти сети были слишком большими и слишком медленными. Хотя было продемонстрировано, что добавление дополнительных слоев повышает производительность, это понимание не было признано, и началась вторая зима искусственного интеллекта. В последнее десятилетие, с увеличением доступности данных и улучшением компьютерного оборудования (как в производительности процессора, так и, что более важно, в производительности графического процессора), нейронные сети наконец-то оправдали свой потенциал. . 6. Что такое графический процессор (GPU)? . GPU расшифровывается как графический процессор (также известный как видеокарта). Стандартные компьютеры имеют различные компоненты, такие как процессоры, оперативная память и т. д. Процессоры, или центральные процессоры, являются основными блоками всех стандартных компьютеров, и они выполняют инструкции, которые составляют компьютерные программы. Графические процессоры, с другой стороны, являются специализированными устройствами, предназначенными для отображения графики, особенно 3D - графики в современных компьютерных играх. Аппаратная оптимизация, используемая в графических процессорах, позволяет ему обрабатывать тысячи задач одновременно. Кстати, эти оптимизации позволяют нам запускать и обучать нейронные сети в сотни раз быстрее, чем на обычных процессорах. . 7. Откройте блокнот и выполните ячейку, содержащую: 1+1 . Что же произойдет? . В блокноте Jupyter мы можем создавать ячейки кода и запускать код в интерактивном режиме. Когда мы выполняем ячейку, содержащую некоторый код (в данном случае: 1+1), код запускает Python, а выходные данные отображаются под ячейкой кода (в данном случае: 2). . 8. Проследите за каждой ячейкой урезанной версии записной книжки для этой главы. Прежде чем выполнить каждую ячейку, угадайте, что произойдет. . Это необходимо сделать самому. . 9. Заполните онлайн-приложение Jupyter Notebook. . Это необходимо сделать самому. . 10. Почему трудно использовать традиционные компьютерные программы для распознавания изображений на фотографии? . Для нас, людей, легко идентифицировать изображения на фотографиях, например, идентифицировать кошек и собак. Это происходит потому, что подсознательно наш мозг узнал, какие черты определяют кошку или собаку. Но трудно определить набор правил для компьютерной программы, которая так же сможет делать это успешно. Можете ли вы придумать универсальное правило, чтобы определить, содержит ли фотография кошку или собаку? Как бы вы закодировали это в виде компьютерной программы? Это очень трудно, потому что кошки, собаки или другие объекты имеют большое разнообразие форм, текстур, цветов и других особенностей, и это почти невозможно вручную закодировать в компьютерной программе. . 11. Что Сэмюэль имел в виду под “распределением веса” (Weight Assignment)? . “распределением веса” относится к текущим значениям параметров модели. Артур Сэмюэл далее упоминает “автоматическое средство проверки эффективности любого текущего распределение веса” и “механизм изменения значения веса таким образом, чтобы максимизировать производительность”. Это относится к оценке и обучению модели с целью получения набора значений параметров, которые максимизируют производительность модели. . 12. Какой термин мы обычно используем в глубоком обучении для тог, что Сэмюэл назвал “Весами”? . Вместо этого мы используем термин параметры. В глубоком обучении термин “вес” имеет другое значение. (Нейронная сеть имеет различные параметры, к которым мы подгоняем наши данные. Как показано в следующих главах, существуют два типа параметров нейронной сети - веса и смещение). . 13. Нарисуйте картинку, которая обобщает взгляд Артура Сэмюэля на модель машинного обучения. . . 14. Почему трудно принять, что модель глубокого обучения делает определенный прогноз? . Это хорошо изученная тема, известная как интерпретируемость моделей глубокого обучения. Модели глубокого обучения трудно понять отчасти из-за их “глубокой” природы. Представьте себе модель линейной регрессии. Просто у нас есть некоторые входные переменные/данные, которые умножаются на некоторые веса, давая нам на выход полученное значение. Мы можем понять, какие переменные более важны, а какие менее важны, основываясь на их весах. Аналогичная логика может применяться и для небольшой нейронной сети с 1-3 слоями. Однако глубокие нейронные сети имеют сотни, если не тысячи слоев. Трудно определить, какие факторы играют важную роль в определении конечного результата. Нейроны в сети взаимодействуют друг с другом, причем выходы из одних нейронов поступают в другие нейроны. В целом, из-за сложной природы моделей глубокого обучения очень трудно понять, почему нейронная сеть делает тот или иной прогноз. . 15. Как называется теорема о том, что нейронная сеть может решить любую математическую задачу с любой точностью? . Универсальная теорема аппроксимации утверждает, что нейронные сети теоретически могут решать любую математическую функцию. Однако важно понимать, что практически, в силу ограниченности имеющихся данных и компьютерного оборудования, обучить любую модель практически невозможно. Но мы можем подойти к решению очень близко! . 16. Что вам нужно для того, чтобы обучить модель? . Вам понадобится архитектура для задачи. Вам понадобятся данные для ввода в вашу модель. Для большинства случаев использования глубокого обучения вам понадобятся метки для ваших данных, чтобы сравнить предсказания вашей модели. Вам понадобится функция потерь, которая будет количественно измерять производительность вашей модели. И вам нужен способ обновить параметры модели, чтобы улучшить ее производительность (это называется оптимизация). . 17. Как цикл обратной связи может повлиять на внедрение прогностической полициской модели? . В прогностической полицейской модели мы могли бы получить положительную обратную связь, что привело бы к очень предвзятой модели с небольшой прогностической силой. Например, нам может понадобиться модель, которая предсказывала бы преступления, но мы используем информацию об арестах в качестве данных . Однако сами эти данные несколько искажены из-за предвзятости существующих полицейских процессов контроля. Обучение с этими данными приводит к необъективной модели. Правоохранительные органы могли бы использовать эту модель для определения того, где сосредоточить полицейскую деятельность, увеличивая число арестов в этих районах. Эти дополнительные аресты будут использоваться при обучении будущих итераций моделей, что приведет к еще более предвзятой модели. Этот цикл продолжается как положительная обратная связь. . 18. Всегда ли мы должны использовать изображения размером 224х224 пикселя в модели распознавания кошек? . Нет, мы этого не должны делаем. 224x224 обычно используется по историческим причинам. Вы можете увеличить размер и получить лучшую производительность, но заплатить скоростью и памятью. . 19. В чем разница между классификацией и регрессией? . Классификация ориентирована на предсказание класса или категории (например, типа домашнего животного). Регрессия ориентирована на предсказание числовой величины (например, возраста домашнего животного). . 20. Что такое проверочный набор (validation set)? Что такое тестовый набор(test set)? Зачем они нам нужны? . Проверочный набор - это часть данных, которая используется не для обучения модели, а для оценки модели во время обучения, чтобы предотвратить переобучение. Он гарантирует, что производительность модели не является результатом “мошейничиства” или запоминания набора данных, а скорее потому, что она изучает соответствующие возможности для прогнозирования. Однако вполне возможно, что мы также переобучаем проверочные данные. Это происходит потому, что разработчик модели также является частью процесса обучения, корректируя гиперпараметры и процедуры обучения в соответствии с значениями проверки. Поэтому для окончательной оценки модели используется другая неиспользованная часть набора данных-тестовый набор. Такое разбиение набора данных необходимо для обеспечения того, чтобы модель обобщалась на неиспользованных данных. . 21. Что сделает fastai, если вы не определите проверочный набор? . fastai автоматически создаст проверочный набор данных. Он случайным образом возьмет 20% данных и назначит их в качестве проверочного набора ( valid_pct = 0.2 . 22. Можем ли мы всегда использовать случайную выборку для проверочного набора? Почему или почему нет? . Хорошые проверочные и тестовые наборы должны быть репрезентативными для новых данных, которые модель будет использовать в будущем. Иногда это не так, если используется случайная выборка. Например, для данных временных рядов случайные выборки не имеют смысла. Вместо этого лучше определить различные периоды времени для тренировки, проверки и тестирования. . 23. Что такое переобучение? Приведите пример. . Переобучение является наиболее сложной проблемой, когда речь заходит о тренировки моделей машинного обучения. Переобучение относится к той ситуации, когда модель слишком близко подходит к ограниченному набору данных, но плохо работает на неиспользованных данных. Это особенно важно, когда речь заходит о нейронных сетях, потому что нейронные сети потенциально могут “запоминать” набор данных, на котором была обучена модель, и будут плохо работать с незадействованными данными, потому что они не “запоминали” основные истинные значения для этих данных. Вот почему необходима логичная структура проверки путем разделения данных на обучающие, проверочные и тестовые. . 24. Что такое метрика? В чем отличие от “потерь” (loss)? . Метрика - это функция, которая измеряет качество прогнозов модели с помощью набора валидации. Она похоже на потери, которая также является мерой производительности модели. Однако потери предназначены для алгоритма оптимизации (например, SGD) с целью эффективного обновления параметров модели, в то время как метрики являются интерпретируемым человеком показателями производительности. Иногда метрика также может быть хорошим выбором для потерь. . 25. Как могут помочь натренированные модели? . Предварительно натренированные модели обученые для задач, которые могут быть весьма схожи с текущей задачей. Например, предварительно обученные модели распознавания изображений часто обучались на наборе данных ImageNet, который содержит 1000 классов, ориентированных на множество различных типов визуальных объектов. Предварительно обученные модели полезны, потому что они уже научились обрабатывать множество простых функций, таких как распознавание краев и цветов. Однако, поскольку модель была обучена для другой задачи, чем решаемая, эта модель не может использоваться как есть. . 26. Что такое “голова”(head) модели? . При использовании предварительно обученной модели более поздние слои модели, которые были полезны для задачи, на которой первоначально обучалась модель, заменяются одним или несколькими новыми слоями с рандомизированными весами, подходящими по размеру для набора данных, с которым вы работаете. Эти новые слои называются “головой” модели. . 27. Какие особенности обнаруживаются в ранних слоях CNN и в более поздних слоях? . Более ранние слои изучают простые объекты, такие как диагональные, горизонтальные и вертикальные ребра. Более поздние слои изучают более продвинутые объекты, такие как автомобильные колеса, лепестки цветов и даже очертания животных. . 28. Являются ли модели изображений полезными только для фотографий? . Нет! Модели изображений могут быть полезны для других типов изображений, таких как чертежи, медицинские данные и т. д. | . Очень много информации можно представить в виде изображений. Например, звук может быть преобразован в спектрограмму, которая является визуальной интерпретацией звука. Временные ряды (например, финансовые данные) можно преобразовать в изображение, построив график. Более того, существуют различные преобразования, которые генерируют изображения из временных рядов и достигли хороших результатов для классификации временных рядов. Есть много других примеров, и, проявив творческий подход, вы можете сформулировать свою проблему как проблему классификации изображений и использовать предварительно подготовленные модели изображений для получения самых современных результатов! . 29. Что такое “архитектура”? . Архитектура-это шаблон или структура модели. Она определяет математическую модель, которую мы пытаемся обучить. . 30. Что такое сегментация? . По своей сути сегментация-это задача классификации по пикселям. Мы пытаемся предсказать метку для каждого отдельного пикселя изображения. В результате получаем шаблон части изображения соответствуют данной метке. . 31. Для чего используется y_range? Когда нам понадобится? . y_range используется для ограничения прогнозируемых значений, когда наша задача сосредоточена на предсказании числового значения в заданном диапазоне (например, прогнозирование рейтингов фильмов в диапазоне 0,5-5). . 32. Что такое “гиперпараметры”? . Обучающие модели требуют различных параметров, определяющих способ обучения модели. Например, нам нужно определить, как долго мы собираемся тренировать или скорость обучения (насколько быстро параметры модели могут изменяться). Такого рода параметры называются гиперпараметрами. . 33. Как лучше всего избежать неудач при использовании ИИ в организации? . Ключевые моменты, которые следует учитывать при использовании ИИ в организации: . Убедитесь, что обучающий, проверочный и тестовый набор правильно определены, чтобы соответствующим образом оценить модель. | Попробуйте построить простую базовую модель, которую будущая модель должны превзойти. Или даже этой простой базовой модели может быть достаточно в некоторых случаях. | .",
            "url": "https://zmey56.github.io/blog//markdown/fastai/russian/deep%20learning/2020/11/12/fastai-chapter1-solution.html",
            "relUrl": "/markdown/fastai/russian/deep%20learning/2020/11/12/fastai-chapter1-solution.html",
            "date": " • Nov 12, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alexander Gladkikh, and I made my own website on GitHub dedicated to my Hobbies: Machine learning, Deep Learning, and algorithmic trading. . I take part in kaggle competitions, have knowledge of R and Python (Pandas, NumPy, Scipy, Scikit-learn, XGBoost), Java . At the main work I participate in projects on the use of new technologies in the field of labor protection and ecology. . I have been engaged in technical analysis of financial markets for a long time. Familiar with software Amibroker, and Metatrader Quik (scripting). . At work I had to deal with the analysis of data in the performance of research in biology at the Institute and writing projects on environmental protection. . My degrees . Corporate Energy University, 2020 . Digital production technologies in the power industry . YANDEX, MIPT, 2019 . Machine learning and data analysis . City Business School, 2019 . MINI-MBA Professional .",
          "url": "https://zmey56.github.io/blog//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://zmey56.github.io/blog//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}